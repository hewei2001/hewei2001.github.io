<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#01213a"><meta name="author" content="He Wei"><meta name="keywords" content="Computer Science and Technology, Machine Learning, Deep Learning, Computer Vision, Natural Language Processing"><meta name="baidu-site-verification" content="code-J3wrn8WJYJ"><meta name="google-site-verification" content="0p_KJKTfB8EcahVDp0vYRjVRhHFw1SBWHi15OakKHY0"><meta name="description" content="学习 PyTorch 时做的笔记，介绍了使用 PyTorch 搭建神经网络的完整流程，分别是：数据加载、模型搭建、训练阶段、评估阶段、模型保存与加载。本文将持续更新。"><meta property="og:type" content="article"><meta property="og:title" content="PyTorch笔记 #2 神经网络"><meta property="og:url" content="https://hwcoder.top/PyTorch-Note-2"><meta property="og:site_name" content="Hwcoder - Life Oriented Programming"><meta property="og:description" content="学习 PyTorch 时做的笔记，介绍了使用 PyTorch 搭建神经网络的完整流程，分别是：数据加载、模型搭建、训练阶段、评估阶段、模型保存与加载。本文将持续更新。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hwcoder.top/img/home/PyTorch.png"><meta property="article:published_time" content="2022-12-15T08:14:15.000Z"><meta property="article:modified_time" content="2023-03-22T08:43:12.362Z"><meta property="article:author" content="He Wei"><meta property="article:tag" content="Python"><meta property="article:tag" content="PyTorch"><meta property="article:tag" content="DL"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://hwcoder.top/img/home/PyTorch.png"><title>PyTorch笔记 #2 神经网络 | Hwcoder - Life Oriented Programming</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/mac.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"hwcoder.top",root:"/",version:"1.9.0",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h2, h3",placement:"left",visible:"always",icon:"§"},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"XGNf4GEnaFLiUijMUvz8HSBF-gzGzoHsz",app_key:"JPeVLcug6EcWdBTpGrDJSdKi",server_url:"https://xgnf4gen.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!0}},search_path:"/local-search.xml"};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.4.2"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Hwcoder</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于我</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/bg/other.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="PyTorch笔记 #2 神经网络"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-12-15 16:14" pubdate>2022年12月15日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 25k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 141 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="技术经验" id="heading-8ee11c06b6f625f35ca6ebfa64545bc3" role="tab" data-toggle="collapse" href="#collapse-8ee11c06b6f625f35ca6ebfa64545bc3" aria-expanded="true">技术经验 <span class="list-group-count">(17)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-8ee11c06b6f625f35ca6ebfa64545bc3" role="tabpanel" aria-labelledby="heading-8ee11c06b6f625f35ca6ebfa64545bc3"><div class="category-post-list"></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="Git" id="heading-0bcc70105ad279503e31fe7b3f47b665" role="tab" data-toggle="collapse" href="#collapse-0bcc70105ad279503e31fe7b3f47b665" aria-expanded="false">Git <span class="list-group-count">(4)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-0bcc70105ad279503e31fe7b3f47b665" role="tabpanel" aria-labelledby="heading-0bcc70105ad279503e31fe7b3f47b665"><div class="category-post-list"><a href="/Git-Note-1" title="Git学习笔记 #1 基础知识介绍" class="list-group-item list-group-item-action"><span class="category-post">Git学习笔记 #1 基础知识介绍</span> </a><a href="/Git-Note-2" title="Git学习笔记 #2 本地仓库使用" class="list-group-item list-group-item-action"><span class="category-post">Git学习笔记 #2 本地仓库使用</span> </a><a href="/Git-Note-3" title="Git学习笔记 #3 远程仓库使用" class="list-group-item list-group-item-action"><span class="category-post">Git学习笔记 #3 远程仓库使用</span> </a><a href="/Git-Note-4" title="Git学习笔记 #4 GitHub合作开发" class="list-group-item list-group-item-action"><span class="category-post">Git学习笔记 #4 GitHub合作开发</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="Linux" id="heading-edc9f0a5a5d57797bf68e37364743831" role="tab" data-toggle="collapse" href="#collapse-edc9f0a5a5d57797bf68e37364743831" aria-expanded="false">Linux <span class="list-group-count">(4)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-edc9f0a5a5d57797bf68e37364743831" role="tabpanel" aria-labelledby="heading-edc9f0a5a5d57797bf68e37364743831"><div class="category-post-list"><a href="/Linux-Note-1" title="Linux学习笔记 #1 终端与Shell" class="list-group-item list-group-item-action"><span class="category-post">Linux学习笔记 #1 终端与Shell</span> </a><a href="/Linux-Note-2" title="Linux学习笔记 #2 高级的Shell与文件管理" class="list-group-item list-group-item-action"><span class="category-post">Linux学习笔记 #2 高级的Shell与文件管理</span> </a><a href="/Linux-Note-3" title="Linux学习笔记 #3 使用远程服务器" class="list-group-item list-group-item-action"><span class="category-post">Linux学习笔记 #3 使用远程服务器</span> </a><a href="/Linux-Note-4" title="Linux学习笔记 #4 服务器环境配置" class="list-group-item list-group-item-action"><span class="category-post">Linux学习笔记 #4 服务器环境配置</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem list-group-item category-item-action col-10 col-md-11 col-xm-11" title="PyTorch" id="heading-95b88f180e9eb5678e0f9ebac2cbe643" role="tab" data-toggle="collapse" href="#collapse-95b88f180e9eb5678e0f9ebac2cbe643" aria-expanded="true">PyTorch <span class="list-group-count">(2)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-95b88f180e9eb5678e0f9ebac2cbe643" role="tabpanel" aria-labelledby="heading-95b88f180e9eb5678e0f9ebac2cbe643"><div class="category-post-list"><a href="/PyTorch-Note-1" title="PyTorch笔记 #1 基础操作" class="list-group-item list-group-item-action"><span class="category-post">PyTorch笔记 #1 基础操作</span> </a><a href="/PyTorch-Note-2" title="PyTorch笔记 #2 神经网络" class="list-group-item list-group-item-action active"><span class="category-post">PyTorch笔记 #2 神经网络</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="Python" id="heading-a7f5f35426b927411fc9231b56382173" role="tab" data-toggle="collapse" href="#collapse-a7f5f35426b927411fc9231b56382173" aria-expanded="false">Python <span class="list-group-count">(4)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-a7f5f35426b927411fc9231b56382173" role="tabpanel" aria-labelledby="heading-a7f5f35426b927411fc9231b56382173"><div class="category-post-list"><a href="/Python-Note-1" title="Python笔记 #1 基础语法" class="list-group-item list-group-item-action"><span class="category-post">Python笔记 #1 基础语法</span> </a><a href="/Python-Note-2" title="Python笔记 #2 NumPy" class="list-group-item list-group-item-action"><span class="category-post">Python笔记 #2 NumPy</span> </a><a href="/Python-Note-3" title="Python笔记 #3 Matplotlib" class="list-group-item list-group-item-action"><span class="category-post">Python笔记 #3 Matplotlib</span> </a><a href="/Python-Note-4" title="Python笔记 #4 常用标准库和第三方库" class="list-group-item list-group-item-action"><span class="category-post">Python笔记 #4 常用标准库和第三方库</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="Zotero" id="heading-eb74b00f2c3d9a56f1e9e6f47c462f15" role="tab" data-toggle="collapse" href="#collapse-eb74b00f2c3d9a56f1e9e6f47c462f15" aria-expanded="false">Zotero <span class="list-group-count">(1)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-eb74b00f2c3d9a56f1e9e6f47c462f15" role="tabpanel" aria-labelledby="heading-eb74b00f2c3d9a56f1e9e6f47c462f15"><div class="category-post-list"><a href="/Zotero-Note-1" title="Zotero使用技巧：坚果云同步、常用插件" class="list-group-item list-group-item-action"><span class="category-post">Zotero使用技巧：坚果云同步、常用插件</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="博客" id="heading-c50d13646ec66f1929da5d491f2dff0f" role="tab" data-toggle="collapse" href="#collapse-c50d13646ec66f1929da5d491f2dff0f" aria-expanded="false">博客 <span class="list-group-count">(2)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-c50d13646ec66f1929da5d491f2dff0f" role="tabpanel" aria-labelledby="heading-c50d13646ec66f1929da5d491f2dff0f"><div class="category-post-list"><a href="/Hello-My-World" title="Hello My World" class="list-group-item list-group-item-action"><span class="category-post">Hello My World</span> </a><a href="/Hexo-Configuration" title="Hexo配置与扩展" class="list-group-item list-group-item-action"><span class="category-post">Hexo配置与扩展</span></a></div></div></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">PyTorch笔记 #2 神经网络</h1><p class="note note-info">本文最后更新于：2023年3月22日 下午</p><div class="markdown-body"><p>本文介绍使用 PyTorch 搭建神经网络模型的方法，由于涉及的内容众多，本文将以<strong>完成一个神经网络实验的顺序</strong>展开介绍。分别是：<strong>数据加载、模型搭建、训练阶段、评估阶段、模型保存与加载</strong>。</p><p>本文内容基于 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">官方 API 文档</a> 及 <a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/">PyTorch 中文文档</a>、网上的博客等，实战部分推荐以下教程：</p><ul><li><a target="_blank" rel="noopener" href="https://github.com/yunjey/pytorch-tutorial">pytorch-tutorial</a>：逐步搭建网络，包括 FFNN、CNN、RNN、ResNet、BiLSTM 等经典模型，代码简练。</li><li><a target="_blank" rel="noopener" href="https://github.com/hunkim/PyTorchZeroToAll">PyTorchZeroToAll</a>：HKUST 的课程，有视频讲解，也是经典模型的搭建过程。</li><li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/396666255">深度学习 PyTorch 训练代码模板</a>：完整的深度学习模板，在处理轻量级任务时可以参考。在<strong>快速开发</strong>时建议不要重复造轮子，优先使用模板（比赛的公开 baseline、要对标的开源模型等）。</li></ul><h2 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h2><p>PyTorch 中的 <code>torch.utils.data</code> 模块封装了所有关于数据集的操作，可以帮助我们使用一些预加载数据集或者自定义的数据，其中 <code>Dataset</code> 可以储存样本以及对应标签，而 <code>Dataloader</code> 则用于将 <code>Dataset</code> 封装成<strong>可迭代对象</strong>，使得我们能够更加容易地获取样本数据。</p><h3 id="预加载-or-自定义"><a href="#预加载-or-自定义" class="headerlink" title="预加载 or 自定义"></a>预加载 or 自定义</h3><p>PyTorch 中提供了大量<strong>预加载</strong>的数据集（如 MNIST、FashionMNIST），存放在不同的领域库中，分别是：<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/datasets.html">torchvision</a>、<a target="_blank" rel="noopener" href="https://pytorch.org/text/stable/datasets.html">torchtext</a>、<a href="">torchaudio</a> 库。</p><p>下面以 FashionMNIST 为例介绍<strong>加载流程</strong>，其包含 60000 个训练样本以及 10000 个测试样本，每个样本由一张灰度图、一个类别标签组成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor<br><br>training_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,			<span class="hljs-comment"># root 是训练/测试数据所存储的路径，相对于当前位置</span><br>    train=<span class="hljs-literal">True</span>,				<span class="hljs-comment"># train 指定训练或测试数据集</span><br>    download=<span class="hljs-literal">True</span>,			<span class="hljs-comment"># 如果数据集在 root 中不存在则从互联网上下载</span><br>    transform=ToTensor()	<span class="hljs-comment"># 特征与标签的变换，这里转成张量</span><br>)<br>test_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">False</span>,			<span class="hljs-comment"># False 表示指定测试集</span><br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br></code></pre></td></tr></table></figure><p>自定义数据集需要通过<strong>面向对象</strong>操作来完成，案例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> torchvision.io <span class="hljs-keyword">import</span> read_image<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><br><span class="hljs-comment"># 定义 MyDataset() 类, 继承自 Dataset 父类，重写 init、getitem、len 方法</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyDataset</span>(<span class="hljs-params">Dataset</span>):</span><br>    <br>    <span class="hljs-comment"># 实例化数据集对象，从文件目录中获取数据集，数据变换也在这里定义</span><br>    <span class="hljs-comment"># 如果数据特征过大，无法一次读入，这里就要记录文件目录，按需读取即可</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, annotations_file, img_dir, transform=<span class="hljs-literal">None</span>, target_transform=<span class="hljs-literal">None</span></span>):</span><br>        self.img_labels = pd.read_csv(annotations_file) <span class="hljs-comment"># (file_name, label) 列表</span><br>        self.img_dir = img_dir<br>        self.transform = transform<br>        self.target_transform = target_transform<br>    <br>    <span class="hljs-comment"># 返回数据集中样本的数量</span><br>	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span><br>		<span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.img_labels)<br>    <br>    <span class="hljs-comment"># 加载并返回数据集中给定索引 idx 的样本特征与标签</span><br>    <span class="hljs-comment"># 如果数据集不在内存中，要从指定目录中读取，并根据实际情况组织数据</span><br>    <span class="hljs-comment"># 通常在这里对数据进行变换和增强，并转为 Tensor 返回</span><br>	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span>(<span class="hljs-params">self, idx</span>):</span><br>		img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="hljs-number">0</span>]) <span class="hljs-comment"># 获取文件路径</span><br>        image = read_image(img_path)<br>        label = self.img_labels.iloc[idx, <span class="hljs-number">1</span>] <span class="hljs-comment"># 获取对应图像的标签</span><br>        <span class="hljs-keyword">if</span> self.transform:<br>            image = self.transform(image)<br>        <span class="hljs-keyword">if</span> self.target_transform:<br>            label = self.target_transform(label)<br>        <span class="hljs-keyword">return</span> image, label<br></code></pre></td></tr></table></figure><h3 id="数据集迭代访问"><a href="#数据集迭代访问" class="headerlink" title="数据集迭代访问"></a>数据集迭代访问</h3><p>获取的数据会以列表的形式存储，特征和标签构成一对<strong>元组</strong>。如果这样通过遍历列表的形式来访问数据集，则效率太低，我们希望以<strong>小批次</strong>（Mini-batch）为单位读取样本，并且在每一个 epoch 中<strong>打乱数据的顺序</strong>以防止模型过拟合，同时利用 Python 的<strong>并行处理</strong>（Multi-Processing）功能加速数据处理。</p><p>使用 <code>DataLoader</code> 模块就可以实现上述功能：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br><span class="hljs-comment"># 声明 DataLoader 模块，得到可迭代对象</span><br>train_dataloader = DataLoader(training_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">4</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">4</span>)<br><br><span class="hljs-comment"># 用 iter() 转迭代器，再用 next() 访问，每次获取一个批次，分包含 64 个特征与标签</span><br>train_features, train_labels = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_dataloader))<br><br><span class="hljs-comment"># 可迭代对象可以用 for 循环遍历，训练时每个 epoch 中的顺序会变化</span><br>num_epoches = <span class="hljs-number">100</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epoches):<br>    <span class="hljs-keyword">for</span> img, label <span class="hljs-keyword">in</span> train_dataloader:<br>        <span class="hljs-comment"># 训练代码，这里每一组 img, label 都是 64 个样本</span><br></code></pre></td></tr></table></figure><p>其中 <code>num_workers</code> 参数代表在声明时一次性创建的工作进程数量，此时 <code>batch_sampler</code> 会将指定 batch 分配给每个 worker，worker 负责将 batch <strong>载入显存</strong>。当 <code>DataLoader</code> 在某轮迭代需要用到该 batch 时，可以<strong>直接使用</strong>。分配的进程数量越多，获取下一 batch 的速度越快，因为它可能早就放在显存中了，但这也加重了 CPU 负担。一般将该值设置为 <strong>CPU 核心数量</strong>，根据性能变化逐步调整。</p><p>除了以上四个常用参数，有时还会用到一个 <code>collate_fn</code> 参数，顾名思义，该函数用于对样本进行<strong>核对、校勘</strong>。在 <code>Dataset</code> 类中调用 <code>__getitem__</code> 后，每次返回的都是一个样本，最后构成一个 Batch 的样本。如果再这个过程中出现出现什么<strong>样本错误</strong>（如存在空值、错误值），或者需要<strong>对批次进行特殊处理</strong>（如对齐整个 Batch 样本的长度），则需要在该函数中完成。</p><p><code>collate_fn</code> 默认是等于 <code>default_collate</code>，这是 PyTorch 自带的处理函数，也可以自己定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入的 batch 是 list of tuple (img, label)，本例子为过滤 None 数据</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">collate_fn</span>(<span class="hljs-params">batch</span>):</span><br>    batch = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>, batch))<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(batch) == <span class="hljs-number">0</span>: <br>        <span class="hljs-keyword">return</span> torch.Tensor()<br>    <span class="hljs-keyword">return</span> default_collate(batch)<br><br><span class="hljs-comment"># 假设 label 是文本，本例子为对齐文本的长度</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">collate_fn</span>(<span class="hljs-params">batch</span>):</span><br>    batch.sort(key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x[<span class="hljs-number">1</span>]), reverse=<span class="hljs-literal">True</span>)<br>    img, label = <span class="hljs-built_in">zip</span>(*batch)<br>    pad_label = []<br>    lens = []<br>    max_len = <span class="hljs-built_in">len</span>(label[<span class="hljs-number">0</span>])<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(label)):<br>        temp_label = [<span class="hljs-number">0</span>] * max_len<br>        temp_label[:<span class="hljs-built_in">len</span>(label[i])] = label[i]<br>        pad_label.append(temp_label)<br>        lens.append(<span class="hljs-built_in">len</span>(label[i]))<br>    <span class="hljs-keyword">return</span> img, pad_label, lens<br></code></pre></td></tr></table></figure><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>前文中已经出现过 <code>torchvision.transforms</code> 模块，该模块的功能是对图像进行预处理，内置各种<strong>数据增强</strong>操作，包括随机裁剪、翻转和旋转、变形和填充、修改属性等。每一个单独的变换都是一个<strong>函数</strong>，输入数据即可得到结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">preprocess = transforms.Resize([<span class="hljs-number">256</span>, <span class="hljs-number">256</span>])<br>img_transformed = preprocess(img)<br></code></pre></td></tr></table></figure><p>如果要连续进行多种操作，则可以用 <code>transforms.Compose()</code> 组合，参数为操作列表。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><br>myTransforms = transforms.Compose([<br>    transforms.RandomResizedCrop(<span class="hljs-number">224</span>),	<span class="hljs-comment"># 将给定图像随机裁剪并缩放到 224x224</span><br>    transforms.RandomHorizontalFlip(),	<span class="hljs-comment"># 给定的概率随机水平旋转，默认 0.5</span><br>    transforms.ToTensor(),				<span class="hljs-comment"># 将给定图像转为 Tensor</span><br>    transforms.Normalize([<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], [<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]) <span class="hljs-comment"># 归一化处理</span><br>])<br><br>data = MyDataset(<br>	annotations_file=<span class="hljs-string">&quot;annotations.csv&quot;</span>,<br>    img_dir=<span class="hljs-string">&quot;data/img&quot;</span>,<br>    transform=myTransforms	<span class="hljs-comment"># 调用自定义的预处理组合</span><br>)<br></code></pre></td></tr></table></figure><blockquote><p>该案例中归一化处理传入的两个参数分别代表均值和标准差，这里取的是 ImageNet 数据集的值，这是 CV 领域一种常见的做法，通常只要数据集是<strong>自然场景的普通照片</strong>就能使用。如果是特殊场景、特殊风格的数据，则需要重新计算。</p></blockquote><h2 id="模型搭建"><a href="#模型搭建" class="headerlink" title="模型搭建"></a>模型搭建</h2><p>网络的定义也是通过<strong>面向对象</strong>操作来完成，<code>torch.nn</code> 模块封装了神经网络的大部分操作，案例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-comment"># 定义 MyNet() 类, 继承自 nn.Module 父类</span><br><span class="hljs-comment"># 父类中封装了 parameters() train() eval() zero_grad() 等常用方法</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <br>    <span class="hljs-comment"># 重写构造函数 __init__(), 在实例化 MyNet() 时自动调用</span><br>    <span class="hljs-comment"># 输入参数除了必须的 self, 还包含其他实例化对象时必备的参数</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">10</span></span>):</span><br>        <span class="hljs-comment"># 由于子类定义了构造函数，父类的构造函数不再自动执行，需要显式调用</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 定义对象变量, 通过实例化 nn.Conv2d() 等类获得</span><br>        self.conv = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>)<br>        self.fc = nn.Linear(<span class="hljs-number">6</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, num_classes)<br><br>    <span class="hljs-comment"># 重写前向传播方法 forward(), 对输入 x 进行操作</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-comment"># 可以调用 self 的对象变量，也可以直接调用 F 函数</span><br>        x = F.max_pool2d(F.relu(self.conv(x)), <span class="hljs-number">2</span>)<br>        x = torch.flatten(x, <span class="hljs-number">1</span>) <span class="hljs-comment"># 将除了 batch 的维度展开到 1 维</span><br>        x = F.relu(self.fc(x))<br>        <span class="hljs-keyword">return</span> x<br><br>net = MyNet()<br><span class="hljs-built_in">input</span> = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>) <span class="hljs-comment"># (batch_size, channel, hight, width)</span><br>out = net(<span class="hljs-built_in">input</span>)<br></code></pre></td></tr></table></figure><p>如果这时候 <code>print(net)</code>，则会打印出类的所有属性，即每个构造函数中的网络部件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">MyNet(<br>  (conv): Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (fc): Linear(in_features=<span class="hljs-number">150</span>, out_features=<span class="hljs-number">10</span>, bias=<span class="hljs-literal">True</span>)<br>)<br></code></pre></td></tr></table></figure><p>由于继承了父类的方法，调用 <code>net.paprameters()</code> 可以得到<strong>参数列表</strong>，各个层的参数依次排列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>params = <span class="hljs-built_in">list</span>(net.parameters())<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(params)) <span class="hljs-comment"># 参数的长度，这里卷积层和全连接层各有 weight 和 bias</span><br><span class="hljs-comment"># 4,</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(params[<span class="hljs-number">0</span>].size()) <span class="hljs-comment"># 按索引访问指定参数</span><br><span class="hljs-comment"># torch.Size([6, 3, 3, 3])</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>([(name, param.shape) <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> net.named_parameters()]) <span class="hljs-comment"># 一次性访问全部</span><br><span class="hljs-comment"># [(&#x27;conv.weight&#x27;, torch.Size([6, 3, 3, 3])),</span><br><span class="hljs-comment">#  (&#x27;conv.bias&#x27;, torch.Size([6])),</span><br><span class="hljs-comment">#  (&#x27;fc.weight&#x27;, torch.Size([10, 150])),</span><br><span class="hljs-comment">#  (&#x27;fc.bias&#x27;, torch.Size([10]))]</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">sum</span>(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> net.parameters() <span class="hljs-keyword">if</span> p.requires_grad)) <span class="hljs-comment"># 计算总参数量</span><br></code></pre></td></tr></table></figure><h3 id="预加载的网络"><a href="#预加载的网络" class="headerlink" title="预加载的网络"></a>预加载的网络</h3><p>PyTorch 中自带了搭建好的经典模型，可以直接调用，包括 <a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/models/alexnet.html">AlexNet</a>、<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/models/vgg.html">VGG</a> 等，详见 <a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/models.html">官方文档</a>。可以选择是否使用<strong>预训练好的权重、原有的转换</strong>。下面的例子展示了使用预加载网络推理一个样本的过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision.io <span class="hljs-keyword">import</span> read_image<br><span class="hljs-keyword">from</span> torchvision.models <span class="hljs-keyword">import</span> resnet50, ResNet50_Weights<br><br>img = read_iamge(<span class="hljs-string">&quot;test.jpg&quot;</span>)<br><br><span class="hljs-comment"># 使用预训练好的权重参数调用模型，也可以不使用预训练，即随机初始化权重</span><br>weights = ResNet50_Weights.DEFAULT<br>model = resnet50(weights=weights) 	<span class="hljs-comment"># model = resnet50(weights=None)</span><br>model.<span class="hljs-built_in">eval</span>() <span class="hljs-comment"># 进入验证模式</span><br><br><span class="hljs-comment"># 使用模型默认的预处理方法（与权重对应）</span><br>preprocess = weights.transforms()	<span class="hljs-comment"># 使用 preprocess(img) 调用变换</span><br><br><span class="hljs-comment"># 此处只有一张照片，因此我们在 dim=0 处补 1 表示 batch_size</span><br>batch = preprocess(img).unsqueeze(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 使用预加载的网络预测出类别，并打印</span><br>prediction = model(batch).squeeze(<span class="hljs-number">0</span>).softmax(<span class="hljs-number">0</span>)<br>class_id = prediction.argmax().item()<br>score = prediction[class_id].item()<br>category_name = weights.meta[<span class="hljs-string">&quot;categories&quot;</span>][class_id]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;category_name&#125;</span>: <span class="hljs-subst">&#123;<span class="hljs-number">100</span> * score:<span class="hljs-number">.1</span>f&#125;</span>%&quot;</span>)<br></code></pre></td></tr></table></figure><p>此外，我们还可以对已经定义好的模型进行<strong>添加、修改</strong>网络层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 添加网络层，第一个参数是网络层的命名</span><br>vgg16.classifier.add_module(<span class="hljs-string">&quot;add_linear&quot;</span>, nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">10</span>)) <span class="hljs-comment"># 在 classifier 里加一层</span><br><span class="hljs-comment"># 修改网络层，直接指定层数修改</span><br>vgg16.classifier[<span class="hljs-number">6</span>] = nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>) <span class="hljs-comment"># 将 classifier 最后一层的输出由 1000 改为 10</span><br><span class="hljs-comment"># print(vgg16)</span><br></code></pre></td></tr></table></figure><h3 id="nn-网络模块"><a href="#nn-网络模块" class="headerlink" title="nn 网络模块"></a>nn 网络模块</h3><p>这里介绍一下在构建网络时常见的一些网络模块，即在 <code>__init__</code> 中定义的部分，当然也可以在 <code>nn.Sequential()</code> 中组合，稍后会介绍。</p><ul><li><strong>全连接层</strong>：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	in_features: 输入特征张量的大小，可以理解为该层神经元个数，即 (batch_size, in_features)</span><br><span class="hljs-string">	out_features: 输出特征张量的大小，可以理解为下一层神经元个数</span><br><span class="hljs-string">	bias: 是否启用偏置，除了 weight 数组，还会初始化一个 bias 数组</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>nn.Flatten()	<span class="hljs-comment"># 展平层，通常放在全连接层之前，如 (64, 3, 5, 5) -&gt; (64, 75)</span><br>nn.Linear(in_features, out_features, bias=<span class="hljs-literal">True</span>) <span class="hljs-comment"># 如 (64, 75) -&gt; (64, 10)</span><br></code></pre></td></tr></table></figure><ul><li><strong>常用的卷积层</strong>，最终输出的张量维度为 $\left( \frac{n_h-k_h+2p_h+s_h}{s_h}, \frac{n_w-k_w+2p_w+s_w}{s_w} \right) $：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	in_channels/out_channels: 输入/输出通道数，四维张量 [N, C, H, W] 中的 C</span><br><span class="hljs-string">	kernel_size: 卷积核大小，可输入一个值或元组</span><br><span class="hljs-string">	stride: 步幅，默认为 1，可输入一个值或元组</span><br><span class="hljs-string">	padding: 填充，默认不填充，可输入一个值或元组</span><br><span class="hljs-string">	padding_mode: 可选 zeros(零) reflect(镜像) replicate(复制) circular(循环)</span><br><span class="hljs-string">	dilation: 是否采用空洞卷积，默认为 1 不采用</span><br><span class="hljs-string">	卷积核参数量: [out_channels, in_channels, kernel_height, kernel_width]</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">True</span>, padding_mode=<span class="hljs-string">&#x27;zeros&#x27;</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>常用的池化层</strong>（无参数），最终输出的张量维度同上：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	kernel_size: 池化窗口大小，可输入一个值或元组</span><br><span class="hljs-string">	stride: 默认为窗口的大小，即每个窗口不重叠，与卷积不同</span><br><span class="hljs-string">	return_indices: 是否返回最大值位置索引</span><br><span class="hljs-string">	ceil_mode: 输出的形状取整方向，默认为 False 向下取整</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>nn.MaxPool2d(kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, return_indices=<span class="hljs-literal">False</span>, ceil_mode=<span class="hljs-literal">False</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	count_include_pad: 计算平均值时是否包括零填充，默认包括</span><br><span class="hljs-string">	divisor_override: 除数大小，默认就是池化窗口大小，但可以自己指定</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>nn.AvgPool2d(kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>, ceil_mode=<span class="hljs-literal">False</span>, count_include_pad=<span class="hljs-literal">True</span>, divisor_override=<span class="hljs-literal">None</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	output_size: 固定输出尺寸，即可自适应窗口大小，可输入一个值或元组</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>nn.MaxPool2d(output_size)<br>nn.AdaptiveAvgPool2d(output_size)<br></code></pre></td></tr></table></figure><ul><li><strong>常用激活函数</strong>：也作为层来定义，但都没有参数，默认 <code>inplace=Flase</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Sigmoid() <span class="hljs-comment"># S 型激活函数，[0, 1]，存在梯度消失/梯度爆炸/不好收敛/效率低的问题</span><br>nn.Tanh() <span class="hljs-comment"># 双曲正切函数，[-1, 1]，相对更好收敛，但还是容易梯度消失/梯度爆炸/效率低</span><br>nn.ReLU() <span class="hljs-comment"># 线性修正单元，计算快，单侧饱和，不存在梯度消失问题，但会有 dead 问题</span><br>nn.LeakyReLU(negative_slope=<span class="hljs-number">0.01</span>) <span class="hljs-comment"># 负区间也采用线性，梯度不再为 0，解决 dead 问题，但不再单侧饱和</span><br>nn.PReLU(num_parameters=<span class="hljs-number">1</span>) <span class="hljs-comment"># 可学习 alpha 的 LeakyReLU，需要有较多数据时才能使用</span><br>nn.ELU(alpha=<span class="hljs-number">1.0</span>) <span class="hljs-comment"># 负区间采用指数函数，既单侧饱和，又解决了 dead 问题</span><br>nn.Softplus(beta=<span class="hljs-number">1</span>, threshold=<span class="hljs-number">20</span>) <span class="hljs-comment"># ReLU 的近似，保证输出为正数</span><br>nn.Softmax(dim=<span class="hljs-literal">None</span>) <span class="hljs-comment"># 最后对输出值的处理，[0, 1] 且和为 1</span><br></code></pre></td></tr></table></figure><ul><li><strong>特殊功能层</strong>：注意 $\textrm{Dropout}$ 只在<strong>训练阶段</strong>启用，并且不带参数。而 $\textrm{BN}$ 层实际是<strong>有参数存储</strong>的，包括仿射参数和<strong>测试阶段</strong>的移动平均。同样 $\textrm{LN}$ 层也是<strong>有参数存储</strong>的，即仿射参数，但不会跟踪全局的移动平均，<strong>所有阶段</strong>都一致。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	p: 张量元素被置为 0 的概率，置为 0 的神经元整个不激活</span><br><span class="hljs-string">	inplace: 是否进行原地操作，默认为否</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-comment"># 主要用于一维数据，一般在线性层、激活函数之后</span><br>nn.Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 主要用于二维数据，一般在卷积层、激活函数之后</span><br>nn.Dropout2d(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	num_features: 通道数，1d 中代表 [N, L] 中的 L, 2d 中代表 [N, C, H, W] 中的 C</span><br><span class="hljs-string">	eps: 归一化时加到分母（方差）上的小量，防止除零溢出</span><br><span class="hljs-string">	momentum: 预测模型下进行移动平均的动量</span><br><span class="hljs-string">	affine: 是否具有可学习的仿射参数 gamma 和 beta，用于拉伸和偏移分布</span><br><span class="hljs-string">	track_running_stats: 是否跟踪均值和方差，进行移动平均</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-comment"># 一维 BN 是取 axis=0 进行归一化的，一般在线性层和激活函数之间</span><br>nn.BatchNorm1d(num_features, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 二维 BN 是取 axis=(0,2,3) 进行归一化的，一般在卷积层和激活函数之间</span><br>nn.BatchNorm2d(num_features, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	normalized_shape: 传入一个整数表示最后一维的长度，传入列表表示最后两维的长度</span><br><span class="hljs-string">	eps: 归一化时加到分母（方差）上的小量，防止除零溢出</span><br><span class="hljs-string">	elementwise_affine: 是否具有可学习的仿射参数 gamma 和 beta，用于拉伸和偏移分布</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-comment"># 取 axis=-1 或 axis=(-2, -1) 进行归一化</span><br>nn.LayerNorm(normalized_shape, eps=<span class="hljs-number">1e-05</span>, elementwise_affine=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>词嵌入层</strong>：在 NLP 任务中经常使用，用来代替 One-Hot 实现降维，并学习词之间的联系。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	num_embeddings: 词典的大小，如果有 5000 词，id 就是 0-4999</span><br><span class="hljs-string">	embedding_dim: 嵌入向量的维度，即用多少维的空间来表示一个词向量</span><br><span class="hljs-string">	padding_idx: 输入句子长度不一致时，末尾的 PAD 的索引值</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>nn.Embedding(num_embeddings, embedding_dim, padding_idx=<span class="hljs-literal">None</span>)<br><span class="hljs-comment"># 使用 Embedding 时要在输入数据前就预处理好格式，具体步骤如下：</span><br><span class="hljs-comment"># 原始句子 			[&#x27;I am a boy.&#x27;,&#x27;How are you?&#x27;,&#x27;I am very lucky.&#x27;]</span><br><span class="hljs-comment"># 分词映射到索引 	  [[3,6,5,6,7],[6,4,7,9,5]，[4,5,8,7]]</span><br><span class="hljs-comment"># 填充[EOS][PAD] 	 [[3,6,5,6,7,1],[6,4,7,9,5,1],[4,5,8,7,1,2]]</span><br><span class="hljs-comment"># 切片用于序列输入   [[3,6,4],[6,4,5],[5,7,8],[6,9,7],[7,5,1],[1,1,2]]</span><br><span class="hljs-comment"># 转为张量			batch = torch.LongTensor(batch)</span><br>embed = torch.nn.Embedding(num_embeddings, embedding_dim)<br>embed_batch = embed(batch) <span class="hljs-comment"># [seq_len, batch_size] -&gt; [seq_len, batch_size, embedding_dim]</span><br></code></pre></td></tr></table></figure><p>如果想观察网络中每层的通道数、数据规模是否符合预期，可以<strong>构造一个单独的样本</strong>来前向传播，并实时获取其形状：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X=layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__, <span class="hljs-string">&#x27;\t\ output shape:\t&#x27;</span>, X.shape)<br></code></pre></td></tr></table></figure><p>需要注意的是，如果要设计全新的网络模块，除了实现一个<strong>运算函数</strong>，还需要定义<strong>可训练的参数</strong>。<code>nn.Parameter()</code> 将一个<strong>不可训练</strong>的 Tensor 转换成<strong>可训练</strong>的类型 Parameter，并将这个 Parameter <strong>绑定</strong>到模型的参数列表里面。比起参数 <code>requires_grad=True</code> 还多了绑定的过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Conv2D</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, kernel_size</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.weight = nn.Parameter(torch.rand(kernel_size))<br>        self.bias = nn.Parameter(torch.zeros(<span class="hljs-number">1</span>))<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-keyword">return</span> corr2d(x, self.weight) + self.bias<br></code></pre></td></tr></table></figure><h3 id="F-函数式编程"><a href="#F-函数式编程" class="headerlink" title="F 函数式编程"></a>F 函数式编程</h3><p>在 PyTorch 的 <code>nn</code> 模块中，我们不需要手动定义网络层的权重和偏置，这就是 PyTorch 比 Tensorflow 简洁的地方。当然，PyTorch 也提供了 <code>nn.Functional</code> 函数式编程的方法，其中的 <code>F.conv2d()</code> 就和 Tensorflow 一样，要<strong>预先定义好</strong>卷积核的权重和偏置 <code>nn.Parameter()</code>，作为形参之一输入。</p><p>当然，为了不多此一举，网络中具有<strong>可学习参数</strong>的层（如全连接层、卷积层）通常会放在 <code>__init__</code> 中<strong>显式定义</strong>，而<strong>不具有参数</strong>的层（如 ReLU、Dropout、BN 层）一般放在 <code>forward</code> 中使用 <code>F</code> 函数调用。</p><ul><li><strong>无参数的池化操作</strong>：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">F.max_pool2d(<span class="hljs-built_in">input</span>, kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>, return_indices=<span class="hljs-literal">False</span>)<br>F.avg_pool2d(<span class="hljs-built_in">input</span>, kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>, ceil_mode=<span class="hljs-literal">False</span>, count_include_pad=<span class="hljs-literal">True</span>, divisor_override=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>无参数的激活函数</strong>：以下函数都支持 <code>F.relu_</code> 的原地版本。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">F.sigmoid(<span class="hljs-built_in">input</span>)<br>F.tanh(<span class="hljs-built_in">input</span>)<br>F.relu(<span class="hljs-built_in">input</span>)<br>F.leaky_relu(<span class="hljs-built_in">input</span>, negative_slope=<span class="hljs-number">0.01</span>)<br>F.prelu(<span class="hljs-built_in">input</span>, weight) 	<span class="hljs-comment"># 特例：这里 weight 是个可学习的参数，一般不会使用函数式编程</span><br>F.elu(<span class="hljs-built_in">input</span>, alpha=<span class="hljs-number">1.0</span>)<br>F.softplus(<span class="hljs-built_in">input</span>, beta=<span class="hljs-number">1</span>, threshold=<span class="hljs-number">20</span>)<br>F.softmax(<span class="hljs-built_in">input</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>无参数的特殊功能函数</strong>：包括 Dropout 和一些计算向量距离的函数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">F.dropout(<span class="hljs-built_in">input</span>, p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>F.dropout2d(<span class="hljs-built_in">input</span>, p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>F.cosine_similarity(x1, x2)  		<span class="hljs-comment"># 计算两个向量的余弦相似度</span><br>F.pairwise_distance(x1, x2, p=<span class="hljs-number">2.0</span>) 	<span class="hljs-comment"># 计算两个向量的 p 范数距离</span><br>F.one_hot(x, num_classes)			<span class="hljs-comment"># 生成长度为 num_classes 的 one_hot 张量组</span><br></code></pre></td></tr></table></figure><blockquote><p>注意在 F 函数式编程中，函数名<strong>全都为小写</strong>，与 nn 网络模块中不同！</p></blockquote><h3 id="使用块搭建深层网络"><a href="#使用块搭建深层网络" class="headerlink" title="使用块搭建深层网络"></a>使用块搭建深层网络</h3><p>块（Block）比单个层（Layer）大，又比整个模型（Model）小，是实现一个深层网络的必备操作，通常用于封装以重复模式排列的若干层，最早在 $\textrm{VGG}$ 中得到应用。</p><p>从编程角度来看，每个块也需要用类定义，并实现<strong>初始化、前向传播</strong>功能。实际中 PyTorch 提供了一个简单的方法 <code>nn.Sequential</code>，可以将不同神经网络层进行<strong>顺序拼接</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">self.net = nn.Sequential(<br>    nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">256</span>),<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>)<br>)<br>output = net(torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">20</span>)) <span class="hljs-comment"># output.shape = (2, 10)</span><br></code></pre></td></tr></table></figure><p>如果希望<strong>多个层共享参数</strong>，只需先实例化一个层，并将其作为网络的多个部分，此时所有层的权重张量<strong>共享一个内存空间</strong>。这就相当于 <code>__init__</code> 中的同一层在 <code>forward</code> 中被调用了两次，因此<strong>每次调用的 grad 也会相加</strong>。如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">sharedLayer = nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>)<br>net = nn.Sequential(nn.Linear(<span class="hljs-number">4</span>, <span class="hljs-number">8</span>), nn.ReLU(),<br>                    sharedLayer, nn.ReLU(),<br>                    sharedLayer, nn.ReLU(),<br>                    nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><p>如果要封装更复杂的块，例如 VGG 中的 <code>VGG_Block</code>，则可以将其单独用类定义，并设置好参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">VGG_Block</span>(<span class="hljs-params">num_convs, in_channels, out_channels</span>):</span><br>    layers = [] <span class="hljs-comment"># 以列表的形式拼接网络模块</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_convs):<br>        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>))<br>        layers.append(nn.ReLU())<br>        in_channels = out_channels<br>    layers.append(nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">return</span> nn.Sequential(*layers) <span class="hljs-comment"># 这里 * 表示解包，将列表拆分</span><br></code></pre></td></tr></table></figure><h3 id="网络初始化"><a href="#网络初始化" class="headerlink" title="网络初始化"></a>网络初始化</h3><p>默认情况下，PyTorch 会根据一个范围<strong>均匀地初始化</strong>权重和偏置矩阵， 这个范围是根据输入和输出维度计算出的。其中<code>nn.init</code> 模块提供了多种预置初始化方法。包括：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.init.zeros_(weight)				<span class="hljs-comment"># 初始化为全零</span><br>nn.init.ones_(weight)				<span class="hljs-comment"># 初始化为全一</span><br>nn.init.eye_(weight)				<span class="hljs-comment"># 对二维矩阵，初始化为单位矩阵</span><br>nn.init.orthogonal_(weight) 		<span class="hljs-comment"># 对二维矩阵，初始化为正交矩阵</span><br>nn.init.constant_(weight, val) 		<span class="hljs-comment"># 初始化为指定常数</span><br>nn.init.uniform_(weight, a=<span class="hljs-number">0</span>, b=<span class="hljs-number">1</span>) 	<span class="hljs-comment"># 初始化为均匀分布 U(a,b)</span><br>nn.init.normal_(weight, mean=<span class="hljs-number">0</span>, std=<span class="hljs-number">1</span>) <span class="hljs-comment"># 从给定均值和标准差，初始化为正态分布；</span><br>nn.init.xavier_uniform_(weight) 	<span class="hljs-comment"># 使用 Xavier 初始化生成均匀分布，范围由扇入、扇出确定</span><br>nn.init.xavier_normal_(weight)		<span class="hljs-comment"># 使用 Xavier 初始化生成正态分布，方差由扇入、扇出确定</span><br>nn.init.kaiming_uniform_(weight, a=<span class="hljs-number">0</span>, mode=<span class="hljs-string">&#x27;fan_in&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;leaky_relu&#x27;</span>)<br>nn.init.kaiming_normal_(weight, a=<span class="hljs-number">0</span>, mode=<span class="hljs-string">&#x27;fan_in&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;leaky_relu&#x27;</span>) <br><span class="hljs-comment"># 使用 He 初始化，范围由 mode 决定，可选扇入或扇出，a 为这层后 Leaky ReLU 的斜率系数，rulu 则不需要</span><br></code></pre></td></tr></table></figure><p>调用上述内置的初始化器，我们可以实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_weights</span>(<span class="hljs-params">m</span>):</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear: <span class="hljs-comment"># 仅当网络层的类型匹配时采用</span><br>        nn.init.normal_(m.weight, mean=<span class="hljs-number">0</span>, std=<span class="hljs-number">0.01</span>)<br>        nn.init.zeros_(m.bias)<br>net.apply(init_weights) <span class="hljs-comment"># 对所有层应用初始化方法</span><br></code></pre></td></tr></table></figure><p>注意到上面的方法要求<strong>网络类型完全匹配</strong>，而 PyTorch 中自带有很多相似的层，如 <code>Conv2d</code>、<code>ConvTranspose2d</code> 等，此时可以获取<strong>类名</strong>进行字符串匹配：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_weights</span>(<span class="hljs-params">m</span>):</span><br>    classname = m.__class__.__name__    	<span class="hljs-comment"># 返回 m 的类名</span><br>    <span class="hljs-keyword">if</span> classname.find(<span class="hljs-string">&#x27;Conv&#x27;</span>) != -<span class="hljs-number">1</span>:		<span class="hljs-comment"># 如果类名包含 Conv</span><br>    	nn.init.kaiming_uniform_(m.weight)<br>        nn.init.uniform_(weight, a=-<span class="hljs-number">1</span>, b=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">elif</span> classname.find(<span class="hljs-string">&#x27;BatchNorm&#x27;</span>) != -<span class="hljs-number">1</span>:           <br>        nn.init.normal_(m.weight, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.02</span>)<br>        nn.init.zeros_(m.bias)<br>net.apply(init_weights)<br></code></pre></td></tr></table></figure><h2 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h2><p>假设我们已经定义好 <code>MyDataset</code> 类和 <code>MyModel</code> 类，现在正式进入训练阶段。除了实例化数据集和模型，还需要额外定义<strong>损失函数和优化器</strong>。下面给出一个完整的训练过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实例化数据集和模型</span><br>dataset = MyDataset(file)<br>train_set = DataLoader(dataset, <span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>)<br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>model = MyModel().to(device) 				<span class="hljs-comment"># 将模型放到指定的设备上</span><br><span class="hljs-comment"># 定义损失函数和优化器</span><br>criterion = nn.MSELoss()<br>optimizer = torch.optim.SGD(model.parameters(), <span class="hljs-number">0.1</span>)<br><span class="hljs-comment"># 训练过程</span><br>model.train()								<span class="hljs-comment"># 将模型切换到「训练模式」，也可以放到循环内</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>	<span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_set:<br>		optimizer.zero_grad()				<span class="hljs-comment"># 每个 batch 前都要清空梯度</span><br>        X, y = X.to(device), y.to(device)	<span class="hljs-comment"># 将数据放到指定的设备上</span><br>        pred = model(X)<br>        loss = criterion(pred, y)			<span class="hljs-comment"># 计算损失</span><br>        loss.backward()						<span class="hljs-comment"># 反向传播</span><br>        optimizer.step()					<span class="hljs-comment"># 每个 batch 更新一次参数</span><br></code></pre></td></tr></table></figure><p>所谓的「<strong>训练模式</strong>」，是相对另一个「<strong>评估模式</strong>」下的模型传播路径。实现上，模型通过 <code>model.train()</code> 将变量 <code>self.training</code> 置为 True，通过 <code>model.eval()</code> 将 <code>self.training</code> 置为 False，所以即便训练与测试共用一个模型，也能通过该变量来<strong>区分现在属于训练还是测试</strong>。</p><p>当模型中使用了 Dropout、BatchNorm 等模块，在训练阶段和测试阶段的<strong>操作不同</strong>，此时区分模式就很有必要。此外，如果网络中有一部分层只在训练阶段启用，则可以用 <code>if self.training:</code> 来执行。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>PyTorch 在 <code>torch.nn</code> 模块中已经定义了大部分常用的 Loss，具体的计算过程详见 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#loss-functions">手册</a>：</p><div class="table-container"><table><thead><tr><th style="text-align:center">损失函数</th><th style="text-align:center">名称</th><th style="text-align:center">适用场景</th></tr></thead><tbody><tr><td style="text-align:center"><code>nn.L1Loss()</code></td><td style="text-align:center">绝对值误差损失</td><td style="text-align:center">回归</td></tr><tr><td style="text-align:center"><code>nn.MSELoss()</code></td><td style="text-align:center">均方误差损失</td><td style="text-align:center">回归</td></tr><tr><td style="text-align:center"><code>nn.SmoothL1Loss()</code></td><td style="text-align:center">Huber 损失（结合 L1 和 L2）</td><td style="text-align:center">回归</td></tr><tr><td style="text-align:center"><code>nn.BCELoss()</code></td><td style="text-align:center">二分类交叉熵损失</td><td style="text-align:center">二分类</td></tr><tr><td style="text-align:center"><code>nn.BCEWithLogitsLoss()</code></td><td style="text-align:center">自带 Sigmoid 的 BCELoss</td><td style="text-align:center">二分类</td></tr><tr><td style="text-align:center"><code>nn.CrossEntropyLoss()</code></td><td style="text-align:center">交叉熵损失</td><td style="text-align:center">多分类</td></tr><tr><td style="text-align:center"><code>nn.NLLLoss()</code></td><td style="text-align:center">负对数似然函数损失</td><td style="text-align:center">多分类</td></tr><tr><td style="text-align:center"><code>nn.NLLLoss2d()</code></td><td style="text-align:center">图像负对数似然函数损失</td><td style="text-align:center">图像分割</td></tr><tr><td style="text-align:center"><code>nn.KLDivLoss()</code></td><td style="text-align:center">KL 散度损失</td><td style="text-align:center">分布学习</td></tr><tr><td style="text-align:center"><code>nn.MarginRankingLoss()</code></td><td style="text-align:center">排序相似度的损失</td><td style="text-align:center">推荐排序</td></tr><tr><td style="text-align:center"><code>nn.MultiLabelMarginLoss()</code></td><td style="text-align:center">多标签分类的损失</td><td style="text-align:center">多标签分类</td></tr><tr><td style="text-align:center"><code>nn.SoftMarginLoss()</code></td><td style="text-align:center">多标签二分类问题的损失</td><td style="text-align:center">多标签二分类</td></tr></tbody></table></div><p>大部分损失函数的用法都是先实例化，再输入<strong>预测值和标签值</strong>，预测值形状为 <code>(batch_size, num_class)</code>，对应<strong>每个类的概率</strong>，标签值形状为 <code>(batch_size, num_class)</code> 或 <code>(batch_size, 1)</code>，前者代表<strong>标签平滑或多标签</strong>，后者仅仅只有<strong>目标类的索引</strong>。具体用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">criterion = nn.MSELoss()<br>loss = criterion(pred, target) <span class="hljs-comment"># 输出的 loss 是零维张量</span><br></code></pre></td></tr></table></figure><blockquote><p>其中 <code>nn.CrossEntropyLoss()</code> 的 <code>target</code> 输入必须是 LongTensor 类型，可以使用 <code>.long()</code> 进行类型强转。而其他 Loss 则是采用 Float 作为目标输入。</p></blockquote><p>需要注意所有损失函数都会<strong>自动对 Loss 取均值</strong>，原因是默认参数 <code>reduction=&#39;mean&#39;</code>，可以修改成 <code>&#39;sum&#39;</code> 进行求和，或 <code>&#39;none&#39;</code> 保留 <code>(batch_size, 1)</code> 形状。这会影响到梯度的大小，用 <code>mean</code> 时，其<strong>学习率大小</strong>应为用 <code>sum</code> 时的 <code>batch_size</code> 倍。有时候自己写的损失函数，也会在调用时加上 <code>.mean()</code> 来对齐结果。</p><p>此外，有的损失函数会有 <code>weight</code> 参数，BCEWithLogitsLoss() 有 <code>pos_weight</code> 参数，这些都是用来帮助模型解决样本类别不均衡问题的。需要传入张量参数 <code>orch.from_numpy(np.array([1,15])).float()</code>。</p><h3 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h3><p>PyTorch 在 <code>torch.optim</code> 中定义了十多种优化器，这里介绍常用的几种：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	params: 待优化的模型参数、或者是模型中参数组的字典（定制不同策略）</span><br><span class="hljs-string">	lr: 学习率起始值，给定常数即可，如 1e-3, 5e-6</span><br><span class="hljs-string">	momentum: 动量因子，默认为 0，通常设置为 0.9 0.8</span><br><span class="hljs-string">	weight_decay: 权重衰减（L2惩罚的系数），默认为 0，通常设置为 1e-4 1e-3 再微调</span><br><span class="hljs-string">	nesterov: 是否采用牛顿动量（NAG），较少设置</span><br><span class="hljs-string">	### 变化量（速度） v = mo * v + dx, 参数更新 x -= lr * v</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>optim.SGD(params, lr, momentum=<span class="hljs-number">0</span>, weight_decay=<span class="hljs-number">0</span>, nesterov=<span class="hljs-literal">False</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	lr: 全局学习率，设定后就不再改变，默认为 0.1 一般不修改（会自适应）</span><br><span class="hljs-string">	### 根据梯度的大小自适应学习率，梯度大则学习率小，更新 x -= lr * dx / sqrt(dx^2)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>optim.Adagrad(params, lr=<span class="hljs-number">0.01</span>, weight_decay=<span class="hljs-number">0</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	alpha: 避免学习率下降太快，采用滑动平均的保留比例，默认为 0.99</span><br><span class="hljs-string">	eps: 为了增加数值计算的稳定性而加到分母里的项，不用理会</span><br><span class="hljs-string">	### 缓存 cache = α * cache + (1-α) * dx^2, 更新 x -= lr * dx / sqrt(cache)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>optim.RMSprop(params, lr=<span class="hljs-number">0.01</span>, alpha=<span class="hljs-number">0.99</span>, eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0</span>, momentum=<span class="hljs-number">0</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	betas: 在缓存的基础上再对梯度加上记忆，推荐参数 (0.9, 0.999)</span><br><span class="hljs-string">	### 记忆 memory = β1 * memory + (1 ‐ β1) * dx</span><br><span class="hljs-string">	### 缓存 cache = β2 * cache + (1 - β2) * dx^2</span><br><span class="hljs-string">	### 更新 x -= lr * memory / sqrt(cache)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>optim.Adam(params, lr=<span class="hljs-number">0.001</span>, betas=(<span class="hljs-number">0.9</span>, <span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0</span>)<br>optim.Adamax(params, lr=<span class="hljs-number">0.002</span>, betas=(<span class="hljs-number">0.9</span>, <span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0</span>) <span class="hljs-comment"># 变体1</span><br>optim.SparseAdam(params, lr=<span class="hljs-number">0.001</span>, betas=(<span class="hljs-number">0.9</span>,<span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>) <span class="hljs-comment"># 变体2，适用于稀疏张量</span><br>optim.AdamW(params, lr=<span class="hljs-number">0.001</span>, betas=(<span class="hljs-number">0.9</span>,<span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0.01</span>) <span class="hljs-comment">#变体3</span><br></code></pre></td></tr></table></figure><p>具体用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br>optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.9</span>) <span class="hljs-comment"># 用于模型训练</span><br>optimizer = optim.Adam([var1, var2], lr=<span class="hljs-number">0.0001</span>) <span class="hljs-comment"># 也可以用于任何变量</span><br><br>optimizer.zero_grad()	<span class="hljs-comment"># 把上一轮 loss 关于 weight 的梯度变成 0，防止自动累计</span><br>optimizer.step()		<span class="hljs-comment"># 进行一次优化迭代，即 x += Δ 的过程</span><br></code></pre></td></tr></table></figure><p>如果想对网络中不同部分采取不同的优化策略，则需要用到<strong>参数组</strong>（param_group），其保存了各个参数及其对应的学习率、动量等，可以通过<strong>列表嵌套字典</strong>的形式来设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 默认情况，一个参数组</span><br>optim.SGD(model.parameters(), lr=<span class="hljs-number">1e-2</span>, momentum=<span class="hljs-number">0.9</span>)<br><span class="hljs-comment"># 两个参数组，此时 len(optim.param_gruops) == 2</span><br>optim.SGD([&#123;<span class="hljs-string">&#x27;params&#x27;</span>: model.base.parameters(), <span class="hljs-string">&#x27;weight_decay&#x27;</span>: wd&#125;,<br>           &#123;<span class="hljs-string">&#x27;params&#x27;</span>: model.classifier.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">1e-3</span>&#125;], <span class="hljs-comment"># 访问模型局部参数即可</span><br>          lr=<span class="hljs-number">1e-2</span>, momentum=<span class="hljs-number">0.9</span>)<br></code></pre></td></tr></table></figure><h3 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h3><p>注意到上述优化器参数都有一个固定的 <code>lr</code>，即使通过不同策略使学习率改变，也会有一个全局 <code>lr</code>。为了更好地收敛模型，PyTorch 在 <code>torch.optim.lr_scheduler</code> 里封装了主动进行学习率衰减的方法。通过一个调度器（Scheduler）来控制优化器，具体用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=<span class="hljs-number">1e-4</span>)<br>scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=<span class="hljs-number">5</span>, verbose=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>	<span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_set:<br>		optimizer.zero_grad()<br>        pred = model(X)<br>        loss = criterion(pred, y)<br>        loss.backward()<br>        optimizer.step()<br>    scheduler.step()					<span class="hljs-comment"># 调度器工作，每个 epoch 调度一次</span><br></code></pre></td></tr></table></figure><p>当然还有许多不同的调度器，可以<strong>有规律地间隔调整</strong>，或者<strong>根据指标的变化情况</strong>调整。参数 <code>verbose=True</code> 表示每次更新都会输出一条信息，适用于所有调度器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 等间隔调整，每隔 step_size 个 epoch，调整为 lr * gamma</span><br>optim.lr_scheduler.StepLR(optimizer, step_size, gamma=<span class="hljs-number">0.1</span>)<br><span class="hljs-comment"># 多间隔调整，milestones 传入参数列表，表示在第几个 epoch 更新</span><br>optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=<span class="hljs-number">0.1</span>)<br><span class="hljs-comment"># 指数衰减调整，每个 epoch 更新为 lr * gamma ^ epoch，gamma 一般设为 0.9</span><br>optim.lr_scheduler.ExponentialLR(optimizer, gamma)<br><span class="hljs-comment"># 余弦退火函数调整，走势类似 cos(x)，当 epoch = T_max 时，学习率取最低点 eta_min</span><br>optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 根据指标调整，当给定指标在最近 patience 个 epoch 中都没有变化，学习率下降</span><br>optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="hljs-string">&#x27;min&#x27;</span>, factor=<span class="hljs-number">0.1</span>, patience=<span class="hljs-number">10</span>)<br>scheduler.step(val_loss) 	<span class="hljs-comment"># loss 对应 mode=&#x27;min&#x27;，表示当指标不再降低时更新</span><br>scheduler.step(val_acc) 	<span class="hljs-comment"># acc 对应 mode=&#x27;max&#x27;，表示当指标不再升高时更新</span><br><br><span class="hljs-comment"># 自定义倍率调整，通过匿名函数为不同参数组设置不同规则，更新为 lr * lambda(epoch)</span><br>optimizer = optim.SGD([&#123;<span class="hljs-string">&#x27;params&#x27;</span>: base_params&#125;,<br>        			   &#123;<span class="hljs-string">&#x27;params&#x27;</span>: net.fc.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">1e-2</span>&#125;],<br>                      lr=<span class="hljs-number">1e-3</span>, momentum=<span class="hljs-number">0.9</span>, weight_decay=<span class="hljs-number">1e-4</span>)<br>lambda1 = <span class="hljs-keyword">lambda</span> epoch: epoch // <span class="hljs-number">3</span><br>lambda2 = <span class="hljs-keyword">lambda</span> epoch: <span class="hljs-number">0.95</span> ** epoch<br>scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])<br></code></pre></td></tr></table></figure><h2 id="评估阶段"><a href="#评估阶段" class="headerlink" title="评估阶段"></a>评估阶段</h2><p>前文提到，「<strong>评估模式</strong>」是相对于「<strong>训练模式</strong>」的模型传播路径，通常在<strong>验证集、测试集</strong>上推理时会开启。开启命令为 <code>model.eval()</code>，也可以用 <code>model.train(False)</code>。在该模式下，Dropout、BatchNorm 等模块会有所区别。并且模型不需要进行反向传播，也就无需进行梯度计算，此时使用 <code>torch.is_grad_enabled()</code> 会返回 False。</p><p>需要注意的是，为了<strong>防止</strong> PyTorch 自动对张量进行梯度计算，要用 <code>with torch.no_grad():</code> 包裹评估阶段的代码。关闭梯度计算后模型的<strong>推理速度</strong>也会大大增加。下面介绍具体用法。</p><ul><li><strong>验证过程</strong>：通常与训练阶段一起运行，即每个 epoch 训练完都在验证集上验证，并计算 Loss：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epoch):<br>	model.train()<br>    <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> tr_set:<br>        <span class="hljs-comment"># 训练过程略去</span><br><br>    model.<span class="hljs-built_in">eval</span>()										<span class="hljs-comment"># 打开评估模式</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():								<span class="hljs-comment"># 关闭梯度计算</span><br>        total_loss = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> val_set:<br>            X, y = X.to(device), y.to(device)<br>            pred = model(X)<br>            loss = criterion(pred, y)<br>            total_loss += loss.cpu().item() * <span class="hljs-built_in">len</span>(X) 	<span class="hljs-comment"># loss 会默认取 mean</span><br></code></pre></td></tr></table></figure><ul><li><strong>测试阶段</strong>：无需优化、无需计算损失，只需前向推理预测答案即可：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">eval</span>()<br>preds = []<br><span class="hljs-keyword">with</span> torch.no_grad():<br>    <span class="hljs-keyword">for</span> X <span class="hljs-keyword">in</span> test_set:<br>        X = X.to(device)<br>        pred = model(X)<br>        preds.append(pred.cpu())<br></code></pre></td></tr></table></figure><h3 id="TensorBoard-可视化"><a href="#TensorBoard-可视化" class="headerlink" title="TensorBoard 可视化"></a>TensorBoard 可视化</h3><p>TensorBoard 用于可视化网络的相关信息，包括训练过程的指标、输出输出、网络内部参数等。使用前需要安装 <code>pip install tensorboard</code>，并在代码中导入 Writer。Writer 会将我们关心的数据保存在一个文件夹中，在<strong>显示到浏览器</strong>上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br>writer = SummaryWriter(<span class="hljs-string">&#x27;./path/to/log&#x27;</span>) 	<span class="hljs-comment"># 实例化 writer</span><br>writer.close()								<span class="hljs-comment"># 关闭 writer，代码结束时用</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	tag/main_tag: 可视化时图像的标签名，是图的唯一标识</span><br><span class="hljs-string">	scalar_value: 代码中的变量名，必须是一个标量</span><br><span class="hljs-string">	tag_scalar_dict: 传入变量字典，key 相当于可视化的 tag，value 是代码中的变量名</span><br><span class="hljs-string">	global_step: 可视化时的横坐标，传入 epoch 就表示随 epoch 的指标变化</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>writer.add_scalar(tag, scalar_value, global_step=<span class="hljs-literal">None</span>, walltime=<span class="hljs-literal">None</span>)<br>writer.add_scalars(main_tag, tag_scalar_dict, global_step=<span class="hljs-literal">None</span>, walltime=<span class="hljs-literal">None</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	img_tensor: 代码中的变量名，可以传入张量，如输入数据、各层的输入输出</span><br><span class="hljs-string">	dataformats: 数据格式，如 CHW，HWC，HW</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>writer.add_image(tag, img_tensor, global_step=<span class="hljs-literal">None</span>, walltime=<span class="hljs-literal">None</span>, dataformats=<span class="hljs-string">&#x27;CHW&#x27;</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">	model: 传入模型，必须是 nn.Module 类</span><br><span class="hljs-string">	input_to_model: 输入给模型的数据</span><br><span class="hljs-string">	verbose: 是否打印计算图结构信息</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>writer.add_graph(model, input_to_model=<span class="hljs-literal">None</span>, verbose=<span class="hljs-literal">False</span>, use_strict_trace=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>运行上述代码后，数据将被保存到文件夹中，接下来在终端中输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tensorboard --logdir=./path/to/<span class="hljs-built_in">log</span> --port 6006<br></code></pre></td></tr></table></figure><p>就可以在 <code>localhost:6006</code> 看到可视化结果。如果是在远程服务器中，则需要将服务器的 6006 端口转发到本机的一个端口，在本机浏览器中访问端口。</p><h2 id="模型保存与加载"><a href="#模型保存与加载" class="headerlink" title="模型保存与加载"></a>模型保存与加载</h2><p>PyTorch 文件后缀为 <code>.pt</code>、<code>.pth</code> 和 <code>.pkl</code>，三者在使用上没有区别，都可以保存<strong>张量</strong>、<strong>模型</strong>（结构、权重参数），张量的保存和加载方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(t, <span class="hljs-string">&#x27;save_tensor.pt&#x27;</span>)		<span class="hljs-comment"># 保存张量包括其 device 属性</span><br>t = torch.load(<span class="hljs-string">&#x27;save_tensor.pt&#x27;</span>)	<span class="hljs-comment"># 是否加载到 GPU 由 save 之前的参数决定</span><br>t = torch.load(<span class="hljs-string">&#x27;save_tensor.pt&#x27;</span>, map_location=torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>))	<span class="hljs-comment"># 加载到 CPU</span><br>t = torch.load(<span class="hljs-string">&#x27;save_tensor.pt&#x27;</span>, map_location=<span class="hljs-string">&quot;cuda:0&quot;</span>)	<span class="hljs-comment"># 加载到指定 GPU</span><br></code></pre></td></tr></table></figure><p>如果有多个张量，可以通过字典的形式进行存储，将字符串映射到张量，这也是模型的保存形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor_dict = &#123;<span class="hljs-string">&#x27;x&#x27;</span>: x, <span class="hljs-string">&#x27;y&#x27;</span>: y&#125;<br>torch.save(tensor_dict, <span class="hljs-string">&#x27;save_dict.pt&#x27;</span>)<br>tensor_dict2 = torch.load(<span class="hljs-string">&#x27;save_dict.pt&#x27;</span>)<br></code></pre></td></tr></table></figure><p>模型（Model）分为<strong>网络的结构和权重参数</strong>两部分，后者位于 <code>model.stack_dict()</code> 中，以<strong>字典</strong>的形式存储，将每一层映射成它的参数张量，通过 <code>.keys()</code> 可以查看所有键。模型的保存和加载方法有两种：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 只保存模型的权重参数，不保存模型结构，速度更快</span><br>torch.save(model.state_dict(), <span class="hljs-string">&#x27;save_param.pt&#x27;</span>)<br>model = Model(*args, **kwargs)	<span class="hljs-comment"># 需要重新 init 模型，模型是否加载到 GPU 由参数决定</span><br>model.load_state_dict(torch.load(<span class="hljs-string">&#x27;save_param.pt&#x27;</span>, map_location=<span class="hljs-string">&quot;cuda:0&quot;</span>))	<span class="hljs-comment"># 参数加载</span><br>model = model.to(device)<br>model.<span class="hljs-built_in">eval</span>()<br><br><span class="hljs-comment"># 保存整个模型，包括网络结构和权重参数</span><br>torch.save(model, <span class="hljs-string">&#x27;save_model.pt&#x27;</span>)<br>model = torch.load(<span class="hljs-string">&#x27;save_model.pt&#x27;</span>)	<span class="hljs-comment"># 模型是否加载到 GPU 由 save 之前的参数决定</span><br>model = model.to(device)<br>model.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></table></figure><p>注意第二种方法中，<strong>网络模型定义的代码</strong>要与 load 代码写在一起，或者从其他文件中 <code>import</code>，这样才能成功加载。</p><h3 id="Checkpoint"><a href="#Checkpoint" class="headerlink" title="Checkpoint"></a>Checkpoint</h3><p>在适当的时候保存<strong>模型的断点</strong>，可以在训练意外终止后<strong>不必从头开始训练</strong>，也方便必要的时候<strong>回滚</strong>到历史状态。除了模型的参数，还包括 Loss、Epoch 等训练信息。最容易忽略的是<strong>优化器的参数</strong>，也是使用 <code>state_dict()</code> 访问，包括：学习率、动量值、衰减系数等参数。一般使用 tar 文件格式来保存这些检查点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % checkpoint_interval == <span class="hljs-number">0</span>:<br>    checkpoint = &#123;<span class="hljs-string">&#x27;epoch&#x27;</span>: epoch+<span class="hljs-number">1</span>,<br>                  <span class="hljs-string">&#x27;loss&#x27;</span>: loss,<br>                  <span class="hljs-string">&#x27;best_loss&#x27;</span>: best_loss,<br>                  <span class="hljs-string">&#x27;model_state_dict&#x27;</span>: net.state_dict(),<br>                  <span class="hljs-string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),<br>                 &#125;<br>    path_checkpoint = <span class="hljs-string">&quot;./history/ckp_&#123;&#125;_epoch.pth.tar&quot;</span>.<span class="hljs-built_in">format</span>(epoch)<br>    torch.save(checkpoint, path_checkpoint)<br><br>model = TheModelClass(*args, **kwargs)<br>optimizer = TheOptimizerClass(*args, **kwargs)<br><br>checkpoint = torch.load(path_checkpoint)<br>model.load_state_dict(checkpoint[<span class="hljs-string">&#x27;model_state_dict&#x27;</span>])<br>optimizer.load_state_dict(checkpoint[<span class="hljs-string">&#x27;optimizer_state_dict&#x27;</span>])<br>epoch = checkpoint[<span class="hljs-string">&#x27;epoch&#x27;</span>]<br>loss = checkpoint[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>best_loss = checkpoint[<span class="hljs-string">&#x27;best_loss&#x27;</span>]<br><br>model.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></table></figure><h3 id="切换设备"><a href="#切换设备" class="headerlink" title="切换设备"></a>切换设备</h3><p>load 提供<strong>设备重载</strong>的功能，可以在 CPU、GPU 之间切换，也可以在不同 GPU 间切换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 强制所有 GPU 张量加载到 CPU 中</span><br>torch.load(<span class="hljs-string">&#x27;tensors.pt&#x27;</span>, map_location=<span class="hljs-keyword">lambda</span> storage, loc: storage)<br>torch.load(<span class="hljs-string">&#x27;tensors.pt&#x27;</span>, map_location=torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>)) <br><span class="hljs-comment"># 把所有的张量加载到 GPU 1 中</span><br>torch.load(<span class="hljs-string">&#x27;tensors.pt&#x27;</span>, map_location=<span class="hljs-keyword">lambda</span> storage, loc: storage.cuda(<span class="hljs-number">1</span>))<br>torch.load(<span class="hljs-string">&#x27;tensors.pt&#x27;</span>, map_location=<span class="hljs-string">&#x27;cuda:1&#x27;</span>)<br><span class="hljs-comment"># 把张量从GPU 1 移动到 GPU 0</span><br>torch.load(<span class="hljs-string">&#x27;tensors.pt&#x27;</span>, map_location=&#123;<span class="hljs-string">&#x27;cuda:1&#x27;</span>:<span class="hljs-string">&#x27;cuda:0&#x27;</span>&#125;)<br></code></pre></td></tr></table></figure><p>上述代码只有模型在一个 GPU 上训练时才生效，如果在<strong>多个 GPU</strong> 中，则需要使用 <code>nn.DataParallel</code> 模块，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = nn.DataParalle(model) <span class="hljs-comment"># 自动将任务发送到所有 GPU 上执行</span><br>torch.save(model.module.state_dict(), <span class="hljs-string">&#x27;model.pt&#x27;</span>)<br></code></pre></td></tr></table></figure><p>注意到此时模型的参数名都带上了 <code>module</code> 前缀，如果再想把模型加载到<strong>单个</strong> GPU 或 CPU 上，则需要手动将 key 中的<strong>前缀去除</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 原始通过 DataParallel 保存的文件</span><br>state_dict = torch.load(<span class="hljs-string">&#x27;model.pt&#x27;</span>)<br><br><span class="hljs-comment"># 创建一个不包含 module. 的新 OrderedDict</span><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br>new_state_dict = OrderedDict()<br><span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> state_dict.items():<br>    name = key[<span class="hljs-number">7</span>:] <span class="hljs-comment"># 去掉 module.</span><br>    new_state_dict[name] = value<br><br><span class="hljs-comment"># 加载参数</span><br>model.load_state_dict(new_state_dict)<br></code></pre></td></tr></table></figure><h3 id="加载部分模型"><a href="#加载部分模型" class="headerlink" title="加载部分模型"></a>加载部分模型</h3><p>在迁移学习或者训练新的复杂模型时，<strong>加载部分模型</strong>是很常见的。利用经过训练的参数，即使只有少数参数可用，也将有助于<strong>预热</strong>训练过程，并且使模型更快收敛。</p><p>在加载部分模型参数进行预训练的时候，很可能会碰到<strong>键不匹配</strong>的情况（模型权重都是按键值对的形式保存并加载回来的）。因此，无论是缺少键还是多出键的情况，都可以通过在 <code>load_state_dict()</code> 函数中设定 <code>strict</code> 参数为 <code>False</code> 来<strong>忽略不匹配的键</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(modelA.state_dict(), PATH)<br>modelB = TheModelBClass(*args, **kwargs)<br>modelB.load_state_dict(torch.load(PATH), strict=<span class="hljs-literal">False</span>) <span class="hljs-comment"># 忽略不匹配的键</span><br></code></pre></td></tr></table></figure><p>如果想将某一层的参数加载到其他层，但是有些键不匹配，可以通过<strong>强制修改</strong> <code>state_dict</code> 中参数的 <code>key</code> 解决。</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%8A%80%E6%9C%AF%E7%BB%8F%E9%AA%8C/" class="category-chain-item">技术经验</a> <span>></span> <a href="/categories/%E6%8A%80%E6%9C%AF%E7%BB%8F%E9%AA%8C/PyTorch/" class="category-chain-item">PyTorch</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Python/">#Python</a> <a href="/tags/PyTorch/">#PyTorch</a> <a href="/tags/DL/">#DL</a></div></div><div class="license-box my-3"><div class="license-title"><div>PyTorch笔记 #2 神经网络</div><div>https://hwcoder.top/PyTorch-Note-2</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>He Wei</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2022年12月15日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i> </span></a><a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - 非商业性使用"><i class="iconfont icon-nc"></i> </span></a><a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - 相同方式共享"><i class="iconfont icon-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/Zotero-Note-1" title="Zotero使用技巧：坚果云同步、常用插件"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Zotero使用技巧：坚果云同步、常用插件</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/PyTorch-Note-1" title="PyTorch笔记 #1 基础操作"><span class="hidden-mobile">PyTorch笔记 #1 基础操作</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.4.16/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"jdbBr3BddTiqSCPnXw6sXFv7-gzGzoHsz","appKey":"2bjgwDr2opjVCwhgjDMpk53c","path":"window.location.pathname","placeholder":"说点什么吧( •̀ ω •́ )✧","avatar":"retro","meta":["nick","mail","link"],"requiredFields":["nick"],"pageSize":10,"lang":"zh-CN","highlight":true,"recordIP":true,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,i=t.getElementById("subtitle");i&&e&&e(i.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init({tocSelector:"#toc-body",contentSelector:".markdown-body",headingSelector:CONFIG.toc.headingSelector||"h1,h2,h3,h4,h5,h6",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",collapseDepth:CONFIG.toc.collapseDepth||0,scrollSmooth:!0,headingsOffset:-t}),0<o.find(".toc-list-item").length&&o.css("visibility","visible"))})</script><script>!function(){var i,t=CONFIG.code_language.enable&&CONFIG.code_language.default,c=CONFIG.copy_btn;(t||c)&&(i="",i+='<div class="code-widget">',i+="LANG",i+="</div>",jQuery(".markdown-body pre").each(function(){var e,a,n=jQuery(this);0<n.find("code.mermaid").length||0<n.find("span.line").length||(e="",t&&(e=CONFIG.code_language.default,0<n[0].children.length&&2<=n[0].children[0].classList.length&&n.children().hasClass("hljs")?e=n[0].children[0].classList[1]:n[0].getAttribute("data-language")?e=n[0].getAttribute("data-language"):n.parent().hasClass("sourceCode")&&0<n[0].children.length&&2<=n[0].children[0].classList.length?(e=n[0].children[0].classList[1],n.parent().addClass("code-wrapper")):n.parent().hasClass("markdown-body")&&0===n[0].classList.length&&n.wrap('<div class="code-wrapper"></div>'),e=e.toUpperCase().replace("NONE",CONFIG.code_language.default)),n.append(i.replace("LANG",e).replace('code-widget">',(a=n[0],(0<=Fluid.utils.getBackgroundLightness(a)?"code-widget-light":"code-widget-dark")+(c?' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>':' code-widget">')))),c&&Fluid.utils.createScript("https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js",function(){new window.ClipboardJS(".copy-btn",{target:function(e){for(var a=e.parentNode.childNodes,n=0;n<a.length;n++)if("CODE"===a[n].tagName)return a[n]}}).on("success",function(e){e.clearSelection(),e.trigger.innerHTML=e.trigger.innerHTML.replace("icon-copy","icon-success"),setTimeout(function(){e.trigger.innerHTML=e.trigger.innerHTML.replace("icon-success","icon-copy")},2e3)})}))}))}()</script><script>Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>MathJax = {
          tex    : {
            inlineMath: { '[+]': [['$', '$']] }
          },
          loader : {
            load: ['ui/lazy']
          },
          options: {
            renderActions: {
              findScript    : [10, doc => {
                document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                  const display = !!node.type.match(/; *mode=display/);
                  const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                  const text = document.createTextNode('');
                  node.parentNode.replaceChild(text, node);
                  math.start = { node: text, delim: '', n: 0 };
                  math.end = { node: text, delim: '', n: 0 };
                  doc.math.push(math);
                });
              }, '', false],
              insertedScript: [200, () => {
                document.querySelectorAll('mjx-container').forEach(node => {
                  let target = node.parentNode;
                  if (target.nodeName.toLowerCase() === 'li') {
                    target.parentNode.classList.add('has-jax');
                  }
                });
              }, '', false]
            }
          }
        };</script><script src="https://lib.baomitu.com/mathjax/3.2.0/es5/tex-svg-full.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>