<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#01213a"><meta name="author" content="Wei He"><meta name="keywords" content="Computer Science and Technology, Machine Learning, Deep Learning, Computer Vision, Natural Language Processing"><meta name="description" content="准备面试过程中的笔记，本系列将记录了一系列经典算法的（伪）代码实现。本文介绍了深度学习中的各种经典函数，包括损失函数、激活函数、指标计算函数。"><meta property="og:type" content="article"><meta property="og:title" content="手撕经典算法 #4 经典函数篇"><meta property="og:url" content="https://hwcoder.top/Manual-Coding-4"><meta property="og:site_name" content="Hwcoder - Life Oriented Programming"><meta property="og:description" content="准备面试过程中的笔记，本系列将记录了一系列经典算法的（伪）代码实现。本文介绍了深度学习中的各种经典函数，包括损失函数、激活函数、指标计算函数。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hwcoder.top/img/home/Manual-coding.jpg"><meta property="article:published_time" content="2024-07-10T01:24:52.000Z"><meta property="article:modified_time" content="2025-04-11T13:07:39.590Z"><meta property="article:author" content="Wei He"><meta property="article:tag" content="NLP"><meta property="article:tag" content="Algorithm"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://hwcoder.top/img/home/Manual-coding.jpg"><title>手撕经典算法 #4 经典函数篇 | Hwcoder - Life Oriented Programming</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/mac.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"hwcoder.top",root:"/",version:"1.9.5",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h2, h3",placement:"left",visible:"always",icon:"§"},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:null,google:{measurement_id:null},tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"XGNf4GEnaFLiUijMUvz8HSBF-gzGzoHsz",app_key:"JPeVLcug6EcWdBTpGrDJSdKi",server_url:"https://xgnf4gen.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!0}},search_path:"/local-search.xml",include_content_in_search:!0};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>Fluid.ctx.dnt||Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=",function(){function a(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],a("js",new Date),a("config","")})</script><meta name="generator" content="Hexo 5.4.2"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Hwcoder</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/bg/other.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="手撕经典算法 #4 经典函数篇"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2024-07-10 09:24" pubdate>2024年7月10日 上午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 10k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 58 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="算法笔记" id="heading-6caacb5aaee15525d7a2b4768aa79852" role="tab" data-toggle="collapse" href="#collapse-6caacb5aaee15525d7a2b4768aa79852" aria-expanded="true">算法笔记 <span class="list-group-count">(22)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-6caacb5aaee15525d7a2b4768aa79852" role="tabpanel" aria-labelledby="heading-6caacb5aaee15525d7a2b4768aa79852"><div class="category-post-list"></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="力扣刷题" id="heading-9ca088cdf0fdc34dd48290c9d13c88b1" role="tab" data-toggle="collapse" href="#collapse-9ca088cdf0fdc34dd48290c9d13c88b1" aria-expanded="false">力扣刷题 <span class="list-group-count">(15)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-9ca088cdf0fdc34dd48290c9d13c88b1" role="tabpanel" aria-labelledby="heading-9ca088cdf0fdc34dd48290c9d13c88b1"><div class="category-post-list"><a href="/LeetCode-Array" title="力扣刷题笔记 #01 数组" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #01 数组</span> </a><a href="/LeetCode-Bitwise" title="力扣刷题笔记 #02 数位&amp;二进制" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #02 数位&amp;二进制</span> </a><a href="/LeetCode-Data-Structure" title="力扣刷题笔记 #03 数据结构" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #03 数据结构</span> </a><a href="/LeetCode-Divide-Conquer" title="力扣刷题笔记 #04 二分&amp;分治" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #04 二分&amp;分治</span> </a><a href="/LeetCode-DP-1" title="力扣刷题笔记 #05-1 一维动态规划" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #05-1 一维动态规划</span> </a><a href="/LeetCode-DP-2" title="力扣刷题笔记 #05-2 二维动态规划" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #05-2 二维动态规划</span> </a><a href="/LeetCode-DP-3" title="力扣刷题笔记 #05-3 复杂动态规划" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #05-3 复杂动态规划</span> </a><a href="/LeetCode-Graph" title="力扣刷题笔记 #06 图论" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #06 图论</span> </a><a href="/LeetCode-Greedy" title="力扣刷题笔记 #07 贪心算法" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #07 贪心算法</span> </a><a href="/LeetCode-LinkList" title="力扣刷题笔记 #08 链表" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #08 链表</span> </a><a href="/LeetCode-Math" title="力扣刷题笔记 #09 数学" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #09 数学</span> </a><a href="/LeetCode-Search" title="力扣刷题笔记 #10 搜索&amp;剪枝" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #10 搜索&amp;剪枝</span> </a><a href="/LeetCode-Stack-Queue" title="力扣刷题笔记 #11 栈&amp;队列" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #11 栈&amp;队列</span> </a><a href="/LeetCode-String" title="力扣刷题笔记 #12 字符串" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #12 字符串</span> </a><a href="/LeetCode-Tree" title="力扣刷题笔记 #13 树" class="list-group-item list-group-item-action"><span class="category-post">力扣刷题笔记 #13 树</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem list-group-item category-item-action col-10 col-md-11 col-xm-11" title="手撕经典算法" id="heading-67ebbc1a1af276ea349725e619870fea" role="tab" data-toggle="collapse" href="#collapse-67ebbc1a1af276ea349725e619870fea" aria-expanded="true">手撕经典算法 <span class="list-group-count">(5)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-67ebbc1a1af276ea349725e619870fea" role="tabpanel" aria-labelledby="heading-67ebbc1a1af276ea349725e619870fea"><div class="category-post-list"><a href="/Manual-Coding-1" title="手撕经典算法 #1 Attention篇" class="list-group-item list-group-item-action"><span class="category-post">手撕经典算法 #1 Attention篇</span> </a><a href="/Manual-Coding-2" title="手撕经典算法 #2 神经网络篇" class="list-group-item list-group-item-action"><span class="category-post">手撕经典算法 #2 神经网络篇</span> </a><a href="/Manual-Coding-3" title="手撕经典算法 #3 Transformer篇" class="list-group-item list-group-item-action"><span class="category-post">手撕经典算法 #3 Transformer篇</span> </a><a href="/Manual-Coding-4" title="手撕经典算法 #4 经典函数篇" class="list-group-item list-group-item-action active"><span class="category-post">手撕经典算法 #4 经典函数篇</span> </a><a href="/Manual-Coding-5" title="手撕经典算法 #5 RLHF 篇" class="list-group-item list-group-item-action"><span class="category-post">手撕经典算法 #5 RLHF 篇</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="算法入门" id="heading-cbb23c60a3cdd7188054b6ba4efbf423" role="tab" data-toggle="collapse" href="#collapse-cbb23c60a3cdd7188054b6ba4efbf423" aria-expanded="false">算法入门 <span class="list-group-count">(2)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-cbb23c60a3cdd7188054b6ba4efbf423" role="tabpanel" aria-labelledby="heading-cbb23c60a3cdd7188054b6ba4efbf423"><div class="category-post-list"><a href="/Algo-Note-1" title="算法入门笔记 #1 杂记" class="list-group-item list-group-item-action"><span class="category-post">算法入门笔记 #1 杂记</span> </a><a href="/Algo-Note-2" title="算法入门笔记 #2 STL标准库" class="list-group-item list-group-item-action"><span class="category-post">算法入门笔记 #2 STL标准库</span></a></div></div></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">手撕经典算法 #4 经典函数篇</h1><p class="note note-info">本文最后更新于：2025年4月11日 晚上</p><div class="markdown-body"><p>本文对深度学习中经典的函数进行了简单的实现和注释。包括：</p><ul><li>损失函数（MSE、CE、BCE、KL、Focal）</li><li>激活函数（Sigmoid、Tanh、ReLU、Leaky ReLU、ELU、Swish、GeLU、SwiGLU、Softmax）</li><li>指标计算（PPL、ROUGE、BLEU）</li></ul><h2 id="损失函数">损失函数</h2><h3 id="mse-loss">MSE Loss</h3><p>均方误差（Mean Squared Error，MSE）衡量预测值与真实值的平方差均值，是<strong>回归任务</strong>中最常用的损失函数：</p><p><span class="math display">\[ L = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2 \]</span></p><p>其中 <span class="math inline">\(y_i\)</span> 为真实值，<span class="math inline">\(\hat{y}_i\)</span> 为预测值。其梯度计算为 <span class="math inline">\(\frac{\partial L}{\partial \hat{y}_i} = \frac{2}{n}(\hat{y}_i - y_i)\)</span>，具有凸函数的良好优化特性，可导且处处平滑，适合梯度下降。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mse_loss</span>(<span class="hljs-params">y_true, y_pred</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算均方误差损失</span><br><span class="hljs-string">    :param y_true: 真实值数组，形状 (n_samples, )</span><br><span class="hljs-string">    :param y_pred: 预测值数组，形状 (n_samples, )</span><br><span class="hljs-string">    :return: 标量损失值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    squared_error = (y_true - y_pred) ** <span class="hljs-number">2</span><br>    <span class="hljs-keyword">return</span> np.mean(squared_error)<br><br><span class="hljs-comment"># 示例用法</span><br>y_true = np.array([<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>])<br>y_pred = np.array([<span class="hljs-number">1.5</span>, <span class="hljs-number">3.8</span>, <span class="hljs-number">4.9</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;MSE Loss: <span class="hljs-subst">&#123;mse_loss(y_true, y_pred):<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)  <span class="hljs-comment"># 输出 MSE Loss: 0.0867</span><br></code></pre></td></tr></table></figure><h3 id="ce-loss">CE Loss</h3><p>交叉熵（Cross Entropy）衡量两个概率分布间的差异，常用于<strong>多分类任务</strong>。给定真实分布 <span class="math inline">\(P\)</span> 和预测分布 <span class="math inline">\(Q\)</span>：</p><p><span class="math display">\[ H(P, Q) = -\sum_{i=1}^N P(x_i) \log Q(x_i) \]</span></p><p>在分类任务中，真实标签常采用 <strong>one-hot 编码</strong>，公式简化为：</p><p><span class="math display">\[ L = -\frac{1}{N}\sum_{i=1}^N \sum_{i=1}^C y_i \log \hat{y}_i \]</span></p><p>其中 <span class="math inline">\(C\)</span> 为类别总数，<span class="math inline">\(\hat{y}_i\)</span> 需经过 <strong>Softmax 归一化</strong>。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cross_entropy</span>(<span class="hljs-params">y_true, y_pred</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算交叉熵损失（需配合 Softmax 使用），带数值稳定处理</span><br><span class="hljs-string">    :param y_true: one-hot 编码的真实标签，形状 (n_samples, n_classes)</span><br><span class="hljs-string">    :param y_pred: 模型输出的 logits，形状 (n_samples, n_classes)</span><br><span class="hljs-string">    :return: 标量损失值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 数值稳定处理：减去最大值防止指数爆炸</span><br>    exps = np.exp(y_pred - np.<span class="hljs-built_in">max</span>(y_pred, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>))<br>    softmax_output = exps / np.<span class="hljs-built_in">sum</span>(exps, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br>    <br>    <span class="hljs-comment"># 避免 log(0) 导致数值问题</span><br>    epsilon = <span class="hljs-number">1e-7</span><br>    clipped = np.clip(softmax_output, epsilon, <span class="hljs-number">1</span> - epsilon)<br>    <br>    <span class="hljs-comment"># 只计算真实类别对应的损失</span><br>    n_samples = y_true.shape[<span class="hljs-number">0</span>]<br>    log_likelihood = -np.log(clipped[<span class="hljs-built_in">range</span>(n_samples), y_true.argmax(axis=<span class="hljs-number">1</span>)])<br>    <span class="hljs-keyword">return</span> np.mean(log_likelihood)<br><br><span class="hljs-comment"># 示例用法（三分类问题）</span><br>y_true = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]])  <span class="hljs-comment"># one-hot 编码</span><br>y_pred = np.array([[<span class="hljs-number">2.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.5</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">0.2</span>]])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;CrossEntropy Loss: <span class="hljs-subst">&#123;cross_entropy(y_true, y_pred):<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)  <span class="hljs-comment"># 输出 0.3184</span><br></code></pre></td></tr></table></figure><h3 id="bce-loss">BCE Loss</h3><p>二元交叉熵（Binary Cross Entropy）处理的是<strong>二分类问题</strong>，其归一化的方式从 Softmax 替换为 Sigmoid，并且<strong>每个类别的概率独立计算</strong>（不像交叉熵仅计算真实类别的损失）： <span class="math display">\[ L = -\frac{1}{N}\sum_{i=1}^N \left[ y_i \cdot \log(\sigma(x_i)) + (1-y_i) \cdot \log(1-\sigma(x_i)) \right] \]</span> 此外，BCE 也可以用于<strong>多分类多标签</strong>任务，此时需要将每个类别看作为 0 或 1 的二分类问题。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">binary_cross_entropy</span>(<span class="hljs-params">y_true, y_pred</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算二元交叉熵损失（需配合 Sigmoid 使用）</span><br><span class="hljs-string">    :param y_true: 二分类的真实标签（0 或 1），形状 (n_samples, n_classes)</span><br><span class="hljs-string">    :param y_pred: 模型输出的 logits，形状 (n_samples, n_classes)</span><br><span class="hljs-string">    :return: 标量损失值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 应用 Sigmoid 将 logits 转换为概率</span><br>    sigmoid_output = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pred))<br>    <br>    <span class="hljs-comment"># 避免 log(0) 导致的数值问题</span><br>    epsilon = <span class="hljs-number">1e-7</span><br>    clipped = np.clip(sigmoid_output, epsilon, <span class="hljs-number">1</span> - epsilon)<br>    <br>    <span class="hljs-comment"># 计算每个样本每个类别的损失</span><br>    loss_per_element = - (y_true * np.log(clipped) + (<span class="hljs-number">1</span> - y_true) * np.log(<span class="hljs-number">1</span> - clipped))<br>    <br>    <span class="hljs-comment"># 对所有元素取平均损失</span><br>    <span class="hljs-keyword">return</span> np.mean(loss_per_element)<br><br><span class="hljs-comment"># 示例用法（两个样本，三分类多标签问题）</span><br>y_true = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])  <span class="hljs-comment"># 多标签真实值</span><br>y_pred = np.array([[<span class="hljs-number">2.0</span>, <span class="hljs-number">1.0</span>, -<span class="hljs-number">1.0</span>], [<span class="hljs-number">0.5</span>, <span class="hljs-number">3.0</span>, -<span class="hljs-number">0.5</span>]])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;BCE Loss: <span class="hljs-subst">&#123;binary_cross_entropy(y_true, y_pred):<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)  <span class="hljs-comment"># 输出 0.1955</span><br></code></pre></td></tr></table></figure><h3 id="kl-散度">KL 散度</h3><p>KL 散度（Kullback-Leibler Divergence）衡量两个概率分布 <span class="math inline">\(P\)</span> 和 <span class="math inline">\(Q\)</span> 的差异程度：</p><p><span class="math display">\[ D_{KL}(P \parallel Q) = \sum_{i=1}^N P(x_i) \log \frac{P(x_i)}{Q(x_i)} \]</span></p><p>其性质包括：</p><ul><li>非对称性：<span class="math inline">\(D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)\)</span></li><li>非负性：<span class="math inline">\(D_{KL} \geq 0\)</span>，当且仅当 P=Q 时等于 0</li><li>与交叉熵的关系：<span class="math inline">\(D_{KL}(P \parallel Q) = H(P, Q) - H(P)\)</span></li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">kl_divergence</span>(<span class="hljs-params">p, q</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算两个离散分布的KL散度</span><br><span class="hljs-string">    :param p: 真实概率分布，形状 (n_classes, )</span><br><span class="hljs-string">    :param q: 预测概率分布，形状 (n_classes, )</span><br><span class="hljs-string">    :return: 标量散度值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 过滤零元素避免数值问题</span><br>    mask = (p != <span class="hljs-number">0</span>)<br>    p = p[mask]<br>    q = q[mask]<br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(p * np.log(p / q))<br><br><span class="hljs-comment"># 示例用法（概率分布差异对比）</span><br>P = np.array([<span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>])<br>Q1 = np.array([<span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>])<br>Q2 = np.array([<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;KL(P||Q1): <span class="hljs-subst">&#123;kl_divergence(P, Q1):<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)  <span class="hljs-comment"># 输出 0.0000</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;KL(P||Q2): <span class="hljs-subst">&#123;kl_divergence(P, Q2):<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)  <span class="hljs-comment"># 输出 0.0204</span><br></code></pre></td></tr></table></figure><h3 id="focal-损失">Focal 损失</h3><p>通过调节难易样本权重解决数据倾斜问题，适用于<strong>长尾分布场景的二分类问题</strong>：</p><p><span class="math display">\[ FL = -\alpha_t (1-p_t)^\gamma \log(p_t) \]</span></p><p>其中：</p><ul><li><p><span class="math inline">\(p_t\)</span> 是模型对真实类别的预测概率： <span class="math display">\[ p_t = \begin{cases} p &amp; \text{正样本} \\ 1-p &amp; \text{负样本} \end{cases} \]</span></p></li><li><p><span class="math inline">\(\alpha \in [0,1]\)</span>：类别平衡因子，通常为稀有类别分配更高权重（如 <span class="math inline">\(\alpha=0.25\)</span>）。</p></li><li><p><span class="math inline">\(\gamma \geq 0\)</span>： 困难样本聚焦参数，调整难易样本的权重比例（通常 $ $ ）。</p></li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">focal_loss</span>(<span class="hljs-params">y_true, y_pred, alpha=<span class="hljs-number">0.25</span>, gamma=<span class="hljs-number">2.0</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算二分类Focal Loss</span><br><span class="hljs-string">    :param y_true: 真实标签 (n_samples, )</span><br><span class="hljs-string">    :param y_pred: 预测概率 (n_samples, )</span><br><span class="hljs-string">    :param alpha: 类别平衡因子</span><br><span class="hljs-string">    :param gamma: 困难样本聚焦参数</span><br><span class="hljs-string">    :return: 标量损失值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    epsilon = <span class="hljs-number">1e-7</span><br>    y_pred = np.clip(y_pred, epsilon, <span class="hljs-number">1</span> - epsilon)<br>    p_t = y_true * y_pred + (<span class="hljs-number">1</span> - y_true) * (<span class="hljs-number">1</span> - y_pred)<br>    alpha_factor = y_true * alpha + (<span class="hljs-number">1</span> - y_true) * (<span class="hljs-number">1</span> - alpha)<br>    loss = -alpha_factor * (<span class="hljs-number">1</span> - p_t) ** gamma * np.log(p_t)<br>    <span class="hljs-keyword">return</span> np.mean(loss)<br><br><span class="hljs-comment"># 示例用法（处理文本分类中的长尾分布）</span><br>y_true = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>])  <span class="hljs-comment"># 多数类别为1</span><br>y_pred = np.array([<span class="hljs-number">0.9</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.1</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Focal Loss: <span class="hljs-subst">&#123;focal_loss(y_true, y_pred):<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)  <span class="hljs-comment"># 输出约0.032</span><br></code></pre></td></tr></table></figure><h2 id="激活函数">激活函数</h2><h3 id="sigmoid">Sigmoid</h3><p>输出区间 (0,1)，符合概率分布特性，常用于二分类输出层。表达式为： <span class="math display">\[ \sigma(x) = \frac{1}{1+e^{-x}} \]</span> 导数： <span class="math display">\[ \sigma&#39;(x) = \sigma(x)(1-\sigma(x)) \]</span> 缺点：</p><ul><li>输入较大或较小时候梯度接近于 0，容易导致<strong>梯度消失</strong>（且导数最大值为 0.25，更新效率不高）；</li><li>函数输出<strong>不是以 0 为中心的</strong>，梯度可能就会向特定方向移动，从而降低权重更新的效率；</li><li>执行指数运算，计算机运行得较慢，比较消耗计算资源。</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">x</span>):</span><br>    x = np.clip(x, -<span class="hljs-number">50</span>, <span class="hljs-number">50</span>)  <span class="hljs-comment"># 防止数值溢出</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br></code></pre></td></tr></table></figure><h3 id="tanh">Tanh</h3><p>输出区间 <span class="math inline">\((-1,1)\)</span>，以零为重心，缓解梯度偏移问题，表达式为： <span class="math display">\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</span> 导数： <span class="math display">\[ \tanh&#39;(x) = 1 - \tanh^2(x) \]</span> 缺点：</p><ul><li>仍然存在<strong>梯度饱和</strong>的问题：但 <span class="math inline">\(x\)</span> 进入饱和区（saturation region）时，其<strong>导数值趋近于零</strong>，最终无法有效更新网络参数；</li><li>依然进行的是指数运算，比较消耗计算资源。</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tanh</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> np.tanh(x)  <span class="hljs-comment"># 内置优化实现比手动计算更稳定</span><br></code></pre></td></tr></table></figure><h3 id="relu">ReLU</h3><p>表达式为： <span class="math display">\[ \text{ReLU}(x) = \max(0, x) \]</span> 导数： <span class="math display">\[ \text{ReLU}&#39;(x) = \begin{cases} 1 &amp; x &gt; 0 \\ 0 &amp; x \leq 0 \end{cases} \]</span> 优点：</p><ul><li>ReLU 解决了<strong>梯度消失</strong>的问题，当输入值为正时，神经元不会饱和（梯度始终为 1）；</li><li>由于 ReLU 线性、非饱和的性质，在 SGD 中能够快速收敛；</li><li>计算复杂度低，不需要进行指数运算。</li></ul><p>缺点：</p><ul><li>Dead ReLU 问题：负区间梯度归零，不再对任何数据有所响应，导致相应参数永远不会被更新；</li><li>与 Sigmoid 一样，其输出不是以 0 为中心的。</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">relu</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> np.maximum(<span class="hljs-number">0</span>, x)  <span class="hljs-comment"># 简洁的向量化实现</span><br></code></pre></td></tr></table></figure><h3 id="leaky-relu">Leaky ReLU</h3><p>缓解神经元死亡问题（负区间保留小梯度 <span class="math inline">\(\alpha\)</span>，常见取值为 <span class="math inline">\(0.01-0.3\)</span>），表达式为： <span class="math display">\[ \text{LeakyReLU}(x) = \begin{cases} x &amp; x &gt; 0 \\ \alpha x &amp; x \leq 0 \end{cases} \quad (\alpha \in (0,1)) \]</span></p><p>导数： <span class="math display">\[ \text{LeakyReLU}&#39;(x) = \begin{cases} 1 &amp; x &gt; 0 \\ \alpha &amp; x \leq 0 \end{cases} \]</span> 代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">leaky_relu</span>(<span class="hljs-params">x, alpha=<span class="hljs-number">0.01</span></span>):</span><br>    <span class="hljs-keyword">return</span> np.where(x &gt; <span class="hljs-number">0</span>, x, alpha * x)  <span class="hljs-comment"># 条件表达式实现分支逻辑</span><br></code></pre></td></tr></table></figure><h3 id="elu">ELU</h3><p>ELU 采用比 ReLU 更平滑的过渡，保持负区间微小梯度（<span class="math inline">\(\alpha\)</span> 控制信息保留程度）解决神经元死亡问题。最重要的是，可以控制<strong>激活函数的输出均值接近于零</strong>（假设输入分布为标准正态输入），使正常梯度更接近于单位自然梯度，从而加快学习速度。表达式为： <span class="math display">\[ \text{ELU}(x) = \begin{cases} x &amp; x &gt; 0 \\ \alpha(e^x - 1) &amp; x \leq 0 \end{cases} \]</span></p><p>导数： <span class="math display">\[ \text{ELU}&#39;(x) = \begin{cases} 1 &amp; x &gt; 0 \\ \text{ELU}(x) + \alpha &amp; x \leq 0 \end{cases} \]</span> 缺点：</p><ul><li>ELU 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息；</li><li>计算的时需要计算指数，计算效率低。</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">elu</span>(<span class="hljs-params">x, alpha=<span class="hljs-number">1.0</span></span>):</span><br>    <span class="hljs-keyword">return</span> np.where(x &gt; <span class="hljs-number">0</span>, x, alpha * (np.exp(x) - <span class="hljs-number">1</span>))  <span class="hljs-comment"># 负区间指数计算</span><br></code></pre></td></tr></table></figure><h3 id="swish">Swish</h3><p>Google Brain (2017) 提出的自门控的智能激活，结合了<strong>线性与非线性</strong>特性的激活函数： <span class="math display">\[ \text{Swish}(x) = x \cdot \sigma(\beta x) \]</span> 导数： <span class="math display">\[ \text{Swish}&#39;(x) = \text{Swish}(x) + \sigma(\beta x)(1 - \text{Swish}(x)) \]</span> 优点： - 自适应门控机制（通过 sigmoid 调整 <span class="math inline">\(\beta\)</span>，Swish 可以模拟不同形状）； - 处处平滑可微，在全体实数域上连续可导（优于 ReLU 系列，<span class="math inline">\(x=0\)</span> 处不可导）； - 在深层网络中表现优于 ReLU（Google 实验证明）。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">swish</span>(<span class="hljs-params">x, beta=<span class="hljs-number">1.0</span></span>):</span><br>    <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-beta * x)))<br></code></pre></td></tr></table></figure><h3 id="gelu">GeLU</h3><p>高斯误差线性单元（Gaussian Error Linear Unit）是一种结合了高斯分布特性的激活函数，旨在通过<strong>概率建模</strong>的方式平滑地调整神经元的激活状态，<strong>被 BERT、GPT 采用</strong>。其数学表达式为：</p><p><span class="math display">\[ \text{GeLU}(x) = x \cdot \Phi(x) \]</span></p><p>其中，<span class="math inline">\(\Phi(x)\)</span> 是标准高斯分布的累积分布函数（CDF）。为了高效计算，常采用近似公式： <span class="math display">\[ \text{GeLU}(x) \approx 0.5x\left(1 + \tanh\left(\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)\right)\right) \]</span></p><p>优点：</p><ul><li>跟 Swish 长得非常像，都是平滑版的 ReLU（保留非线性同时可微分）；</li><li>通过概率权重调整激活强度，避免 ReLU 的神经元死亡现象（负值完全被抑制）。</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gelu</span>(<span class="hljs-params">x</span>):</span><br>	<span class="hljs-comment"># 使用近似公式</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * x * (<span class="hljs-number">1</span> + np.tanh(np.sqrt(<span class="hljs-number">2</span>/np.pi) * (x + <span class="hljs-number">0.044715</span> * x**<span class="hljs-number">3</span>)))<br></code></pre></td></tr></table></figure><h3 id="swiglu">SwiGLU</h3><p>SwiGLU 是一种结合了 Swish 激活函数和<strong>门控线性单元</strong>（GLU）的复合激活函数，近年来被广泛应用于大型语言模型（LLM）<strong>如 LLaMA、PaLM 等</strong>。</p><p>GLU 的原始形式为： <span class="math display">\[ \text{GLU}(x) = \sigma(W x + b) \otimes (Vx) \]</span> 其中 <span class="math inline">\(\otimes\)</span> 是逐元素乘法，<span class="math inline">\(\sigma\)</span> 是 Sigmoid 函数，用于门控信息流。而 SwiGLU 将 GLU 中的 Sigmoid 替换为 Swish： <span class="math display">\[ \text{SwiGLU}(x) = \text{Swish}(W x + b) \otimes (Vx) \]</span></p><p>在实际实现中，SwiGLU 通常被整合到前馈网络（FFN）中，传统的 FFN 可以记为： <span class="math display">\[ \text{FFN}_\text{ReLU} = (\text{ReLU}(W_1 x+b))W_2 + c \]</span> 而 SwiGLU 通过 <span class="math inline">\(W_1x\)</span> 完成升维操作的同时，还会用 <span class="math inline">\(Vx\)</span> 完成门控操作，最后再用 <span class="math inline">\(W_2\)</span> 降维： <span class="math display">\[ \text{FFN}_\text{SwiGLU} = (\text{Swish}(W_1 x + b) \otimes (Vx))W_2 + c \]</span> 优点：</p><ul><li>Swish 的连续梯度缓解了梯度消失问题，而门控机制进一步平衡了信息流，使深层网络训练更稳定；</li><li>实验表明其计算效率优于 GeLU，且下游任务表现更好。</li></ul><p>Llama 中的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LlamaMLP</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">        self,</span></span><br><span class="hljs-params"><span class="hljs-function">        hidden_size: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># 4096</span></span></span><br><span class="hljs-params"><span class="hljs-function">        intermediate_size: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># 11008</span></span></span><br><span class="hljs-params"><span class="hljs-function">    </span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=<span class="hljs-literal">True</span>)<br>        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=<span class="hljs-literal">False</span>)<br>        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-keyword">return</span> self.down_proj(self.gate_proj(x) * self.up_proj(x))<br></code></pre></td></tr></table></figure><h3 id="softmax">Softmax</h3><p>基本表达式为： <span class="math display">\[ \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}} \]</span> 在实际使用中，为了消除指数爆炸风险（原始值超过 20 时可能发生浮点溢出），通常会等价变形为： <span class="math display">\[ \text{Softmax}(x_i) = \frac{e^{x_i - \text{max}(x)}}{\sum_{j=1}^n e^{x_j - \text{max}(x)}} \]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">softmax</span>(<span class="hljs-params">x, eps=<span class="hljs-number">1e-8</span></span>):</span><br>    <span class="hljs-comment"># x 的形状为 (n_samples, n_classes)</span><br>    exps = np.exp(x - np.<span class="hljs-built_in">max</span>(x, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>))<br>    <span class="hljs-keyword">return</span> exps / np.<span class="hljs-built_in">sum</span>(exps, axis=-<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h2 id="指标计算">指标计算</h2><h3 id="ppl">PPL</h3><p>困惑度（Perplexity）是语言模型的核心评估指标，用于衡量模型对测试数据的预测能力。其本质是交叉熵的指数形式，可理解为模型在预测时面临的平均「选择困境」。 <span class="math display">\[ PPL = \exp\left(-\frac{1}{N}\sum_{i=1}^N \log p(w_i|w_{&lt;i})\right) \]</span></p><p>其中 <span class="math inline">\(N\)</span> 为测试集词数，<span class="math inline">\(p(w_i|w_{&lt;i})\)</span> 是模型预测当前词的概率。PPL 值越低，说明模型预测越准确。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_ppl</span>(<span class="hljs-params">log_probs</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算困惑度</span><br><span class="hljs-string">    :param log_probs: 模型输出的对数概率序列</span><br><span class="hljs-string">    :return: PPL值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> np.exp(-np.mean(log_probs))<br><br><span class="hljs-comment"># 示例：假设模型对5个词的预测对数概率分别为-0.2, -0.3, -0.1, -0.4, -0.2</span><br>log_probs = [-<span class="hljs-number">0.2</span>, -<span class="hljs-number">0.3</span>, -<span class="hljs-number">0.1</span>, -<span class="hljs-number">0.4</span>, -<span class="hljs-number">0.2</span>]<br><span class="hljs-built_in">print</span>(calculate_ppl(log_probs))  <span class="hljs-comment"># 输出：1.284</span><br></code></pre></td></tr></table></figure><h3 id="rouge指标">ROUGE指标</h3><p>自动摘要任务的黄金标准，主要变体：</p><ul><li><strong>ROUGE-N</strong>：基于 n-gram 重叠的召回率</li><li><strong>ROUGE-L</strong>：基于最长公共子序列（LCS）</li></ul><p><span class="math display">\[ ROUGE{\text-L} = \frac{(1+\beta^2)R_{\text{lcs}}P_{\text{lcs}}}{R_{\text{lcs}}+\beta^2 P_{\text{lcs}}} \]</span></p><p>其中 <span class="math inline">\(\beta\)</span> 控制召回率权重，通常设为 1.2。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rouge_l</span>(<span class="hljs-params">reference, candidate</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算 ROUGE-L 分数</span><br><span class="hljs-string">    :param reference: 参考摘要（词列表）</span><br><span class="hljs-string">    :param candidate: 生成摘要（词列表）</span><br><span class="hljs-string">    :return: F1分数</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># LCS 长度计算（动态规划实现）</span><br>    m, n = <span class="hljs-built_in">len</span>(reference), <span class="hljs-built_in">len</span>(candidate)<br>    dp = [[<span class="hljs-number">0</span>]*(n+<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m+<span class="hljs-number">1</span>)]<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, m+<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n+<span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">if</span> reference[i-<span class="hljs-number">1</span>] == candidate[j-<span class="hljs-number">1</span>]:<br>                dp[i][j] = dp[i-<span class="hljs-number">1</span>][j-<span class="hljs-number">1</span>] + <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                dp[i][j] = <span class="hljs-built_in">max</span>(dp[i-<span class="hljs-number">1</span>][j], dp[i][j-<span class="hljs-number">1</span>])<br>    lcs = dp[m][n]<br>    <br>    precision = lcs / <span class="hljs-built_in">len</span>(candidate)<br>    recall = lcs / <span class="hljs-built_in">len</span>(reference)<br>    beta = <span class="hljs-number">1.2</span><br>    <span class="hljs-keyword">return</span> ( (<span class="hljs-number">1</span> + beta**<span class="hljs-number">2</span>) * precision * recall ) / ( recall + beta**<span class="hljs-number">2</span> * precision )<br></code></pre></td></tr></table></figure><h3 id="bleu分数">BLEU分数</h3><p>机器翻译经典指标，基于修正 n-gram 精度和长度惩罚：</p><p><span class="math display">\[ BLEU = BP \cdot \exp\left(\sum_{n=1}^N w_n \log p_n\right) \]</span></p><p>其中 BP（Brevity Penalty）惩罚过短输出：</p><p><span class="math display">\[ BP = \begin{cases} 1 &amp; \text{if } c &gt; r \\ e^{(1-r/c)} &amp; \text{otherwise} \end{cases} \]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bleu</span>(<span class="hljs-params">references, candidate, weights=[<span class="hljs-number">0.25</span>]*<span class="hljs-number">4</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算 BLEU 分数</span><br><span class="hljs-string">    :param references: 多个参考译文列表</span><br><span class="hljs-string">    :param candidate: 候选译文（词列表）</span><br><span class="hljs-string">    :param weights: n-gram 权重（默认 4-gram 平均）</span><br><span class="hljs-string">    :return: BLEU 分数（0-1）</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 计算各n-gram精度</span><br>    p_n = []<br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(weights)+<span class="hljs-number">1</span>):<br>        candidate_grams = [<span class="hljs-built_in">tuple</span>(candidate[i:i+n]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(candidate)-n+<span class="hljs-number">1</span>)]<br>        refs_grams = [ [<span class="hljs-built_in">tuple</span>(ref[i:i+n]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(ref)-n+<span class="hljs-number">1</span>)] <span class="hljs-keyword">for</span> ref <span class="hljs-keyword">in</span> references]<br>        <br>        count_clip = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> gram <span class="hljs-keyword">in</span> <span class="hljs-built_in">set</span>(candidate_grams):<br>            max_ref_count = <span class="hljs-built_in">max</span>([ref.count(gram) <span class="hljs-keyword">for</span> ref <span class="hljs-keyword">in</span> refs_grams])<br>            count_clip += <span class="hljs-built_in">min</span>(candidate_grams.count(gram), max_ref_count)<br>            <br>        p_n.append(count_clip / <span class="hljs-built_in">len</span>(candidate_grams) <span class="hljs-keyword">if</span> candidate_grams <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-comment"># 计算长度惩罚</span><br>    c = <span class="hljs-built_in">len</span>(candidate)<br>    r = <span class="hljs-built_in">min</span>(<span class="hljs-built_in">len</span>(ref) <span class="hljs-keyword">for</span> ref <span class="hljs-keyword">in</span> references)<br>    bp = <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> c &gt; r <span class="hljs-keyword">else</span> math.exp(<span class="hljs-number">1</span> - r/c)<br>    <br>    <span class="hljs-comment"># 综合得分</span><br>    score = <span class="hljs-built_in">sum</span>(w * math.log(p) <span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> w, p <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(weights, p_n))<br>    <span class="hljs-keyword">return</span> bp * math.exp(score)<br></code></pre></td></tr></table></figure></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/" class="category-chain-item">算法笔记</a> <span>></span> <a href="/categories/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E6%89%8B%E6%92%95%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/" class="category-chain-item">手撕经典算法</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/NLP/" class="print-no-link">#NLP</a> <a href="/tags/Algorithm/" class="print-no-link">#Algorithm</a></div></div><div class="license-box my-3"><div class="license-title"><div>手撕经典算法 #4 经典函数篇</div><div>https://hwcoder.top/Manual-Coding-4</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Wei He</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2024年7月10日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - 非商业性使用"><i class="iconfont icon-nc"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - 相同方式共享"><i class="iconfont icon-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/OpenAI-o1-Series" title="OpenAI o1 系列模型背后的技术猜测"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">OpenAI o1 系列模型背后的技术猜测</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/Manual-Coding-3" title="手撕经典算法 #3 Transformer篇"><span class="hidden-mobile">手撕经典算法 #3 Transformer篇</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"jdbBr3BddTiqSCPnXw6sXFv7-gzGzoHsz","appKey":"2bjgwDr2opjVCwhgjDMpk53c","path":"window.location.pathname","placeholder":"说点什么吧( •̀ ω •́ )✧","avatar":"retro","meta":["nick","mail","link"],"requiredFields":["nick"],"pageSize":10,"lang":"zh-CN","highlight":true,"recordIP":true,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,i=t.getElementById("subtitle");i&&e&&e(i.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;0<t.find(".toc-list-item").length&&t.css("visibility","visible")}}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-svg-full.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>