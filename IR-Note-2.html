<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#01213a"><meta name="description" content="「信息检索」课程笔记，本文介绍统计语言模型基本概念、多元语言模型，以及查询排序问题、零频问题。"><meta name="author" content="He Wei"><meta name="keywords" content="Computer Science and Technology, Machine Learning, Deep Learning, Computer Vision, Natural Language Processing"><meta name="baidu-site-verification" content="code-J3wrn8WJYJ"><meta name="google-site-verification" content="0p_KJKTfB8EcahVDp0vYRjVRhHFw1SBWHi15OakKHY0"><meta name="description" content="「信息检索」课程笔记，本文介绍统计语言模型基本概念、多元语言模型，以及查询排序问题、零频问题。"><meta property="og:type" content="article"><meta property="og:title" content="IR学习笔记 #2 统计语言模型"><meta property="og:url" content="https://hwcoder.top/IR-Note-2"><meta property="og:site_name" content="Hwcoder - Life Oriented Programming"><meta property="og:description" content="「信息检索」课程笔记，本文介绍统计语言模型基本概念、多元语言模型，以及查询排序问题、零频问题。"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2021-08-22T12:26:20.000Z"><meta property="article:modified_time" content="2021-10-05T04:54:36.431Z"><meta property="article:author" content="He Wei"><meta property="article:tag" content="IR"><meta property="article:tag" content="NLP"><meta name="twitter:card" content="summary_large_image"><title>IR学习笔记 #2 统计语言模型 | Hwcoder - Life Oriented Programming</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/night-owl.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/css/mac.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"hwcoder.top",root:"/",version:"1.8.12",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h2, h3",placement:"left",visible:"always",icon:"§"},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"XGNf4GEnaFLiUijMUvz8HSBF-gzGzoHsz",app_key:"JPeVLcug6EcWdBTpGrDJSdKi",server_url:"https://xgnf4gen.lc-cn-n1-shared.com",path:"window.location.pathname"}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Hwcoder</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于我</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(/img/bg/other.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="IR学习笔记 #2 统计语言模型"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-08-22 20:26" pubdate>2021年8月22日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.9k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 16 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">IR学习笔记 #2 统计语言模型</h1><p class="note note-info">本文最后更新于：2021年10月5日 下午</p><div class="markdown-body"><p>基于对布尔模型的改进，提出一种新的最佳匹配模型。</p><h2 id="统计语言模型-Statistical-Language-Models"><a href="#统计语言模型-Statistical-Language-Models" class="headerlink" title="统计语言模型 | Statistical Language Models"></a>统计语言模型 | Statistical Language Models</h2><p>首先探讨的是 Doc (文档) 的呈现形式，引入 Topic (<strong>主题</strong>) 来表述一个文档的隐含语义，起到索引作用。基于以下两个假设：</p><ul><li>Words common in document are common in topic.</li><li>Words not in document much less likely.</li></ul><p>可以得出，Topic 是由 Doc 中的一些关键词勾勒出来的。于是引入 $P(w|Doc)$ 概率分布表：统计每个词在文档中出现频度（频率）——基于大数定律。</p><p>但 Topic 的难确定性（语义理解不同、可能有多个主题）导致其难以直接计算，因此可以用近似估算。</p><script type="math/tex;mode=display">P\left( w|Topic_D \right) \approx P\left( w|D \right) =tf\left( w,D \right) /len\left( D \right)</script><p>事实上，我们可以认为 Topic 是一种「<strong>语言模型</strong>」，$P\left( w|Topic_D \right)$ 可以认为是在 Topic 下生成该 word 的概率，即该 word 在这个「语言模型」中被生成的概率，故 word 可以不在 Topic 中出现，但也有概率生成。</p><h3 id="语言模型化-Language-Modeling"><a href="#语言模型化-Language-Modeling" class="headerlink" title="语言模型化 | Language Modeling"></a>语言模型化 | Language Modeling</h3><p>定义 <em>M</em> 为我们试图描述的 language (语言)，<em>s</em> 为该语言下观测到的文本串（由许多词条构成）。</p><ul><li><p><em>M</em> can be thought of as a “source” or a generator - a <strong>mechanism</strong> that can spit out strings that are legal in the language.</p></li><li><p>$P(s|M)$ is the probability of getting “<em>s</em>” during random sampling from <em>M</em>.</p></li></ul><p>语言的规模可大可小，把每种语言的规模缩小为一个 Topic（对应着语料库中的一个文档）；这个 Topic 就决定了任意一个字符串在这个 Topic 所对应的「语言模型」中出现的概率：比如，在一个描述信息检索发展历史的文档中，“Washington” 出现的概率就会远远小于 “Robertson”。</p><p>那么，一旦我们确定了这个 Doc 所对应的「语言模型」$M_D$ ，而 <em>Q</em> 是用户的 Query，我们是不是可以求出这个「语言模型」下<strong>生成</strong> <em>Q</em> 的概率？概率最大者就是与查询最相关的文档。那么，我们就可以根据 $P(Q|M_D) $ 给所有的 Doc 排序，得到我们的查询结果。</p><h2 id="多元语言模型-N-gram-Language-Models"><a href="#多元语言模型-N-gram-Language-Models" class="headerlink" title="多元语言模型 | N-gram Language Models"></a>多元语言模型 | N-gram Language Models</h2><p>对于一个较长的 Query，我们采用<strong>分词</strong>的方法来计算它的生成概率。为此，首先通过几个例子明确语言模型中 N-gram 的概念：</p><ul><li>Unigram 一元分词，把句子分成一个一个的汉字，如：哈/工/大/深/圳</li><li>Bigram 二元分词，每两个字组成一个词语，如：哈工/工大/大深/深圳</li><li>Trigram 三元分词，每三个字组成一个词语，如：哈工大/工大深/大深圳</li></ul><p>在以上例子中，我们可以知道一个文本串在一元语言中生成的概率将这样计算：</p><script type="math/tex;mode=display">P\left( w_1w_2w_3 \right) =P\left( w_1 \right) \cdot P\left( w_2 \right) \cdot P\left( w_3 \right)</script><p>在二元语言中将这样计算：</p><script type="math/tex;mode=display">P\left( w_1w_2w_3 \right) =P\left( w_1 \right) \cdot P\left( w_2|w_1 \right) \cdot P\left( w_3|w_2 \right)</script><p>可以发现，在 Unigram 中我们假设了单词之间的<strong>独立性</strong>，这就意味着它的本质是词的多项分布，而一个文本串可以看作是这个分布的一个实例。</p><p>对于更多元的分词 N-gram，我们是假设每个单词出现的概率只与它之前的 n-1 个单词<strong>相关</strong>，因此采用了条件概率。事实上，这是一种基于马尔可夫假设的模型，此时的文本串应是有序相关的，这就不属于 BoW 的范畴。</p><p>一般情况下，N 的取值都很小，实际<strong>自然语言处理</strong>应用中最多的是将 N = 3 的三元分词模型。原因如下：</p><ul><li>N 元模型的空间复杂度，是 N 的指数函数，即 $O\left( \left| V \right|^N \right) $，<em>V</em> 是一种语言的词汇量，一般在几万到几十万个。时间复杂度也是一个指数函数$O\left( \left| V \right|^{N-1} \right) $。</li><li>即使使用 N = 4 、N = 5 也不可能覆盖所有词与词之间的相关性。某两个词可能是一段话和一段话之间才会出现的。</li></ul><h3 id="多元语言模型的参数估计"><a href="#多元语言模型的参数估计" class="headerlink" title="多元语言模型的参数估计"></a>多元语言模型的参数估计</h3><p>针对一元模型，只需要统计该「语言模型」生成的文档中，出现该 term 的频率，用频率近似概率即可——<strong>大数定律</strong>。</p><p>这里对二元模型展开探讨：估计 $P\left( w_i|w_{i-1} \right)$，利用条件概率：</p><script type="math/tex;mode=display">P\left(w_{i} \mid w_{i-1}\right)=\frac{P\left(w_{i-1}, w_{i}\right)}{P\left(w_{i-1}\right)}</script><p>于是，我们只需要统计 $\left(w_{i-1}, w_{i}\right)$ 的有序词对在文档中的出现次数，再统计 $w_{i-1}$ 的出现次数，即可估计其概率。</p><p>然而，存在这样一个问题：在文本中，两个词没有连续出现过，即频度为 0，那么它的概率就是 0 吗？如果词对 $\left(w_{i-1}, w_{i}\right)$ 和 $w_{i-1}$ 的出现次数相同，其概率就是 1 吗？这就涉及到了统计的可靠性问题，也称「<strong>不平滑问题</strong>」。</p><p>解决这些问题的主要方法是<strong>古德-图灵估计</strong>（Good-Turing Estimate）和<strong>卡茨退避法</strong>（Katz backoff）。</p><ul><li><p>对出现次数大于某个阈值的词，频率不下调，即用频率代替概率；</p></li><li><p>对出现次数小于这个阈值的词，频率才下调，利用古德-图灵估计的相对频度来调整；</p></li><li><p>对出现次数等于 0 的词，利用卡茨退避法给予一个比较小的概率值。</p></li></ul><p>这部分的内容属于语料库的自然语言处理，本文中不赘述，仅在后文针对零频问题介绍几种方法。</p><h2 id="查询排序问题-Ranking"><a href="#查询排序问题-Ranking" class="headerlink" title="查询排序问题 | Ranking"></a>查询排序问题 | Ranking</h2><p>当给定查询 <em>Q</em> 时，怎么根据统计语言模型进行排序呢？有三种排序方法，分别是：</p><ol><li><strong>查询似然排序</strong> | Query-likelihood</li></ol><p>为每个 Doc 确定其所对应的 $M_D$，而用户的 Query 记为 $q=(q_1,q_2,\cdots,q_k)$ 。则该查询在每个文档的「语言模型」下生成的概率可如下计算：</p><script type="math/tex;mode=display">P\left(q_{1} \ldots q_{k} \mid M_{D}\right)=\prod_{i=1}^{k} P\left(q_{i} \mid M_{D}\right)</script><p>将所有计算结果排序，即可得到检索结果。要注意，这种方法对每个 Doc 计算出的概率都独立于其他 Doc，相关文档没有被利用到。</p><ol><li><strong>文档似然排序</strong> | Document-likelihood</li></ol><p>查询似然的翻转版本，为每个 Query 确定其所对应的 $M_Q$，计算任意一个文档在该查询的「语言模型」下生成的概率：</p><script type="math/tex;mode=display">P\left(D \mid M_{Q}\right)=\prod_{w \in D} P\left(w \mid M_{Q}\right)</script><p>但是，这种方法存在如下问题：</p><ul><li>文档的长度相差很大，很难比较。</li><li>由于文档中出现的词很多没有出现在查询中，将会出现零频问题。</li><li>将会出现无意义的作弊网页，如将 Query 中的关键词无限重复。</li></ul><p>要解决这些问题，需要引入 Likelihood Ratio (似然比)，对文档长度加以归一。</p><script type="math/tex;mode=display">P\left(M_{Q} \mid D\right)=\frac{P\left(M_{Q}\right) P\left(D \mid M_{Q}\right)}{P(D)} \approx \frac{c \prod_{w \in D} P\left(w \mid M_{Q}\right)}{\prod_{w \in D} P(w \mid G E)}</script><p>其中，对每个文档计算其可能 「生成 $M_Q$」的概率，在用贝叶斯公式展开，其中的 $P\left(M_{Q}\right)$ 对于每个文档可视作常数，再由分母的约束，对文档加以限制。</p><ol><li>Ranking by <strong>Model Comparison</strong></li></ol><p>结合前两种方法，提出了<strong>交叉熵</strong>（cross-entropy）的概念：</p><script type="math/tex;mode=display">H\left(M_{Q} \| M_{D}\right)=-\sum_{w} P\left(w \mid M_{Q}\right) \log P\left(w \mid M_{D}\right)</script><p>这种方法同时考虑了查询 $M_Q$ 和文档 $M_D$，直接比较两种模型的相似度。要注意，$M_Q$ 和 $M_D$ 在公式中的顺序不能调换。</p><h3 id="零频问题-Zero-frequency-problem"><a href="#零频问题-Zero-frequency-problem" class="headerlink" title="零频问题 | Zero frequency problem"></a>零频问题 | Zero frequency problem</h3><p>有了上述排序模型，现在我们只需要从查询和文档中估算出 $M_Q$ 和 $M_D$。</p><p>在本文的「语言模型」中，我们只需采用<strong>一元分词模型</strong>，独立性和独立分布可以简化许多问题。然而，在<strong>极大似然估计</strong>下，还是有个问题急需解决——零频问题，即有的 term 根本不出现在观测集中，我们该如何估算其概率？</p><p>这里介绍三种 Discounting Methods (折扣法) 来 Smoothing (平滑) ：</p><ol><li><p>Laplace correction：把每个词的词频都加 1，分母的总频数加上词项数 N。但是这种方法不适合较大的词典表。</p></li><li><p>Lindstone correction：把每个词都加一个很小的值 ε，分母的总频数加上 Nε。</p></li><li><p>Absolute Discounting：把词频不等于 0 的词减去一个很小的值 ε，再把减去的总值平均分配到词频为 0 的词上去，不改变分母。</p></li></ol><p>除了折扣法，还有诸如插值法、退避法等方法也可以用于平滑。</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="hover-with-bg" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/">信息检索</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/IR/">IR</a> <a class="hover-with-bg" href="/tags/NLP/">NLP</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow noopener noopener"><u>CC BY-NC-SA 4.0</u></a> 协议，转载请注明来自 <a href="https://hwcoder.top" rel="noopener"><u>Hwcoder</u></a>！</p><div class="post-prevnext"><article class="post-prev col-6"><a href="/IR-Note-3"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">IR学习笔记 #3 向量空间模型</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/IR-Note-1"><span class="hidden-mobile">IR学习笔记 #1 概论&布尔模型</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"jdbBr3BddTiqSCPnXw6sXFv7-gzGzoHsz","appKey":"2bjgwDr2opjVCwhgjDMpk53c","path":"window.location.pathname","placeholder":"说点什么吧( •̀ ω •́ )✧","avatar":"retro","meta":["nick","mail","link"],"requiredFields":["nick"],"pageSize":10,"lang":"zh-CN","highlight":true,"recordIP":true,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          Fluid.plugins.initFancyBox('#valine .vcontent img:not(.vemoji)');
        })
      });
    });</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/local-search.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>