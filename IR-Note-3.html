<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#01213a"><meta name="author" content="He Wei"><meta name="keywords" content="Computer Science and Technology, Machine Learning, Deep Learning, Computer Vision, Natural Language Processing"><meta name="baidu-site-verification" content="code-J3wrn8WJYJ"><meta name="google-site-verification" content="0p_KJKTfB8EcahVDp0vYRjVRhHFw1SBWHi15OakKHY0"><meta name="description" content="「信息检索」课程笔记，本文介绍向量空间模型的概念与应用、潜层语义索引及奇异值分解的应用。"><meta property="og:type" content="article"><meta property="og:title" content="IR学习笔记 #03 向量空间模型"><meta property="og:url" content="https://hwcoder.top/IR-Note-3"><meta property="og:site_name" content="Hwcoder - Life Oriented Programming"><meta property="og:description" content="「信息检索」课程笔记，本文介绍向量空间模型的概念与应用、潜层语义索引及奇异值分解的应用。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hwcoder.top/img/home/IR.jpg"><meta property="article:published_time" content="2021-08-22T14:26:20.000Z"><meta property="article:modified_time" content="2022-11-28T07:48:31.818Z"><meta property="article:author" content="He Wei"><meta property="article:tag" content="IR"><meta property="article:tag" content="NLP"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://hwcoder.top/img/home/IR.jpg"><title>IR学习笔记 #03 向量空间模型 | Hwcoder - Life Oriented Programming</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/mac.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"hwcoder.top",root:"/",version:"1.9.0",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h2, h3",placement:"left",visible:"always",icon:"§"},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"XGNf4GEnaFLiUijMUvz8HSBF-gzGzoHsz",app_key:"JPeVLcug6EcWdBTpGrDJSdKi",server_url:"https://xgnf4gen.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!0}},search_path:"/local-search.xml"};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.4.2"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Hwcoder</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于我</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/bg/other.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="IR学习笔记 #03 向量空间模型"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-08-22 22:26" pubdate>2021年8月22日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.6k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 20 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="学习笔记" id="heading-078425eaf316a180b0989442e53f920b" role="tab" data-toggle="collapse" href="#collapse-078425eaf316a180b0989442e53f920b" aria-expanded="true">学习笔记 <span class="list-group-count">(32)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-078425eaf316a180b0989442e53f920b" role="tabpanel" aria-labelledby="heading-078425eaf316a180b0989442e53f920b"><div class="category-post-list"></div><div class="category-sub row nomargin-x"><a class="category-subitem list-group-item category-item-action col-10 col-md-11 col-xm-11" title="信息检索" id="heading-b10f39c13b5f9af517dfde91c4da0ec4" role="tab" data-toggle="collapse" href="#collapse-b10f39c13b5f9af517dfde91c4da0ec4" aria-expanded="true">信息检索 <span class="list-group-count">(11)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-b10f39c13b5f9af517dfde91c4da0ec4" role="tabpanel" aria-labelledby="heading-b10f39c13b5f9af517dfde91c4da0ec4"><div class="category-post-list"><a href="/IR-Note-1" title="IR学习笔记 #01 概论&amp;布尔模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #01 概论&amp;布尔模型</span> </a><a href="/IR-Note-2" title="IR学习笔记 #02 统计语言模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #02 统计语言模型</span> </a><a href="/IR-Note-3" title="IR学习笔记 #03 向量空间模型" class="list-group-item list-group-item-action active"><span class="category-post">IR学习笔记 #03 向量空间模型</span> </a><a href="/IR-Note-4" title="IR学习笔记 #04 概率模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #04 概率模型</span> </a><a href="/IR-Note-5" title="IR学习笔记 #05 检索系统评价" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #05 检索系统评价</span> </a><a href="/IR-Note-6" title="IR学习笔记 #06 网络信息检索" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #06 网络信息检索</span> </a><a href="/IR-Note-7" title="IR学习笔记 #07 IRLbot" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #07 IRLbot</span> </a><a href="/IR-Note-8" title="IR学习笔记 #08 倒排索引模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #08 倒排索引模型</span> </a><a href="/IR-Note-9" title="IR学习笔记 #09 网页排序" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #09 网页排序</span> </a><a href="/IR-Note-10" title="IR学习笔记 #10 查询相关反馈" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #10 查询相关反馈</span> </a><a href="/IR-Note-11" title="IR学习笔记 #11 问答系统" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #11 问答系统</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="本科课程" id="heading-d577b913fa605a2342ab24bd1b0bea8b" role="tab" data-toggle="collapse" href="#collapse-d577b913fa605a2342ab24bd1b0bea8b" aria-expanded="false">本科课程 <span class="list-group-count">(4)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-d577b913fa605a2342ab24bd1b0bea8b" role="tabpanel" aria-labelledby="heading-d577b913fa605a2342ab24bd1b0bea8b"><div class="category-post-list"><a href="/Database-System" title="数据库系统 应试笔记" class="list-group-item list-group-item-action"><span class="category-post">数据库系统 应试笔记</span> </a><a href="/Mathematical-Logic" title="数理逻辑 应试笔记" class="list-group-item list-group-item-action"><span class="category-post">数理逻辑 应试笔记</span> </a><a href="/Software-Engineering-1" title="软件工程 应试笔记 #1" class="list-group-item list-group-item-action"><span class="category-post">软件工程 应试笔记 #1</span> </a><a href="/Software-Engineering-2" title="软件工程 应试笔记 #2" class="list-group-item list-group-item-action"><span class="category-post">软件工程 应试笔记 #2</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="机器学习" id="heading-61bb1751fd355596e307767d1927c855" role="tab" data-toggle="collapse" href="#collapse-61bb1751fd355596e307767d1927c855" aria-expanded="false">机器学习 <span class="list-group-count">(12)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-61bb1751fd355596e307767d1927c855" role="tabpanel" aria-labelledby="heading-61bb1751fd355596e307767d1927c855"><div class="category-post-list"><a href="/ML-Note-1" title="ML学习笔记 #01 梯度下降：一元线性回归" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #01 梯度下降：一元线性回归</span> </a><a href="/ML-Note-2" title="ML学习笔记 #02 梯度下降：多元线性回归" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #02 梯度下降：多元线性回归</span> </a><a href="/ML-Note-3" title="ML学习笔记 #03 正规方程：多元线性回归" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #03 正规方程：多元线性回归</span> </a><a href="/ML-Note-4" title="ML学习笔记 #04 逻辑回归：二分类到多分类" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #04 逻辑回归：二分类到多分类</span> </a><a href="/ML-Note-5" title="ML学习笔记 #05 过拟合与正则化" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #05 过拟合与正则化</span> </a><a href="/ML-Note-6" title="ML学习笔记 #06 神经网络基础" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #06 神经网络基础</span> </a><a href="/ML-Note-7" title="ML学习笔记 #07 神经网络：反向传播" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #07 神经网络：反向传播</span> </a><a href="/ML-Note-8" title="ML学习笔记 #08 数据集划分与误差分析" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #08 数据集划分与误差分析</span> </a><a href="/ML-Note-9" title="ML学习笔记 #09 支持向量机" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #09 支持向量机</span> </a><a href="/ML-Note-10" title="ML学习笔记 #10 K-Means 聚类" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #10 K-Means 聚类</span> </a><a href="/ML-Note-11" title="ML学习笔记 #11 PCA：主成分分析" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #11 PCA：主成分分析</span> </a><a href="/ML-Note-12" title="ML学习笔记 #12 异常检测" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #12 异常检测</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="模式识别" id="heading-03b289aaa6b21d8730ca1e736a2796eb" role="tab" data-toggle="collapse" href="#collapse-03b289aaa6b21d8730ca1e736a2796eb" aria-expanded="false">模式识别 <span class="list-group-count">(5)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-03b289aaa6b21d8730ca1e736a2796eb" role="tabpanel" aria-labelledby="heading-03b289aaa6b21d8730ca1e736a2796eb"><div class="category-post-list"><a href="/PR-Note-1" title="PR学习笔记 #1 KNN 分类器" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #1 KNN 分类器</span> </a><a href="/PR-Note-2" title="PR学习笔记 #2 贝叶斯分类器" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #2 贝叶斯分类器</span> </a><a href="/PR-Note-3" title="PR学习笔记 #3 概率密度：参数估计" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #3 概率密度：参数估计</span> </a><a href="/PR-Note-4" title="PR学习笔记 #4 概率密度：非参数估计" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #4 概率密度：非参数估计</span> </a><a href="/PR-Note-5" title="PR学习笔记 #5 判别式 vs. 生成式" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #5 判别式 vs. 生成式</span></a></div></div></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">IR学习笔记 #03 向量空间模型</h1><p class="note note-info">本文最后更新于：2022年11月28日 下午</p><div class="markdown-body"><p>回忆前两个模型，我们发现统计语言模型在布尔模型上，做出了最佳匹配和排序结果的改进。但是，仍然没有考虑到「<strong>词项的权重</strong>」。</p><p>在向量空间模型中，我们容易联想到用向量来表示文档和查询，再通过计算余弦来得到两个向量的距离，从而得到相似性度量。</p><p>那么，如何选取向量空间 basis vector (基向量)？如何将目标转化为向量？如何为各个维度选取 magnitide (幅值)，从而考虑权重？如何在高维空间计算向量距离？</p><h2 id="向量空间模型-Vector-Space-Model"><a href="#向量空间模型-Vector-Space-Model" class="headerlink" title="向量空间模型 | Vector Space Model"></a>向量空间模型 | Vector Space Model</h2><p>通常地，我们选择用 linearly independent (线性独立) 或 orthogonal (正交) 的基向量来张成<strong>向量空间</strong>，这样可以使得维度最少。那么，如何选取基向量？</p><p>这是一个特征选择问题，在 IR 中，通常有两种方式：</p><ol><li><p>Core concept (核心概念) 的思想：把词语的类型分类，按照其在不同分类上的「倾斜程度」决定向量的值，可以使维度尽量少。但是，由于语义上的多样性，很难实现。目前有 WordNet, HowNet, HNC 等模型。</p></li><li><p>把出现过的 term 都当作是一个基向量，并<strong>假设</strong>所有的基向量都是相互正交、相互独立的。这样将会得到一个维度不断增长的向量空间（随着词典表扩大）。</p></li></ol><p>以下我们采用第二种方式。一个 Doc 或 Query 的向量表示就是：所有出现在文档中的 term 的向量之和。</p><h3 id="词项权重-Term-Weighting"><a href="#词项权重-Term-Weighting" class="headerlink" title="词项权重 | Term Weighting"></a>词项权重 | Term Weighting</h3><p>当一个 term 在文档中不断出现时，在这个方向上的向量幅值就会很大。这样比起布尔模型的 0/1 二值，更能反映了这个 term 的重要性。这便是决定权重的 <em>tf</em> (<strong>term frequency</strong>，词项频率) 方法。</p><p>然而，原始的 <em>tf</em> 值会面临这样一个严重的问题：即在和查询进行相关度计算时，所有 term 都被认为是同等重要的。</p><p>实际上，某些 term 对于相关度计算来说几乎没有或很少有区分能力。一个很直接的想法就是给包含在较多文档中的词项赋予较低的权重。为此，引入变量 <em>df</em> (<strong>document frequency</strong>，文档集频率)，即有多少文档包含了该 term。df 值越大，说明该 term 越不重要。</p><p>为了计算的方便，将其标准化得到 <em>idf</em> (<strong>inverse document frequency</strong>，逆文档频率)：</p><script type="math/tex;mode=display">idf_t=\log \left( \frac{N}{df_t} \right)</script><p>观察该式发现，<em>idf</em> 虽然可以使得在较多文档中的词项权值降低，但与 <em>tf</em> 相反的是，这样做的缺点是：对那些极少出现的词极度敏感。</p><p>为此，我们将二者结合在一起，诞生了 <strong><em>tf·idf</em></strong> 方法——在文本处理领域中使用最广泛的数值权重计算方法。方法基于的思想和构造的统计量都很简单，但是在实际中却表现了很好的性能。</p><p>在 VSM 中，我们会将词项的 <em>tf·idf</em> 存储在词典表（词项-文档）矩阵中，作为向量的幅值，用于后续的计算。</p><h3 id="相似度计算-Similarity"><a href="#相似度计算-Similarity" class="headerlink" title="相似度计算 | Similarity"></a>相似度计算 | Similarity</h3><p>当我们已经把文档表示成 $R^{v}$ 上的向量，从而可以计算文档与文档之间的相似度（根据向量内积或者<strong>余弦夹角</strong>）。</p><p>设 $D_1$ 和 $D_2$ 表示 VSM 中的两个向量：</p><script type="math/tex;mode=display">\begin{aligned}
&D_{1}=D_{1}\left(w_{11}, w_{12}, \ldots, w_{1 n}\right) \\
&D_{2}=D_{2}\left(w_{21}, w_{22}, \ldots, w_{2 n}\right)
\end{aligned}</script><p>可以借助于 N 维空间中两个向量之间的某种距离来表示文档之间的相似度，常用的方法是使用向量之间的內积来计算：</p><script type="math/tex;mode=display">\operatorname{Sim}\left(D_{1}, D_{2}\right)=\sum_{k=1}^{n} w_{1 k} \times w_{2 k}</script><p>考虑到向量的<strong>归一化</strong>，则可以使用两个向量的余弦值来表示相似系数：</p><script type="math/tex;mode=display">\operatorname{Sim}\left(D_{1}, D_{2}\right)=\cos \theta=\frac{\sum_{k=1}^{n} w_{1 k} \times w_{2 k}}{\sqrt{\sum_{k=1}^{n} w_{1 k}^{2} \sum_{k=1}^{n} w_{2 k}^{2}}}</script><p>要注意，这里使用向量内积，是基于对所有向量相互独立、相互正交的假设，否则计算内积也就失去了意义。对于相关的基向量，应该评估 Term 之间的相关度 $T_{i,j}$，再把向量当成多项式计算，最后代入 $T_{i,j}$。</p><p>此外，在其他的考虑权重的模型中，如 Lucene，在计算相似度时引入了更多的因子，如 <em>tf·idf</em>，$boost_t$，<em>overlap(q,d)</em> 等，对应用情形、平滑度加以考量。</p><h3 id="VSM-实际应用"><a href="#VSM-实际应用" class="headerlink" title="VSM 实际应用"></a>VSM 实际应用</h3><p>在 IR 中应用 VSM 模型时，相似度在检索结果中有两种体现：</p><ol><li><strong>Threshold</strong> (阈值)：对于每个查询，只在相似度大于一定阈值的文档中检索，如 Sim &gt; 0.50 的文档中，减少查询范围。</li><li><strong>Ranking</strong>：对于每个查询，返回相似度排名 Top n 的文档，以相似度排序。</li></ol><p>而 VSM 模型也有着致命的<strong>缺点</strong>：</p><ul><li><p>对于大的文档集（10w+ term），向量维度太多导致难以存储和计算。</p></li><li><p>一篇文档的词数（1k+ term）远低于总的词数——高维稀疏矩阵。</p></li><li>词项之间的相关性，导致了大量冗余的基向量。</li></ul><h2 id="潜层语义索引-Latent-Semantic-Indexing"><a href="#潜层语义索引-Latent-Semantic-Indexing" class="headerlink" title="潜层语义索引 | Latent Semantic Indexing"></a>潜层语义索引 | Latent Semantic Indexing</h2><p>潜层语义索引，也被称为 LSA (Latent Semantic Analysis，潜在语义分析)，是针对向量空间的「<strong>高维稀疏</strong>」问题提出的解决方法，利用线性代数中的<strong>奇异值分解</strong>降低维度（去除噪音），同时尽量减少信息的损失。</p><h3 id="奇异值分解-Singular-Value-Decomposition"><a href="#奇异值分解-Singular-Value-Decomposition" class="headerlink" title="奇异值分解 | Singular Value Decomposition"></a>奇异值分解 | Singular Value Decomposition</h3><p>参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6251584.html">https://www.cnblogs.com/pinard/p/6251584.html</a></p><p>对于一个 $t\times d$​​ 矩阵 $A$​​​，可以分解为下面三个矩阵：</p><script type="math/tex;mode=display">A_{t\times d}=U_{t\times t}\varSigma _{t\times d}V^T_{d\times d}</script><p>其中 $U$​ 和 $V$​ 都是<strong>酉矩阵</strong>，即满足 $U^TU=I, V^TV=I$​。$\varSigma$​ 一个 $t\times d$​ 矩阵，除了主对角线上的元素以外全为 0，主对角线上的每个元素都称为<strong>奇异值</strong>。</p><p>利用酉矩阵性质得：</p><script type="math/tex;mode=display">A=U\Sigma V^T \Rightarrow A^T=V\Sigma^T U^T \Rightarrow A^TA = V\Sigma^T U^TU\Sigma V^T = V\Sigma^2V^T</script><p>可以看出 $A^TA$ 的特征向量组成的矩阵，就是我们 SVD 中的 $V^T_{d\times d}$​ 矩阵。进一步我们还可以看出我们的特征值矩阵等于奇异值矩阵的平方。</p><p>利用以上原理，我们可以得出 SVD <strong>分解步骤</strong>：</p><ol><li>假设词典矩阵为 $A$，首先求出 $AA^T$，会得到一个 $t\times t$ 的方阵。</li><li>既然是方阵，就可以进行特征值分解，得到 <em>t</em> 个特征值和对应的特征向量。</li><li>将特征值按方差大小排序，用所有的列向量张成一个 $t\times t$ 的矩阵 $U_{t\times t}$。</li><li>同理可以用 $A^TA$ 求出 $d\times d$ 的矩阵 $V^T_{d\times d}$。</li><li>利用前面求出的特征值，开方后得到 $\varSigma _{t\times d}$。​</li></ol><h3 id="利用-SVD-降维"><a href="#利用-SVD-降维" class="headerlink" title="利用 SVD 降维"></a>利用 SVD 降维</h3><p>对于奇异值，它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列。通常，奇异值的<strong>衰减</strong>得特别快，在很多情况下，前 10% 甚至 1% 的奇异值之和就占了全部的奇异值之和的 99% 以上的比例。</p><p>也就是说，我们也可以用最大的 k 个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：</p><script type="math/tex;mode=display">A_{t\times d}=U_{t\times t}\varSigma _{t\times d}V^T_{d\times d}\approx U_{t\times k}\varSigma _{k\times k}V^T_{k\times d}</script><p>其中 <em>k</em> 要比 <em>t</em> 小很多，也就是一个大的矩阵可以用三个小的矩阵，此时存储空间可以大量节省。通常 <em>k</em> 的值即为我们假设的<strong>主题数</strong>。</p><p>SVD 分解后，$U_{il}$ 对应第 <em>i</em> 个词和第 <em>l</em> 个词义的相关度。$V_{jm}$ 对应第 <em>j</em> 个文档和第 <em>m</em> 个主题的相关度。$\Sigma_{lm}$ 对应第 <em>l</em> 个词义和第 <em>m</em> 个主题的相关度。</p><p>这样我们通过一次 SVD，就可以得到词和词义的相关度，词义和主题的相关度，以及文档和主题的相关度。</p><h3 id="LSI-的使用"><a href="#LSI-的使用" class="headerlink" title="LSI 的使用"></a>LSI 的使用</h3><p>通过计算后，我们关注新的矩阵 $V^T_{k\times d}$​ ，所有的文档已经简化成了和 k 个主题的相关度。假设此时的查询为 $Q=q_1q_2\cdots q_t$​​​​，​其中 <em>q</em> 取 0 或 1，则</p><script type="math/tex;mode=display">Q_{1\times k}=Q_{1\times t}U_{t\times k}\varSigma _{k\times k}</script><p>可将 <em>t</em> 维的查询转化成 <em>k</em> 维的「<strong>与主题的相关度</strong>」，此时就可以与文档进行相似度计算了。</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="category-chain-item">学习笔记</a> <span>></span> <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/" class="category-chain-item">信息检索</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/IR/">#IR</a> <a href="/tags/NLP/">#NLP</a></div></div><div class="license-box my-3"><div class="license-title"><div>IR学习笔记 #03 向量空间模型</div><div>https://hwcoder.top/IR-Note-3</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>He Wei</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2021年8月22日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i> </span></a><a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - 非商业性使用"><i class="iconfont icon-nc"></i> </span></a><a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - 相同方式共享"><i class="iconfont icon-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/IR-Note-4" title="IR学习笔记 #04 概率模型"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">IR学习笔记 #04 概率模型</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/IR-Note-2" title="IR学习笔记 #02 统计语言模型"><span class="hidden-mobile">IR学习笔记 #02 统计语言模型</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.4.16/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"jdbBr3BddTiqSCPnXw6sXFv7-gzGzoHsz","appKey":"2bjgwDr2opjVCwhgjDMpk53c","path":"window.location.pathname","placeholder":"说点什么吧( •̀ ω •́ )✧","avatar":"retro","meta":["nick","mail","link"],"requiredFields":["nick"],"pageSize":10,"lang":"zh-CN","highlight":true,"recordIP":true,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,i=t.getElementById("subtitle");i&&e&&e(i.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init({tocSelector:"#toc-body",contentSelector:".markdown-body",headingSelector:CONFIG.toc.headingSelector||"h1,h2,h3,h4,h5,h6",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",collapseDepth:CONFIG.toc.collapseDepth||0,scrollSmooth:!0,headingsOffset:-t}),0<o.find(".toc-list-item").length&&o.css("visibility","visible"))})</script><script>!function(){var i,t=CONFIG.code_language.enable&&CONFIG.code_language.default,c=CONFIG.copy_btn;(t||c)&&(i="",i+='<div class="code-widget">',i+="LANG",i+="</div>",jQuery(".markdown-body pre").each(function(){var e,a,n=jQuery(this);0<n.find("code.mermaid").length||0<n.find("span.line").length||(e="",t&&(e=CONFIG.code_language.default,0<n[0].children.length&&2<=n[0].children[0].classList.length&&n.children().hasClass("hljs")?e=n[0].children[0].classList[1]:n[0].getAttribute("data-language")?e=n[0].getAttribute("data-language"):n.parent().hasClass("sourceCode")&&0<n[0].children.length&&2<=n[0].children[0].classList.length?(e=n[0].children[0].classList[1],n.parent().addClass("code-wrapper")):n.parent().hasClass("markdown-body")&&0===n[0].classList.length&&n.wrap('<div class="code-wrapper"></div>'),e=e.toUpperCase().replace("NONE",CONFIG.code_language.default)),n.append(i.replace("LANG",e).replace('code-widget">',(a=n[0],(0<=Fluid.utils.getBackgroundLightness(a)?"code-widget-light":"code-widget-dark")+(c?' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>':' code-widget">')))),c&&Fluid.utils.createScript("https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js",function(){new window.ClipboardJS(".copy-btn",{target:function(e){for(var a=e.parentNode.childNodes,n=0;n<a.length;n++)if("CODE"===a[n].tagName)return a[n]}}).on("success",function(e){e.clearSelection(),e.trigger.innerHTML=e.trigger.innerHTML.replace("icon-copy","icon-success"),setTimeout(function(){e.trigger.innerHTML=e.trigger.innerHTML.replace("icon-success","icon-copy")},2e3)})}))}))}()</script><script>Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>MathJax = {
          tex    : {
            inlineMath: { '[+]': [['$', '$']] }
          },
          loader : {
            load: ['ui/lazy']
          },
          options: {
            renderActions: {
              findScript    : [10, doc => {
                document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                  const display = !!node.type.match(/; *mode=display/);
                  const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                  const text = document.createTextNode('');
                  node.parentNode.replaceChild(text, node);
                  math.start = { node: text, delim: '', n: 0 };
                  math.end = { node: text, delim: '', n: 0 };
                  doc.math.push(math);
                });
              }, '', false],
              insertedScript: [200, () => {
                document.querySelectorAll('mjx-container').forEach(node => {
                  let target = node.parentNode;
                  if (target.nodeName.toLowerCase() === 'li') {
                    target.parentNode.classList.add('has-jax');
                  }
                });
              }, '', false]
            }
          }
        };</script><script src="https://lib.baomitu.com/mathjax/3.2.0/es5/tex-svg-full.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>