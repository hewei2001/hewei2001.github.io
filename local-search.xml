<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>手撕经典算法 #5 RLHF 篇</title>
    <link href="/Manual-Coding-5.html"/>
    <url>/Manual-Coding-5.html</url>
    
    <content type="html"><![CDATA[<p>本文对 RLHF 中经典的算法进行了简单的实现和注释。包括：</p><ul><li>广义优势估计（GAE）</li><li>各种损失函数（PPO、DPO、GRPO）</li></ul><h2 id="gae">GAE</h2><p>PPO 中的<strong>广义优势估计</strong>（Generalized AdvantageEstimation，GAE） 通过<strong>在时间步上对 TD Error进行加权累加</strong>，提供了一个在偏差和方差之间可调的优势估计器。其定义为：<span class="math display">\[\begin{aligned}A_t^{\text{GAE}(\gamma, \lambda)} &amp;= \delta_t + (\gamma \lambda)\delta_{t+1} + (\gamma \lambda)^2 \delta_{t+2} + \cdots \\&amp;= \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}\\\end{aligned}\]</span> 其中：</p><ul><li><span class="math inline">\(\delta_t\)</span> 是第 <spanclass="math inline">\(t\)</span> 步的 TD Error：<spanclass="math inline">\(\delta_t = r_t + \gamma v_t(s_{t+1}) -v_t(s_t)\)</span>，这里为了简洁将第 <spanclass="math inline">\(t\)</span> 步的即时奖励记为 <spanclass="math inline">\(r_t\)</span>；</li><li><span class="math inline">\(\gamma\)</span> 是折扣因子，<spanclass="math inline">\(\lambda \in [0, 1]\)</span> 是 <strong>GAE的衰减系数，控制偏差和方差之间的平衡</strong>。</li></ul><p>通过调整 <span class="math inline">\(\lambda\)</span>的值，可以在偏差和方差之间进行调节：</p><ul><li>当 <span class="math inline">\(\lambda = 0\)</span> 时，只考虑一步的TD Error，偏差较大，但方差较小。</li><li>当 <span class="math inline">\(\lambda = 1\)</span>时，优势估计等价于蒙特卡洛方法，偏差较小，但方差较大。</li></ul><p>在 PPO 实现的过程中，我们需要对每个 Token 都计算出GAE，因此需要<strong>反向遍历</strong>，用递推的形式逐步计算。实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">GAE</span>(<span class="hljs-params">values, rewards, gamma, lambd</span>):</span><br><span class="hljs-comment"># values: (batch_size, seq_len)</span><br><span class="hljs-comment"># rewards: (batch_size, seq_len)</span><br>    <br>    suffix_gae = <span class="hljs-number">0</span><br>    advantages_reversed = []<br>    response_length = rewards.size(<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 从最后一个时间步开始向前遍历</span><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(response_length)):<br>        nextvalues = values[:, t + <span class="hljs-number">1</span>] <span class="hljs-keyword">if</span> t &lt; response_length - <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0.0</span><br>        delta = rewards[:, t] + gamma * nextvalues - values[:, t]   <span class="hljs-comment"># 计算 TD error</span><br>        suffix_gae = delta + gamma * lambd * suffix_gae             <span class="hljs-comment"># 递推计算 GAE</span><br>        advantages_reversed.append(suffix_gae)                      <span class="hljs-comment"># 当前时间步的优势</span><br>    <br>    <span class="hljs-comment"># advantages: [batch_size, seq_len]</span><br>    advantages = torch.stack(advantages_reversed[::-<span class="hljs-number">1</span>], dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 将逆序列表反转为正序</span><br>    <span class="hljs-keyword">return</span> advantages.detach()<br></code></pre></td></tr></table></figure><h2 id="ppo">PPO</h2><p>在真实的 PPO代码中，可能包含<strong>策略模型</strong>损失函数、<strong>价值模型</strong>损失函数、<strong>语言模型</strong>损失函数，用于优化Actor、Critic 和防止模型遗忘预训练知识。这里我们仅介绍常见的 PolicyLoss和 ValueLoss。</p><h3 id="policy-loss">Policy Loss</h3><p>策略模型损失函数可以表示为： <span class="math display">\[L^{\mathrm{CLIP}}(\theta) = \mathbb{E}_{t} \left[ \min \left(r_t(\theta) A_t,\; \operatorname{clip}(r_t(\theta), 1 - \epsilon, 1 +\epsilon) A_t \right) \right]\]</span> 用于优化 Actor模型，为确保策略更新不会偏离旧策略太远，通过计算新旧策略的概率比率，并使用裁剪参数<span class="math inline">\(\epsilon\)</span> 与裁剪函数<code>clamp</code> 限制更新幅度，从而稳定训练过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">PolicyLoss</span>(<span class="hljs-params">log_probs, old_log_probs, advantages</span>)</span><br><span class="hljs-function">&quot;&quot;&quot;</span><br><span class="hljs-function"><span class="hljs-title">log_probs</span>:</span> (batch_size, seq_len)，表示某个被选中的 action 的对数概率<br>    advantages: (batch_size, seq_len)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">ratio = (log_probs - old_log_probs).exp()# 计算概率比率</span><br><span class="hljs-string">    surr1 = ratio * advantages# 计算未裁剪的目标函数</span><br><span class="hljs-string">    surr2 = ratio.clamp(1 - self.clip_eps, 1 + self.clip_eps) * advantages# 计算裁剪后的目标函数</span><br><span class="hljs-string">    loss = -torch.min(surr1, surr2)# 取两者中的较小值</span><br><span class="hljs-string">    return loss.mean()</span><br></code></pre></td></tr></table></figure><blockquote><p>注意：这里的 <code>log_probs</code> 表示采样的 Experience 中选中Action 的对数概率，经过一层 Log 变换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 模型输出原始 logits (batch_size, seq_len, vocab_size)</span><br>logits = actor_model(input_ids)<br><br><span class="hljs-comment"># 计算概率分布（softmax 后的概率）</span><br>probs = F.softmax(logits, dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch_size, seq_len, vocab_size)</span><br><br><span class="hljs-comment"># 提取被采样 token 的 log 概率</span><br>log_probs = torch.gather(probs.log(), dim=-<span class="hljs-number">1</span>, index=actions.unsqueeze(-<span class="hljs-number">1</span>)).squeeze(-<span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch_size, seq_len)</span><br></code></pre></td></tr></table></figure></blockquote><h3 id="value-loss">Value Loss</h3><p>价值模型损失函数可以表示为： <span class="math display">\[L^{\text{Critic}}(\phi) = \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \|V_\phi(x) - V_t^{\text{target}} \|^2 \right]\]</span></p><p>其中，<span class="math inline">\(V_\phi(x)\)</span> 是价值模型对状态<span class="math inline">\(x\)</span> 的价值估计，<spanclass="math inline">\(V_t^{\text{target}}\)</span> 在 RLHF-PPO中一般定义为<strong>回报值</strong> <span class="math inline">\(R_t =\hat{A}_{t}^{\mathrm{GAE}(\gamma,\lambda)} + V(s_t)\)</span>。</p><p>虽然在 PPO 原论文中并未对价值函数引入裁剪，但在一些 PPO的变体中，为了提高训练的稳定性，也在价值函数的训练中引入了裁剪操作。同样，通过参数<span class="math inline">\(\epsilon\)</span> ，约束新策略 <spanclass="math inline">\(V_\phi^{new}\)</span> 与旧策略 <spanclass="math inline">\(V_\phi^{old}\)</span> 之间的变化幅度。</p><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ValueLoss</span>(<span class="hljs-params">values, old_values, returns</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    values: (batch_size, seq_len)</span><br><span class="hljs-string">    returns: (batch_size, seq_len)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    values_clipped = old_values + (values - old_values).clamp(-self.clip_eps, self.clip_eps)<br>    surr1 = (values_clipped - returns) ** <span class="hljs-number">2</span><br>    surr2 = (values - returns) ** <span class="hljs-number">2</span><span class="hljs-comment"># 未经裁剪的价值损失</span><br>    loss = torch.<span class="hljs-built_in">max</span>(surr1, surr2)<span class="hljs-comment"># 取较大的损失（最大值）</span><br><span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * loss.mean()<br></code></pre></td></tr></table></figure><h2 id="dpo-loss">DPO Loss</h2><p>通过极大似然估计，BT 模型的优化目标可以转化为二元交叉熵损失： <spanclass="math display">\[\mathcal{L}_{\text{DPO}}(\theta) = -\mathbb{E}_{(x,y_w,y_l) \sim D}\left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w\midx)}{\pi_{\text{ref}}(y_w\mid x)} - \beta \log \frac{\pi_\theta(y_l\midx)}{\pi_{\text{ref}}(y_l\mid x)} \right) \right]\]</span></p><p>注意，DPO的目标是比较<strong>两个完整序列的整体偏好差异</strong>，而不是逐个token的生成质量。因此：被选中/拒绝响应的对数概率是对序列所有token的对数概率求和或者取平均，<strong>维度为<code>(batch_size,)</code></strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">policy_chosen_logps, policy_rejected_logps = concatenated_forward(<br>    self.policy_model, chosen_ids, c_mask, reject_ids, r_mask, prompt_id_lens<br>)<span class="hljs-comment"># 策略模型的概率，(batch_size,)</span><br><br>reference_chosen_logps, reference_rejected_logps = concatenated_forward(<br>    self.ref_model, chosen_ids, c_mask, reject_ids, r_mask, prompt_id_lens<br>)<span class="hljs-comment"># 参考模型的概率，(batch_size,)</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DPOLoss</span>(<span class="hljs-params">policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    policy_chosen_logps: (batch_size,)</span><br><span class="hljs-string">    policy_rejected_logps: (batch_size,)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 计算策略模型和参考模型的 log 概率比</span><br>    pi_logratios= policy_chosen_logps - policy_rejected_logps<br>ref_logratios = reference_chosen_logps - reference_rejected_logps<br>    logits = pi_logratios - ref_logratios<span class="hljs-comment"># 构造 logits 差值</span><br>    <br>    losses = -F.logsigmoid(self.beta * logits)<span class="hljs-comment"># -log σ(beta * (策略比值 - 参考比值))</span><br>    <span class="hljs-keyword">return</span> losses.mean()<br></code></pre></td></tr></table></figure><h2 id="grpo-loss">GRPO Loss</h2><h3 id="reward-归一化处理">Reward 归一化处理</h3><p>对于每个问题 <span class="math inline">\(q\)</span>，首先从旧策略模型 <spanclass="math inline">\(\pi_{o_\mathrm{old}}\)</span> 中采样生成一组包含<span class="math inline">\(G\)</span> 个输出的结果 <spanclass="math inline">\(\{o_1,o_2,\cdots,o_G\}\)</span>。利用奖励模型对这些输出进行评分，得到对应的奖励值<spanclass="math inline">\(r=\{r_1,r_2,\cdots,r_G\}\)</span>。对奖励进行归一化处理，该输出中所有token 的优势值 <span class="math inline">\(\hat{A}_{i,t}\)</span>设为归一化的奖励值，即<spanclass="math inline">\(\hat{A}_{i,t}=\tilde{r}_i\quad(\forall t \ino_i).\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">rewards = [experience.info[<span class="hljs-string">&quot;reward&quot;</span>] <span class="hljs-keyword">for</span> experience <span class="hljs-keyword">in</span> experiences]<br><br><span class="hljs-comment"># 每行对应一个 prompt 的 n 个样本</span><br>rewards = torch.cat(rewards).reshape(-<span class="hljs-number">1</span>, args.n_samples_per_prompt)<span class="hljs-comment"># [batch_size, n_samples_per_prompt]</span><br><span class="hljs-comment"># 计算均值与组标准差</span><br>rewards = (rewards - rewards.mean(-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)) / (rewards.std(-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>) + <span class="hljs-number">1e-9</span>)<br>rewards = rewards.reshape(-<span class="hljs-number">1</span>).chunk(<span class="hljs-built_in">len</span>(experiences))<br></code></pre></td></tr></table></figure><h3 id="loss-计算">Loss 计算</h3><p>GRPO的核心改进在于<strong>消除价值函数</strong>，通过组内相对优势估计代替绝对优势评估。其目标函数结合PPO 的剪切机制与 KL 惩罚项，具体实现可分为以下步骤：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">GRPOLoss</span>(<span class="hljs-params">log_probs, old_log_probs, ref_log_probs, advantages, clip_eps=<span class="hljs-number">0.2</span>, beta=<span class="hljs-number">0.04</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    log_probs: (batch_size * G, seq_len) 新策略的对数概率</span><br><span class="hljs-string">    old_log_probs: (batch_size * G, seq_len) 旧策略的对数概率</span><br><span class="hljs-string">    ref_log_probs: (batch_size * G, seq_len) 参考模型（如 SFT 模型）的对数概率</span><br><span class="hljs-string">    advantages: (batch_size * G, seq_len) 归一化后的优势值（每个 token 相同）</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 计算策略比率与剪切损失</span><br>    ratio = (log_probs - old_log_probs).exp() <span class="hljs-comment"># (batch*G, seq_len)</span><br>    surr1 = ratio * advantages                <span class="hljs-comment"># 未裁剪目标</span><br>    surr2 = ratio.clamp(<span class="hljs-number">1</span> - clip_eps, <span class="hljs-number">1</span> + clip_eps) * advantages  <span class="hljs-comment"># 裁剪目标</span><br>    policy_loss = -torch.<span class="hljs-built_in">min</span>(surr1, surr2)    <span class="hljs-comment"># 取较小值防止过激更新</span><br>    <br>    <span class="hljs-comment"># 计算 KL 散度惩罚项（当前策略 vs 参考模型）</span><br>    kl_loss = log_probs - ref_log_probs       <span class="hljs-comment"># 近似 KL 散度</span><br>    <br>    total_loss = (policy_loss - beta * kl_loss).mean()<br>    <span class="hljs-keyword">return</span> total_loss<br></code></pre></td></tr></table></figure><h3 id="无偏-kl-散度">无偏 KL 散度</h3><p>为了有效地计算策略模型与参考模型之间的 KL散度，我们可以从<strong>对数概率</strong>（log_prob）的角度来实现这一过程：<span class="math display">\[\mathrm{KL}\left(\pi_\theta^{\mathrm{RL}}(y \mid x) \parallel\pi^{\mathrm{SFT}}(y \mid x)\right) = \sum_{t} \left( \log\pi_\theta^{\mathrm{RL}}(y_t \mid x, y_{&lt;t}) - \log\pi^{\mathrm{SFT}}(y_t \mid x, y_{&lt;t}) \right)\]</span> 具体步骤如下：</p><ol type="1"><li>使用策略模型进行<strong>采样</strong>：对输入 <spanclass="math inline">\(x\)</span> 生成的输出 <spanclass="math inline">\(y\)</span>。</li><li>计算<strong>策略模型的对数概率</strong>（log_probs）：计算策略模型在每个时间步（Token）的对数概率。</li><li>计算<strong>参考模型的对数概率</strong>（ref_log_probs）：对<strong>相同的输入输出</strong>，使用参考模型计算逐Token 的对数概率。</li><li>计算 KL散度：通过<strong>对数概率之差累加</strong>得到策略模型与参考模型之间的KL 散度。</li><li>整合奖励：Token 级的 KL 散度与奖励模型在输出 <spanclass="math inline">\(y\)</span> 上序列级 Reward 组合，得到 <spanclass="math inline">\(r_{\text {total}}\)</span>。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_kl</span>(<span class="hljs-params">log_probs, log_probs_base</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    log_probs: (batch_size, seq_len)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    kl = log_probs.<span class="hljs-built_in">float</span>() - log_probs_base.<span class="hljs-built_in">float</span>()   <span class="hljs-comment"># log(pi) - log(pi_ref)</span><br></code></pre></td></tr></table></figure><p>在实际使用的时候，KL散度还有一种<strong>非负、无偏、低方差</strong>的估计方式：</p><p><span class="math display">\[\mathrm{KL}\left(\pi_\theta^{\mathrm{RL}}(y \mid x) \parallel\pi^{\mathrm{SFT}}(y \mid x)\right) = \frac{\pi^{\mathrm{SFT}}(y_t \midx, y_{&lt;t})}{\pi_\theta^{\mathrm{RL}}(y_t \mid x, y_{&lt;t})} - \log\frac{\pi^{\mathrm{SFT}}(y_t \mid x,y_{&lt;t})}{\pi_\theta^{\mathrm{RL}}(y_t \mid x, y_{&lt;t})} - 1\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_approx_kl</span>(<span class="hljs-params">log_probs, log_probs_base</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    log_probs: (batch_size, seq_len)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    log_ratio = log_probs.<span class="hljs-built_in">float</span>() - log_probs_base.<span class="hljs-built_in">float</span>()<span class="hljs-comment"># 保持原来的计算</span><br>    log_ratio = -log_ratio <span class="hljs-comment"># 注意公式里是反过来的</span><br>    kl = log_ratio.exp() - <span class="hljs-number">1</span> - log_ratio<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>手撕经典算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL 学习笔记 #13 直接偏好优化（DPO）理论</title>
    <link href="/RL-Note-13.html"/>
    <url>/RL-Note-13.html</url>
    
    <content type="html"><![CDATA[<p>以 PPO 为代表的经典 RLHF 流程中，存在两个显著缺陷：</p><ol type="1"><li><strong>奖励模型偏差</strong>：PPO 需要训练一个RM，奖励建模误差可能会通过 RL 传导到策略网络。</li><li><strong>训练流程复杂</strong>：PPO 需要交替训练 Actor 和 Critic网络，加载多个模型，涉及大量超参数调整（如 clip 阈值、GAE参数等），导致实现成本极高。</li></ol><p>DPO（Direct PreferenceOptimization）的提出正是为了解决这些问题。其核心思想是<strong>绕过奖励模型建模，直接利用人类偏好数据优化策略网络</strong>。这种方法的革命性在于发现：语言模型本身可以视为一个隐式的奖励函数，通过数学变换可以直接建立策略与偏好的映射关系。</p><p>附上一些参考资料：</p><ul><li><p>猛猿老师的文章：<ahref="https://mp.weixin.qq.com/s/c_qkRj7cxzHGIR_3oqnkHA">人人都能看懂的DPO 数学原理</a></p></li><li><p>DPO 论文：<a href="http://arxiv.org/abs/2305.18290">DirectPreference Optimization: Your Language Model is Secretly a RewardModel</a></p></li></ul><h2 id="dpo-理论推导">DPO 理论推导</h2><p>给定一个预训练语言模型 <spanclass="math inline">\(\pi_{\text{ref}}(y\midx)\)</span>，以及一个人类偏好数据集 <spanclass="math inline">\(\mathcal{D} = \{(x, y_w, y_l)\}\)</span>，其中<span class="math inline">\(y_w\)</span> 是人类偏好的回答，<spanclass="math inline">\(y_l\)</span>是人类不偏好的回答。我们的目标是找到一个新的策略 <spanclass="math inline">\(\pi_{\theta}(y\mid x)\)</span>来最大化与人类偏好的一致性。</p><p>不管是 PPO 还是DPO，我们的优化目标通常是一个<strong>正则化（Behavior-Regularized）的 RL问题</strong>： <span class="math display">\[\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim\pi_\theta(y|x)}\left[r(x, y) - \beta\mathbb{D}_{\mathrm{KL}}(\pi_\theta(y\mid x) \parallel\pi_{\text{ref}}(y\mid x))\right]\]</span> 其中： - <span class="math inline">\(r(x, y)\)</span>是奖励函数，评估响应 <span class="math inline">\(y\)</span> 对于输入<span class="math inline">\(x\)</span> 的质量； - <spanclass="math inline">\(\beta\)</span> 是控制 KL 散度惩罚强度的超参数； -<span class="math inline">\(\mathbb{D}_{\mathrm{KL}}\)</span> 是 KL散度，防止优化后的策略 <span class="math inline">\(\pi_\theta\)</span>与参考策略 <span class="math inline">\(\pi_{\text{ref}}\)</span>相差太远，两个模型都初始化自 SFT 模型。</p><h3 id="最优奖励与最优策略模型">最优奖励与最优策略模型</h3><p>现在我们尽可能去<strong>简化这个优化目标</strong>，考虑到 KL散度的表达式： <span class="math display">\[\mathbb{D}_{\mathrm{KL}}\left[\pi(y \mid x) \| \pi_{\mathrm{ref}}(y \midx)\right]=\mathbb{E}_{y \sim \pi(y \mid x)}\left[\log \frac{\pi(y \midx)}{\pi_{\mathrm{ref}}(y \mid x)}\right]\]</span> 优化目标可以改写为： <span class="math display">\[\max _\pi \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \midx)}\left[r(x, y)-\beta \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y\mid x)}\right]\]</span> 除以 <span class="math inline">\(\beta\)</span>，再取反，因此<span class="math inline">\(\max\)</span> 改为 <spanclass="math inline">\(\min\)</span> 得到： <span class="math display">\[\min _\pi \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \midx)}\left[\log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \midx)}-\frac{1}{\beta} r(x, y)\right]\]</span> 将 <span class="math inline">\(\frac{1}{\beta} r(x,y)\)</span> 改为 <span class="math inline">\(\log\exp\left(\frac{1}{\beta} r(x, y)\right)\)</span> 后可以将两项合并：<span class="math display">\[\min _\pi \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \midx)}\left[\log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)}\right]\]</span>根据这里的分母，我们人为定义一个<strong>配分函数</strong>（PartitionFunction）： <span class="math display">\[Z(x) = \sum_{y} \pi_{\mathrm{ref}}(y \mid x) \exp \left(\frac{1}{\beta}r(x, y)\right)\]</span> 其中 <span class="math inline">\(\sum_{y}\)</span>表示在给定某个 Prompt <span class="math inline">\(x\)</span>的前提下，参考模型可能生成的所有 <spanclass="math inline">\(y\)</span>，因此有 <spanclass="math inline">\(\sum_{y} \pi_{\mathrm{ref}}(y \midx)=1\)</span>。由这个定义可知，<span class="math inline">\(Z(x)\)</span>是<strong>关于 <span class="math inline">\(x\)</span>的函数</strong>，且和我们准备优化的模型 <spanclass="math inline">\(\pi\)</span> 没有关系。</p><blockquote><p>暂时可以理解这个式子是<strong>为了跟分母对齐而凑出来的归一化因子</strong>，但实际上这种配分函数在逆强化学习、离线强化学习中大量存在，涉及到比较麻烦的理论推导，这里就不展开解释了。</p></blockquote><p>为了引入 <spanclass="math inline">\(Z(x)\)</span>，将优化目标进一步变形为： <spanclass="math display">\[\min _\pi \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \midx)}\left[\log \frac{\pi(y \mid x)}{\frac{1}{Z(x)} \pi_{\mathrm{ref}}(y\mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)}-\log Z(x)\right]\]</span> 对于式子的左半部分，可以发现它非常像 <strong>KL散度的形式</strong>（即衡量了两个分布之间的相似性），因此我们将分母再定义为<span class="math inline">\(\pi^*(y \mid x)\)</span>： <spanclass="math display">\[\pi^*(y \mid x)=\frac{1}{Z(x)} \pi_{\mathrm{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)\]</span> 可以证明上述 <span class="math inline">\(\pi^*\)</span>的满足<strong>概率分布大于 <span class="math inline">\(0\)</span> 且和为<span class="math inline">\(1\)</span></strong>的定义，是一个合法的概率分布——这也是我们将 <spanclass="math inline">\(Z(x)\)</span> 定义为这个形式的原因！</p><p>此时，将期望移入括号，优化目标进一步改写为： <spanclass="math display">\[\begin{aligned}&amp; \min _\pi \mathbb{E}_{x \sim \mathcal{D}}\left[\mathbb{E}_{y \sim\pi(y \mid x)}\left[\log \frac{\pi(y \mid x)}{\pi^*(y \midx)}\right]-\log Z(x)\right] \\&amp; = \min _\pi \mathbb{E}_{x \sim\mathcal{D}}\left[\mathbb{D}_{\mathrm{KL}}\left(\pi(y \mid x) \parallel\pi^*(y \mid x)\right)-\log Z(x)\right]\end{aligned}\]</span> 前面我们说过和 <span class="math inline">\(Z(x)\)</span>和我们准备优化的模型 <span class="math inline">\(\pi\)</span>没有关系，所以在 <span class="math inline">\(\min_{\pi}\)</span>中可以<strong>将第二项忽略</strong>。那么现在我们只需要关心 KL散度这一项。我们知道 KL散度在<strong>两个分布完全相等时达到最小</strong>，由此我们可以写出模型的显式解：<span class="math display">\[\pi(y \mid x)=\pi^*(y \mid x)=\frac{1}{Z(x)} \pi_{\mathrm{ref}}(y \midx) \exp \left(\frac{1}{\beta} r(x, y)\right)\]</span> 因为以上推导都是在<strong>假设我们有一个固定的奖励函数 <spanclass="math inline">\(r\)</span>的基础上进行的</strong>，所以我们可以加一个下标来强调这一点： <spanclass="math display">\[\pi_r(y \mid x)=\frac{1}{Z(x)} \pi_{\mathrm{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)\]</span> 可是，在正常的对齐训练中，这个奖励函数 <spanclass="math inline">\(r(x, y)\)</span>可不是任意的，它是我们先用数据训练出来的<strong>最优奖励模型</strong>，然后在这个最优奖励模型的基础上，我们再通过训练去找到<strong>最优策略模型</strong>。最优奖励模型和基于它训练出的最优的对齐模型依然满足上式，我们分别设它们为<span class="math inline">\(r^*(x, y)\)</span> 和 <spanclass="math inline">\(\pi^*(y \mid x)\)</span>，则有： <spanclass="math display">\[\pi^*(y \mid x)=\frac{1}{Z(x)} \pi_{\mathrm{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r^*(x, y)\right)\]</span></p><h3 id="建立策略到偏好的映射">建立策略到偏好的映射</h3><p>虽然我们现在得到了策略模型的显式解 <spanclass="math inline">\(\pi_r(y \midx)\)</span>，但是我们却很难直接利用起这个显式解形式，原因如下：</p><ul><li><span class="math inline">\(Z(x)\)</span> 的值很难估计。根据 <spanclass="math inline">\(Z(x)\)</span> 的形式可知，想要估计它，就需要对一个Prompt <span class="math inline">\(x\)</span> 采样足够多的回答 <spanclass="math inline">\(y\)</span>。这个代价是十分昂贵的。</li><li>回顾最开始我们的目标：<strong>省略训练奖励模型这个步骤，直接优化策略模型</strong>。而目前我们得到的的显式解仍然需要一个确定的奖励函数<span class="math inline">\(r\)</span>。</li></ul><p>所以现在我们继续来迭代。考虑到第二个原因，我们可以先从 <spanclass="math inline">\(\pi^*\)</span> 的显式解中推出最优奖励模型 <spanclass="math inline">\(r^*\)</span> 的形式： <spanclass="math display">\[r^*(x, y)=\beta \log \frac{\pi^*(y \mid x)}{\pi_{\mathrm{ref}}(y \midx)}+\beta \log Z(x)\]</span> 既然有这层映射关系，我们就可以直接<strong>将 <spanclass="math inline">\(\pi^*\)</span>代入到奖励模型的训练优化目标</strong>中去，从而实现「<strong>明面上训练奖励模型，实际上却一步到位训练出了对齐模型</strong>」！</p><p>所以现在回到奖励模型的训练优化目标——在介绍 PPO的时候我们曾提过，我们通常使用「偏好排序」这种数据标注方式来对奖励模型进行训练，一般有两种偏好排序方法：</p><ul><li>只生成 <span class="math inline">\(2\)</span>个回答，<code>&lt;prompt x, chosen y1, reject y2&gt;</code>，即对于一个Prompt，我们生成 2个回答，人工对这两个回答的偏好做排序，我们希望奖励模型对 Chosen回答的给分尽量高于对 Reject 回答的给分。</li><li>生成 <span class="math inline">\(K &gt; 2\)</span>个回答，<code>&lt;prompt x, y1, ..., yK&gt;</code>，假设人工标注后的偏好排序组合为<span class="math inline">\(\tau\)</span>，那么我们希望奖励模型对 <spanclass="math inline">\(\tau\)</span>这个排序的总得分要大于其余任何可能的偏好排序。</li></ul><p>在某些框架（比如 InstructGPT）的训练中，当生成的回答 <spanclass="math inline">\(&gt;2\)</span>个时，会将回答<strong>两两配对</strong>，这样就可以和只生成 <spanclass="math inline">\(2\)</span>个回答时的目标函数做统一。但在更一般的场景中，对于 <spanclass="math inline">\(&gt;2\)</span>个回答的场景，我们是把<strong>每一种可能的回答偏好排序当成一个整体数据</strong>进行处理的，然后希望真值排序的得分最高，DPO的推导就是基于后者进行的。</p><p>接下来，我们也对 <span class="math inline">\(K=2\)</span> 和 <spanclass="math inline">\(K&gt;2\)</span> 这两种情况下 DPO最终的目标函数形式进行推导。</p><h3 id="bradley-terry-bt-偏好模型">Bradley-Terry (BT) 偏好模型</h3><p>我们希望一个好的奖励模型对 Chosen 回答的给分尽量高于对 Reject回答的给分，这其实表示了一种相对偏好。1952 年提出的统计模型Bradley-Terry (BT) Model可用于<strong>分析成对数据间的相对优势或者偏好</strong>，其被广泛应用于体育比赛分析、市场研究等场景。</p><p>在该模型下，我们假设有两个回答 <spanclass="math inline">\(y_w\)</span> 和 <spanclass="math inline">\(y_l\)</span>，<strong>人类偏好 <spanclass="math inline">\(y_w\)</span> 而非 <spanclass="math inline">\(y_l\)</span> 的概率</strong>可以表示为： <spanclass="math display">\[P(y_w \succ y_l\mid x) = \frac{\exp[r(x, y_w)]}{\exp[r(x, y_w)] +\exp[r(x, y_l)]}\]</span> 现在，我们希望对于整个标注数据集 <spanclass="math inline">\(\mathcal{D} = \{(x, y_w, y_l)\}\)</span>，Chosen打败 Reject的期望概率尽量大，所以奖励函数的总体优化目标可以设计成<strong>负对数似然</strong>（NegativeLog-Likelihood，NLL）： <span class="math display">\[L_R\left(r_\phi, D\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \simD}\left[\log P\left(y_w \succ y_l \mid x\right)\right]\]</span> 将 <span class="math inline">\(P\)</span>的具体形式代入化简，则有： <span class="math display">\[\begin{aligned}L_R\left(r_\phi, D\right) &amp; =-\mathbb{E}_{\left(x, y_w, y_l\right)\sim D}\left[\log P\left(y_w \succ y_l \mid x\right)\right] \\&amp; =-\mathbb{E}_{\left(x, y_w, y_l\right) \sim D}\left[\log\frac{\exp \left[r\left(x, y_w\right)\right]}{\exp \left[r\left(x,y_w\right)\right]+\exp \left[r\left(x, y_l\right)\right]}\right] \\&amp; =-\mathbb{E}_{\left(x, y_w, y_l\right) \sim D}\left[\log\frac{1}{1+\frac{\exp \left[r\left(x, y_l\right)\right]}{\exp\left[r\left(x, y_w\right)\right]}}\right] \\&amp; =-\mathbb{E}_{\left(x, y_w, y_l\right) \sim D}\left[\log\frac{1}{1+e^{-\left(r\left(x, y_w\right)-r\left(x,y_l\right)\right)}}\right] \\&amp; =-\mathbb{E}_{\left(x, y_w, y_l\right) \sim D}\left[\log\sigma\left(r\left(x, y_w\right)-r\left(x, y_l\right)\right)\right]\end{aligned}\]</span></p><p>其中 <span class="math inline">\(\sigma\)</span> 是 sigmoid函数。这时我们震惊地发现：最后一个式子就是 OpenAI 2020 年论文《<ahref="https://arxiv.org/abs/2009.01325">Learning to Summarize with HumanFeedback</a>》介绍 PPO 的<strong>成对排序损失（Pairwise RankingLoss）</strong>： <span class="math display">\[\mathcal{L}^{\text{RM}} = \log \sigma(r(x, y_w) - r(x, y_l))\]</span></p><p>这个公式表明，<strong>两个回答之间的相对偏好与它们奖励值的差异成正比</strong>。</p><p>现在，我们将前面求出的最优奖励模型 <spanclass="math inline">\(r^*\)</span> 代入 Bradley-Terry偏好模型，可以得到： <span class="math display">\[\begin{aligned}P(y_w \succ y_l \mid x) &amp; = \sigma(r^*(x, y_w) - r^*(x, y_l))\\&amp;= \sigma\left(\beta \log \frac{\pi^*(y_w\midx)}{\pi_{\text{ref}}(y_w\mid x)} + \beta \log Z(x) - \beta \log\frac{\pi^*(y_l\mid x)}{\pi_{\text{ref}}(y_l\mid x)} - \beta \logZ(x)\right)\\\end{aligned}\]</span> 注意到 <span class="math inline">\(\log Z(x)\)</span>项相互抵消，我们得到： <span class="math display">\[P^*(y_w \succ y_l \mid x) = \sigma\left(\beta \log \frac{\pi^*(y_w\midx)}{\pi_{\text{ref}}(y_w\mid x)} - \beta \log \frac{\pi^*(y_l\midx)}{\pi_{\text{ref}}(y_l\mid x)}\right)\]</span></p><p>到这里，我们已经把训<strong>练奖励模型的目标函数转化成只和策略模型<span class="math inline">\(\pi\)</span>相关</strong>了！也就是说，我们可以一步到位，绕开训练奖励模型的过程，直接用标注好的成对偏好数据，像SFT 一样直接训练策略模型了。</p><h3 id="plackett-luce-偏好模型">Plackett-Luce 偏好模型</h3><p>现在，如果我不想使用成对偏好数据，而是对于一个 Prompt，我给 <spanclass="math inline">\(K&gt;2\)</span>个回答进行偏好排序，在这种方式下，我要怎么设计奖励模型优化目标呢？</p><p>类似于 BT 模型的一个 Plackett-Luce模型可以<strong>对多数据的偏好排序进行建模</strong>：假设 <spanclass="math inline">\(\tau\)</span>为人工标注出的真值排序，我们希望其能够打败其余任何一种可能的偏好排序，这一概率可以表示成：<span class="math display">\[P\left(\tau \mid y_1, \ldots, y_K, x\right)=\prod_{k=1}^K \frac{\exp\left[r\left(x, y_{\tau(k)}\right)\right]}{\sum_{j=k}^K \exp\left[r\left(x, y_{\tau(j)}\right)\right]}\]</span> 其中，<span class="math inline">\(\tau_k\)</span>表示人类标注的偏好序列 <span class="math inline">\(\tau\)</span> 中的第<span class="math inline">\(k\)</span> 个数据，序列 <spanclass="math inline">\(\tau\)</span> 中的 <spanclass="math inline">\(K\)</span>个回答已经按照偏好从高到低进行排序。</p><p>这个公式从直观上理解的话：</p><ul><li><p>对于真值 <span class="math inline">\(\tau\)</span> 中的第一个回答<span class="math inline">\(\tau_1\)</span>，它是人工标注的偏好最高的数据，我们当然希望它的得分在 <spanclass="math inline">\(\tau_1 \sim \tau_K\)</span> 中占大头；</p></li><li><p>对于真值 <span class="math inline">\(\tau\)</span> 中的第一个回答<span class="math inline">\(\tau_2\)</span> ，我们当然希望它的得分在<span class="math inline">\(\tau_2 \sim \tau_K\)</span>中占大头；</p></li><li><p>对于真值 <span class="math inline">\(\tau\)</span> 中的第一个回答<span class="math inline">\(\tau_3\)</span> ，我们当然希望它的得分在<span class="math inline">\(\tau_3 \sim \tau_K\)</span>中占大头；</p></li><li><p>以此类推，则不难理解上述在 Plackett-Luce模型下概率的表达方式。</p></li></ul><p>同样，我们把最优奖励函数 <span class="math inline">\(r^*(x,y)\)</span> 代入上面的 P 中，则有：</p><p><span class="math display">\[P\left(\tau \mid y_1, \ldots, y_K, x\right)=\prod_{k=1}^K \frac{\exp\left[r^*\left(x, y_{\tau(k)}\right)\right]}{\sum_{j=k}^K \exp\left[r^*\left(x, y_{\tau(j)}\right)\right]}\]</span></p><p>然后我们再用 <span class="math inline">\(\pi^*\)</span> 去表示 <spanclass="math inline">\(r^*\)</span> ，则有（这里我们可以把 <spanclass="math inline">\(Z(x)\)</span>省略掉，因为正如前文所说，它和对齐模型 <spanclass="math inline">\(\pi\)</span> 没有关系）：</p><p><span class="math display">\[P^*\left(\tau \mid y_1, \ldots, y_K, x\right)=\prod_{k=1}^K \frac{\exp\left(\beta \log \frac{\pi^*\left(y_{\tau(k)} \midx\right)}{\pi_{\mathrm{ref}}\left(y_{\tau(k)} \midx\right)}\right)}{\sum_{j=k}^K \exp \left(\beta \log\frac{\pi^*\left(y_{\tau(j)} \midx\right)}{\pi_{\mathrm{ref}}\left(y_{\tau(j)} \mid x\right)}\right)}\]</span></p><h2 id="dpo-实现细节">DPO 实现细节</h2><h3 id="损失函数推导">损失函数推导</h3><p>通过极大似然估计，我们将 BT 模型的优化目标转化为二元交叉熵损失：<span class="math display">\[\mathcal{L}_{\text{DPO}}(\theta) = -\mathbb{E}_{(x,y_w,y_l) \sim D}\left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w\midx)}{\pi_{\text{ref}}(y_w\mid x)} - \beta \log \frac{\pi_\theta(y_l\midx)}{\pi_{\text{ref}}(y_l\mid x)} \right) \right]\]</span> 这个损失函数具有三个关键特性：</p><ol type="1"><li><strong>隐式奖励建模</strong>：通过策略网络与参考策略的比值隐式学习奖励函数，避免显式奖励模型的偏差；</li><li><strong>数据高效性</strong>：直接利用成对偏好数据，无需从策略网络中采样（对比PPO 需要 on-policy 采样）；</li><li><strong>动态权重调节</strong>：<spanclass="math inline">\(\beta\)</span>控制策略更新幅度，防止过度偏离参考策略（类似 PPO 中的 KL 约束）。</li></ol><p>链式法则求梯度可以得到： <span class="math display">\[\nabla_\theta \mathcal{L}_{\mathrm{DPO}}\left(\theta \right)=-\beta \mathbb{E}_{\left(x, y_w, y_l\right) \simD}[\underbrace{\sigma\left(\hat{r}_\theta\left(x,y_l\right)-\hat{r}_\theta\left(x, y_w\right)\right)}_{\text {higherweight when reward estimate is wrong }}[\underbrace{\nabla_\theta \log\pi\left(y_w \mid x\right)}_{\text {increase likelihood of }y_w}-\underbrace{\nabla_\theta \log \pi\left(y_l \mid x\right)}_{\text{decrease likelihood of } y_l}]]\]</span> 其中 <span class="math inline">\(\hat{r}_\theta\left(x,y\right)=\beta \log \frac{\pi_\theta(y\mid x)}{\pi_{\text{ref}}(y\midx)}\)</span>，注意这一项是 <spanclass="math inline">\(l-w\)</span>，表示当隐式奖励估计错误时，会用更大的幅度去纠正策略模型。</p><p>有了这个损失函数后，理想的结果就是，Chosen 回答概率上升，Reject回答概率下降，从而达到跟人类偏好对齐的自的。当然实践过程中可能会到<strong>Chosen 和 Reject 的概率都下降的情况</strong>，毕竟 DPO只约束差值。之后会介绍一些改进工作，包含了相应的优化。</p><h3 id="dpo-loss-简易实现">DPO Loss 简易实现</h3><p>DPO 原论文附录给出了 DPO Loss 的简易实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dpo_loss</span>(<span class="hljs-params">pi_logps, ref_logps, yw_idxs, yl_idxs, beta</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算DPO（Direct Preference Optimization）损失和奖励</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    参数说明：</span><br><span class="hljs-string">    pi_logps:   策略模型（待优化模型）的log概率，形状为 (B,)，B是批量大小</span><br><span class="hljs-string">    ref_logps:  参考模型（固定模型）的log概率，形状为 (B,)</span><br><span class="hljs-string">    yw_idxs:    偏好完成的索引（优选答案），形状为 (T,)，每个元素在 [0, B-1] 之间</span><br><span class="hljs-string">    yl_idxs:    非偏好完成的索引（非优选答案），形状为 (T,)</span><br><span class="hljs-string">    beta:       控制KL惩罚强度的温度参数</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    每个 (yw_idxs[i], yl_idxs[i]) 对表示一个偏好对</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <br>    <span class="hljs-comment"># 从策略模型中提取优选和非优选的 log 概率</span><br>    <span class="hljs-comment"># 结果形状均为 (T,)</span><br>    pi_yw_logps = pi_logps[yw_idxs]  <span class="hljs-comment"># 策略模型对优选答案的 log 概率</span><br>    pi_yl_logps = pi_logps[yl_idxs]  <span class="hljs-comment"># 策略模型对非优选答案的 log 概率</span><br>    <br>    <span class="hljs-comment"># 从参考模型中提取对应的 log 概率</span><br>    ref_yw_logps = ref_logps[yw_idxs]  <span class="hljs-comment"># 参考模型对优选答案的 log 概率</span><br>    ref_yl_logps = ref_logps[yl_idxs]  <span class="hljs-comment"># 参考模型对非优选答案的 log 概率</span><br>    <br>    <span class="hljs-comment"># 计算策略模型和参考模型的 log 概率比（logits 差）</span><br>    pi_logratios = pi_yw_logps - pi_yl_logps  <span class="hljs-comment"># 策略模型的优选 vs 非优选概率比的对数</span><br>    ref_logratios = ref_yw_logps - ref_yl_logps  <span class="hljs-comment"># 参考模型的对应概率比</span><br>    <br>    <span class="hljs-comment"># 计算DPO损失（核心公式）</span><br>    <span class="hljs-comment"># 公式等效于 -log σ(beta * (策略比值 - 参考比值))</span><br>    losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))<br>    <br>    <span class="hljs-comment"># 计算奖励信号（用于监控训练过程，不影响梯度）</span><br>    <span class="hljs-comment"># 奖励 = beta * (策略模型logp - 参考模型logp)</span><br>    <span class="hljs-comment"># detach() 使参考模型的梯度不反向传播</span><br>    rewards = beta * (pi_logps - ref_logps).detach()<br>    <br>    <span class="hljs-keyword">return</span> losses, rewards<br></code></pre></td></tr></table></figure><h2 id="讨论online-vs.-offlineon-policy-vs.-off-policy">讨论：Online vs.Offline，On-Policy vs. Off-Policy</h2><p>引用知乎上 <a href="https://zhuanlan.zhihu.com/p/688806682"><spanclass="citation" data-cites="Wei">@Wei</span> Xiong</a> 的说法：</p><blockquote><p>我们用 Offline指代从一个给定的偏好数据集中学习，并且在学习中，我们无法进一步让 Human给出偏好信号，而相应的 Online 指的是我们可以在训练过程中让 Human为我们标数据。</p><p><strong>换言之，区分 online / offline 最关键的在于 preference signal采集的模式不同。</strong>因此，下面的这些算法都是 offline 的：</p><ul><li>DPO 直接在给定的数据集上进行偏好学习：offline</li><li>我们从一个给定的数据集训练得到一个奖励函数，并使用 PPO优化这个奖励函数：offline</li><li>我们从一个给定的数据集训练得到一个奖励函数，并使用 rejectionsampling finetuning 优化：offline</li></ul><p>一个相关的概念是 on-policy 与 off-policy，我们用 on-policy指代那些采集数据的策略与要改进的策略是同一个策略的情况，而 off-policy指代那些使用某个行为策略采集数据，但是用以改进另一个策略的算法。</p><p><strong>换言之，区分 on-policy / off-policy 最关键的在于 responses采集的模型不同。</strong>我们给出以下例子：</p><ul><li>DPO 是 off-policy 的</li><li>我们从一个给定的数据集训练得到一个奖励函数，并使用 PPO优化这个奖励函数：on-policy</li></ul></blockquote><p>总而言之，对于 DPO 而言，没有争议，就是 offline、off-policy算法。因为他<strong>使用离线的偏好数据集</strong>（不与环境交互，甚至没有采样环节）。</p><p>而对于 PPO 就比较灵活： - 如果从偏好信号采集的角度，<strong>PPO 在System level 上是不与环境交互的</strong>，只从固定的 RM 获取，所以认为是offline【主流观点】；但有些人也会认为 PPO 是实时从 RM处获取偏好，所以也是 online。 -从采样和更新策略角度，对于整个训练过程，<strong>PPO使用一个策略模型采样并更新之</strong>，所以是on-policy【主流观点】；但是如果细到每个 step，由于 rollout 模型跟training 模型不一定一致（存在延迟），PPO 也可以说是 off-policy。</p><h2 id="讨论dpo-的局限性">讨论：DPO 的局限性</h2><blockquote><p>参考资料：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/11913305485">DPO vsPPO：深度解读谁是 LLM Alignment 的未来 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/688806682">为什么我们应该做online RLHF/DPO? - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/1082394115">DPO 的局限性 -知乎</a></li><li>论文：Is DPO Superior to PPO for LLM Alignment? A ComprehensiveStudy.</li><li>论文：Towards Analyzing and Understanding the Limitations of DPO: ATheoretical Perspective.</li></ul></blockquote><p>以下截取一些简单易懂的局限性进行分析：</p><ol type="1"><li><strong>训练目标不一致</strong>：DPO 的训练目标实际上是 RM的训练目标，只不过将其迁移到了策略模型上。而 RM的训练目标其实是让模型拥有 Evaluate 能力（拉开好坏响应的Margin），但我们希望策略模型拥有的是 Generate 能力 ——「<strong>模型的Evaluate 能力和 Generate能力到底是不是相互促进的？</strong>」这个前提成立与否，才是 DPO有效的关键。在训练 DPO 时，基本都遇到过 good_sentence 和 bad_sentence 的Loss 都上升的尴尬现象，这就表示模型的 Generate能力不一定同步上升了，往往需要魔改 Loss 才能有效。</li><li><strong>缺少在线采样</strong>：PPO 通过在线采样<strong>将 RM 的Evaluate 能力转化为 Generate 能力</strong>让策略模型学习。这种 Online方式让模型能够探索出更多的结果并获取来自真实世界的反馈，也能有更大的Generate 空间来优化。因此，一些 Online-DPO 迭代策略，就是<strong>通过用DPO 采样多个 path后构造偏好对</strong>，从而引入采样和探索机制来增大策略空间。</li><li><strong>场景适应性限制</strong>：DPO在生成多样性要求高的任务（如创意写作）中表现较弱，因为其偏好学习偏向「安全回答」。此外，其标注方式也注定其只能用于<strong>偏好关系简单且静态</strong>的任务（如文本风格迁移、简单对话生成）。</li></ol>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL 学习笔记 #12 OpenRLHF-PPO 实践</title>
    <link href="/RL-Note-12.html"/>
    <url>/RL-Note-12.html</url>
    
    <content type="html"><![CDATA[<p>在本节中，我们将深入探讨 <ahref="https://github.com/OpenRLHF/OpenRLHF">OpenRLHF</a>框架的整体结构和核心组件，了解 PPO 在 RLHF 中的实际应用。以下是 OpenRLHF整体的代码架构，我们将围绕 PPO 有关的训练脚本、训练器Trainer、损失函数、模型架构展开讨论。部分内容参考仓库作者的博客：<ahref="https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361">PPO实现技巧</a>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c">OpenRLHF<br> ├── openrlhf<br> │   ├── cli<span class="hljs-comment">// 训练入口函数</span><br> │   │   ├── ...<br> │   │   └── train_ppo.py<br> │   ├── datasets<span class="hljs-comment">// 数据集处理相关</span><br> │   ├── models<span class="hljs-comment">// 定义模型、loss相关</span><br> │   │   ├── __init__.py<br> │   │   ├── actor.py<span class="hljs-comment">// 定义 actor model </span><br> │   │   ├── loss.py<span class="hljs-comment">// 定义不同的 loss 函数</span><br> │   │   ├── model.py<span class="hljs-comment">// 定义 critic model 和 reward model</span><br> │   │   └── utils.py<br> │   ├── trainer<br> │   │   └── ppo_trainer.py<span class="hljs-comment">// 定义 ppo 训练方法</span><br> │   └── utils<span class="hljs-comment">// 工具类、函数定义</span><br> │   │   └── remote_rm_utils.py <span class="hljs-comment">// 可以自定义 reward model，通过 http 部署</span><br> └── examples<span class="hljs-comment">// 示例启动脚本</span><br>     └── scripts<br> ├── ...<br>         └── train_ppo_llama.sh<br></code></pre></td></tr></table></figure><h2 id="主流程入口-train_ppo.py">主流程入口<code>train_ppo.py</code></h2><p>在 OpenRLHF 框架中，主要的 PPO 训练流程在<code>cli/train_ppo.py</code>脚本中实现。该脚本负责模型的初始化、数据加载、训练过程控制等核心功能。</p><h3 id="参数列表">参数列表</h3><p>一、Checkpoint 相关参数</p><table><thead><tr class="header"><th style="text-align: left;">参数名</th><th style="text-align: center;">类型（默认值）</th><th style="text-align: left;">补充解释</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><code>save_path</code></td><td style="text-align: center;">str (<code>./ckpt</code>)</td><td style="text-align: left;">模型保存路径，用于存储训练后的 Actor模型。</td></tr><tr class="even"><td style="text-align: left;"><code>save_steps</code></td><td style="text-align: center;">int (-1)</td><td style="text-align: left;">保存间隔步数，-1表示按训练周期（episode）保存。</td></tr><tr class="odd"><td style="text-align: left;"><code>load_checkpoint</code></td><td style="text-align: center;">bool (False)</td><tdstyle="text-align: left;">是否从检查点恢复训练，用于长时间训练任务或调试时中断后继续训练的场景。</td></tr><tr class="even"><td style="text-align: left;"><code>max_ckpt_num</code></td><td style="text-align: center;">int (3)</td><tdstyle="text-align: left;">最大保留检查点数量，防止存储空间溢出。</td></tr></tbody></table><p>二、PPO 核心参数</p><table><thead><tr class="header"><th style="text-align: left;">参数名</th><th style="text-align: left;">类型（默认值）</th><th style="text-align: left;">补充解释</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><strong>策略优化参数</strong></td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;"><code>num_episodes</code></td><td style="text-align: left;">int (1)</td><td style="text-align: left;">训练轮次数量，控制整个 PPO流程的执行次数。</td></tr><tr class="odd"><td style="text-align: left;"><code>eps_clip</code></td><td style="text-align: left;">float (0.2)</td><td style="text-align: left;">PPO的核心超参数，限制新旧策略概率比的剪切范围。越大允许策略变化越大，但可能降低稳定性。</td></tr><tr class="even"><td style="text-align: left;"><code>value_clip</code></td><td style="text-align: left;">float (0.2)</td><td style="text-align: left;">Critic 值函数更新的剪切范围，防止 Critic网络更新过快。</td></tr><tr class="odd"><td style="text-align: left;"><code>init_kl_coef</code></td><td style="text-align: left;">float (0.01)</td><td style="text-align: left;">KL散度惩罚项的初始系数，约束策略更新幅度，总损失为<code>policy_loss + value_loss + kl_coef * KL_divergence</code>。</td></tr><tr class="even"><td style="text-align: left;"><code>kl_target</code></td><td style="text-align: left;">float (None)</td><td style="text-align: left;">若设置，KL 系数会动态调整以逼近目标值（如0.01）。自适应 KL 控制可平衡探索与稳定性。</td></tr><tr class="odd"><td style="text-align: left;"><strong>优势估计</strong></td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;"><code>gamma</code></td><td style="text-align: left;">float (1)</td><td style="text-align: left;">折扣因子。1 表示无折扣，接近 0更关注即时奖励。</td></tr><tr class="odd"><td style="text-align: left;"><code>lambd</code></td><td style="text-align: left;">float (0.95)</td><td style="text-align: left;">GAE 的 λ 参数，平衡偏差与方差。1等价于蒙特卡洛方法（高方差），0 仅用一步 TD 误差（高偏差）。</td></tr><tr class="even"><td style="text-align: left;"><code>advantage_estimator</code></td><td style="text-align: left;">str (gae)</td><td style="text-align: left;">优势估计方法。<code>gae</code>：基于Critic 的广义优势估计（需Critic网络）；<code>reinforce</code>：蒙特卡洛策略梯度（无Critic，高方差）；<code>rloo</code>：基于多个样本的相对奖励比较（需n_samples_per_prompt&gt;1）</td></tr><tr class="odd"><td style="text-align: left;"><strong>混合训练</strong></td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;"><code>ptx_coef</code></td><td style="text-align: left;">float (0.05)</td><td style="text-align: left;">预训练损失的权重系数，公式为<code>total_loss = ppo_loss + ptx_coef * pretrain_loss</code>。</td></tr></tbody></table><p>三、模型配置</p><table><thead><tr class="header"><th style="text-align: left;">参数名</th><th style="text-align: left;">类型（默认值）</th><th style="text-align: left;">补充解释</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><code>pretrain</code></td><td style="text-align: left;">str (None)</td><td style="text-align: left;">Actor模型的预训练权重路径，必需参数。</td></tr><tr class="even"><td style="text-align: left;"><code>reward_pretrain</code></td><td style="text-align: left;">str (None)</td><td style="text-align: left;">奖励模型的预训练权重路径。</td></tr><tr class="odd"><td style="text-align: left;"><code>critic_pretrain</code></td><td style="text-align: left;">str (None)</td><td style="text-align: left;">Critic 模型的预训练权重路径。</td></tr><tr class="even"><td style="text-align: left;"><code>flash_attn</code></td><td style="text-align: left;">bool (False)</td><td style="text-align: left;">启用 FlashAttention-2加速注意力计算。</td></tr><tr class="odd"><td style="text-align: left;"><code>bf16</code></td><td style="text-align: left;">bool (False)</td><td style="text-align: left;">使用 bfloat16混合精度训练，降低显存消耗。</td></tr></tbody></table><p>四、优化器参数</p><table><thead><tr class="header"><th style="text-align: left;">参数名</th><th style="text-align: left;">类型（默认值）</th><th style="text-align: left;">补充解释</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><code>actor_learning_rate</code></td><td style="text-align: left;">float (1e-6)</td><td style="text-align: left;">Actor 网络学习率，通常比 Critic小一个量级。</td></tr><tr class="even"><td style="text-align: left;"><code>critic_learning_rate</code></td><td style="text-align: left;">float (9e-6)</td><td style="text-align: left;">Critic网络学习率，需快速适应价值估计。</td></tr><tr class="odd"><td style="text-align: left;"><code>adam_betas</code></td><td style="text-align: left;">float (0.9,0.95)</td><td style="text-align: left;">Adam 优化器的 β1/β2参数，控制动量衰减率。</td></tr><tr class="even"><td style="text-align: left;"><code>lr_warmup_ratio</code></td><td style="text-align: left;">float (0.03)</td><tdstyle="text-align: left;">学习率预热比例，帮助训练初期稳定收敛。</td></tr></tbody></table><p>五、数据集参数</p><table><thead><tr class="header"><th style="text-align: left;">参数名</th><th style="text-align: left;">类型（默认值）</th><th style="text-align: left;">补充解释</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><code>prompt_data</code></td><td style="text-align: left;">str (None)</td><tdstyle="text-align: left;">提示数据集路径，用于生成阶段采样提示。</td></tr><tr class="even"><td style="text-align: left;"><code>pretrain_data</code></td><td style="text-align: left;">str (None)</td><td style="text-align: left;">预训练数据集路径，用于 PTX 混合训练。</td></tr><tr class="odd"><td style="text-align: left;"><code>n_samples_per_prompt</code></td><td style="text-align: left;">int (1)</td><tdstyle="text-align: left;">每个提示生成的响应数量，影响数据多样性。</td></tr></tbody></table><p>六、生成控制</p><table><thead><tr class="header"><th style="text-align: left;">参数名</th><th style="text-align: left;">类型（默认值）</th><th style="text-align: left;">补充解释</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><code>top_p</code></td><td style="text-align: left;">float (1.0)</td><td style="text-align: left;">Nucleus采样概率阈值，控制生成多样性。</td></tr><tr class="even"><td style="text-align: left;"><code>temperature</code></td><td style="text-align: left;">float (1.0)</td><td style="text-align: left;">温度参数，调整 softmax 分布平滑度。</td></tr><tr class="odd"><td style="text-align: left;"><code>generate_max_len</code></td><td style="text-align: left;">int (1024)</td><tdstyle="text-align: left;">生成响应的最大长度，影响计算资源消耗。</td></tr></tbody></table><p>七、DeepSpeed 配置</p><table><thead><tr class="header"><th style="text-align: left;">参数名</th><th style="text-align: left;">类型（默认值）</th><th style="text-align: left;">补充解释</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><code>zero_stage</code></td><td style="text-align: left;">int (2)</td><td style="text-align: left;">ZeRO 优化阶段，1/2/3对应不同显存优化级别。</td></tr><tr class="even"><td style="text-align: left;"><code>gradient_checkpointing</code></td><td style="text-align: left;">bool (False)</td><tdstyle="text-align: left;">激活梯度检查点，用计算时间换显存空间。</td></tr></tbody></table><p>八、LoRA 参数</p><table><thead><tr class="header"><th style="text-align: left;">参数名</th><th style="text-align: left;">类型（默认值）</th><th style="text-align: left;">补充解释</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><code>lora_rank</code></td><td style="text-align: left;">int (0)</td><td style="text-align: left;">LoRA 的秩大小，决定低秩矩阵的维度。</td></tr><tr class="even"><td style="text-align: left;"><code>lora_alpha</code></td><td style="text-align: left;">int (16)</td><td style="text-align: left;">LoRA缩放系数，控制适配器对原模型的影响强度。</td></tr></tbody></table><h3 id="模型加载">模型加载</h3><p>在 <code>train.py</code> 中，初始化并加载了 Actor 模型和 Reference模型用于生成和评估回复（二者的加载方式和路径几乎一样）。通过函数<code>get_llm_for_sequence_regression</code> 加载 Reward 模型和 Critic模型。</p><p><strong>Actor Model</strong>：</p><p>在 OpenRLHF 中，Actor 模型负责生成文本动作（即生成的 token序列）并计算动作的对数概率。Actor 通过 <code>generate</code>方法调用模型的生成功能，生成文本序列后通过<code>process_sequences</code> 处理生成结果：</p><ol type="1"><li><strong>定位有效 Token</strong>：通过翻转<code>attention_mask</code>并寻找第一个非填充位置，确定每个序列的结束位置（EOS），避免中间填充干扰。</li><li><strong>掩码生成</strong>：<code>attention_mask</code> 标记有效Token 范围（首个 Token 到 EOS），<code>action_mask</code>标记强化学习中需优化的动作位置。</li><li><strong>序列截断</strong>：根据输入长度 <code>input_len</code>分割状态序列 <code>state_seq</code>，确保动作与状态的对应关系。</li></ol><p>Actor 的 <code>forward</code>方法计算每个动作的对数概率：在处理位置编码后，调用基础模型<code>self.model</code> 获取 <code>logits</code>，通过<code>log_probs_from_logits</code> 计算每个 Token的对数概率，最后提取目标位置的 <code>action_log_probs</code>。</p><p>核心代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Actor</span>(<span class="hljs-params">nn.Module</span>):</span><br><span class="hljs-meta">    @torch.no_grad()</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">generate</span>(<span class="hljs-params">self, input_ids: torch.Tensor, **kwargs</span>):</span><br>        <span class="hljs-comment"># 调用基础模型的生成功能</span><br>        sequences = self.model.generate(input_ids, **kwargs)<br>        <span class="hljs-comment"># 处理生成结果，提取有效 token 范围</span><br>        <span class="hljs-keyword">return</span> self.process_sequences(sequences, input_ids.size(<span class="hljs-number">1</span>))<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_sequences</span>(<span class="hljs-params">self, sequences: torch.Tensor, input_len</span>):</span><br>        <span class="hljs-comment"># 生成 attention_mask（标记有效 token）</span><br>        attention_mask = sequences.ne(pad_token_id) &amp; sequences.ne(eos_token_id)<br>        <span class="hljs-comment"># 定位序列结束位置（EOS），将 EOS 标记显式插入到序列的结束位置（确保终止性）</span><br>        eos_indices = sequences.size(<span class="hljs-number">1</span>) - <span class="hljs-number">1</span> - attention_mask.long().flip(dims=[<span class="hljs-number">1</span>]).argmax(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        sequences.scatter_(dim=<span class="hljs-number">1</span>, index=eos_indices, value=eos_token_id)<br>        <br>        <span class="hljs-comment"># 对于处理中间 EOS 的特殊情况（如 Llama3/Qwen2），找到每个序列的第一个有效 token 的起始位置</span><br>        first_token_indices = attention_mask.long().argmax(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># 生成一个序列长度的范围掩码 [0, 1, 2, ..., seq_len-1]</span><br>        mask = torch.arange(sequences.size(<span class="hljs-number">1</span>), device=sequences.device).expand_as(sequences)<br>        <span class="hljs-comment"># 更新 attention_mask：仅保留从第一个有效 token 到 EOS 之间的部分</span><br>        attention_mask = (mask &gt;= first_token_indices) &amp; (mask &lt;= eos_indices)<br>        <span class="hljs-keyword">return</span> sequences, attention_mask<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, sequences, num_actions, attention_mask=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-comment"># 动态生成位置编码（处理填充）</span><br>        position_ids = attention_mask.long().cumsum(-<span class="hljs-number">1</span>) - <span class="hljs-number">1</span>  <span class="hljs-comment"># [batch_size, seq_len]</span><br>        <span class="hljs-comment"># 模型推理获取 logits</span><br>        output = self.model(sequences, attention_mask, position_ids)<br>        <span class="hljs-comment"># 计算对数概率（公式：log_softmax + 标签对齐）</span><br>        log_probs = log_probs_from_logits(output.logits[:, :-<span class="hljs-number">1</span>], sequences[:, <span class="hljs-number">1</span>:])<br>        <span class="hljs-comment"># 提取动作位置的对数概率（强化学习优化目标）</span><br>        action_log_probs = log_probs[:, -num_actions:]  <span class="hljs-comment"># [batch_size, num_actions]</span><br>        <br>        <span class="hljs-keyword">return</span> action_log_probs<br></code></pre></td></tr></table></figure><p><strong>Reward Model</strong>：</p><p>奖励模型通过 <code>_get_reward_model</code> 函数加载。由于在 PPO训练过程中，<strong>奖励模型的参数是冻结的</strong>，不参与更新，因此可以指定一个预训练好的模型地址<code>remote_rm_url</code>。需要特别注意的是要准确识别 <strong>EOS标记</strong>，确保从模型的输出中提取正确的奖励值，即整个序列的得分。</p><p>核心代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RewardModel</span>(<span class="hljs-params">base_pretrained_model</span>):</span>        <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, input_ids, attention_mask</span>):</span><br>        outputs = <span class="hljs-built_in">getattr</span>(self, self.base_model_prefix)(<br>            input_ids, attention_mask=attention_mask<br>        )<br>        last_hidden_states = outputs[<span class="hljs-string">&quot;last_hidden_state&quot;</span>]  <span class="hljs-comment"># [batch_size, seq_length, hidden_size]</span><br>        values = <span class="hljs-built_in">getattr</span>(self, self.value_head_prefix)(last_hidden_states).squeeze(-<span class="hljs-number">1</span>)  <span class="hljs-comment"># [batch_size, seq_length]</span><br><br>        <span class="hljs-comment"># 找到最后一个非填充 token（通常是 EOS Token）的索引</span><br>        eos_indices = attention_mask.size(<span class="hljs-number">1</span>) - <span class="hljs-number">1</span> - attention_mask.long().flip(dims=[<span class="hljs-number">1</span>]).argmax(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># 从 values 中提取每个序列在结束位置的得分</span><br>        reward = values.gather(dim=<span class="hljs-number">1</span>, index=eos_indices).squeeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># [batch_size]</span><br><br>        <span class="hljs-keyword">return</span> reward<br></code></pre></td></tr></table></figure><p>在上述代码中，通过翻转 <code>attention_mask</code>并寻找第一个非填充位置，定位到序列中最后一个有效的 Token 的索引<code>eos_indices</code>。然后，从模型的输出 <code>values</code>中提取对应位置的得分，即为该序列的奖励值 <code>reward</code>。</p><p><strong>Critic Model</strong>：</p><p>Critic 模型使用 <code>_get_critic_model</code> 函数加载。由于在 PPO训练过程中，<strong>Critic模型的参数需要更新</strong>，因此需要对其进行训练。</p><p>在序列生成任务中，Critic模型需要对序列中每个动作（Token）之后的状态进行价值估计。但需要注意的是，通常不对最后一个时间步（终止状态）的价值进行估计，因为其未来累积奖励一般为零，无需估计。</p><p>Critic 模型的核心代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CriticModel</span>(<span class="hljs-params">base_pretrained_model</span>):</span><br>    <span class="hljs-comment"># num_actions：动作的数量，可以是整数或整数列表，用于指示要计算价值的动作数量。</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, input_ids, num_actions, attention_mask</span>):</span><br>        outputs = <span class="hljs-built_in">getattr</span>(self, self.base_model_prefix)(<br>            input_ids, attention_mask=attention_mask<br>        )<br>        last_hidden_states = outputs[<span class="hljs-string">&quot;last_hidden_state&quot;</span>]  <span class="hljs-comment"># [batch_size, seq_length, hidden_size]</span><br>        values = <span class="hljs-built_in">getattr</span>(self, self.value_head_prefix)(last_hidden_states).squeeze(-<span class="hljs-number">1</span>)    <span class="hljs-comment"># [batch_size, seq_length]</span><br>        values = values[:, :-<span class="hljs-number">1</span>]  <span class="hljs-comment"># [batch_size, seq_length - 1]</span><br><br>        <span class="hljs-comment"># 提取与动作对应的价值估计</span><br>        action_values = values[:, -num_actions:]  <span class="hljs-comment"># [batch_size, num_actions]</span><br><br>        <span class="hljs-keyword">return</span> action_values<br></code></pre></td></tr></table></figure><p>在上述代码中，<code>values</code>包含了序列中每个位置的价值估计。通过 <code>values[:, :-1]</code>去除最后一个时间步的价值估计。随后，使用<code>num_actions</code>指定要提取的价值数量，即对应于实际动作的价值估计<code>action_values</code>。</p><h3 id="模型训练">模型训练</h3><p>通过指定 <code>PPOTrainer</code>，调用 <code>fit</code>函数，开始模型的训练过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">trainer = PPOTrainer(<br>    strategy,<br>    actor,<br>    critic,<br>    reward_model,<br>    initial_model,<br>    ema_model,<br>    actor_optim,<br>    critic_optim,<br>    actor_scheduler,<br>    critic_scheduler,<br>    max_epochs=args.max_epochs,<br>    micro_train_batch_size=args.micro_train_batch_size,<br>    micro_rollout_batch_size=args.micro_rollout_batch_size,<br>    ...<br>)<br>trainer.fit(args, prompts_dataloader, pretrain_dataloader, consumed_samples, num_update_steps_per_episodes)<br></code></pre></td></tr></table></figure><p>接下来，我们将详细解析 <code>PPOTrainer</code> 的实现和训练流程。</p><h2 id="ppo-trainer">PPO Trainer</h2><p><code>PPOTrainer</code> 是 OpenRLHF 框架中用于实现 PPO算法的核心模块。它负责管理策略模型的训练过程，使模型能够在与环境交互的过程中不断优化，生成更符合人类偏好的回复。</p><h3 id="关键参数解读">关键参数解读</h3><ul><li><p><strong>strategy</strong>：指定训练策略，管理分布式训练、混合精度、模型并行、数据并行等等。</p></li><li><p><strong>actor 和 critic 的 optimizer 和scheduler</strong>：模型的优化器和学习率调度器。</p></li><li><p><strong>init_kl_coef</strong>：KL 散度的初始系数，用于对 PPO损失函数中限制策略偏移的 KL 约束项进行加权。</p></li><li><p><strong>ptx_coef</strong>：预训练数据的监督损失权重。在强化学习训练过程中，可以加入少量原始预训练任务的监督损失（PTX），以防止模型遗忘预训练知识，默认为0。</p></li><li><p><strong>buffer_limit</strong>：回放缓冲区 （replaybufer）最大容量。若使用经验重放机制，存储生成的轨迹数据并用于后续训练，0表示无限制。</p></li><li><p><strong>eps_clip</strong>：策略更新时的裁剪范围，PPO重要超参数，防止新策略与旧策略概偏离过大。</p></li><li><p><strong>value_clip</strong>：对价值函数更新进行裁剪，防止价值估计在更新过程中发生过大偏移，稳定Critic 的训练。</p></li><li><p><strong>gradient_checkpointing</strong>：若为True，在反向传播时重新计算部分前向传播，牺牲计算时间以节约显存。</p></li><li><p><strong>remote_rm_url</strong>：若有远程的 reward model服务，可通过 URL 访问。</p></li><li><p><strong>reward_fn</strong>：自定义奖励函数，用于根据生成的文本序列为策略分配奖励。</p></li><li><p><strong>generate_kwargs</strong>：策略模型在 rollout或推理阶段的参数，如 max_length、temperature、top_k、top_p 等。</p></li></ul><h3 id="初始化函数-init">初始化函数 <code>init</code></h3><p>初始化主要完成模型的加载、超参数的设置、优化器和调度器的配置，以及损失函数的定义等。具体包括：</p><ol type="1"><li><strong>损失函数</strong>：<ul><li><code>actor_loss_fn</code>： 策略损失函数，使用 PPO的裁剪策略。</li><li><code>critic_loss_fn</code>： 价值损失函数，使用价值裁剪。</li><li><code>ptx_loss_fn</code>：预训练任务的损失函数（如语言模型的交叉熵损失）。</li></ul></li><li><strong>经验和重放缓冲区</strong>：<ul><li><code>experience_maker</code>：经验生成器，用于与环境交互，生成训练所需的样本数据，包括状态、动作、奖励、优势等信息。</li><li><code>replay_buffer</code>：重放缓冲区，用于存储和采样训练数据。</li></ul></li></ol><h3 id="循环层级解读">循环层级解读</h3><p>初始化后，会进入 <code>PPOTrainer</code>的核心训练循环，在这里需要先理清楚循环层级的概念：</p><ol type="1"><li><strong>Episode</strong>：一个完整的 PPO训练周期，通常包含「经验收集」和「模型更新」两个流程，每个 Episode会遍历所有 Prompt（<code>num_episodes</code>）。</li><li><strong>Batch</strong>：分别表示两个流程中的批处理样本数量。<ul><li><strong>RolloutBatch</strong>：每次生成经验时并行处理的提示数量（<code>rollout_batch_size</code>），并且由于每个Prompt 会采样 <code>n_samples_per_prompt</code> 个响应，二者相乘就是replay_buffer 的大小。</li><li><strong>TrainBatch</strong>：每次参数更新时使用的经验子集（<code>train_batch_size</code>），通常会更小，因为训练需要的显存较大。例如同时用32 个提示生成响应后，每次梯度下降用 8 条经验计算损失。</li></ul></li><li><strong>Epoch</strong>：对当前收集的一批经验数据重复训练的轮数（<code>max_epochs</code>）。一批经验会重复被利用多次（每次都会打乱顺序），但不需要重新计算优势等信息，都会存储在buffer 中。</li><li><strong>Step</strong>：既表示参数更新步，又表示环境交互步。<ul><li><strong>Update Step</strong>：参数更新步，每个 Step 会更新一次 Actor和 Critic Model，是一次参数更新的最小单位，对应一个 Train Batch的处理（<code>training_step</code>）。</li><li><strong>Global Step</strong>：环境交互步，表示 Episode 中的 Rollout次数，用于跟踪训练进度，有时候 Actor会先冻结一些步骤，就需要用这个变量来区分不同阶段（<code>global_steps</code>）。</li></ul></li></ol><p>下图展示了各个层级的关系：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs text">Episode 1<br>├─ Rollout Batch 1 → 生成经验 → 存入 Replay Buffer → Global Step 1<br>│  ├─ Epoch 1<br>│  │  ├─ Train Batch 1 → Update Step 1<br>│  │  ├─ Train Batch 2 → Update Step 2<br>│  │  └─ ...<br>│  ├─ Epoch 2<br>│  │  ├─ Train Batch 1 → Update Step 3<br>│  │  └─ ...<br>├─ Rollout Batch 2 → 生成新经验 → 更新 Buffer → Global Step 2<br>│  ├─ Epoch 1<br>│  │  ├─ Train Batch 1 → Update Step n<br>│  │  └─ ...<br>└─ ...<br>Episode 2 → 重复上述流程<br></code></pre></td></tr></table></figure><h3 id="拟合函数-fit">拟合函数 <code>fit</code></h3><p><code>fit</code> 函数是 <code>PPOTrainer</code>的核心训练循环，主要包括以下步骤：</p><ol type="1"><li><strong>训练状态恢复</strong>：在中断训练后，可以根据已消耗的样本数恢复训练步数和起始的episode，以继续未完成的训练过程。</li><li><strong>主训练循环</strong>：遍历每个训练 epoch，在每个 epoch中执行以下操作：<ul><li><strong>数据加载</strong>：从数据加载器（<code>prompts_dataloader</code>）中获取随机的提示（prompts）。</li><li><strong>收集经验</strong>：使用 <code>experience_maker</code>生成经验数据。将生成的经验数据添加到重放缓冲区中。</li><li><strong>PPO 训练过程</strong>：进行 loss 的计算与参数更新。</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>():</span><br>    <span class="hljs-comment"># 训练状态恢复</span><br>    <span class="hljs-comment"># 根据已消耗的样本数恢复训练步数和起始 episode</span><br>    steps = consumed_samples // args.rollout_batch_size + <span class="hljs-number">1</span><br>    start_episode = consumed_samples // args.rollout_batch_size // num_rollouts_per_episodes<br>    consumed_samples = consumed_samples % (num_rollouts_per_episodes * args.rollout_batch_size)<br><br>    <span class="hljs-comment"># 主循环函数</span><br>    <span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_episode, args.num_episodes):<br>        <span class="hljs-keyword">for</span> rand_prompts <span class="hljs-keyword">in</span> self.prompts_dataloader:<br>            <span class="hljs-comment"># 1. 收集经验</span><br>            <span class="hljs-keyword">for</span> i, experience <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<br>            self.experience_maker.make_experience_list(rand_prompts, **self.generate_kwargs)<br>            ): <span class="hljs-comment"># 采样训练数据</span><br>                self.replay_buffer.append(experience)  <span class="hljs-comment"># 收集经验到 replay buffer</span><br><br>          <span class="hljs-comment"># 2. PPO训练过程</span><br>            torch.cuda.empty_cache()<span class="hljs-comment"># 清除缓存</span><br>            self.replay_buffer.normalize(<span class="hljs-string">&quot;advantages&quot;</span>, self.strategy) <span class="hljs-comment"># 标准化优势值</span><br>            status = self.ppo_train(steps)<span class="hljs-comment"># PPO 参数更新</span><br>            self.replay_buffer.clear()<span class="hljs-comment"># 清空 replay buffer</span><br>            torch.cuda.empty_cache()<span class="hljs-comment"># 清除缓存</span><br><br>            self.save_logs_and_checkpoints()<span class="hljs-comment"># 日志和检查点保存</span><br></code></pre></td></tr></table></figure><h3 id="训练函数-ppo_train">训练函数 <code>ppo_train</code></h3><p><code>ppo_train</code> 函数负责具体的训练过程，主要包括以下步骤：</p><ol type="1"><li><strong>主训练循环</strong> <code>ppo_train</code>：控制训练的 epoch和 train_batch，遍历当前批次的经验数据 <code>batch_experiences</code> 用<code>training_step</code> 执行每个训练步骤。</li><li><strong>训练步骤</strong><code>training_step</code>：根据当前的全局步数<code>global_steps</code>，决定是否训练 <code>actor</code> 和<code>critic</code> 模型<ul><li>训练策略模型<code>training_step_actor</code>：计算策略损失，并更新策略网络的参数。</li><li>训练价值模型<code>training_step_critic</code>：计算价值损失，并更新价值网络的参数。</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ppo_train</span>(<span class="hljs-params">self, global_steps=<span class="hljs-number">0</span></span>):</span><br>  status_list = [] <br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_epochs):<br>        <span class="hljs-keyword">for</span> experience <span class="hljs-keyword">in</span> batch_experiences: <span class="hljs-comment"># 遍历当前 batch 的 experience</span><br>            experience.to_device(device)<br>            status = self.training_step(experience, global_steps)<br>            status_list.append(status)<br>            <br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">training_step</span>(<span class="hljs-params">self, experience, global_steps</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">float</span>]:</span><br>    status = &#123;&#125;<br>    status = self.training_step_actor(experience)<br>    status.update(self.training_step_critic(experience))<br>    <span class="hljs-keyword">return</span> status<br></code></pre></td></tr></table></figure><h2 id="模型更新">模型更新</h2><h3 id="更新-actor">更新 Actor</h3><p><code>training_step_actor</code> 负责 Actor Model的更新，主要包括以下步骤：</p><ol type="1"><li><strong>训练模式切换</strong>：设置 Actor 网络为训练模式，启用dropout 等训练专用模块</li><li><strong>经验数据准备</strong>：将传入的经验数据拆为轨迹序列<code>sequences</code>、旧策略概率<code>old_action_log_probs</code>（用于重要性采样计算）、优势值<code>advantages</code> 等。</li><li><strong>策略网络前向推理</strong>：通过 Actor模型计算当前策略的动作概率分布 <spanclass="math inline">\(\pi_\theta\left(a_t \mid s_t\right)\)</span>。</li><li><strong>策略损失计算和更新</strong>：根据前面计算的结果调用损失函数，更新。这里还可以选择加上<strong>混合预训练更新</strong>。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">training_step_actor</span>(<span class="hljs-params">self, experience</span>):</span><br>    <span class="hljs-comment"># 训练模式切换</span><br>    self.actor.train()<br><span class="hljs-comment"># 读取采样经验</span><br>    sequences = experience.sequences<br>    old_action_log_probs = experience.action_log_probs<br>    advantages = experience.advantages<br>    num_actions = experience.action_mask.size(<span class="hljs-number">1</span>)<br>    attention_mask = experience.attention_mask<br>    <span class="hljs-comment"># 计算当前策略的 action 分布</span><br>    action_log_probs = self.actor(<br>        sequences,<br>        num_actions,<br>        attention_mask=attention_mask,<br>    )<br>    <span class="hljs-comment"># 带重要性采样的 loss function，反向传播更新</span><br>    actor_loss = self.actor_loss_fn(<br>        action_log_probs,<br>        old_action_log_probs,<br>        advantages,<br>        action_mask=experience.action_mask,<br>    )<br>    self.strategy.backward(actor_loss, self.actor, self.actor_optim)<br>    <br>    <span class="hljs-comment"># 如果有预训练的数据集，计算预训练损失（ptx loss）</span><br>    <span class="hljs-keyword">if</span> self.pretrain_dataloader <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        data = <span class="hljs-built_in">next</span>(self.pretrain_dataloader)<br>        output = self.actor(inputs, data=data, return_output=<span class="hljs-literal">True</span>)<br>        ptx_log_probs = output[<span class="hljs-string">&quot;logits&quot;</span>]<br>        <span class="hljs-comment"># 预训练 loss function</span><br>        ptx_loss = self.ptx_loss_fn(ptx_log_probs, data.label)<br>        self.strategy.backward(self.ptx_coef * ptx_loss, self.actor, self.actor_optim)<br><br>    <span class="hljs-comment"># 学习率调整</span><br>self.strategy.optimizer_step(self.actor_optim, self.actor, self.actor_scheduler, name=<span class="hljs-string">&quot;actor&quot;</span>)<br>    <span class="hljs-keyword">return</span> status<br></code></pre></td></tr></table></figure><h3 id="更新-critic">更新 Critic</h3><p><code>training_step_critic</code> 负责 Critic Model的更新，主要步骤和前面的类似，不再赘述。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">training_step_critic</span>(<span class="hljs-params">self, experience</span>):</span><br>    self.critic.train()<br><span class="hljs-comment"># 读取采样经验</span><br>    sequences = experience.sequences<br>    old_values = experience.values<br>    returns = experience.returns<br>    num_actions = experience.action_mask.size(<span class="hljs-number">1</span>)<br>    packed_seq_lens = <span class="hljs-literal">None</span><br>    attention_mask = experience.attention_mask<br>    <span class="hljs-comment"># critic loss</span><br>    values = self.critic(<br>        sequences,<br>        num_actions=num_actions,<br>        attention_mask=attention_mask,<br>    )<br>    <span class="hljs-comment"># loss function</span><br>    critic_loss = self.critic_loss_fn(<br>        values,<br>        old_values,<br>        returns,<br>        action_mask=experience.action_mask,<br>    )<br>    self.strategy.backward(critic_loss, self.critic, self.critic_optim)<br>    <br>    <span class="hljs-comment"># 学习率调整</span><br>    self.strategy.optimizer_step(self.critic_optim, self.critic, self.critic_scheduler, name=<span class="hljs-string">&quot;critic&quot;</span>)<br>    <span class="hljs-keyword">return</span> status<br></code></pre></td></tr></table></figure><h3 id="损失函数-loss.py">损失函数 <code>loss.py</code></h3><p>在 PPO trainer 的 init函数中，分别加载了<strong>策略模型</strong>损失函数、<strong>价值模型</strong>损失函数、<strong>语言模型</strong>损失函数，用于优化Actor、Critic 和防止模型遗忘预训练知识。接下来我们一一分析：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">self.actor_loss_fn = PolicyLoss(eps_clip)<br>self.critic_loss_fn = ValueLoss(value_clip)<br>self.ptx_loss_fn = GPTLMLoss()<br></code></pre></td></tr></table></figure><ol type="1"><li><strong>策略模型损失函数 (Policy Loss)</strong></li></ol><p>用于优化 Actor模型，为确保策略更新不会偏离旧策略太远，通过计算新旧策略的概率比率，并使用裁剪参数<span class="math inline">\(\epsilon\)</span> 与裁剪函数<code>clamp</code> 限制更新幅度，从而稳定训练过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PolicyLoss</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">        self,</span></span><br><span class="hljs-params"><span class="hljs-function">        log_probs: torch.Tensor,</span></span><br><span class="hljs-params"><span class="hljs-function">        old_log_probs: torch.Tensor,</span></span><br><span class="hljs-params"><span class="hljs-function">        advantages: torch.Tensor,</span></span><br><span class="hljs-params"><span class="hljs-function">        action_mask: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">    </span>) -&gt; torch.Tensor:</span><br>        ratio = (log_probs - old_log_probs).exp()<span class="hljs-comment"># 计算概率比率</span><br>        surr1 = ratio * advantages<span class="hljs-comment"># 计算未裁剪的目标函数</span><br>        surr2 = ratio.clamp(<span class="hljs-number">1</span> - self.clip_eps, <span class="hljs-number">1</span> + self.clip_eps) * advantages<span class="hljs-comment"># 计算裁剪后的目标函数</span><br>        loss = -torch.<span class="hljs-built_in">min</span>(surr1, surr2)<span class="hljs-comment"># 取两者中的较小值</span><br>        loss = masked_mean(loss, action_mask, dim=-<span class="hljs-number">1</span>).mean() <span class="hljs-comment"># 使用动作掩码计算平均损失</span><br>        <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure><ol start="2" type="1"><li><strong>价值模型损失函数 (Value Loss)</strong></li></ol><p>用于优化 Critic模型，其目标是<strong>最小化当前价值的估计与目标价值之间的差值</strong>，从而使得Critic 模型能够<strong>准确拟合真实的回报</strong>，即 <spanclass="math inline">\(L^{\text{Critic}}(\phi) = \mathbb{E}_{(x, y) \sim\mathcal{D}} \left[ \| V_\phi(x) - \hat{R}(x, y) \|^2\right]\)</span>。</p><p>虽然在 PPO 原论文中并未对价值函数引入裁剪，但在一些 PPO的变体中，为了提高训练的稳定性，也在价值函数的训练中引入了裁剪操作。同样，通过参数<span class="math inline">\(\epsilon\)</span> ，约束新策略 <spanclass="math inline">\(V_\phi^{new}\)</span> 与旧策略 <spanclass="math inline">\(V_\phi^{old}\)</span> 之间的变化幅度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ValueLoss</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">        self,</span></span><br><span class="hljs-params"><span class="hljs-function">        values: torch.Tensor,</span></span><br><span class="hljs-params"><span class="hljs-function">        old_values: torch.Tensor,</span></span><br><span class="hljs-params"><span class="hljs-function">        returns: torch.Tensor,</span></span><br><span class="hljs-params"><span class="hljs-function">        action_mask: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">    </span>) -&gt; torch.Tensor:</span><br>        <span class="hljs-keyword">if</span> self.clip_eps <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<span class="hljs-comment"># 计算裁剪的损失</span><br>            values_clipped = old_values + (values - old_values).clamp(-self.clip_eps, self.clip_eps)<br>            surr1 = (values_clipped - returns) ** <span class="hljs-number">2</span><br>            surr2 = (values - returns) ** <span class="hljs-number">2</span><span class="hljs-comment"># 未经裁剪的价值损失</span><br>            loss = torch.<span class="hljs-built_in">max</span>(surr1, surr2)<span class="hljs-comment"># 取较大的损失（最大值）</span><br>        <span class="hljs-keyword">else</span>:<br>            loss = (values - returns) ** <span class="hljs-number">2</span><span class="hljs-comment"># 直接计算均方误差</span><br><br>        loss = masked_mean(loss, action_mask, dim=-<span class="hljs-number">1</span>).mean()<span class="hljs-comment"># 使用动作掩码计算平均损失</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * loss<br></code></pre></td></tr></table></figure><p>注意：在优化价值函数时，采用了 <code>torch.max(surr1, surr2)</code>来选择较大的损失值，这与策略模型损失函数中使用<code>torch.min(surr1, surr2)</code> 的方式不同。</p><p>这是因为：</p><ul><li><strong>策略损失函数</strong>：在策略更新中，我们的目标是让 <spanclass="math inline">\(r_t(\theta)\)</span> 趋向于 <spanclass="math inline">\(1\)</span>，因此仅当 <spanclass="math inline">\(\pi_\theta\)</span> 远离 <spanclass="math inline">\(\pi_{\theta_\text{old}}\)</span>时，我们需要缩小更新的幅度；而当 <spanclass="math inline">\(\pi_\theta\)</span> 靠近 <spanclass="math inline">\(\pi_{\theta_\text{old}}\)</span>时，则不需要缩小。</li><li><strong>价值损失函数</strong>：在价值更新中，我们的目标是让 <spanclass="math inline">\(V_\phi\)</span> 趋向于 <spanclass="math inline">\(V_t^{\text{target}}\)</span>，因此无论 <spanclass="math inline">\(V_\phi\)</span> 偏离 <spanclass="math inline">\(V_{\phi_\text{old}}\)</span>多少都不需要缩小，但如果 <span class="math inline">\(V_\phi\)</span>离目标越远，则其 Loss越大，更新的速度也就越快，此时就需要选最大者。这里的 CLIP操作只是单纯<strong>为了多一种选择</strong>（且这个选择比较靠谱，因为离<span class="math inline">\(V_{\phi_\text{old}}\)</span> 不太远）。</li></ul><ol start="3" type="1"><li><strong>语言模型损失函数 (GPTLM Loss)</strong></li></ol><p>GPT 语言模型损失函数用于预测序列中的下一个token。通过移位操作，模型基于前面的 token 预测下一个token。使用交叉熵损失函数来计算预测与真实标签之间的误差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GPTLMLoss</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        self.IGNORE_INDEX = -<span class="hljs-number">100</span><span class="hljs-comment"># 通过 IGNORE_INDEX 实现 loss_mask</span><br>        self.loss = nn.CrossEntropyLoss(ignore_index=self.IGNORE_INDEX)<span class="hljs-comment"># 交叉熵损失函数</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, logits: torch.Tensor, labels: torch.Tensor</span>) -&gt; torch.Tensor:</span><br>        <span class="hljs-comment"># 通过错位操作实现下一个词的预测</span><br>shift_logits = logits[..., :-<span class="hljs-number">1</span>, :].contiguous() <span class="hljs-comment"># 取除了最后一个位置的所有预测</span><br>shift_labels = labels[..., <span class="hljs-number">1</span>:].contiguous()<span class="hljs-comment"># 取除了第一个位置的所有标签</span><br><br>        loss = self.loss(shift_logits.view(-<span class="hljs-number">1</span>, shift_logits.size(-<span class="hljs-number">1</span>)), shift_labels.view(-<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure><ol start="4" type="1"><li><strong>奖励模型损失函数（PairWise Loss）</strong></li></ol><p>奖励模型的损失函数用于比较两个生成结果的优劣。在前文 Reward Model介绍中 Pairwise loss 的计算方式为，<spanclass="math inline">\(L^{\text{RM}}= -\log \sigma(r_\text{chosen} -r_\text{reject})\)</span>。通过 sigmoid 函数将奖励差异映射到 <spanclass="math inline">\((0,1)\)</span>区间，然后取负对数。在实现过程中，margin参数可以用于强化区分度，即不仅要求 <spanclass="math inline">\(y_\text{chosen} &gt;y_\text{reject}\)</span>，还要求 <spanclass="math inline">\(y_\text{chosen} &gt; y_\text{reject} +margin\)</span>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PairWiseLoss</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">        self, chosen_reward: torch.Tensor, reject_reward: torch.Tensor, margin: torch.Tensor = <span class="hljs-literal">None</span></span></span><br><span class="hljs-params"><span class="hljs-function">    </span>) -&gt; torch.Tensor:</span><br>        <span class="hljs-keyword">if</span> margin <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            loss = -F.logsigmoid(chosen_reward - reject_reward - margin)<br>        <span class="hljs-keyword">else</span>:<br>            loss = -F.logsigmoid(chosen_reward - reject_reward)<br>        <span class="hljs-keyword">return</span> loss.mean()<br></code></pre></td></tr></table></figure><p>第二种计算方法同样是比较两个生成结果优劣的奖励模型损失，在公式上是上述实现的等价变换<span class="math inline">\(-\log(\mathrm{sigmoid}(x)) =\log(1+e^{-x})\)</span>。当 <span class="math inline">\(r_\text{reject}- r_\text{chosen}\)</span> 为负值（即选中的序列更好）时，损失接近 0；当<span class="math inline">\(r_\text{reject} - r_\text{chosen}\)</span>为正值时，损失随差值线性增长。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LogExpLoss</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">        self, chosen_reward: torch.Tensor, reject_reward: torch.Tensor, margin: torch.Tensor = <span class="hljs-literal">None</span></span></span><br><span class="hljs-params"><span class="hljs-function">    </span>) -&gt; torch.Tensor:</span><br>        loss = torch.log(<span class="hljs-number">1</span> + torch.exp(reject_reward - chosen_reward)).mean()<br>        <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure><ol start="5" type="1"><li><strong>过程奖励模型损失函数（PRMLoss）</strong></li></ol><p>在强化学习的奖励建模中，传统的奖励模型往往只评估完整序列的好坏。然而，在一些复杂任务中，我们希望模型能够对生成过程中的每个步骤进行评估，这就引入了过程奖励模型（ProcessReward Model，PRM）。PRM能够对生成序列中的每个步骤打分，以更细粒度地指导模型的生成过程。</p><p>PRM的损失函数旨在学习<strong>模型对每个步骤的奖励评估</strong>。具体的实现方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PRMLoss</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, placeholder_token_id: <span class="hljs-built_in">int</span>, reward_token_ids: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>]] = <span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.IGNORE_INDEX = -<span class="hljs-number">100</span><br>        self.loss = nn.CrossEntropyLoss(ignore_index=self.IGNORE_INDEX)<br>        self.placeholder_token_id = placeholder_token_id<br>        self.reward_token_ids = reward_token_ids<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, inputs: torch.Tensor, logits: torch.Tensor, labels: torch.Tensor, *, return_acc: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span>):</span><br>        <span class="hljs-comment"># 提取出所有步骤结束位置对应的数据</span><br>        placeholder_mask = inputs == self.placeholder_token_id <span class="hljs-comment"># bool, (batch_size, seq_len)</span><br>        logits = logits[placeholder_mask] <span class="hljs-comment"># (num_placeholders, vocab_size)</span><br>        labels = labels[placeholder_mask] <span class="hljs-comment"># (num_placeholders,)</span><br><br>        <span class="hljs-keyword">if</span> labels.dtype == torch.<span class="hljs-built_in">float</span>:<br>            <span class="hljs-comment"># soft label</span><br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(self.reward_token_ids) == <span class="hljs-number">2</span>, <span class="hljs-string">&quot;reward_token_ids should have 2 tokens for soft labels&quot;</span><br>            logits = logits[..., self.reward_token_ids] <span class="hljs-comment"># (num_placeholders, 2)</span><br>            positive_labels = labels.to(logits.dtype)<br>            negative_labels = <span class="hljs-number">1</span> - positive_labels <span class="hljs-comment"># (num_placeholders,)</span><br>            labels = torch.stack([positive_labels, negative_labels], dim=-<span class="hljs-number">1</span>) <span class="hljs-comment"># (num_placeholders, 2)</span><br>        <span class="hljs-keyword">elif</span> self.reward_token_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># hard label with reward_token_ids set. (otherwise the whole vocab will be trained together.)</span><br>            logits = logits[..., self.reward_token_ids] <span class="hljs-comment"># (num_placeholders, reward_token_size)</span><br>            <span class="hljs-keyword">for</span> i, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.reward_token_ids):<br>                <span class="hljs-comment"># 在每个位置放上对应的 reward_token_id</span><br>                labels = torch.where(labels == token, i, labels) <span class="hljs-comment"># (num_placeholders,)</span><br><br>        <span class="hljs-comment"># 计算交叉熵损失</span><br>        <span class="hljs-comment"># logits形状: (num_placeholders, num_classes) </span><br>        <span class="hljs-comment"># labels形状: (num_placeholders,) 或 (num_placeholders, 2)</span><br>        loss = self.loss(logits, labels)<br>        <br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_acc:<br>            <span class="hljs-keyword">return</span> loss<br><br>        <span class="hljs-comment"># 准确率计算部分</span><br>        <span class="hljs-keyword">if</span> labels.dtype == logits.dtype:<br>            <span class="hljs-comment"># 当使用 soft labels 时，取最大概率索引</span><br>            <span class="hljs-comment"># labels 形状变化: (num_placeholders, 2) =&gt; (num_placeholders,)</span><br>            labels = labels.argmax(dim=-<span class="hljs-number">1</span>)<br>            <br>        <span class="hljs-comment"># 计算准确率，logits.argmax形状: (num_placeholders,)</span><br>        acc = (logits.argmax(dim=-<span class="hljs-number">1</span>) == labels).<span class="hljs-built_in">float</span>().mean()<br>        <span class="hljs-keyword">return</span> loss, acc<br></code></pre></td></tr></table></figure><p>我们以一个 PRM 数据集的示例来说明：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs json">&#123;<br>    <span class="hljs-attr">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Janet pays $40/hour for 3 hours per week of clarinet lessons and $28/hour for 5 hours a week of piano lessons. How much more does she spend on piano lessons than clarinet lessons in a year? Step 1: Janet spends 3 hours + 5 hours = &lt;&lt;3+5=8&gt;&gt;8 hours per week on music lessons. ки Step 2: She spends 40 * 3 = &lt;&lt;40*3=120&gt;&gt;120 on clarinet lessons per week. ки Step 3: She spends 28 * 5 = &lt;&lt;28*5=140&gt;&gt;140 on piano lessons per week. ки Step 4: Janet spends 120 + 140 = &lt;&lt;120+140=260&gt;&gt;260 on music lessons per week. ки Step 5: She spends 260 * 52 = &lt;&lt;260*52=13520&gt;&gt;13520 on music lessons in a year. The answer is: 13520 ки&quot;</span>,<br>    <span class="hljs-attr">&quot;labels&quot;</span>: <span class="hljs-string">&quot;Janet pays $40/hour for 3 hours per week of clarinet lessons and $28/hour for 5 hours a week of piano lessons. How much more does she spend on piano lessons than clarinet lessons in a year? Step 1: Janet spends 3 hours + 5 hours = &lt;&lt;3+5=8&gt;&gt;8 hours per week on music lessons. + Step 2: She spends 40 * 3 = &lt;&lt;40*3=120&gt;&gt;120 on clarinet lessons per week. + Step 3: She spends 28 * 5 = &lt;&lt;28*5=140&gt;&gt;140 on piano lessons per week. + Step 4: Janet spends 120 + 140 = &lt;&lt;120+140=260&gt;&gt;260 on music lessons per week. + Step 5: She spends 260 * 52 = &lt;&lt;260*52=13520&gt;&gt;13520 on music lessons in a year. The answer is: 13520 -&quot;</span>,<br>    <span class="hljs-attr">&quot;values&quot;</span>: [ <span class="hljs-string">&quot;+&quot;</span>, <span class="hljs-string">&quot;+&quot;</span>, <span class="hljs-string">&quot;+&quot;</span>, <span class="hljs-string">&quot;+&quot;</span>, <span class="hljs-string">&quot;-&quot;</span> ]<br>&#125;<br></code></pre></td></tr></table></figure><p>在这个例子中：</p><ul><li><code>inputs</code>：每个步骤后面都有一个特殊标记<code>ки</code>，用于标记步骤的结束位置。</li><li><code>labels</code>：每个步骤后面有一个标签 <code>+</code> 或<code>-</code>，表示当前步骤的推理是否正确。当使用<strong>软标签</strong>（SoftLabels）时，每个步骤的标签是不再是 id，而是一个 float 概率值，比如<code>[0.8, 0.85, 0.9, 0.78, 0.1]</code>，表示该步骤为正样本的概率。</li></ul><p>在代码中的实现为：</p><ul><li><code>placeholder_token_id</code> 标记每个步骤结束位置的特殊标记的ID，用于定位需要评估的步骤位置。</li><li><code>reward_token_ids</code> 用于表示奖励的标签 token ID 列表，比如<code>['+', '-']</code> 对应的 tokenIDs。也可以有更多的标签进行多分类，但在使用<strong>软标签</strong>时只支持二分类。</li></ul><h2 id="experience_maker-类"><code>experience_maker</code> 类</h2><p>在强化学习中，<strong>经验</strong>（experience）是指智能体与环境交互时收集的数据。这些数据通常以元组形式表示<span class="math inline">\((s_t, a_t, r_t,s_{t+1})\)</span>，包括智能体采取的动作、所处的状态、获得的奖励以及下一个状态。</p><p>OpenRLHF 框架中定义了一个 <code>NaiveExperienceMaker</code>类，其核心函数 <code>make_experience_list</code>实现了从提示（prompt）生成响应（response），构建样本（samples），处理经验（experience），以及计算奖励和优势的完整流程。接下来，我们将按照这一流程，逐步解析各部分的实现。</p><ol type="1"><li><strong>采样并构建 Samples</strong></li></ol><p>为了方便管理和操作，框架中定义了一个 <code>Samples</code>类，用于存储生成过程中的响应和相关信息，例如响应长度、注意力掩码（<code>attention_mask</code>）和动作掩码（<code>action_mask</code>）等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Samples</span>:</span><br>    <span class="hljs-string">&quot;&quot;&quot;Samples is a batch of data.</span><br><span class="hljs-string">    &quot;B&quot; 是批次大小。</span><br><span class="hljs-string">        &quot;S&quot; 是序列长度。</span><br><span class="hljs-string">        &quot;A&quot; 是动作的数量，即生成的token长度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    sequences: torch.Tensor<span class="hljs-comment"># (B, S) ，包含 Prompt + response 的拼接序列</span><br>    attention_mask: <span class="hljs-type">Optional</span>[torch.LongTensor]<span class="hljs-comment"># (B, S) ，标识去掉 padding 有效的 attention 位置</span><br>    action_mask: <span class="hljs-type">Optional</span>[torch.BoolTensor]<span class="hljs-comment"># (B, A) 或 None，标识有效的生成 token</span><br>    num_actions: <span class="hljs-type">Union</span>[<span class="hljs-built_in">int</span>, torch.Tensor]<span class="hljs-comment"># int 或 (B,)，响应中的动作（令牌）数量</span><br>    packed_seq_lens: <span class="hljs-type">Optional</span>[torch.Tensor]<br>    response_length: torch.Tensor<span class="hljs-comment"># 响应的令牌数量</span><br>    total_length: torch.Tensor<span class="hljs-comment"># (B,)，sequences 所有 token（prompt + response）的数量</span><br></code></pre></td></tr></table></figure><p>通过 <code>generate_samples</code> 收集每个属性，构建 samples。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">samples_list = self.generate_samples(all_prompts, **generate_kwargs)<br></code></pre></td></tr></table></figure><p>首先，基于 <code>args.micro_rollout_batch_size</code>的配置，将数据进行批处理。</p><p><img src="/img/blog/RL-Note-12-images/batching.png" alt="采样批处理" width=90% /></p><p>然后，调用 <code>tokenize_fn</code>，将 Prompt 进行 token化，并进行左 padding 处理。</p><p><img src="/img/blog/RL-Note-12-images/padding.png" alt="采样前做左 padding"  width=90% /></p><p>接下来，调用 <code>Actor.generate()</code> 方法，根据给定的提示<code>input_ids</code> 生成包含提示和响应的完整序列<code>sequences</code>，同时生成 <code>attention_mask</code> 和<code>action_mask</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sequences, attention_mask, action_mask = self.actor.generate(**inputs, **generate_kwargs)<br></code></pre></td></tr></table></figure><ul><li><code>sequences</code>：形状为 <code>(B, S)</code>，包含了 Prompt和生成的 Response 的拼接序列。</li></ul><p><img src="/img/blog/RL-Note-12-images/response.png" alt="采样的序列"  width=90%/></p><ul><li><code>attention_mask</code>：形状为<code>(B, S)</code>，用于标记序列中哪些位置是有效的，将填充（padding）部分置为1，在计算时忽略。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">attention_mask = (mask &gt;= first_token_indices) &amp; (mask &lt;= eos_indices).to(dtype=torch.long)<br></code></pre></td></tr></table></figure><p><img src="/img/blog/RL-Note-12-images/attention_mask.png" alt="注意力掩码" width=90% /></p><ul><li><code>action_mask</code>：形状为<code>(B, A)</code>，也就是序列长度是生成的 token数，num_actions。对有效状态位置值1，用于区分 Prompt 和 Response部分，标记 Response 部分的动作，用于后续的策略优化。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">state_seq = sequences[:, input_len - <span class="hljs-number">1</span> : -<span class="hljs-number">1</span>]<br>action_mask = state_seq.ne(eos_token_id) &amp; state_seq.ne(pad_token_id)<br>action_mask[:, <span class="hljs-number">0</span>] = <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p><img src="/img/blog/RL-Note-12-images/action_mask.png" alt="动作掩码" width=90% /></p><p>通过以上步骤，我们构建了一个包含序列数据和相关掩码信息的<code>Samples</code> 对象，为后续的经验处理做好了准备。</p><ol start="2" type="1"><li><strong>处理 Samples 构建 Experience</strong></li></ol><p>生成了 <code>Samples</code>后，需要对其进行进一步处理，计算每个动作的对数概率、价值估计和 KL散度等信息，构建强化学习所需的 <code>Experience</code>对象。<code>Experience</code> 类在 <code>Samples</code>的基础上，增加了价值估计（values）、回报（returns）和优势（advantages）等信息，用于强化学习的训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Experience</span>:</span><br>    <span class="hljs-string">&quot;&quot;&quot;Experience is a batch of data.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    sequences: torch.Tensor<span class="hljs-comment"># (B, S), 存储状态</span><br>    action_log_probs: torch.Tensor<span class="hljs-comment"># (B, A), 每个动作的对数概率</span><br>    values: torch.Tensor<span class="hljs-comment"># (B, A), 每个动作的价值估计</span><br>    returns: <span class="hljs-type">Optional</span>[torch.Tensor]<span class="hljs-comment"># (B, A), 每个动作的回报</span><br>    advantages: <span class="hljs-type">Optional</span>[torch.Tensor]<span class="hljs-comment"># (B, A), 每个动作的优势，按照 GAE 方法计算</span><br>    attention_mask: <span class="hljs-type">Optional</span>[torch.LongTensor]<span class="hljs-comment"># (B, S), 掩码序列中的无效部分</span><br>    action_mask: <span class="hljs-type">Optional</span>[torch.BoolTensor]<span class="hljs-comment"># (B, A), 掩码无效动作</span><br>    kl: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span><span class="hljs-comment"># (B, A), 每个动作的 kl 散度</span><br></code></pre></td></tr></table></figure><p>在处理过程中，调用 <code>make_experience</code>函数，计算得到强化学习训练所需的经验数据：<strong>对数概率（logprobs）</strong>、<strong>价值（values）</strong>、<strong>奖励（rewards）</strong>，以及<strong>KL 散度（kl divergence）</strong>。这里的 <code>reward</code>代表一个交互轨迹的总分。<code>advantage</code> 和 <code>return</code>两个元素在后续步骤计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">make_experience</span>(<span class="hljs-params">self, samples: Samples</span>) -&gt; Experience:</span><br>  <span class="hljs-comment"># 计算 actor 动作的对数概率</span><br>    action_log_probs = self.actor(sequences, num_actions, attention_mask)<br><span class="hljs-comment"># 计算 ref 动作的对数概率</span><br>    base_action_log_probs = self.initial_model(sequences, num_actions, attention_mask)<br>    <span class="hljs-comment"># 估计每个动作的价值</span><br>    value = self.critic(sequences, num_actions, attention_mask)<br>    <span class="hljs-comment"># 计算奖励模型给出的奖励，这里还不是最终的奖励 r_total</span><br>    r = self.reward_model(sequences, attention_mask)<br>    <span class="hljs-comment"># 计算 KL 散度，在这里全都一次性算好、存储</span><br>    kl = compute_approx_kl(action_log_probs, base_action_log_probs, action_mask)<br>    <span class="hljs-comment"># 构建 Experience 对象返回</span><br><span class="hljs-keyword">return</span> Experience(...)<br></code></pre></td></tr></table></figure><ol start="3" type="1"><li><strong>处理经验数据</strong></li></ol><p><code>process_experiences</code>函数用于对经验数据进行进一步处理。当使用 RLOO作为优势估计器时，需要对奖励进行 Leave-One-Out 处理（RewardShaping）。并且得到 RM 给出的 <code>reward</code>，这里的维度还是<code>[B, 1]</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">experiences, rewards = self.process_experiences(experiences)<br></code></pre></td></tr></table></figure><ol start="4" type="1"><li><strong>计算每个时间步的奖励</strong>：<code>compute_reward</code>函数</li></ol><p>前面步骤中，<code>rewards</code>是由奖励模型给出的，<strong>每个样本只提供了一个最终总奖励</strong>。但是，在强化学习中，策略优化需要每个时间步的奖励。为了在时间步上进行策略优化，我们需要将总奖励分配到每个时间步。</p><p><code>compute_reward</code>函数的作用是将总奖励分配到序列的特定时间步（通常是最后一个有效动作），并结合KL 散度惩罚，计算得到每个时间步的总奖励序列。输出的 <code>reward</code>是一个与动作序列长度匹配的张量，维度是 <code>[B, A]</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_reward</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">    r: <span class="hljs-type">Union</span>[torch.Tensor, <span class="hljs-built_in">float</span>],</span></span><br><span class="hljs-params"><span class="hljs-function">    kl_coef: <span class="hljs-built_in">float</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">    kl: <span class="hljs-type">Union</span>[torch.Tensor, <span class="hljs-built_in">list</span>[torch.Tensor]],</span></span><br><span class="hljs-params"><span class="hljs-function">    action_mask: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">    num_actions: <span class="hljs-type">Optional</span>[<span class="hljs-type">Union</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>]]] = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function"></span>) -&gt; <span class="hljs-type">Union</span>[torch.Tensor, <span class="hljs-built_in">list</span>[torch.Tensor]]:</span><br><br>    <span class="hljs-keyword">if</span> action_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        kl_reward = -kl_coef * kl<br>        <span class="hljs-comment"># 找到最后一个有效动作的位置</span><br>        eos_indices = action_mask.size(<span class="hljs-number">1</span>) - <span class="hljs-number">1</span> - action_mask.long().fliplr().argmax(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># 在最后一个有效动作位置添加环境奖励</span><br>        last_reward = torch.zeros_like(kl).scatter_(dim=<span class="hljs-number">1</span>, index=eos_indices, src=r.unsqueeze(<span class="hljs-number">1</span>).to(kl.dtype))<br>        reward = last_reward + kl_reward    <span class="hljs-comment"># 总奖励 = 环境奖励（仅在最后一个动作） + KL 惩罚（每个时间步）</span><br>    <span class="hljs-keyword">else</span>:<br>        reward = []<br>        <span class="hljs-keyword">for</span> i, (kl_seg, action_len) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(kl, num_actions)):<br>            kl_reward = -kl_coef * kl_seg<br>            kl_reward[action_len - <span class="hljs-number">1</span>] += r[i]  <span class="hljs-comment"># 在最后一个动作位置添加环境奖励</span><br>            reward.append(kl_reward)<br><br>    <span class="hljs-keyword">return</span> reward<br></code></pre></td></tr></table></figure><p>Scatter 操作如下：</p><p><img src="/img/blog/RL-Note-12-images/scatter.png" alt="Scatter 操作" width=60%  /></p><p>这里可选的一个操作是在每个时间步（上图红色位置）加上 KL惩罚，得到所谓的 <spanclass="math inline">\(r_\text{total}\)</span>。计算得到的<strong>每个时间步</strong>的最终<code>rewards</code>仅用于后续的<strong>优势函数和回报</strong>的计算。</p><ol start="5" type="1"><li><strong>计算优势和回报</strong></li></ol><p>在强化学习中，为了更新策略，我们需要计算每个时间步的<strong>优势</strong>（advantages）和<strong>回报</strong>（returns），用于指导Actor 和 Critic的改进。在代码实现中，有两种计算优势和回报的方法：<strong>GAE</strong>和 <strong>REINFORCE</strong>。</p><p>第一种 <strong>GAE 算法</strong>通过<code>get_advantages_and_returns</code> 函数实现，在 PPO训练中广泛使用。优势计算公式为 $ <em>{t}^{(,)}=</em>{l=0}<sup>()</sup>l_{t+l}. <spanclass="math inline">\(，通过计算每个时间步的时间差分误差（TDerror）：\)</span><em>t=r_t+V(s</em>{t+1})-V(s_t)$，递归计算得到优势，最后计算回报<span class="math inline">\(R_t =\hat{A}_{t}^{\mathrm{GAE}(\gamma,\lambda)} +V(s_t)\)</span>。实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_advantages_and_returns</span>(<span class="hljs-params">self, values, rewards, action_mask, gamma, lambd</span>):</span><br>    lastgaelam = <span class="hljs-number">0</span><br>    advantages_reversed = []<br>    response_length = rewards.size(<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># Mask invalid responses</span><br>    <span class="hljs-keyword">if</span> action_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        values = action_mask * values<br>        rewards = action_mask * rewards<br><br>    <span class="hljs-comment"># 从最后一个时间步开始向前遍历</span><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(response_length)):<br>        nextvalues = values[:, t + <span class="hljs-number">1</span>] <span class="hljs-keyword">if</span> t &lt; response_length - <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0.0</span><br>        delta = rewards[:, t] + gamma * nextvalues - values[:, t]   <span class="hljs-comment"># 计算 TD error：δ_t = r_t + γV(s_&#123;t+1&#125;) - V(s_t)</span><br>        lastgaelam = delta + gamma * lambd * lastgaelam             <span class="hljs-comment"># 递推计算 GAE</span><br>        advantages_reversed.append(lastgaelam)                      <span class="hljs-comment"># 当前时间步的优势</span><br>    advantages = torch.stack(advantages_reversed[::-<span class="hljs-number">1</span>], dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 将逆序列表反转为正序</span><br>    returns = advantages + values <span class="hljs-comment"># 计算回报：R_t = A_t + V(s_t)</span><br>    <span class="hljs-keyword">return</span> advantages.detach(), returns<br></code></pre></td></tr></table></figure><p><img src="/img/blog/RL-Note-12-images/gae.jpg" alt="GAE 计算过程" width=100% /></p><p>另一种方法是使用 <strong>REINFORCE</strong> 算法，无需 Critic Model直接计算累积回报用于策略的更新。回报 <spanclass="math inline">\(R_t\)</span> 定义为从时间步 <spanclass="math inline">\(t\)</span> 开始的折扣累积奖励 <spanclass="math inline">\(R_t=\sum_{k=0}^{\infty}\gamma^kr_{t+k}\)</span>。实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_cumulative_returns</span>(<span class="hljs-params">self, rewards, action_mask, gamma</span>):</span><br>    <span class="hljs-comment"># Calculate returns by accumulating discounted rewards</span><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(response_length)):<span class="hljs-comment"># 从最后一个时间步开始反向遍历</span><br>        cumulative_return = rewards[:, t] + gamma * cumulative_return<br>        returns[:, t] = cumulative_return<br>   <span class="hljs-keyword">return</span> returns <span class="hljs-comment"># 只返回 returns 累积奖励</span><br></code></pre></td></tr></table></figure><p>通过以上步骤，函数完整地构建了强化学习算法中的经验数据，并计算了策略优化所需的关键量。这些数据将用于后续的策略更新。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL 学习笔记 #11 PPO 在 RLHF 中的应用</title>
    <link href="/RL-Note-11.html"/>
    <url>/RL-Note-11.html</url>
    
    <content type="html"><![CDATA[<p>在前面的章节中，我们介绍了通用的 RL流程和基本概念，这些内容通常应用于游戏、机器人控制等序列决策任务。但随着大型语言模型（LLM）的发展，RL在自然语言任务中的作用变得越来越重要。</p><p>那么，<strong>如何将强化学习的框架映射到 NLP任务中呢？</strong>我们可以将强化学习的核心元素（MDP序列决策）与文本生成任务中的组件对应起来：</p><ul><li>智能体（Agent）：对应于语言模型，负责生成文本序列。</li><li>环境（Environment）：代表外部的反馈机制，例如人类用户的互动反馈。</li><li>回合（Episode）：始于输入文本，结束于模型输出文本的 EOS_token。</li><li>状态（State, <spanclass="math inline">\(s_t\)</span>）：是当前的文本Context，包括输入和已经生成的 Token 序列。</li><li>动作（Action, <spanclass="math inline">\(a_t\)</span>）：是在给定状态下选择生成下一个Token，通常会从词表中选择。</li><li>奖励（Reward, <spanclass="math inline">\(r_t\)</span>）：反映生成文本的质量（序列级或 Token级），由环境提供，用于指导模型的学习。</li></ul><p>自 2023年以来，<strong>基于人类反馈的强化学习</strong>（Reinforcement Learningfrom Human Feedback，RLHF）在 LLM 领域逐渐成为研究热点。RLHF的核心思想是通过人类反馈来指导模型的学习过程，使其生成的文本更加符合人类的期望和价值观。</p><p>与传统的监督学习（SFT）不同，RLHF通过训练一个<strong>奖励模型</strong>（RewardModel）来量化人类的偏好，并使用强化学习算法（如PPO）来优化策略模型（PolicyModel）。这种方法的优势在于能够处理复杂、多样的反馈信号，并在生成过程中动态调整策略，从而生成更符合人类期望的文本。接下来，我们将对RLHF-PPO 算法进行详细介绍。</p><h2 id="rlhf-ppo-中的四个模型">RLHF-PPO 中的四个模型</h2><p>在 RLHF的训练过程中，涉及到四个模型的协同工作：策略模型、参考模型、价值模型、奖励模型。这四个模型各自承担不同的角色，一起实现策略的优化：</p><ul><li><strong>策略模型</strong>（Policy Model）：即 ActorModel，用于生成文本，并根据奖励信号不断优化；</li><li><strong>参考模型</strong>（ReferenceModel）：文本任务特有的模型，使用 SFT模型初始化，防止其过度偏离初始点；</li><li><strong>奖励模型</strong>（RewardModel）：文本任务特有的模型，衡量整个序列的人类偏好，并分配到每个 Token上；</li><li><strong>价值模型</strong>（Value Model）：即 Critic model，与配合Reward 来估计优势和回报，从而引导策略更新。</li></ul><p>PPO 的完整工作流程如下：</p><figure><img src="/img/blog/RL-Note-11-images/PPO.png" alt="PPO 工作流" /><figcaption aria-hidden="true">PPO 工作流</figcaption></figure><h3 id="策略模型-policy-model">策略模型 | Policy Model</h3><p>策略模型的基本用法与经典 PPO 中的 Actor 类似，需要注意的是，在 RLHF中，它一般使用<strong>预训练语言模型的 SFT后的版本</strong>初始化。其目标是通过生成符合人类偏好的文本来获得更高的奖励。</p><p>与经典 PPO类似，直接优化策略模型可能会导致<strong>策略崩溃</strong>（PolicyCollapse），即模型过度优化某些特定的生成模式来骗取奖励，导致生成的文本缺乏多样性甚至出现乱码。为此，RLHF中也会使用 <strong>KL散度</strong>来限制策略模型的更新幅度。但此时我们不会直接使用PPO-Penalty 作为优化目标，而是<strong>使用 Refernce Model 计算完 KL散度后直接与 Reward 结合</strong>： <span class="math display">\[r_{\text {total }}=r(x, y)-\eta\mathrm{KL}\left(\pi_\theta^{\mathrm{RL}}(y \mid x) \parallel\pi^{\mathrm{SFT}}(y \mid x)\right)\]</span> 其中，<span class="math inline">\(r(x, y)\)</span>是奖励模型给出的 Reward，<span class="math inline">\(\eta\)</span> 是 KL散度的惩罚系数。通过引入惩罚，策略模型在优化过程中不仅会追求更高的奖励，还会尽量保持与参考模型的生成行为一致，从而避免策略崩溃。</p><p>为了有效地计算策略模型与参考模型之间的 KL散度，我们可以从<strong>对数概率</strong>（log_prob）的角度来实现这一过程：<span class="math display">\[\mathrm{KL}\left(\pi_\theta^{\mathrm{RL}}(y \mid x) \parallel\pi^{\mathrm{SFT}}(y \mid x)\right) = \sum_{t} \left( \log\pi_\theta^{\mathrm{RL}}(y_t \mid x, y_{&lt;t}) - \log\pi^{\mathrm{SFT}}(y_t \mid x, y_{&lt;t}) \right)\]</span> 具体步骤如下：</p><ol type="1"><li>使用策略模型进行<strong>采样</strong>：对输入 <spanclass="math inline">\(x\)</span> 生成的输出 <spanclass="math inline">\(y\)</span>。</li><li>计算<strong>策略模型的对数概率</strong>（log_probs）：计算策略模型在每个时间步（Token）的对数概率。</li><li>计算<strong>参考模型的对数概率</strong>（ref_log_probs）：对<strong>相同的输入输出</strong>，使用参考模型计算逐Token 的对数概率。</li><li>计算 KL散度：通过<strong>对数概率之差累加</strong>得到策略模型与参考模型之间的KL 散度。</li><li>整合奖励：Token 级的 KL 散度与奖励模型在输出 <spanclass="math inline">\(y\)</span> 上序列级 Reward 组合，得到 <spanclass="math inline">\(r_{\text {total}}\)</span>。</li></ol><blockquote><p>在实际使用的时候，KL散度还有一种<strong>非负、无偏、低方差</strong>的估计方式： <spanclass="math display">\[\mathrm{KL}\left(\pi_\theta^{\mathrm{RL}}(y \mid x) \parallel\pi^{\mathrm{SFT}}(y \mid x)\right) = \frac{\pi^{\mathrm{SFT}}(y_t \midx, y_{&lt;t})}{\pi_\theta^{\mathrm{RL}}(y_t \mid x, y_{&lt;t})} - \log\frac{\pi^{\mathrm{SFT}}(y_t \mid x,y_{&lt;t})}{\pi_\theta^{\mathrm{RL}}(y_t \mid x, y_{&lt;t})} - 1\]</span> 参考 John Schulman’s Blog：<ahref="http://joschu.net/blog/kl-approx.html">Approximating KLDivergence</a></p></blockquote><h3 id="参考模型-reference-model">参考模型 | Reference Model</h3><p>参考模型在 RLHF中扮演着「锚点」的角色，同样使用<strong>预训练语言模型的 SFT后的版本</strong>初始化，并且全程<strong>冻结参数</strong>不更新。参考模型只在采样经验后计算log_prob 时用到。</p><p>初学者可能会误认为参考模型就是经典 PPO 中的 <spanclass="math inline">\(\pi_{\theta_{\mathrm{old}}}^{\mathrm{RL}}\)</span>，但实际上<spanclass="math inline">\(\pi_{\theta_{\mathrm{old}}}^{\mathrm{RL}}\)</span>只是相对不断更新的 <spanclass="math inline">\(\pi_\theta^{\mathrm{RL}}\)</span>的一个延迟拷贝（甚至<strong>不需要显式拷贝</strong>，只需要用 <spanclass="math inline">\(\pi_\theta^{\mathrm{RL}}\)</span> 采样经验、计算log_prob 后就可以放任其更新）。</p><h3 id="奖励模型-reward-model">奖励模型 | Reward Model</h3><p>奖励模型是负责量化人类对生成文本的偏好。在传统 RL任务中，奖励代表环境对于每个状态-动作对的即时反馈，但是在 LLM训练中则是对整个序列赋予奖励。奖励模型通常是一个经过特殊微调的语言模型，<strong>其输入是生成的文本，输出是一个标量值</strong>，表示该文本的「质量」或「符合人类偏好的程度」。</p><p>从架构上看，奖励模型是在原始 SFT 模型基础上，添加了一个 <strong>ValueHead</strong> 线性层（MLP）。这个 Head 的输入是序列的隐藏状态（HiddenStates），输出是一个标量（Scalar），表示每个位置的得分预测。具体而言：</p><ul><li>输入：模型的隐藏状态，形状为<code>[batch_size, seq_length, hidden_size]</code>。</li><li>价值头：线性层，将隐藏状态映射到奖励值，输出形状为<code>[batch_size, seq_length, 1]</code>。</li><li>输出：通常<strong>只取最后一个标记</strong>（比如EOS_token）的奖励值作为整个回复的得分，形状为<code>[batch_size, 1]</code>；也可以根据需要<strong>对所有位置的奖励值取平均</strong>（但考虑到Decoder-only 模型的性质，只有 EOS_token输出时才能看完整句子给出一个整体评价，一般不会用平均）。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Reward Model 模型架构伪代码</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RewardModel</span>(<span class="hljs-params">AutoModelForCausalLM</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, config, tokenizer</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__(config)<br>        self.tokenizer = tokenizer<br>        self.value_head = torch.nn.Linear(config.hidden_size, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, input_ids, attention_mask</span>):</span><br>      <span class="hljs-comment"># 获取基础模型的输出</span><br>        outputs = self.model(<br>            input_ids=input_ids,<br>            attention_mask=attention_mask,<br>            return_dict=<span class="hljs-literal">True</span>,<br>            use_cache=<span class="hljs-literal">False</span><br>        )<br>        hidden_states = outputs.last_hidden_state  <span class="hljs-comment"># [batch_size, seq_length, hidden_size]</span><br>        rewards = self.value_head(hidden_states)  <span class="hljs-comment"># [batch_size, seq_length, 1]</span><br>        <span class="hljs-comment"># 取最后一个标记的得分作为整体得分</span><br>        final_reward = rewards[:, -<span class="hljs-number">1</span>, :]           <span class="hljs-comment"># [batch_size, 1]</span><br>        <span class="hljs-keyword">return</span> final_reward.squeeze(-<span class="hljs-number">1</span>)            <span class="hljs-comment"># [batch_size]</span><br></code></pre></td></tr></table></figure><p>在 PPO训练阶段，奖励模型的参数通常是<strong>冻结的</strong>，不参与更新。而在这之前，奖励模型需要依赖<strong>成对偏好数据</strong>（PairwisePreferenceData）进行训练。这是因为直接对文本标注绝对分数可能存在标注者主观性和噪音，成对比较可以<strong>更有效地捕捉人类偏好</strong>—— 出自 OpenAI 2020 年论文《<ahref="https://arxiv.org/abs/2009.01325">Learning to Summarize with HumanFeedback</a>》。</p><p>具体而言，针对一组提示，我们使用指令微调后的模型生成多个不同的回复。再由人类标注者根据偏好进行<strong>排序，两两组合</strong>形成了成对的偏好数据，用于训练奖励模型。其中，每条训练数据由<span class="math inline">\((x,y_\text{win},y_\text{lose})\)</span>三元组构成，其中 <span class="math inline">\(x\)</span> 是查询，<spanclass="math inline">\(y_\text{win}\)</span> 和 <spanclass="math inline">\(y_\text{lose}\)</span>分别是被选中的回复和被拒绝的回复。</p><p>奖励模型通过<strong>最大化选中回复与拒绝回复得分的差异</strong>，学习人类的偏好，即<strong>成对排序损失（PairwiseRanking Loss）</strong>： <span class="math display">\[L^{\text{RM}}(\psi) = \log \sigma(r(x, y_w) - r(x, y_l))\]</span> 其中，<span class="math inline">\(r(x, y_w)\)</span> 和 <spanclass="math inline">\(r(x, y_l)\)</span>分别是奖励模型对选中回复与拒绝回复的标量预测值，<spanclass="math inline">\(\sigma\)</span> 是 sigmoid 函数。在 2021 年Anthropic 的论文《<a href="https://arxiv.org/abs/2112.00861">A GeneralLanguage Assistant as a Laboratory forAlignment</a>》中，为了使奖励模型不仅能够比较两个回复的优劣，还能够生成更符合人类偏好的回复，可以在损失函数中加入<strong>自回归语言模型的损失</strong>，即在选中回复上引入语言模型的<strong>对数似然项</strong>。奖励模型的整体损失函数可以表示为：</p><p><span class="math display">\[L^{\text{RM}}(\psi) = -\lambda \mathbb{E}_{(x, y_w, y_l) \sim\mathcal{D}_{\text{rm}}} \left[ \log \sigma(r(x, y_w) - r(x, y_l))\right] + \beta_{\text{rm}} \mathbb{E}_{(x, y_w) \sim\mathcal{D}_{\text{rm}}} \left[ \log(r&#39;(x, y_w)) \right]\]</span></p><p>其中，<span class="math inline">\(\lambda\)</span> 和 <spanclass="math inline">\(\beta_{\text{rm}}\)</span> 是超参数。<spanclass="math inline">\(r^′\)</span> 与 <spanclass="math inline">\(r\)</span> 是除线性层外相同的模型，<spanclass="math inline">\(r^′(x,y_w)\)</span> 是在给定提示 <spanclass="math inline">\(x\)</span> 下，生成选中响应 <spanclass="math inline">\(y_w\)</span>时的似然。通过同时最小化排序损失和语言模型损失，奖励模型既学习了人类的偏好排序，也保留了生成高质量文本的能力。</p><h3 id="价值模型-value-model">价值模型 | Value Model</h3><p>价值模型在 RLHF中负责估计<strong>每个状态</strong>的价值函数，扮演经典 PPO 算法中的Critic Model角色。其作用是预测当前状态下（前缀序列）未来累积奖励的期望值，记为<strong>期望总收益</strong><span class="math inline">\(V_t\)</span>。这个 <spanclass="math inline">\(V_t\)</span>则在采样时为策略模型提供<strong>优势函数</strong>（AdvantageFunction），帮助策略模型更好地理解哪些动作能够带来更高的奖励。</p><p>通常，价值模型是由<strong>奖励模型初始化</strong>而来，利用奖励模型对即时收益的评估能力，拓展到对未来累积奖励的估计。从架构上看，它们有着相似的结构，都是在语言模型的基础上添加了一个ValueHead，但价值模型会对序列中的<strong>每个状态</strong>进行评分，并全部返回。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Critic Model 模型架构</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CriticModel</span>(<span class="hljs-params">AutoModelForCausalLM</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, config, tokenizer</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__(config)<br>        self.tokenizer = tokenizer<br>        self.value_head = torch.nn.Linear(config.hidden_size, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, input_ids, attention_mask</span>):</span><br>        <span class="hljs-comment"># 获取基础模型的输出</span><br>        outputs = self.model(<br>            input_ids=input_ids,<br>            attention_mask=attention_mask,<br>            return_dict=<span class="hljs-literal">True</span>,<br>            use_cache=<span class="hljs-literal">False</span><br>        )<br>        <span class="hljs-comment"># 获取每个时间步的隐藏状态</span><br>        hidden_states = outputs.last_hidden_state  <span class="hljs-comment"># [batch_size, seq_length, hidden_size]</span><br><br>        <span class="hljs-comment"># 计算每个时间步的价值估计</span><br>        values = self.value_head(hidden_states)    <span class="hljs-comment"># [batch_size, seq_length, 1]</span><br>        values = values.squeeze(-<span class="hljs-number">1</span>)                <span class="hljs-comment"># [batch_size, seq_length]</span><br><br>        <span class="hljs-keyword">return</span> values  <span class="hljs-comment"># 返回序列中每个位置的价值估计</span><br></code></pre></td></tr></table></figure><p>价值模型在 PPO训练过程中会不断更新，其学习目标是最小化其预测值与目标价值之间的差异，损失函数通常使用<strong>均方误差</strong>（MeanSquared Error，MSE）来定义： <span class="math display">\[L^{\text{Critic}}(\phi) = \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \|V_\phi(x) - V_t^{\text{target}} \|^2 \right]\]</span></p><p>其中，<span class="math inline">\(V_\phi(x)\)</span> 是价值模型对状态<span class="math inline">\(x\)</span> 的价值估计，<spanclass="math inline">\(V_t^{\text{target}}\)</span>可以是<strong>实际累积奖励的估计值</strong> <spanclass="math inline">\(\hat{R}(x, y)=\sum_{l=0}^{\infty}\gamma^lr_{t+l}\)</span>，也可以是<strong>使用 TDError 估计的优势价值</strong>。</p><h2 id="instructgpt-论文解读">InstructGPT 论文解读</h2><p>2022 年，OpenAI 发表了论文《Training language models to followinstructions with humanfeedback》，利用人类反馈微调语言模型，在多种任务上实现模型与用户意图对齐，得到<strong>InstructGPT</strong>。InstructGPT 有三个版本，分别是1.3B、6B、175B 参数量。在人类评估中，1.3B 参数 InstructGPT模型的输出好于 175B GPT-3 的输出，同时参数少 100 倍。此外，InstructGPT模型在输出的真实性、毒性输出等方面有改进，证明该方法是使语言模型与人类意图对齐的有前景方向。</p><p><img src="/img/blog/RL-Note-11-images/Performance.png" alt="InstructGPT(PPO-ptx)的表现" width=60% /></p><p>文中提出的三阶段训练方法开启了 LLM 训练的新范式：</p><figure><img src="/img/blog/RL-Note-11-images/InstructGPT.png"alt="训练LLM的三阶段：SFT、RM、RLHF" /><figcaptionaria-hidden="true">训练LLM的三阶段：SFT、RM、RLHF</figcaption></figure><ol type="1"><li><p><strong>构造人类指令数据集，进行监督微调</strong>：作者聘请了 40个人的团队来标注数据，主要是针对用户提交给 OpenAI API的问题标了一些期望的回复（human-writtendemonstrations），并使用这个标注的数据微调GPT-3，这一步是有监督的微调（SupervisedFine-Tuning，SFT）。但是值得注意的是，SFT的数据集是一种<strong>对话形式的数据集</strong>，这种数据集的采集成本很大，所以通常数据的量级不大。</p></li><li><p><strong>构造偏好对数据集，训练 RewardModel</strong>：对第一步训练的模型喂入一些Prompt，使它输出多个结果。再让人类来打分，所得到的这些包含人类偏好的数据集用于训练一个奖励模型。这个模型的作用是预测人类更喜欢模型的哪个输出。</p></li><li><p><strong>使用第二步的 RM + PPO算法优化第一步训练的模型以最大化奖励</strong>：将第一步训练的 LLM模型视为策略模型，采用 PPO 的 RL 方法训练 LLM模型，得到最终的模型。</p></li></ol><h3 id="对齐税与-ppo-ptx">对齐税与 PPO-ptx</h3><p>对齐税（Alignment Tax）指 LLM 在通过 RLHF进行偏好对齐时，其通用任务能力可能出现的退化现象。其本质是对齐过程中施加的偏好约束（如安全准则、伦理规范）与预训练阶段的自由探索之间<strong>存在目标冲突</strong>。</p><p>在大部分时候 PPO 算法中会使用 PPO-clip 作为目标函数： <spanclass="math display">\[\mathcal{L}_{\mathrm{ppo}-\mathrm{clip}}(\theta)=\hat{\mathbb{E}}_t\left[\min\left(\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old}}}\left(a_t \mid s_t\right)} \hat{A}_t,\operatorname{clip}\left(\frac{\pi_\theta\left(a_t \mids_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)},1-\epsilon, 1+\epsilon\right) \hat{A}_t\right)\right]\]</span> 为了缓解对齐税的影响，作者还探索了将预处理数据纳入 RL阶段。利用此方法的模型表示为 PPO-ptx，组合后的目标函数如下： <spanclass="math display">\[\mathcal{L}_{\text {ppo-ptx }}(\theta)=\mathcal{L}_{\text {ppo-clip}}(\theta)+\lambda_{\text {ptx }} \mathbb{E}_{x \sim \mathcal{D}_{\text{pretrain }}}\left[\log \left(\pi_\theta^{\mathrm{RL}}(x)\right)\right]\]</span> 其中，<span class="math inline">\(\lambda_{\text{ptx}}\)</span> 是预训练损失的占比，论文中将其设为 <spanclass="math inline">\(27.8\)</span>。</p><h2 id="讨论rlhf-vs.-sft">讨论：RLHF vs. SFT</h2><ol type="1"><li><strong>奖励建模角度</strong>：有一些 LLM需要的目标函数是难以通过规则定义的，比如说<strong>无害性、有帮助性</strong>，如果我们希望模型最后具有这些好的特性，就需要制定这样的训练目标函数；而用人类的偏好学习一个reward model 再用 RL 来训练，就自然的可以将这些特性融合到 LLM里面。</li><li><strong>探索性角度</strong>：对于SFT，人类的高质量样本确实很快速让模型学会指令输出，但<strong>样本始终是有限的</strong>，并且很多任务我们<strong>没法给出标准答案</strong>；而对于RL，我们可以探索出无穷多的样本用于训练，偏好排序的形式也更适合筛选更合适的样本，甚至可能会<strong>探索到比SFT 数据更优质的解</strong>。</li><li><strong>监督信号角度</strong>：SFT中模型只要稍微偏移答案就会收到惩罚；而 RL对不同的答案也可能给出正反馈，间接鼓励了模型输出的多样性。</li><li><strong>幻觉角度</strong>：对于知识密集型任务，如果模型内部具有这个知识，那么SFT 会让其将知识和问题联系起来；但如果模型预训练时压根没见过这个知识，则<strong>SFT可能会让模型学会说谎</strong>（强行拟合输出）。相比之下，RLHF则可以根据答案的正确性和模型的置信度给出多样化的反馈，一定程度上减少了幻觉。</li></ol><h2 id="讨论rlhf-与幻觉问题">讨论：RLHF 与幻觉问题</h2><blockquote><p>相关资料：<a href="https://www.youtube.com/watch?v=hhiLw5Q_UFg">JohnSchulman - Reinforcement Learning from Human Feedback: Progress andChallenges - YouTube</a></p></blockquote><p>LLMs有一个众所周知的「硬伤」——它们经常会一本正经编造貌似真实的内容，俗称幻觉（Hallucination）。</p><p>当人们说幻觉时，主要指的是两类不同情况：</p><ul><li>第一类幻觉是<strong>语言模型的模式完成（PatternCompletion）行为</strong>。它们的训练目的是最大化文本可能性，使生成的内容看起来很像互联网上的文本。这主要有三个原因：<ol type="1"><li><strong>它不知道自己可以回答“我不知道”或者表达不确定性</strong>。如果告诉模型可以回答“我不知道”，那么在一定程度上能解决幻觉问题；</li><li><strong>模型有时不愿意去质疑前提（premise）</strong>，它认为前提是数据分布的一部分；</li><li><strong>模型有时会陷入谎言之中</strong>。如果模型已经犯了一个错误，那么它会认为自己应该继续回答下去，生成一连串响应，这也意味着它会继续说谎。</li></ol></li><li>第二类幻觉就是<strong>单纯的「猜错了」</strong>。就像人类一样，你可能只遇到过一次某件事情，自己不能确定，感到很模糊，所以在回答时必须带点猜测，有时可能就会猜错。这主要有两个原因：<ol type="1"><li><strong>模型在训练时被倾向于输出更全面的答案</strong>。当答案中包含模糊的东西时，模型自动地将其补全，以便「骗过」奖励模型，取得更高的分数。</li><li><strong>有监督微调（SFT）时教会了模型乱猜</strong>。这就涉及到<strong>行为克隆</strong>（BehaviorCloning）这一微调过程。</li></ol></li></ul><h3 id="行为克隆造成幻觉">行为克隆造成幻觉</h3><p>行为克隆（BehaviorCloning）是强化学习领域的一个术语，意思是<strong>有监督微调或最大化似然</strong>（MaximizingLikelihood），其目的是完成给定 Prompt的最大化似然或最大化对数概率。通常我们认为在模型训练的三个阶段中，SFT最容易引入幻觉。</p><p>这是由于，当我们进行 SFT时，使用的是人类编写的正确指令对来训练，此时的监督训练目标是模仿指令对的输出（包括格式和内容）。当然，我们的本意是「<strong>教会模型按照我们想要的方式完成指令</strong>」，但如果此时训练集中含有关于2022 年世界杯的问题，但第一阶段预训练的知识库截至于 2021年，此时模型并不了解 2022年比赛的结果。在这种情况下，我们实际上<strong>不是在训练模型输出正确答案，而是在训练它在这种问题上进行猜测</strong>。</p><p>如果你使用行为克隆来训练模型，那么无法避免出现上述幻觉问题。同时也会出现相反的问题，即<strong>如果你想训练模型在某些情况下回答「我不知道」，那么它可能会隐瞒实际上已经知道的信息。</strong>例如，如果标注者不知道答案，他们可能会将「我不知道」列为目标答案，但实际上网络可能已经有了答案，而模型在预训练时已经学会，你只是在训练模型隐瞒信息。</p><p>因此，行为克隆或监督学习的问题在于：<strong>正确的目标实际上取决于神经网络中包含了哪些知识，而这对于收集数据或进行实验的人来说是未知的。</strong>因此，除非你有一种方法来查看模型中的内容，否则无法使用行为克隆训练出真实可信的模型。</p><h3 id="模型对自身不确定性的认知">模型对自身不确定性的认知</h3><p>我们希望模型在不知道正确答案时能<strong>输出其知识的实际状态并表明不确定性，而不是进行猜测</strong>。那么模型是否知道自己的不确定性呢？比如给定一个问题，模型清楚自身是否知道答案吗？这个问题很难回答。什么是「知道」？如果模型知道某些东西，那么能否用最简单的东西将其反应出来？</p><p>John Schulman认为模型肯定是知道自身的不确定性的。这是因为模型被训练为最小化对数损失，为此它必须输出概率。模型的下一个token预测是经过校准的（calibrated），<strong>校准后的对数损失</strong>是一个合适的表示不确定性的指标。</p><p>预训练目标产生了一个校准的模型，它必定输出合理的概率，这意味着，模型知道自身的不确定性，至少对于任何可以被转化为预测单个token 的短答案的问题，它可以为该 token 给出一个合理的概率分布。Anthropic的论文《<a href="https://arxiv.org/abs/2207.05221">Language Models(Mostly) Know What They Know</a>》对这个现象给出了更详尽的分析。</p><p>因此，<strong>如果模型确实知道自己的不确定性，行为克隆无法利用这一点来避免幻觉，强化学习才是解决这个问题正道</strong>。</p><p>部分幻觉仅因为模型陷入想要「给出完整答案」的模式或不知道如何表达不确定性而产生，因此这类幻觉很好解决。比如可在训练模型时给出一些表明「我不知道」、「我的知识截止于××日期」的示范，或者给出一些质疑用户提问的范例，这样模型至少能够表达不确定性，只是表达的时机可能不是那么恰当。</p><h3 id="rlhf-用于缓解幻觉">RLHF 用于缓解幻觉</h3><p>一个很朴素的想法是，当模型给出一个答案，如果是<strong>非常自信的正确答案</strong>，将得到高额奖励；如果是<strong>模糊的正确答案</strong>，将得到稍差的奖励；如果是<strong>无信息的答案</strong>，例如“我不知道”，将得到一些惩罚；如果是<strong>模糊的错误答案和完全错误的答案</strong>，将得到更多的惩罚。这基本是一个适当的评分规则，能够激励模型给出自信的答案，如果它对错误答案过于自信，就会给出相应惩罚。</p><p>但通过强化学习训练语言模型来实现这一目标并不容易，因为你需要知道答案是否正确，但答案究竟正确与否我们无从知道。更复杂的情况，当模型输出Long-form Answer时，其中很可能包含正确与错误的部分——<strong>没有完美的答案，也没有完美的评判标准</strong>。</p><p>因此，更合理的办法是让标注者对回答进行排序，并说出哪个更好。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL 学习笔记 #10 近端策略优化（PPO）理论</title>
    <link href="/RL-Note-10.html"/>
    <url>/RL-Note-10.html</url>
    
    <content type="html"><![CDATA[<p>赵世钰老师的课程到 Actor-Critic方法就结束了，接下来我们趁热打铁来学习 OpenAI 于 2017年提出的<strong>近端策略优化</strong>（Proximal PolicyOptimization，PPO）算法。它通过结合<strong>策略梯度方法</strong>和<strong>近端优化思想</strong>的优点，提供了一个高效且稳定的策略优化方案。</p><p>接下来的内容将会涵盖 PPO 的理论基础、LLM-RLHF 中 PPO的应用、OpenRLHF-PPO 代码实践。</p><p>附上一些参考资料：</p><ul><li>猛猿老师的文章：<ahref="https://zhuanlan.zhihu.com/p/677607581">人人都能看懂的 PPO原理与源码解读</a>、<ahref="https://mp.weixin.qq.com/s/XU9MznCUFYkoHCrdQmV68w">人人都能看懂的RL-PPO 理论知识</a></li><li>OpenAI 论文：<a href="https://arxiv.org/abs/1707.06347">ProximalPolicy Optimization Algorithms</a></li><li>FudanNLP 论文：<a href="https://arxiv.org/abs/2307.04964">Secrets ofRLHF in Large Language Models Part I: PPO</a></li><li>GitHub 仓库：首推 <ahref="https://github.com/OpenRLHF/OpenRLHF">OpenRLHF</a>、其次 <ahref="https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning">DeepSpeed-Chat</a></li></ul><h2 id="动机朴素-actor-critic-的局限性">动机：朴素 Actor-Critic的局限性</h2><p>在深入探讨 PPO 之前，我们先回顾一下在上一节 Actor-Critic方法中，我们做出的两个重要改进：</p><ul><li><strong>引入 TD Error作为优势函数</strong>，以降低策略梯度估计的方差，提高了学习效率。</li><li><strong>引入重要性采样</strong>，从 Off-Policy数据中学习，提高了数据利用率。</li></ul><p>然而，这些方法在实践中仍存在一些局限性。</p><h3 id="td-error-估计不准">TD Error 估计不准</h3><p>在 Advantage Actor-Critic 方法中，Critic 通过 TD Error来估计优势函数： <span class="math display">\[\delta_t\left(s_t, a_t\right) = r_{t+1} +\gamma v_t(s_{t+1}) -v_t(s_t)\]</span> 然而，TD Error 的估计依赖于当前策略的价值函数 <spanclass="math inline">\(v_t\)</span>，这个估计本身可能存在<strong>偏差</strong>。特别是在训练的早期阶段，Critic网络的参数尚未收敛，导致估计不准确。这种不准确性会直接影响策略更新的方向，可能导致策略性能的波动甚至下降。</p><p>此外，TD Error的估计还受到<strong>采样噪声</strong>的影响。由于强化学习中的采样过程通常是随机的，Critic网络可能会受到噪声的干扰，进一步加剧了估计的不准确性。因此，如何平衡 TDError 的估计偏差和方差，是提升 Actor-Critic 方法稳定性的关键。</p><h3 id="行为策略分布差异过大">行为策略分布差异过大</h3><p>在 Off-Policy Actor-Critic 方法中，我们使用重要性采样来修正行为策略<span class="math inline">\(\pi_b\)</span> 和目标策略 <spanclass="math inline">\(\pi_\theta\)</span>之间的分布差异。然而，当行为策略和目标策略的分布差异过大时，可能会使得重要性权重<span class="math inline">\(\rho_t = \dfrac{\pi_\theta(a_t \mids_t)}{\pi_b(a_t \mid s_t)}\)</span>变得非常大，导致<strong>梯度估计的方差急剧增大</strong>，进而使得训练过程不稳定。</p><p>通常，为了避免这种高方差，我们不会使用一个完全无关的行为策略，而是用前几个时间步的旧策略<spanclass="math inline">\(\pi_{\theta_\text{old}}\)</span>。这样既能够充分利用经验数据，又能够使训练变得稳定。唯一的问题就是我们需要<strong>控制策略更新的幅度</strong>，确保<span class="math inline">\(\pi_{\theta_\text{old}}\)</span> 和 <spanclass="math inline">\(\pi_\theta\)</span> 不能差异过大。</p><h2 id="proximal-policy-optimization-ppo">Proximal Policy Optimization |PPO</h2><p>为了解决上述问题，<strong>近端策略优化</strong>（Proximal PolicyOptimization，PPO） 应运而生。PPO的核心目标是<strong>避免策略更新中的剧烈变化</strong>，确保每次更新都不会使策略偏离原有策略太远，从而保持训练的稳定性。</p><p>「近端」一词来源于优化问题中的<strong>近端优化</strong>（ProximalOptimization）概念。它指的是在优化过程中，通过引入某种约束或惩罚机制，限制每次更新的幅度，使得新策略不会偏离当前策略太远，从而保证优化的稳定性和安全性。这种约束可以通过以下两种方式实现：</p><ol type="1"><li><strong>显式约束</strong>：例如，限制新旧策略之间的 KL散度（Kullback-LeiblerDivergence），确保它们之间的差异不超过某个阈值。</li><li><strong>隐式约束</strong>：例如，通过裁剪（Clipping）重要性权重来间接限制策略更新的幅度。</li></ol><p>下面，我们将介绍 PPO 如何解决我们前面说到的两个问题：TD Error估计不准、行为策略分布差异过大。</p><h3 id="平衡偏差与方差">平衡偏差与方差</h3><p>在 TD Error 的估计过程中，偏差和方差始终是最关键的问题：</p><ul><li><strong>偏差（Bias）</strong>：反映策略更新的方向是否准确，这里的偏差主要来自于<span class="math inline">\(v_t\)</span>。</li><li><strong>方差（Variance）</strong>：反映策略更新的稳定性，这里的方差主要来自于采样噪音。</li></ul><p>我们自然希望能够实现一个<strong>低偏差、低方差</strong>的算法：当<span class="math inline">\(v_t\)</span> 能够准确估计策略 <spanclass="math inline">\(\pi\)</span> 的价值 <spanclass="math inline">\(v_\pi\)</span> 时，<spanclass="math inline">\(\delta_t\left(s_t, a_t\right)\)</span>至少就满足了低偏差的需求。注意，虽然优势函数已经显著降低了方差，但依然不能完全避免采样过程中随机变量<span class="math inline">\(r_{t+1}\)</span> 和 <spanclass="math inline">\(s_{t+1}\)</span> 带来的方差。</p><p>然而，大部分时候 <span class="math inline">\(v_t\)</span>都不能够准确估计 <spanclass="math inline">\(v_\pi\)</span>，为了缓解高偏差问题，一个直接的想法是：减少对<span class="math inline">\(v_t\)</span>的依赖，使用<strong>蒙特卡洛估计</strong>（类似 N-Step SARSA）！对于<span class="math inline">\(r_{t+1} +\gamma v_t(s_{t+1})-v_t(s_t)\)</span>，我们可以将 <spanclass="math inline">\(v_t(s_{t+1})\)</span> 项展开： <spanclass="math display">\[\begin{aligned}A_t^{(\infty)}&amp;= r_{t+1} +\gamma v_t(s_{t+1}) -v_t(s_t)\\&amp;= r_{t+1} +\gamma (r_{t+1} + \gamma (r_{t+2} + \cdots) ) - v_t(s_t)\\&amp;= \sum_{l=1}^{\infty}\gamma^{l-1} r_{t+l} - v_t(s_t) \\\end{aligned}\]</span></p><p>其中，<span class="math inline">\(\{r_i\}\)</span>是采样到的即时奖励数据，如果 <span class="math inline">\(v_t\)</span>不准，我们就<strong>信任实际采样结果</strong>，这样至少不会造成太大偏差。</p><p>然而，这样做又会引发一个新问题：前面我们提到优势函数中的随机变量<span class="math inline">\(r_{t+1}\)</span> 和 <spanclass="math inline">\(s_{t+1}\)</span>会带来方差，此时我们使用了更多采样数据，相当于引入<strong>更多的随机变量</strong>，方差也将变得更大。</p><h3 id="广义优势估计-gae">广义优势估计 | GAE</h3><p>一个自然而然的想法就是：在两种估计间寻找平衡。为此，PPO引入了<strong>广义优势估计</strong>（Generalized AdvantageEstimation，GAE）。GAE 通过<strong>在时间步上对 TD Error进行加权累加</strong>，提供了一个在偏差和方差之间可调的优势估计器。其定义为：<span class="math display">\[\begin{aligned}A_t^{\text{GAE}(\gamma, \lambda)} &amp;= \delta_t + (\gamma \lambda)\delta_{t+1} + (\gamma \lambda)^2 \delta_{t+2} + \cdots \\&amp;= \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}\\\end{aligned}\]</span> 其中：</p><ul><li><span class="math inline">\(\delta_t\)</span> 是第 <spanclass="math inline">\(t\)</span> 步的 TD Error：<spanclass="math inline">\(\delta_t = r_t + \gamma v_t(s_{t+1}) -v_t(s_t)\)</span>，这里为了简洁将第 <spanclass="math inline">\(t\)</span> 步的即时奖励记为 <spanclass="math inline">\(r_t\)</span>；</li><li><span class="math inline">\(\gamma\)</span> 是折扣因子，<spanclass="math inline">\(\lambda \in [0, 1]\)</span> 是 <strong>GAE的衰减系数，控制偏差和方差之间的平衡</strong>。</li></ul><p>其推导过程涉及到对展开项 <spanclass="math inline">\(A_t^{(1)},A_t^{(2)},\cdots,A_t^{(\infty)}\)</span>的 <span class="math inline">\(\lambda\)</span>指数衰减求和，可以查看发表于 ICLR 2016 的<ahref="https://arxiv.org/abs/1506.02438">原论文</a>。</p><p>通过调整 <span class="math inline">\(\lambda\)</span>的值，可以在偏差和方差之间进行调节：</p><ul><li>当 <span class="math inline">\(\lambda = 0\)</span> 时，只考虑一步的TD Error，偏差较大，但方差较小。</li><li>当 <span class="math inline">\(\lambda = 1\)</span>时，优势估计等价于蒙特卡洛方法，偏差较小，但方差较大。</li></ul><p>通过调整 <spanclass="math inline">\(\lambda\)</span>，我们可以在偏差和方差之间取得最佳平衡，估计出更准确的优势。</p><h3 id="ppo-的前身trpo">PPO 的前身：TRPO</h3><p>在 PPO 出现之前，<strong>信赖域策略优化</strong>（Trust Region PolicyOptimization，TRPO）是一种解决策略更新过大问题的重要方法。其基本思想是，在策略更新过程中，<strong>通过限制新旧策略之间的距离，确保每次更新都不会偏离当前策略太远</strong>。</p><p>这里的「距离」使用的是 <strong>KL 散度</strong>（Kullback-LeiblerDivergence），要求不超过一个预设的阈值。于是 TRPO 的优化目标为： <spanclass="math display">\[\max_\theta \mathbb{E}_{s \sim \pi_{\theta_{\text{old}}}, a \sim\pi_{\theta_{\text{old}}}} \left[ \frac{\pi_\theta(a\mids)}{\pi_{\theta_{\text{old}}}(a\mid s)} A^{\pi_{\theta_{\text{old}}}}(s,a) \right]\]</span></p><p>约束条件：</p><p><span class="math display">\[\mathbb{E}_{s \sim \pi_{\theta_\text{old}}} \left[ \mathrm{KL} \left(\pi_{\theta_\text{old}}(\cdot \mid s)  \parallel  \pi_\theta(\cdot \mids) \right) \right] \leq \delta\]</span></p><p>其中：</p><ul><li><p><span class="math inline">\(D_{\mathrm{KL}}\)</span> 表示 KL散度，在这里用于度量新旧策略的差异；</p></li><li><p><span class="math inline">\(\delta\)</span>是预设的<strong>信赖域</strong>（Trust Region）阈值，所谓信赖域就是要让<span class="math inline">\(\pi_\theta\)</span> 落在 <spanclass="math inline">\(\pi_{\theta_\text{old}}\)</span>附近，这个区域是可信的；</p></li><li><p>注意：这里的 <spanclass="math inline">\(A^{\pi_{\theta_{\text{old}}}}(s, a)\)</span>是在<strong>旧策略上计算的优势</strong>，与之前都不同。</p></li></ul><blockquote><p>KL 散度是衡量两个概率分布差异的非对称性指标。对于分布 <spanclass="math inline">\(P\)</span> 和 <spanclass="math inline">\(Q\)</span>，其定义为：</p><p><span class="math display">\[D_{KL}(P \parallel Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}\]</span></p><p>其值满足非负性且 <span class="math inline">\(D_{KL}(P \parallel Q)\neq D_{KL}(Q \parallel P)\)</span>，反映用 <spanclass="math inline">\(Q\)</span> 近似 <spanclass="math inline">\(P\)</span> 时的信息损失。定义推导来自交叉熵 $ H(P,Q) = -P(x)Q(x)$ 与真实分布熵 <span class="math inline">\(H(P) = -\sumP(x)\log P(x)\)</span> 的差值。</p></blockquote><p>TRPO在理论上具有良好的收敛性，但实现复杂，需要进行二阶导数的计算和共轭梯度优化，在高维参数空间中计算成本较高。更多细节可以参考<ahref="https://arxiv.org/abs/1502.05477">原论文</a>，发表于 ICML2015。</p><h3 id="ppo-目标函数">PPO 目标函数</h3><p>之所以 TRPO的优化过程复杂，就是因为它<strong>将约束条件独立于目标函数之外</strong>。因此，为了简化TRPO 的实现，PPO提出了两种主要形式的目标函数：<strong>PPO-Penalty</strong> 和<strong>PPO-Clip</strong>。</p><h4 id="显式约束ppo-penalty">显式约束：PPO-Penalty</h4><p>PPO-Penalty 在目标函数中<strong>直接加入了 KL散度的惩罚项</strong>，将约束条件转化为惩罚项，使得优化过程可以使用一阶优化方法完成。</p><p>目标函数为：</p><p><span class="math display">\[L^{\mathrm{PEN}}(\theta) = \mathbb{E}_{t} \left[ r_t(\theta)A_t^{\text{GAE}(\gamma, \lambda)} - \beta D_{\mathrm{KL}} \left(\pi_{\theta_{\text{old}}}(\cdot \mid s_t) \,||\, \pi_\theta(\cdot \mids_t) \right) \right]\]</span></p><p>其中：</p><ul><li><span class="math inline">\(r_t(\theta) =\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}\)</span>是新旧策略的概率比率；</li><li><span class="math inline">\(A_t^{\text{GAE}(\gamma,\lambda)}\)</span> 是优势估计，这里引入了 GAE 来平衡；</li><li><span class="math inline">\(\beta\)</span>是<strong>惩罚系数</strong>，控制 KL 散度对目标函数的影响。</li></ul><p>PPO-Penalty 通过在优化过程中监控 KL 散度，根据实际值调整 <spanclass="math inline">\(\beta\)</span>的大小，实现对策略更新幅度的动态控制：</p><ul><li>首先，我们对 <span class="math inline">\(D_{\mathrm{KL}}\)</span>也会设置 threshold，我们分别记为 <spanclass="math inline">\(D_{\max}\)</span> 和 <spanclass="math inline">\(D_{\min}\)</span>。</li><li>当 <span class="math inline">\(D_{\mathrm{KL}} \ge D_{\max}\)</span>时，说明当前策略已经<strong>偏离旧策略较远</strong>了，这时我们应该增大<span class="math inline">\(\beta\)</span>，把分布拉回来。</li><li>当 <span class="math inline">\(D_{\mathrm{KL}} \le D_{\min}\)</span>时，说明当前策略很可能<strong>找到了一条捷径</strong>，即它只优化 KL散度一项，而不去优化前面优势相关的项，所以这时我们应该减小 <spanclass="math inline">\(\beta\)</span>，降低惩罚项的影响。</li></ul><h4 id="隐式约束ppo-clip">隐式约束：PPO-Clip</h4><p>如果觉得 KL 散度的动态调整也很麻烦，还有一个更简单的 PPO-Clip算法。它在目标函数中<strong>直接对概率比率 <spanclass="math inline">\(r_t(\theta)\)</span>进行裁剪</strong>，避免了引入二阶导数或 KL 散度计算。</p><p>那么如何进行裁剪呢？首先，我们要明确一点，我们的目标是防止 <spanclass="math inline">\(\pi_\theta\)</span> 偏离 <spanclass="math inline">\(\pi_{\theta_\text{old}}\)</span> 太远——因此当<span class="math inline">\(\pi_\theta\)</span> 远离 <spanclass="math inline">\(\pi_{\theta_\text{old}}\)</span>时，我们需要缩小更新的幅度。</p><p>考虑到当优势 <span class="math inline">\(A_t&gt;0\)</span>时，此时我们倾向于让 <span class="math inline">\(\pi_\theta(a_t\mids_t)\)</span> 变大，因此 <span class="math inline">\(r_t(\theta) =\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}\)</span>会变大：</p><ul><li>如果两个分布相同，则 <spanclass="math inline">\(r_t(\theta)=1\)</span>，此时训练稳定，<strong>无需裁剪</strong>；</li><li>如果 <span class="math inline">\(\pi_\theta(a_t\mid s_t)\)</span>在之前的更新过程中已经变大，则 <spanclass="math inline">\(r_t(\theta)&gt;1\)</span>，且我们<strong>需要进行裁剪</strong>，将其限制在<span class="math inline">\(1 + \epsilon\)</span>内防止越变越大，分布越偏越远；</li><li>如果 <span class="math inline">\(\pi_\theta(a_t\mid s_t)\)</span>在之前的更新过程中变小了（注：这是可能发生的，因为策略作为一个参数网络，在优化其他状态和动作的时候也会相互影响），则<spanclass="math inline">\(r_t(\theta)&lt;1\)</span>，但此时的更新会导致<span class="math inline">\(r_t(\theta)\to1\)</span>，这是我们希望看到的变化，于是我们<strong>不再对其进行裁剪</strong>。</li></ul><p>反之，当优势 <span class="math inline">\(A_t&lt;0\)</span>时，我们也有类似的操作。</p><p>最终，目标函数可以写作： <span class="math display">\[L^{\mathrm{CLIP}}(\theta) = \mathbb{E}_{t} \left[ \min \left(r_t(\theta) A_t,\; \operatorname{clip}(r_t(\theta), 1 - \epsilon, 1 +\epsilon) A_t \right) \right]\]</span></p><p>其中：</p><ul><li><span class="math inline">\(\operatorname{clip}(r_t(\theta), 1 -\epsilon, 1 + \epsilon)\)</span> 表示将 <spanclass="math inline">\(r_t(\theta)\)</span> 限制在 <spanclass="math inline">\([1 - \epsilon, 1 + \epsilon]\)</span>的范围内；</li><li><span class="math inline">\(\epsilon\)</span>是超参数，控制策略变化的范围，一般取值为 <spanclass="math inline">\(0.1\)</span> 或 <spanclass="math inline">\(0.2\)</span>；</li><li><span class="math inline">\(\min(*)\)</span>操作实现了在优势为正的时候，仅对 <span class="math inline">\(1 +\epsilon\)</span> 裁剪；在优势为负的时候，仅对 <spanclass="math inline">\(1 - \epsilon\)</span> 裁剪。</li></ul><h2 id="ppo-算法实现">PPO 算法实现</h2><p>这里以 PPO-Clip 为例，简单介绍其算法实现。在这之前，我们知道 Actor 和Critic 是需要交替更新的。而在 PyTorch 中，其更新步骤一般是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 对于每个训练 Epoch</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-comment"># 使用旧策略采集交互数据，这批数据将被多次使用</span><br>    <span class="hljs-comment"># 此时随着 exps 被计算出来的还有 Value_old, Reward, Adavantage，之后将多次复用</span><br>    exps = generate_experience(prompts, actor, critic, reward, ref)<br>    <br>    <span class="hljs-comment"># 计算目标函数，使用最新 Actor 和 Critic 模型 + 前面采集的经验，多次更新</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(steps):<br>        actor_loss = cal_actor_loss(exps, actor)<br>        critic_loss = cal_critic_loss(exps, critic)<br>        <br>        actor.backward(actor_loss)<br>        actor.step()<br>        <br>        critic.backward(critic_loss)<br>        critic.step()<br></code></pre></td></tr></table></figure><p>其中的优化问题我们不再需要去计算 <spanclass="math inline">\(\theta_{t+1}\)</span> 或 <spanclass="math inline">\(w_{t+1}\)</span>，而是使用<strong>损失函数和优化器（如Adam）</strong>直接更新。唯一的问题就是需要明确两个 Loss 的表达式。</p><h3 id="actor-loss">Actor Loss</h3><p>策略网络的更新目标是最大化 PPO-Clip 的目标函数 <spanclass="math inline">\(L^{\text{CLIP}}(\theta)\)</span>。具体来说，策略网络的参数<span class="math inline">\(\theta\)</span>应该通过梯度上升法进行更新。现在我们已经知道其目标函数：</p><p><span class="math display">\[L^{\mathrm{CLIP}}(\theta) = \mathbb{E}_{t} \left[ \min \left(r_t(\theta) A_t,\; \operatorname{clip}(r_t(\theta), 1 - \epsilon, 1 +\epsilon) A_t \right) \right]\]</span></p><p>其具体计算步骤如下：</p><ol type="1"><li><strong>计算概率比率</strong>：对于每个经验池中的每个样本 <spanclass="math inline">\((s_t, a_t)\)</span>，用当前策略进行推断后，计算<span class="math inline">\(r_t(\theta)\)</span>。</li><li><strong>计算裁剪后的目标</strong>：根据经验中 <spanclass="math inline">\(A_t\)</span> 的符号，对 <spanclass="math inline">\(r_t(\theta)\)</span>进行裁剪，得到裁剪后的目标值。</li><li><strong>计算目标函数</strong>：将裁剪后的目标值与未裁剪的目标值进行比较，取最小值作为最终的目标函数值。</li></ol><h3 id="critic-loss">Critic Loss</h3><p>在前面的笔记中，我们知道价值函数的目标是满足贝尔曼方程： <spanclass="math display">\[V^{\pi}(s_t) = \mathbb{E} \left[ r_t + \gamma V^{\pi}(s_{t+1}) \right]\]</span> 在实际应用中，通过采样轨迹数据，目标值 <spanclass="math inline">\(V_{\text{target}}\)</span> 可近似为： <spanclass="math display">\[V_{\text{target}}(s_t) = r_t + \gamma V_{\phi_{\text{old}}}(s_{t+1})\]</span> 其中 <span class="math inline">\(\phi_{\text{old}}\)</span>是<strong>旧的 Critic网络</strong>，也就是在生成经验时使用的模型，这种设计类似于 DQN中的目标网络机制：通过分离新旧网络（延迟副本）来稳定训练。</p><p>而在引入优势估计后，我们知道优势函数定义为： <spanclass="math display">\[A_t^{\text{GAE}(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}\]</span> 其中单步 TD 误差 <span class="math inline">\(\delta_t\)</span>在生成经验就可以用 <spanclass="math inline">\(\phi_{\text{old}}\)</span> 表示： <spanclass="math display">\[\delta_t = r_t + \gamma V_{\phi_{\text{old}}}(s_{t+1}) -V_{\phi_{\text{old}}}(s_t)\]</span> 结合这些式子，我们可以将价值函数的目标值进一步表示为： <spanclass="math display">\[V_{\text{target}}(s_t) = V_{\phi_{\text{old}}}(s_t) + A_t^{\text{GAE}}\]</span>这个公式的意义是：<strong>目标值等于当前状态的估计值加上优势估计</strong>，即采取特定动作的额外收益。而价值网络的更新目标是让当前的<strong>主网络</strong><span class="math inline">\(V_\phi(s_t)\)</span> 尽可能接近目标值 <spanclass="math inline">\(V_t^{\text{target}}\)</span>。因此，我们使用<strong>均方误差（MSE）</strong>作为损失函数：</p><p><span class="math display">\[L^{\text{VF}}(\phi) = \frac{1}{N} \sum_{t} \left( V_{\phi}(s_t) -V_t^{\text{target}} \right)^2\]</span> 其中，<span class="math inline">\(N\)</span>是样本数量。其具体计算步骤如下：</p><ol type="1"><li><strong>计算 GAE 优势</strong>：从轨迹的末端开始，对于每个样本 <spanclass="math inline">\((s_t, a_t, r_t, s_{t+1})\)</span>反向计算每个时刻的 <spanclass="math inline">\(A_t^{\text{GAE}}\)</span>。</li><li><strong>构造目标值</strong>：<spanclass="math inline">\(V_{\text{target}}(s_t) =V_{\phi_{\text{old}}}(s_t) +A_t^{\text{GAE}}\)</span>，算好后与经验存在一起备用。</li><li><strong>计算均方误差</strong>：计算价值网络输出 <spanclass="math inline">\(V_{\phi}(s_t)\)</span> 与目标值 <spanclass="math inline">\(V_t^{\text{target}}\)</span> 之间的均方误差。</li></ol><h3 id="算法步骤">算法步骤</h3><p>PPO 算法的整体步骤如下：</p><ol type="1"><li><p><strong>初始化策略网络 <spanclass="math inline">\(\pi_\theta\)</span> 和价值网络 <spanclass="math inline">\(V_\phi\)</span></strong>，设定超参数 <spanclass="math inline">\(\epsilon\)</span>、<spanclass="math inline">\(\gamma\)</span>、<spanclass="math inline">\(\lambda\)</span> 等。</p></li><li><p><strong>采集交互数据</strong>：使用旧策略网络 <spanclass="math inline">\(\pi_{\theta_{\text{old}}}\)</span>与环境进行交互，收集一批轨迹数据 <span class="math inline">\(\{ (s_t,a_t, r_t, s_{t+1}) \}\)</span>。</p></li><li><p><strong>计算优势</strong>：使用 GAE 计算每个时间步的优势估计<span class="math inline">\(A_t\)</span>。 <span class="math display">\[A_t^{\text{GAE}(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}\]</span> 其中 <span class="math inline">\(\delta_t = r_t + \gammaV_{\phi}(s_{t+1}) -V_{\phi}(s_t)\)</span>，使用旧价值网络计算。</p></li><li><p><strong>计算策略目标，更新策略网络</strong>：对于每个样本，计算概率比率<span class="math inline">\(r_t(\theta)\)</span> 和目标函数 <spanclass="math inline">\(L^{\text{CLIP}}(\theta)\)</span>，使用优化器更新策略网络参数<span class="math inline">\(\theta\)</span>。 <spanclass="math display">\[L^{\text{CLIP}}(\theta) = \mathbb{E}_{t} \left[ \min \left( r_t(\theta)A_t,\; \operatorname{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) A_t\right) \right]\]</span></p></li><li><p><strong>计算状态价值的目标</strong>：对于每个样本，使用旧价值网络计算：<span class="math display">\[V_t^{\text{target}} = A_t + V_{\phi}(s_t)\]</span></p></li><li><p><strong>更新价值网络</strong>：最小化均方误差损失，使用优化器更新价值网络参数<span class="math inline">\(\phi\)</span>。 <spanclass="math display">\[L^{\text{VF}}(\phi) = \frac{1}{N} \sum_{t} \left( V_{\phi}(s_t) -V_t^{\text{target}} \right)^2\]</span></p></li><li><p><strong>重复迭代</strong>：在更新多次后，重置 <spanclass="math inline">\(\theta_{\text{old}} \leftarrow \theta,\phi_{\text{old}} \leftarrow\phi\)</span>，并继续下一个周期的采样和更新，直到策略收敛或达到预定的停止条件。</p></li></ol><blockquote><p>注意：这里的新旧价值网络和拷贝步骤 <spanclass="math inline">\(\theta_{\text{old}} \leftarrow \theta,\phi_{\text{old}} \leftarrow \phi\)</span>实际上不需要显式进行。因为在实际使用中，当使用 <spanclass="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 和 <spanclass="math inline">\(V_{\phi_{\text{old}}}\)</span>采样并处理一批经验数据后，我们会将所有结果存储起来，并在之后的多次ppo_step 更新中复用。</p><p>在更新的时候这些「随机选取的经验」其实还是 <spanclass="math inline">\(\text{old}\)</span> 版本的采样结果，而当前 Actor和 Critic 模型已经在不断进行更新，自然就呈现出了新旧网络分离的效果。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OpenAI o3 与 Monte-Carlo 思想</title>
    <link href="/OpenAI-o3-Series.html"/>
    <url>/OpenAI-o3-Series.html</url>
    
    <content type="html"><![CDATA[<p>o3 来了，分享一些个人的浅见。关于 Test-time Scaling Law的进展，比我们想象中的要快得多。但我想说的是，这条路其实有些曲折——它是OpenAI 在追求 AGI 的道路上，采取的<strong>曲线救国</strong>之策。</p><h2 id="强化学习与捷径思维">强化学习与捷径思维</h2><p>为什么会这样说呢？我们通过两个例子来探讨。</p><p>第一个例子来自强化学习。在 RL 中，折扣因子 <spanclass="math inline">\(\gamma\)</span>扮演着关键角色，它意味着越往后的决策步骤，所获得的奖励将会逐渐减少。因此，强化学习的目标通常是尽量<strong>以最短的时间和最少的步骤获得最大化的奖励</strong>。这种策略的核心，是强调「捷径」，即尽可能快速地得到回报。</p><p>第二个例子是大模型的微调过程。一个未经微调的预训练模型，往往没有明确的指向性和控制能力。当我们询问模型「中国的首都在哪里？」时，它可能会先说「这是一个好问题！」，然后绕着话题扯一大堆，而最终才给出「北京」这个答案。然而，当同样的问题问到一个经过微调的模型时，答案直接而明确：「北京」。</p><p>这种微调后的模型展示了一种<strong>通过优化策略获得捷径</strong>的方式——与人类的进化历程相似——总是在追求最少的能量消耗和最短的路径。</p><h2 id="为什么是-reasoning">为什么是 Reasoning？</h2><p>如果把 Reasoning 采样的过程可视化为一棵树：</p><p><img src="/img/blog/OpenAI-o3-Series-images/shortcut-and-journey.png" alt="O1 Replication Journey: Part 1" width=60% /></p><p>左侧的是过去我们追求的<strong>捷径学习</strong>：以最少的步骤到达正确结果。而右侧则是以OpenAI o1 为代表的「反思、回溯」范式。</p><p>我们知道，在 o1进行搜索的过程中，模型会进行不断的<strong>反思和回溯</strong>，而这一过程往往伴随着额外的开销。问题是，如果模型真的能一遍给出正确答案，谁还愿意花时间、花钱去做复杂的搜索呢？OpenAI也不傻，大家都知道捷径更好！</p><p>对于越困难的问题，这棵潜在的思维树就越宽、每一步的搜索空间就更大，走捷径能到达正确答案的概率就越小。那怎么办呢？一个直观的思路就是去<strong>剪枝</strong>！把那些不可能到达终点的树节点提前剪掉，压缩搜索空间——<strong>让这棵树变回窄窄的</strong>。这也是当前许多工作努力的方向，比如：</p><figure><img src="/img/blog/OpenAI-o3-Series-images/CPO.png"alt="Chain of Preference Optimization (CPO)" /><figcaption aria-hidden="true">Chain of Preference Optimization(CPO)</figcaption></figure><p>Chain of Preference Optimization就是从思维树上天然地构造出偏好数据，再用 DPO去优化，让模型有更大概率去选择能到达终点的树节点。</p><figure><img src="/img/blog/OpenAI-o3-Series-images/OVM.png"alt="Outcome-supervised Value Models (OVM)" /><figcaption aria-hidden="true">Outcome-supervised Value Models(OVM)</figcaption></figure><p>Outcome-supervised Value Models 就是将 Reasoning 建模为 MDP过程，用当前步骤到达正确答案的概率（Value）来指导策略优化。</p><h2 id="为何-openai-选择突破传统捷径">为何 OpenAI选择突破传统捷径？</h2><p>回到 o1 上，为什么选择打破传统的捷径思想，去走 Tree Search 这条“<strong>弯路</strong>” 呢？</p><p>如果说在过去，我们倾向于<strong>利用</strong>（Exploit）模型的基本能力，就会认为现有GPT-4模型已经能够满足大部分对话和简单推理需求。并且这些任务能够很好地采样、评估偏好、迭代优化。</p><p>但这一视角忽视了更复杂任务的需求——例如数学推理（AIME、FrontierMath）、代码生成（SWE-Bench、CodeForce）等，这些任务往往难以在短期内获得回报——它们的奖励是非常稀疏的，只有在最终得出正确答案时，回报才会显现出来。</p><p>因此，传统的捷径学习不再适合处理这类复杂任务：<strong>你连一条正确路径都采样不到，何谈去优化模型选择正确路径的概率呢</strong>？</p><p>回到本文的标题中的「<strong>Monte-Carlo思想</strong>」，我们可以发现这其实是一回事：蒙特卡洛方法在强化学习中的应用，核心在于通过多次采样来估计策略的价值，进而优化模型。然而，这一方法有天然的局限性——如果采样的策略无法采样到最优路径，那模型优化的终点永远只是<strong>局部最优</strong>。这也是为什么我们会在MC Learning 中选择更具有探索性的策略。</p><p>于是 OpenAI选择打破强化学习的天平，摆脱了传统的捷径思维，转而强化<strong>探索</strong>（Explore）。</p><h2 id="o1-的突破从探索到优化">o1 的突破：从探索到优化</h2><p>在这一背景下，OpenAI 提出了 o1范式。这一改变使得模型能够在面对复杂任务时，<strong>开始逐渐能够获得稀疏的奖励了</strong>！并且通过这些奖励，可以不断地优化策略。尽管这一探索过程可能显得繁琐和低效，但它为模型的进一步优化奠定了基础。</p><p>那么 o1 是怎么来的呢？最近也出现了很多复刻 o1的工作，他们都在做什么呢？如果用于探索的行为策略是 On-Policy方法的话，那就是用当前模型（例如 GPT-4o）去采样，效率还是太低。</p><p>于是大伙不约而同地选择了 Off-Policy 方法：</p><ul><li>OpenAI 花重金聘请在读博士生来标注 Long CoT 数据；</li><li>没钱咋办？那就搞点人机协作标注数据（人工蒸馏o1），降低对标注者的要求；</li><li>连找标注者的钱都没有？那就只能去蒸馏 R1 /QwQ、或者想其他的办法（Critique、PRM 等）。</li></ul><p>这里我也想提醒积极复刻 o1的大厂、实验室们，不要忘了：<strong>探索的终极目标还是优化</strong>！</p><blockquote><p>插一句题外话，虽然大家都在骂 o1 隐藏了真正的思维链，只展示 Summary的捷径版本。殊不知这个 Summary 才是优化策略的关键数据！但 OpenAI并不害怕其他人蒸馏这些Summary，因为蒸馏这些数据还有一个前提——基础模型的能力足够强大，不然步子迈太大还容易闪了腰。</p><p>并且 OpenAI还将<strong>探索的成本</strong>转嫁给了用户。虽然在初期花很多钱来标注探索型数据，但现在有了o1 后，用户使用的过程中又无形地为他标注了更多数据。OpenAI再次实现了伟大的<strong>数据飞轮</strong>！</p></blockquote><h2 id="o1-到-o3-的快速进化">o1 到 o3 的快速进化</h2><p>o1 才刚发布没几个月，o3 就来了。</p><p>其实这也侧面验证了前面的猜想：如果说 GPT-4 代表了<strong>从 0 到 1的进步</strong>——即从简单任务到获得奖励的过程；那么 o1则代表了<strong>从 1 到 10的飞跃</strong>——通过探索复杂任务并获得稀有的回报，为进一步优化策略提供了前所未有的高质量数据。</p><p>因此它的进展之快，超出了所有人的预期：</p><p><img src="/img/blog/OpenAI-o3-Series-images/o3.jpeg" alt="飞速进展" width=40% /></p><p>这不仅是对探索策略的成功应用，更是人工智能技术迈向 AGI的重要一步。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>前沿热点</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLMs</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL 学习笔记 #09 Actor-Critic 方法</title>
    <link href="/RL-Note-9.html"/>
    <url>/RL-Note-9.html</url>
    
    <content type="html"><![CDATA[<p>在今天的强化学习中，<strong>Actor-Critic</strong>方法依旧是最流行的一类算法。它将 Value-based<strong>值函数</strong>方法和 Policy-based<strong>策略梯度</strong>方法结合起来，改进了各自的一些缺陷（但仍属于策略梯度方法）。正如其名所示，Actor-Critic方法由两个主要部分组成：</p><ul><li><strong>Actor</strong>：负责策略更新（PU），决定了智能体在每个状态下的行动选择。</li><li><strong>Critic</strong>：负责策略评估（PE），通常通过计算状态价值函数或优势函数来给出反馈。</li></ul><p>这两个模块相互协作，Actor 负责采取动作并推动策略的优化，Critic负责评估当前策略的效果并为 Actor 提供更新的指导。</p><h2 id="q-value-based-actor-critic-qac">Q-Value based Actor-Critic(QAC)</h2><p>我们先回顾一下上一节的策略梯度算法。首先，我们定义了两个目标函数的指标<span class="math inline">\(\bar{v}_\pi\)</span> 和 <spanclass="math inline">\(\bar{r}_\pi\)</span>，并使用梯度上升来最大化目标函数<span class="math inline">\(J(\theta)\)</span>： <spanclass="math display">\[\begin{aligned}\theta_{t+1} &amp;= \theta_t + \alpha \nabla_\theta J(\theta) \\&amp;= \theta_t + \alpha \mathbb{E} \left[\nabla_\theta \ln \pi(A \midS;\theta) \cdot q_\pi(S, A)\right]\\\end{aligned}\]</span> 对于其中的期望，我们使用随机采样的 <spanclass="math inline">\((s_t,a_t)\)</span> 替换： <spanclass="math display">\[\theta_{t+1} =\theta_t + \alpha \nabla_\theta \ln \pi(a_t \mids_t;\theta_t) \cdot q_t(s_t, a_t)\]</span> 我们可以发现，根据 Actor 和 Critic的定义：这个<strong>随机梯度上升算法</strong>就是负责策略更新的Actor，其中<strong>用于估计 <span class="math inline">\(q_t(s_t,a_t)\)</span> 的算法</strong>就是负责策略评估的 Critic。</p><p>在上篇笔记的末尾，我们提到 REINFORCE 算法使用蒙特卡洛方法来估计 <spanclass="math inline">\(q_t(s_t,a_t)\)</span>，导致它没法边采样边更新策略；而一旦引入<strong>时序差分方法</strong>，就能完美解决这一问题。此时我们就得到了最简单的<strong>Q-Value based Actor-Critic (QAC)</strong> 算法。</p><h3 id="基于-sarsa-的-critic-定义">基于 SARSA 的 Critic 定义</h3><p>在 TD Learning 中，我们用于估计动作价值函数的方法就是<strong>SARSA</strong>。其根据策略 <spanclass="math inline">\(\pi\)</span> 产生的经验数据 <spanclass="math inline">\(\{(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})\}\)</span>进行更新： <span class="math display">\[q_{t+1}\left(s_t, a_t\right) =q_t\left(s_t,a_t\right)-\alpha_t\left(s_t, a_t\right)\left[q_t\left(s_t,a_t\right)-\left[r_{t+1}+\gamma q_t\left(s_{t+1},a_{t+1}\right)\right]\right]\]</span>为了能够实现对高维空间的近似，我们引入<strong>值函数估计</strong>的<span class="math inline">\(\hat{q}(s_t, a_t;w_t)\)</span>，也就是价值网络作为 Critic 模型： <spanclass="math display">\[w_{t+1} = w_t + \alpha_w \left( r_{t+1} + \gamma \hat{q}(s_{t+1},a_{t+1}; w_t) - \hat{q}(s_t, a_t; w_t) \right) \nabla_w \hat{q}(s_t,a_t; w_t)\]</span></p><blockquote><p>这里使用 <span class="math inline">\(w\)</span>表示价值网络的参数，用于和策略网络的参数 <spanclass="math inline">\(\theta\)</span> 区分。</p></blockquote><h3 id="算法步骤">算法步骤</h3><p>QAC 的算法步骤通常如下：</p><ol type="1"><li><p><strong>初始化策略网络和价值网络</strong>，并设定学习率 <spanclass="math inline">\(\alpha_w\)</span> 和 <spanclass="math inline">\(\alpha_\theta\)</span>。</p></li><li><p><strong>采样</strong>：根据当前策略网络 <spanclass="math inline">\(\pi(a \mid s; \theta_t)\)</span>与环境进行一次交互，得到 <span class="math inline">\(\{(s_t, a_t,r_{t+1}, s_{t+1}, a_{t+1})\}\)</span>。</p></li><li><p><strong>策略估计</strong>：使用 SARSA + 值函数近似更新价值网络：<span class="math display">\[w_{t+1} = w_t + \alpha_w \left( r_{t+1} + \gamma q(s_{t+1}, a_{t+1};w_t) - q(s_t, a_t; w_t) \right) \nabla_w q(s_t, a_t; w_t)\]</span></p></li><li><p><strong>更新策略</strong>：根据价值网络的估计结果，使用策略梯度方法来更新策略网络：<span class="math display">\[\theta_{t+1} =\theta_t + \alpha \nabla_\theta \ln \pi(a_t \mids_t;\theta_t) \cdot q_t(s_t, a_t;w_{t+1})\]</span></p></li><li><p><strong>迭代</strong>：继续采样，直到策略网络和价值网络均收敛。</p></li></ol><p>需要注意的是，QAC 算法：</p><ul><li>属于 On-Policy 算法，因为每次采样都要基于 <spanclass="math inline">\(\pi_\theta\)</span> 去获取 <spanclass="math inline">\(a_t\)</span> 和 <spanclass="math inline">\(a_{t+1}\)</span>；</li><li>属于随机性策略，因为基于 Softmax 的概率 <spanclass="math inline">\(\pi(A \mid S;\theta) &gt;0\)</span>。</li></ul><h2 id="advantage-actor-critic-a2c">Advantage Actor-Critic (A2C)</h2><p><strong>Advantage Actor-Critic (A2C)</strong> 方法是 Actor-Critic方法的一个扩展，它通过引入<strong>优势函数</strong>（AdvantageFunction）作为偏置量来减少估计的<strong>方差</strong>，提高学习效率。优势函数<span class="math inline">\(A(s, a)\)</span>衡量了<strong>某个特定动作相对于平均水平的表现</strong>。通常，优势函数定义为：<span class="math display">\[A(s, a) = Q(s, a) - V(s)\]</span> 其中：</p><ul><li><span class="math inline">\(Q(s, a)\)</span>是状态-动作值函数，表示从状态 <span class="math inline">\(s\)</span>开始采取动作 <span class="math inline">\(a\)</span> 后的预期回报。</li><li><span class="math inline">\(V(s)\)</span> 是状态值函数，表示在状态<span class="math inline">\(s\)</span>下，智能体能够获得的预期回报，可以认为是状态 <spanclass="math inline">\(s\)</span> 下所有动作价值的平均水平。</li></ul><p>下面我们将介绍 Actor-Critic的高方差问题，并解释为什么需要引入优势函数来指导策略更新。</p><h3 id="baseline-invariance-性质">Baseline Invariance 性质</h3><p>首先要介绍一个重要性质——在策略梯度定理中，引入一个<strong>与状态分布相关</strong>的<strong>基线</strong>（baseline）作为偏置量，不会改变梯度的值：<span class="math display">\[\begin{aligned}\nabla_\theta J(\theta) &amp; =\mathbb{E}_{S\sim \eta, A \sim \pi}\left[\nabla_\theta \ln \pi(A \mid S;\theta) \cdot q_\pi(S, A)\right]\\&amp;=\mathbb{E}_{S\sim \eta, A \sim \pi} \left[\nabla_\theta \ln \pi(A\mid S;\theta) \cdot \left( q_\pi(S, A) - b(S) \right) \right]\end{aligned}\]</span> 为什么引入 baseline 不会改变梯度值？我们需要证明下式成立：<span class="math display">\[\mathbb{E}_{S\sim \eta, A \sim \pi} \left[\nabla_\theta \ln \pi(A \midS;\theta) \cdot  b(S)\right]=0\]</span> 我们将其原式展开，并依次将与 <spanclass="math inline">\(a\)</span> 无关的项提到 <spanclass="math inline">\(\sum\)</span> 外： <span class="math display">\[\begin{aligned}\mathbb{E}_{S \sim \eta, A \sim \pi}\left[\nabla_\theta \ln \pi\left(A\mid S, \theta\right) b(S)\right] &amp; =\sum_{s \in \mathcal{S}}\eta(s) \sum_{a \in \mathcal{A}} \pi\left(a \mid s, \theta\right)\nabla_\theta \ln \pi\left(a \mid s, \theta\right) b(s) \\&amp; =\sum_{s \in \mathcal{S}} \eta(s) \sum_{a \in \mathcal{A}}\nabla_\theta \pi\left(a \mid s, \theta\right) b(s) \\&amp; =\sum_{s \in \mathcal{S}} \eta(s) b(s) \sum_{a \in \mathcal{A}}\nabla_\theta \pi\left(a \mid s, \theta\right) \\&amp; =\sum_{s \in \mathcal{S}} \eta(s) b(s) \nabla_\theta \sum_{a \in\mathcal{A}} \pi\left(a \mid s, \theta\right) \\&amp; =\sum_{s \in \mathcal{S}} \eta(s) b(s) \nabla_\theta 1=0\end{aligned}\]</span> 于是，我们就证明了策略梯度的 <strong>Baseline Invariance性质</strong>：对于 <span class="math display">\[X(S,A) \doteq \nabla_\theta \ln \pi(A \mid S;\theta) \cdot \left(q_\pi(S, A) - b(S) \right)\]</span> 期望 <span class="math inline">\(\mathbb{E}[X]\)</span> 对<span class="math inline">\(b(S)\)</span> 具有不变性。</p><p>此外，我们还注意到，方差 <spanclass="math inline">\(\mathrm{Var}(X)\)</span>是<strong>不具有不变性</strong>的：因为 <spanclass="math inline">\(\mathrm{tr}(\mathrm{Var}(X))=\mathbb{E}[X^\topX]-\bar{x}^\top \bar{x}\)</span>，其中后者不会受到 <spanclass="math inline">\(b(S)\)</span> 的影响，但前者 <spanclass="math display">\[\begin{aligned}\mathbb{E}\left[X^T X\right] &amp; =\mathbb{E}\left[\left(\nabla_\theta\ln \pi\right)^T\left(\nabla_\theta \ln \pi\right)(q(S,A)-b(S))^2\right] \\&amp; =\mathbb{E}\left[\left\|\nabla_\theta \ln \pi\right\|^2(q(S,A)-b(S))^2\right]\end{aligned}\]</span> 此时 <span class="math inline">\(b(S)\)</span>显然会带来影响。因此，我们的目标就是：<strong>寻找一个最好的 Baseline来尽可能降低梯度的方差</strong>，进而提高学习效率。</p><p>在 REINFORCE 和 QAC 中，我们可以认为 <spanclass="math inline">\(b=0\)</span>，这并不是一个很好的 Baseline。</p><h3 id="最优-baseline-与优势函数">最优 Baseline 与优势函数</h3><p>可以证明，最优的 Baseline 为： <span class="math display">\[b^*(s)=\frac{\mathbb{E}_{A \sim \pi}\left[\left\|\nabla_\theta \ln\pi\left(A \mid s, \theta_t\right)\right\|^2 q(s,A)\right]}{\mathbb{E}_{A \sim \pi}\left[\left\|\nabla_\theta \ln\pi\left(A \mid s, \theta_t\right)\right\|^2\right]}\]</span>具体的证明不再展开。虽然这个基线可以达到最小的方差，但是由于太过复杂，在实际中我们会去掉权重项，直接使用：<span class="math display">\[b(s) =\mathbb{E}_{A \sim \pi}\left[q(s, A)\right]=v_\pi(s)\]</span> 将其代入更新表达式： <span class="math display">\[\begin{aligned}\theta_{t+1} &amp;= \theta_t + \alpha \mathbb{E} \left[\nabla_\theta \ln\pi(A \mid S;\theta) \cdot \left( q_\pi(S, A) - v_\pi(S)\right)\right]\\&amp; \doteq  \theta_t + \alpha \mathbb{E} \left[\nabla_\theta \ln \pi(A\mid S;\theta) \cdot \delta_\pi(S,A) \right]\end{aligned}\]</span> 其中，我们定义优势函数： <span class="math display">\[\delta_\pi(S,A) \doteq q_\pi(S, A) - v_\pi(S)\]</span> 考虑到随机采样，我们还可以将更新表达式写作： <spanclass="math display">\[\theta_{t+1} =\theta_t + \alpha  \nabla_\theta \ln \pi(a_t \mids_t;\theta_t)\delta_t(s_t,a_t)\]</span> 并且，和上一节介绍的类似，<spanclass="math inline">\(\delta_t(s_t,a_t)\)</span>也起到<strong>平衡探索和采样</strong>的作用： <spanclass="math display">\[\begin{aligned}\theta_{t+1} &amp; =\theta_t+\alpha \nabla_\theta \ln \pi\left(a_t \mids_t; \theta_t\right) \delta_t\left(s_t, a_t\right) \\&amp; =\theta_t+\alpha \underbrace{\left(\frac{\delta_t\left(s_t,a_t\right)}{\pi\left(a_t \mid s_t; \theta_t\right)}\right)}_{\text{stepsize}} \nabla_\theta \pi\left(a_t \mid s_t; \theta_t\right)\end{aligned}\]</span> 分子 <span class="math inline">\(\delta_t(s_t,a_t)\)</span>与之前的 <span class="math inline">\(q_t\left(s_t, a_t\right)\)</span>类似，意味着新策略会将更大的概率分配给更好的动作，并且此时<strong>不再关注绝对价值，而是关注相对价值</strong>！防止出现<span class="math inline">\(q_t\left(s_t, a_t\right)\)</span>绝对值过大但是没有相对优势的情况。</p><h3 id="算法步骤-1">算法步骤</h3><p>在实际算法中，我们通常会将优势函数进一步改写成： <spanclass="math display">\[\begin{aligned}\delta_t\left(s_t, a_t\right)&amp; = q_t\left(s_t, a_t\right) -v_t(s_t)\\&amp;= r_{t+1} +\gamma v_t(s_{t+1}) -v_t(s_t)\end{aligned}\]</span>这样做的好处是：我们可以<strong>只用一个价值网络</strong>来近似 <spanclass="math inline">\(v(s)\)</span>，而不需要再引入一个 <spanclass="math inline">\(q(s,a)\)</span> 网络。这个式子形如 TD Error，因此A2C 算法有时也被称为 <strong>TD Actor-Cricic</strong>。</p><p>A2C 的算法步骤通常如下：</p><ol type="1"><li><p><strong>初始化策略网络和价值网络</strong>，并设定学习率 <spanclass="math inline">\(\alpha_w\)</span> 和 <spanclass="math inline">\(\alpha_\theta\)</span>。</p></li><li><p><strong>采样</strong>：根据当前策略网络 <spanclass="math inline">\(\pi(a \mid s; \theta_t)\)</span>与环境进行一次交互，得到 <span class="math inline">\(\{(s_t, a_t,r_{t+1}, s_{t+1})\}\)</span>。</p></li><li><p><strong>计算优势</strong>：使用 TD Error 形式： <spanclass="math display">\[\delta_t = r_{t+1} +\gamma v(s_{t+1};w_t) -v(s_t;w_t)\]</span></p></li><li><p><strong>策略估计</strong>：使用优势函数更新价值网络： <spanclass="math display">\[w_{t+1} = w_t + \alpha_w \delta_t \nabla_w v(s_t; w_t)\]</span></p></li><li><p><strong>更新策略</strong>：再次<strong>复用优势函数</strong>，使用策略梯度方法来更新策略网络：<span class="math display">\[\theta_{t+1} =\theta_t + \alpha \delta_t \nabla_\theta \ln \pi(a_t \mids_t;\theta_t)\]</span></p></li><li><p><strong>迭代</strong>：继续采样，直到策略网络和价值网络均收敛。</p></li></ol><p>需要注意的是，A2C 算法：</p><ul><li>属于 On-Policy 算法，因为每次采样都要基于 <spanclass="math inline">\(\pi_\theta\)</span> 去获取 <spanclass="math inline">\(a_t\)</span>；</li><li>属于随机性策略，因为基于 Softmax 的概率 <spanclass="math inline">\(\pi(A \mid S;\theta) &gt;0\)</span>。</li></ul><h2 id="off-policy-actor-critic">Off-Policy Actor-Critic</h2><p>在某些应用场景中，我们希望能够使用 <strong>Off-Policy</strong>方法来进行学习，以利用现有的经验数据。在之前的方法中： <spanclass="math display">\[\nabla_\theta J(\theta) =\mathbb{E}_{S\sim \eta, \textcolor{blue}{A \sim\pi}} \left[*\right]\]</span> 我们的目标策略 <spanclass="math inline">\(\pi\)</span>，同时也是采样中使用的行为策略。如果要使用其他行为策略采样的数据，就会遇到<strong>分布差异</strong>的问题。为此，我们需要引入<strong>重要性采样</strong>（ImportanceSampling），解决分布问题，从而提高数据效率。</p><blockquote><p>注意：虽然我们到这里才首次介绍重要性采样，但实际上任何求期望（采样）的算法，只要涉及On-Policy 到 Off-Policy 的转换，都可以用到这个技术。</p></blockquote><h3 id="重要性采样-importance-sampling">重要性采样 | ImportanceSampling</h3><p>我们先从一个例子开始：给定一个随机变量 <span class="math inline">\(X\in \mathcal{X}=\{+1, -1\}\)</span>，其概率分布为 <spanclass="math inline">\(p_0\)</span>： <span class="math display">\[p_0(X=+1)=0.5,\quad p_0(X=-1)=0.5\]</span> 在这个分布下采样得到 <spanclass="math inline">\(\{x_i\}\)</span>后，我们可以用这组数的平均值来估计其期望： <span class="math display">\[\bar{x}=\sum_{i=1}^n \frac{1}{n} x_i \rightarrow \mathbb{E}_{X \simp_0}[X]=0\]</span> 然而，如果 <span class="math inline">\(\{x_i\}\)</span>是<strong>采样自另一个分布</strong> <spanclass="math inline">\(p_1\)</span>： <span class="math display">\[p_1(X=+1)=0.8,\quad p_1(X=-1)=0.2\]</span> 此时用平均值去估计期望就是造成差异： <spanclass="math display">\[\bar{x}=\sum_{i=1}^n \frac{1}{n} x_i \rightarrow \mathbb{E}_{X \simp_1}[X]=0.6 \neq \mathbb{E}_{X \sim p_0}[X]\]</span> 那么要如何使用 <span class="math inline">\(\{x_i\} \simp_1\)</span> 去估计 <span class="math inline">\(\mathbb{E}_{X \simp_0}[X]\)</span>？这就是重要性采样试图解决的问题。在强化学习中，我们希望使用来自行为策略<span class="math inline">\(\pi_b\)</span> 的数据去估计目标策略 <spanclass="math inline">\(\pi_\theta\)</span> 下的 <spanclass="math inline">\(\mathbb{E}_{A \sim \pi_\theta}\left[*\right]\)</span>。</p><p>我们记： <span class="math display">\[\mathbb{E}_{X \sim p_0}[X]=\sum_x p_0(x) x=\sum_x p_1(x)\underbrace{\frac{p_0(x)}{p_1(x)} x}_{f(x)}=\mathbb{E}_{X \simp_1}[f(X)]\]</span> 其中 <span class="math inline">\(f(x)\)</span>就是<strong>重要性变换</strong>，通过这个函数映射，我们可以将分布 <spanclass="math inline">\(p_1\)</span> 下的样本 <spanclass="math inline">\(x_i\)</span> 模拟为分布 <spanclass="math inline">\(p_0\)</span> 下的样本 <spanclass="math inline">\(f_i\)</span>。那么如何估计 <spanclass="math inline">\(\mathbb{E}_{X \sim p_1}[f(X)]\)</span>呢？还是依赖大数定律： <span class="math display">\[\mathbb{E}_{X \sim p_0}[X] \approx \bar{f}=\frac{1}{n} \sum_{i=1}^nf\left(x_i\right)=\frac{1}{n} \sum_{i=1}^n\frac{p_0\left(x_i\right)}{p_1\left(x_i\right)} x_i\]</span> 其中，$ $ 就是<strong>重要性权重</strong>（ImportanceWeight）。之所以叫「重要性」也很直观：如果 $ p_0(x_i) &gt;p_1(x_i)$，那么可以认为 <span class="math inline">\(x_i\)</span> 在<span class="math inline">\(p_0\)</span> 下容易采到，而在 <spanclass="math inline">\(p_1\)</span> 下则不容易采到，因此当我们想计算<span class="math inline">\(p_0\)</span>下的期望，就应该很珍惜这个重要的样本。</p><blockquote><p>这个时候初学者可能会有一个问题：在求权重的时候，我如果知道 <spanclass="math inline">\(p_0\left(x_i\right)\)</span>，那我直接根据定义<span class="math inline">\(\mathbb{E}_{X \sim p_0}[X] = \sump_0\left(x\right)x\)</span> 不就能求出期望了吗？</p><p>考虑一个情况：<span class="math inline">\(p_0\)</span>是一个神经网络，此时它的输出是连续的，定义式需要改为积分符号 <spanclass="math inline">\(\int\)</span>，并且<strong>神经网络是没有表达式的，自然也没法求积分</strong>。我们只能给定一个<span class="math inline">\(x_i\)</span>，返回一个 <spanclass="math inline">\(p_0(x_i)\)</span>，然后慢慢采样累加。这就是我们用在策略网络<span class="math inline">\(\pi(a \mid s; \theta)\)</span>中的情况。</p></blockquote><h3 id="重要性采样应用于-actor-critic">重要性采样应用于Actor-Critic</h3><p>在 <strong>Off-Policy Actor-Critic</strong>方法中，我们使用<strong>重要性采样</strong>来修正策略的更新。与<strong>On-Policy</strong> 方法不同，行为策略 <spanclass="math inline">\(\pi_b\)</span> 和目标策略 <spanclass="math inline">\(\pi_\theta\)</span>不再相同。我们首先要获取新的梯度表达式，再将其用到梯度上升算法中进行优化。此时的目标函数为：<span class="math display">\[J(\theta) = \sum_{s\in\mathcal{S}}d_{\pi_b}v_{\pi_\theta}=\mathbb{E}_{S\simd_{\pi_b}}[v_{\pi_\theta}(S)]\]</span> 其中 <span class="math inline">\(d_{\pi_b}\)</span> 为行为策略<span class="math inline">\(\pi_b\)</span> 下的稳态分布。在<strong>Q-value based</strong> 方法中，策略梯度更新公式通常为：</p><p><span class="math display">\[\nabla_\theta J(\theta) = \mathbb{E}_{S \sim \eta, A \sim \pi_b}\left[\frac{\pi_\theta(A \mid S)}{\pi_b(A \mid S)} \nabla_\theta \ln\pi_\theta(A \mid S) \cdot q_{\pi_\theta}(S, A)\right]\]</span></p><p>在这个公式中，<span class="math inline">\(\frac{\pi_\theta(A \midS)}{\pi_b(A \mid S)}\)</span>就是<strong>重要性采样比率</strong>，它校正了采样分布与目标策略之间的差异。其中<span class="math inline">\(S\)</span> 的分布通常不需要在意，<spanclass="math inline">\(A\)</span> 的分布则需要使用行为策略 <spanclass="math inline">\(\pi_b\)</span> 获取。</p><p>同理，我们也可以应用优势函数进行改进： <span class="math display">\[\nabla_\theta J(\theta) = \mathbb{E}_{S \sim \eta, A \sim \pi_b}\left[\frac{\pi_\theta(A \mid S)}{\pi_b(A \mid S)} \nabla_\theta \ln\pi_\theta(A \mid S) \cdot (q_{\pi_\theta}(S,A)-v_{\pi_\theta}(S))\right]\]</span> 引入随机采样的梯度上升更新表达式： <spanclass="math display">\[\theta_{t+1} =\theta_t + \alpha_\theta \frac{\pi_\theta(a_t \mids_t)}{\pi_b(a_t \mid s_t)} \nabla_\theta \ln \pi(a_t \mids_t;\theta_t)\delta_t(s_t,a_t)\]</span></p><h3 id="算法步骤-2">算法步骤</h3><p>Off-Policy Actor-Critic 的算法步骤通常如下：</p><ol type="1"><li><p><strong>初始化策略网络和价值网络</strong>，并设定学习率 <spanclass="math inline">\(\alpha_w\)</span> 和 <spanclass="math inline">\(\alpha_\theta\)</span>。</p></li><li><p><strong>初始化行为策略</strong> <spanclass="math inline">\(\pi_b\)</span>，它通常是一个探索性策略（例如 <spanclass="math inline">\(\epsilon\)</span>-greedy或者是通过一个探索性噪声的策略）。</p></li><li><p><strong>采样</strong>：根据行为策略 <spanclass="math inline">\(\pi_b\)</span>从环境中采样（也可以从经验池中获取），得到经验数据 <spanclass="math inline">\(\{(s_t, a_t, r_{t+1},s_{t+1})\}\)</span>。</p></li><li><p><strong>计算重要性权重</strong>：计算每个时间步的采样权重： <spanclass="math display">\[\rho_t = \frac{\pi_\theta(a_t \mid s_t)}{\pi_b(a_t \mid s_t)}\]</span></p></li><li><p><strong>计算优势</strong>：使用 TD Error 形式优势函数： <spanclass="math display">\[\delta_t = r_{t+1} +\gamma v(s_{t+1};w_t) -v(s_t;w_t)\]</span></p></li><li><p><strong>更新价值网络</strong>：使用<strong>重要性采样权重修正</strong>的优势函数来更新价值网络：<span class="math display">\[w_{t+1} = w_t + \alpha_w \rho_t \delta_t  \nabla_w v(s_t; w_t)\]</span></p></li><li><p><strong>更新策略网络</strong>：根据<strong>重要性采样权重修正</strong>的策略梯度更新策略网络：<span class="math display">\[\theta_{t+1} = \theta_t + \alpha_\theta \rho_t \delta_t  \nabla_\theta\ln \pi(a_t \mid s_t; \theta_t)\]</span></p></li><li><p><strong>迭代</strong>：继续采样，直到策略网络和价值网络均收敛。</p></li></ol><h2 id="deterministic-policy-gradient-dpg">Deterministic Policy Gradient(DPG)</h2><p>在前面的方法中，我们都是在讨论随机性策略。什么情况下需要一个确定性策略呢？</p><p>考虑到随机性策略函数 <span class="math inline">\(\pi_\theta\)</span>的一个天然缺陷——它要求输出的动作 <span class="math inline">\(a\)</span>必须是有限的，因为我们需要一个函数 <span class="math inline">\(\pi(a\mid s;\theta)\)</span> 将状态 <span class="math inline">\(s\)</span>映射到一个<strong>有限的动作集合</strong>中。这对于离散动作空间是没问题的，但如果<strong>动作空间是连续的</strong>（如控制任务中的物理机器人），那么使用这种方式就不再适用。</p><p>这个时候我们可以换一种形式表示<strong>确定性策略</strong>： <spanclass="math display">\[a=\mu(s;\theta)\]</span> 这里的 <span class="math inline">\(\mu(s; \theta)\)</span>是一个<strong>确定性函数</strong>，它会根据当前的状态 <spanclass="math inline">\(s\)</span> 输出一个特定的动作 <spanclass="math inline">\(a\)</span>，而不再是基于概率的选择。这种方法尤其适用于连续动作空间（例如控制系统），并且通常能更高效地进行学习。</p><h3 id="dpg-的梯度计算">DPG 的梯度计算</h3><p>在 <strong>Deterministic Policy Gradient (DPG)</strong>方法中，策略函数 Actor不再是一个概率分布，而是确定性的函数。此时，Critic 的任务是评估由 Actor产生的每个动作的价值。</p><p>为了更新策略网络 <span class="math inline">\(\mu(s;\theta)\)</span>，我们依然使用策略梯度方法，但由于策略是确定性的，我们需要调整梯度的计算。首先，我们定义目标函数：<span class="math display">\[J(\theta)= \mathbb{E}_{s \sim d} \left[ v_\mu(s) \right]=\sum_{s\in\mathcal{S}}d(s) v_\mu(s)\]</span> 这里的 <span class="math inline">\(d(s)\)</span>是一个概率分布，满足 <span class="math inline">\(\sumd(s)=1\)</span>。它可以和策略 <span class="math inline">\(\mu\)</span>相关，也可以无关，此时又两种特殊情况：</p><ul><li>与任何策略都无关的 <spanclass="math inline">\(d_0\)</span>，此时为了方便我们一般设定 <spanclass="math inline">\(d_0(s_0)=1\)</span> 而 <spanclass="math inline">\(d_0(s\nes_0)=0\)</span>，这样做可以最大化起始状态的权重，也符合需求；</li><li>与另一个行为策略 <span class="math inline">\(b\)</span> 相关的 <spanclass="math inline">\(d_b\)</span>，此时 <spanclass="math inline">\(d_b\)</span> 是策略 <spanclass="math inline">\(b\)</span> 下的稳态分布，构成了 Off-Policy的方法。注意，待会我们就会发现，其实 <strong>DPG 天然就是 Off-Policy算法</strong>。</li></ul><p>现在我们给出目标函数的梯度： <span class="math display">\[\begin{aligned}\nabla_\theta J(\theta) &amp; =\sum_{s \in \mathcal{S}} \rho_{\mu}\nabla_\theta \mu(s;\theta)\left(\nabla_a q_\mu(s,a)\right)|_{a=\mu(s;\theta)} \\&amp; =\mathbb{E}_{S \sim \rho_\mu}\left[\nabla_\theta\mu(S;\theta)\left(\nabla_a q_\mu(S, a)\right)|_{a=\mu(S;\theta)}\right]\end{aligned}\]</span> 其中：</p><ul><li><span class="math inline">\(S\)</span> 需要服从 <spanclass="math inline">\(\rho_\mu\)</span>的分布，具体推导过程这里不再展开；</li><li><span class="math inline">\(\left(\nabla_a q_\mu(S,a)\right)|_{a=\mu(S;\theta)}\)</span> 表示先对 <spanclass="math inline">\(q_\mu\)</span> 求梯度，再将 <spanclass="math inline">\(a\)</span> 替换为 <spanclass="math inline">\(\mu(S;\theta)\)</span> 后计算外层的链式求导。</li></ul><p>注意这个表达式中与前面不同的一点：因为没有涉及到 <spanclass="math inline">\(A\)</span>，因此在求期望 <spanclass="math inline">\(\mathbb{E}_{S \sim \rho_\mu}[*]\)</span>的时候也不需要分布 <span class="math inline">\(A\sim\pi(S)\)</span>。所以这里<strong>天然就是 Off-Policy的，之后也无需重要性采样来修正分布</strong>。</p><p>于是，我们就可以给出引入随机采样的梯度上升更新表达式： <spanclass="math display">\[\theta_{t+1} =\theta_t + \alpha_\theta\nabla_\theta\mu(s_t;\theta)\left(\nabla_a q_\mu(s_t,a)\right)|_{a=\mu(s_t;\theta)}\]</span></p><h3 id="dpg-算法步骤">DPG 算法步骤</h3><p>DPG 算法的步骤通常如下：</p><ol type="1"><li><p><strong>初始化动作选择策略网络和价值网络</strong>，并设定学习率<span class="math inline">\(\alpha_w\)</span> 和 <spanclass="math inline">\(\alpha_\theta\)</span>。</p></li><li><p><strong>采样</strong>：使用<strong>任何一个策略</strong>与环境进行交互（也可以从经验池中获取），得到一批经验<span class="math inline">\(\{(s_t, a_t, r_{t+1},s_{t+1})\}\)</span>。</p></li><li><p><strong>计算优势</strong>：使用 TD Error 形式优势函数： <spanclass="math display">\[\delta_t= r_{t+1} + \gamma q(s_{t+1}, \mu(s_{t+1}; \theta_t); w_t) -q(s_t, a_t; w_t)\]</span></p></li><li><p><strong>更新价值网络</strong>：使用的优势函数来直接更新价值网络，<strong>无需修正</strong>：<span class="math display">\[w_{t+1} = w_t + \alpha_w \delta_t \nabla_w q(s_t, a_t; w_t)\]</span></p></li><li><p><strong>更新策略网络</strong>：使用确定性策略梯度更新策略网络：<span class="math display">\[\theta_{t+1} = \theta_t + \alpha_\theta \nabla_\theta \mu(s_t; \theta_t)\cdot \nabla_a q(s_t, a_t; w_{t+1}) \big|_{a_t = \mu(s_t; \theta_t)}\]</span></p></li><li><p><strong>迭代</strong>：重复采样和更新过程，直到策略和价值网络都收敛。</p></li></ol><p>需要注意的是，DPG 算法：</p><ul><li>属于 Off-Policy 算法，但采样所用的行为策略也可以从 <spanclass="math inline">\(\mu\)</span>中复制，但此时要对策略进行扰动，使得策略具有探索性。例如，在连续动作空间内，加上一个正态分布的微小噪音。</li><li>这里的 <span class="math inline">\(q(s_t, a_t; w_t)\)</span>可以选取线性函数 <spanclass="math inline">\(\phi^\top(s,a)w\)</span>（出自 DPG原始论文）或神经网络模型（出自改进的 DDPG 论文）。</li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL 学习笔记 #08 策略梯度方法</title>
    <link href="/RL-Note-8.html"/>
    <url>/RL-Note-8.html</url>
    
    <content type="html"><![CDATA[<p>在之前的学习中，我们讨论了 <strong>Value-based</strong> 方法，例如Q-Learning 和SARSA。这类方法都是以值函数为核心，先估计或学习值函数再间接推导出策略，不断进行采样和迭代优化。然而，在许多复杂场景中，直接优化策略往往是更为高效的做法。</p><p>因此，从本节开始，我们将介绍 <strong>Policy-based</strong>方法，核心是直接对策略进行建模和优化。本节主要介绍<strong>策略梯度</strong>（PolicyGradient），而下一节会扩展到 Actor-Critic 方法。</p><h2 id="基于策略的目标函数">基于策略的目标函数</h2><p>之前我们的所有策略也都是表格形式的 —— 使用 <spanclass="math inline">\((s,a)\)</span> 作为索引的二维表格，每个值代表状态<span class="math inline">\(s\)</span> 下采取动作 <spanclass="math inline">\(a\)</span>的概率。这种策略往往是基于值函数推导出来的，例如 <spanclass="math inline">\(\varepsilon\)</span>-greedy。</p><p>Policy-based 方法的基本思想是直接用<strong>参数化</strong>的策略<span class="math inline">\(\pi(a \mid s; \theta)\)</span>代替基于值函数推导的策略。这里，策略 <span class="math inline">\(\pi(a\mid s; \theta)\)</span> 是某个概率分布的模型，参数为 <spanclass="math inline">\(\theta\)</span>（例如，最常用的神经网络模型）。这个策略的输入是<span class="math inline">\(s\)</span>，输出是状态 <spanclass="math inline">\(s\)</span> 下可执行的所有 <spanclass="math inline">\(a_i \in A(s)\)</span>的概率。这里的策略有时候也会写作 <spanclass="math inline">\(\pi_\theta(a \mid s)\)</span>。</p><p>与 Value-based 方法相比，参数化策略有以下优点：</p><ol type="1"><li><strong>自然处理连续动作空间</strong>：Value-based方法通常依赖离散化处理（即使是值函数近似，最终策略也是基于表格计算的），而参数化策略能够高效地直接优化连续的动作概率分布。</li><li><strong>更高效和可泛化的优化过程</strong>：Value-based方法只有当索引访问到 <span class="math inline">\((s,a)\)</span>时才会更新对应的策略，而 Policy-based 方法在访问一个 <spanclass="math inline">\((s,a)\)</span>时对函数的优化也会作用到其他状态-动作。</li><li><strong>探索更加多样化</strong>：由于策略是一个概率分布，天然地支持随机探索。</li></ol><p>为了优化这个策略，我们需要定义最优策略 <spanclass="math inline">\(\pi^*\)</span>。在之前的表格形式中，我们定义：<span class="math display">\[v_{\pi^*}(s) \ge v_{\pi}(s),\;\; \forall s \in S\]</span>而在参数化情况下，我们需要定义一个<strong>标量目标函数</strong> <spanclass="math inline">\(J(\theta)\)</span>，而这个目标函数通常与策略的长期表现（长期回报）相关。在优化过程中，我们无法直接修改<span class="math inline">\(\pi(a \mid s)\)</span>的概率分布，而是需要对参数 <span class="math inline">\(\theta\)</span>进行更新从而修改概率。</p><p>接下来我们将介绍这个目标函数 <spanclass="math inline">\(J(\theta)\)</span> 的构成。</p><h3 id="指标一average-value">指标一：Average Value</h3><p>我们定义策略 <span class="math inline">\(\pi\)</span>的<strong>平均状态值</strong>为： <span class="math display">\[\bar{v}_\pi= \mathbb{E}_{s \sim d} \left[ v_\pi(s) \right]=\sum_{s\in\mathcal{S}}d(s) v_\pi(s)=d^\top v_\pi\]</span> 其中，<span class="math inline">\(d(s)\)</span> 是状态 <spanclass="math inline">\(s\)</span> 的权重，满足 <spanclass="math inline">\(\sum_{s\in \mathcal{S}}d(s)=1\)</span>。在 AverageValue 中，<span class="math inline">\(d\)</span>可以有以下两种形式：</p><ol type="1"><li><p><strong>策略无关的分布</strong>：我们将其记为 <spanclass="math inline">\(d_0\)</span>。此时在 <spanclass="math inline">\(\bar{v}_\pi\)</span> 中仅有 <spanclass="math inline">\(v_\pi\)</span> 与策略有关，记为 <spanclass="math inline">\(\bar{v}_\pi^0\)</span>，因此在求梯度 <spanclass="math inline">\(\nabla_\pi \bar{v}_\pi^0\)</span>时就不需要考虑分布 <spanclass="math inline">\(d_0\)</span>，更方便计算。常见的一种情况是<strong>将所有状态同等看待</strong>：<span class="math display">\[d_0(s) = \frac{1}{|\mathcal{S}|},\]</span> 也可以<strong>自定义偏好</strong>，例如赋予初始状态高权重：<span class="math display">\[d_0(s_0) = 1,\quad d_0(s\ne s_0) = 0.\]</span></p></li><li><p><strong>稳态分布</strong>（StationaryDistribution）：与上一节定义值函数近似目标函数类似，我们考虑智能体在策略<span class="math inline">\(\pi\)</span>下在每个状态停留的概率。此时访问次数多的状态自然就被赋予了高权重。</p></li></ol><p>考虑到 <span class="math inline">\(v_\pi(s)\)</span> 可以展开为状态<span class="math inline">\(s\)</span> 的平均回报： <spanclass="math display">\[v_\pi(s) =\mathbb{E}\left[\sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_0=s\right]\]</span> 因此，<span class="math inline">\(\bar{v}_\pi\)</span>还有另一个等价的形式： <span class="math display">\[\begin{aligned}\bar{v}_\pi&amp;= \sum_{s\in \mathcal{S}}d(s)\mathbb{E}\left[\sum_{t=0}^\infty\gamma^t R_{t+1} \mid S_0=s \right] \\&amp;=\mathbb{E}\left[\sum_{t=0}^\infty \gamma^t R_{t+1} \right]\\\end{aligned}\]</span></p><h3 id="指标二average-reward">指标二：Average Reward</h3><p>对于策略 <spanclass="math inline">\(\pi\)</span>，我们也可以定义一个更加通用的<strong>平均奖励/ 平均单步奖励</strong>： <span class="math display">\[\bar{r}_\pi= \mathbb{E}_{s \sim d_\pi} \left[ r_\pi(s)\right]=\sum_{s\in \mathcal{S}}d_\pi(s) r_\pi(s)\]</span> 其中：</p><ul><li>与 Average Value 不同的是，这里的 <spanclass="math inline">\(d_\pi\)</span> 直接是稳态分布，依赖于策略 <spanclass="math inline">\(\pi\)</span>。</li><li><span class="math inline">\(r_\pi(s)\)</span> 表示状态 <spanclass="math inline">\(s\)</span> 下的平均奖励： <spanclass="math inline">\(r_\pi(s) = \sum_{a\in \mathcal{A}}\pi(a\mid s)r(s,a)\)</span>；</li><li><span class="math inline">\(r(s,a)\)</span> 表示状态 <spanclass="math inline">\(s\)</span> 时执行动作 <spanclass="math inline">\(a\)</span> 的平均奖励：<spanclass="math inline">\(r(s,a)= \sum_{r}r p(r\mid s,a)\)</span>。</li></ul><p>它还有另一个等价的形式： <span class="math display">\[\begin{aligned}&amp; \lim _{n \rightarrow \infty} \frac{1}{n}\mathbb{E}\left[R_{t+1}+R_{t+2}+\cdots+R_{t+n} \mid S_t=s_0\right] \\= &amp; \lim _{n \rightarrow \infty} \frac{1}{n}\mathbb{E}\left[\sum_{k=1}^n R_{t+k} \mid S_t=s_0\right]\end{aligned}\]</span> 其中，<span class="math inline">\(\{(R_{t+1}, R_{t+1},\ldots)\}\)</span> 是智能体从状态 <spanclass="math inline">\(s_0\)</span> 出发基于策略 <spanclass="math inline">\(\pi_\theta\)</span>生成的一个轨迹下的每步奖励。考虑到当 <span class="math inline">\(n\rightarrow \infty\)</span> 时，起点 <spanclass="math inline">\(s_0\)</span> 也失去了意义，所以有： <spanclass="math display">\[\bar{r}_\pi=\lim _{n \rightarrow \infty} \frac{1}{n}\mathbb{E}\left[\sum_{k=1}^n R_{t+k} \right]\]</span> 需要注意的是，这里的 <spanclass="math inline">\(\bar{r}_\pi\)</span>只考虑<strong>即时奖励</strong>，因此不含有折扣因子 <spanclass="math inline">\(\gamma\)</span>。关于这两个表达形式等价的证明，这里不再展开介绍。</p><h3 id="两个指标的联系">两个指标的联系</h3><p>两个指标都是关于 <span class="math inline">\(\theta\)</span>的函数，因此我们的目标就是去最大化这些指标，这就是<strong>策略梯度</strong>（PolicyGradient）的基本思路。</p><p>直观上看，Average Reward 好像更短视，因为它只关心即时奖励，而 AverageValue 关注到整体回报。但可以证明，在含有折扣因子 <spanclass="math inline">\(\gamma &lt; 1\)</span>的情况下，<strong>二者等效</strong>： <span class="math display">\[\bar{r}_\pi=(1-\gamma)\bar{v}_\pi\]</span>在优化其中一个指标时，另一个指标也会跟随着优化，并且同时达到极值。</p><h2 id="策略梯度-policy-gradient">策略梯度 | Policy Gradient</h2><p>有了目标函数后，为了优化策略，我们需要计算目标函数 <spanclass="math inline">\(J(\theta)\)</span> 对策略参数 <spanclass="math inline">\(\theta\)</span> 的梯度 ——这是策略梯度方法中最复杂的一步 ——因为涉及到不同形式的指标、是否有折扣等等。</p><h3 id="目标函数的梯度计算">目标函数的梯度计算</h3><p>为了简单起见，这里先给出<strong>统一的梯度表达式</strong>： <spanclass="math display">\[\nabla_\theta J(\theta)=\sum_{s \in \mathcal{S}} \eta(s) \sum_{a \in\mathcal{A}} \nabla_\theta \pi(a \mid s, \theta) q_\pi(s, a)\]</span> 其中：</p><ul><li>目标函数 <span class="math inline">\(J(\theta)\)</span> 可以是 <spanclass="math inline">\(\bar{v}_\pi\)</span>，<spanclass="math inline">\(\bar{r}_\pi\)</span> 或 <spanclass="math inline">\(\bar{v}_\pi^0\)</span>；</li><li>等号「<span class="math inline">\(=\)</span>」可以是严格相等 <spanclass="math inline">\(=\)</span>、约等 <spanclass="math inline">\(\simeq\)</span> 或成比例等 <spanclass="math inline">\(\propto\)</span>，取决于 <spanclass="math inline">\(J(\theta)\)</span>；</li><li><span class="math inline">\(\eta\)</span>是一个特殊的分布，对于不同的 <spanclass="math inline">\(J(\theta)\)</span> 会呈现不同的形式。</li></ul><p>具体而言： <span class="math display">\[\begin{gathered}\nabla_\theta \bar{r}_\pi \simeq \sum_s d_\pi(s) \sum_a \nabla_\theta\pi(a \mid s, \theta) q_\pi(s, a), \\\nabla_\theta \bar{v}_\pi=\frac{1}{1-\gamma} \nabla_\theta \bar{r}_\pi,\\\nabla_\theta \bar{v}_\pi^0=\sum_{s \in \mathcal{S}} \rho_\pi(s) \sum_{a\in \mathcal{A}} \nabla_\theta \pi(a \mid s, \theta) q_\pi(s, a).\end{gathered}\]</span> 这组公式也被称为<strong>策略梯度定理</strong>（Policy GradientTheorem）。它的推导基于对轨迹概率的链式分解，具体证明详见：<ahref="https://zhuanlan.zhihu.com/p/491647161">【策略梯度定理】推导、证明、深入理解与代码实现- 知乎</a>。</p><p>当然最重要的是，<span class="math inline">\(\nabla_\thetaJ(\theta)\)</span> 还可以进一步写成<strong>期望形式</strong>： <spanclass="math display">\[\begin{aligned}\nabla_\theta J(\theta) &amp; =\sum_{s \in \mathcal{S}} \eta(s) \sum_{a\in \mathcal{A}} \nabla_\theta \pi(a \mid s, \theta) q_\pi(s, a)\\&amp;=\sum_{s \in \mathcal{S}} \eta(s) \sum_{a \in \mathcal{A}} \pi(a\mid s, \theta) \nabla_\theta \ln \pi(A \mid S;\theta) q_\pi(s, a)\\&amp;=\mathbb{E}_{S\sim \eta} \left[ \sum_{a \in \mathcal{A}} \pi(a \mids, \theta) \nabla_\theta \ln \pi(A \mid S;\theta) q_\pi(s, a)\right]\\&amp;=\mathbb{E}_{S\sim \eta, A \sim \pi} \left[\nabla_\theta \ln \pi(A\mid S;\theta) \cdot q_\pi(S, A)\right]\\&amp;=\mathbb{E} \left[\nabla_\theta \ln \pi(A \mid S;\theta) \cdotq_\pi(S, A)\right]\\\end{aligned}\]</span> 其中，<span class="math inline">\(\ln \pi(a \mids;\theta)\)</span>是策略函数的<strong>自然对数</strong>，这里有一个关键的变形： <spanclass="math display">\[\nabla_\theta \ln \pi(A \mid S;\theta)=\frac{\nabla_\theta  \pi(A \midS;\theta)}{\pi(A \mid S;\theta)}\]</span>为什么需要期望形式呢？因为原始表达式中的分布均无法准确计算，而一旦引入期望，我们就能用采样来近似这个值！进而使用随机梯度上升来最大化这个目标。</p><p>注意到，当我们想计算 <span class="math inline">\(\ln \pi(A \midS;\theta)\)</span>，还需要满足 <span class="math inline">\(\pi(A \midS;\theta) &gt;0\)</span>，因此为了我们需要在神经网络的输出层引入<strong>Softmax</strong> 运算，将 <spanclass="math inline">\((-\infty,+\infty)\)</span> 映射到 <spanclass="math inline">\((0,1)\)</span>。</p><p>因此，最终策略 <span class="math inline">\(\pi\)</span>会呈现如下形式： <span class="math display">\[\pi(a \mid s; \theta)=\frac{e^{h(a\mid s;\theta)}}{\sum_{a^{\prime} \in\mathcal{A}} e^{h\left(a^{\prime}\mid s;\theta\right)}}\]</span> 其中的 <span class="math inline">\(h(a\mid s;\theta)\)</span>就类似上一节介绍的 Feature Function，使用神经网络来自动学习特征。输入<span class="math inline">\(s\)</span> 后经过参数为 <spanclass="math inline">\(\theta\)</span> 的网络，得到若干个 <spanclass="math inline">\(h(a^{\prime}\mid s;\theta)\)</span>，再一起映射到<span class="math inline">\((0,1)\)</span> 输出。</p><blockquote><p>由于 <span class="math inline">\(\pi(A \mid S;\theta)&gt;0\)</span>，显然此时的策略就是<strong>随机性</strong>策略，具有较强的探索性。在下一节中我们将介绍Deterministic Policy Gradient (DPG) 就是确定性策略。</p></blockquote><h3 id="reinforce-算法中的近似">REINFORCE 算法中的近似</h3><p>在得到梯度的期望形式后，策略梯度方法的最基础实现是<strong>REINFORCE</strong>算法，也称<strong>随机梯度策略优化</strong>。这是一个基于 Monte-Carlo方法的策略优化算法，稍后我们将介绍它如何从环境中采样得到的回报来更新策略参数。</p><p>首先要明确，策略参数 <span class="math inline">\(\theta\)</span>的更新本质上是一个<strong>梯度上升</strong>（GradientAscent）过程，因为我们希望最大化目标函数 <spanclass="math inline">\(J(\theta)\)</span>，于是有： <spanclass="math display">\[\begin{aligned}\theta_{t+1} &amp;= \theta_t + \alpha \nabla_\theta J(\theta_t) \\&amp;= \theta_t + \alpha \mathbb{E} \left[\nabla_\theta \ln \pi(A \midS;\theta_t) \cdot q_\pi(S, A)\right]\\\end{aligned}\]</span>其中涉及到无法获取的期望，于是我们<strong>用随机采样来代替</strong>：<span class="math display">\[\theta_{t+1} =\theta_t + \alpha \nabla_\theta \ln \pi(a_t \mids_t;\theta_t) \cdot q_\pi(s_t, a_t)\]</span> 但这个表达式中也涉及到未知的 <spanclass="math inline">\(q_\pi\)</span>，因此我们还需要将其也近似为 <spanclass="math inline">\(q_t(s_t,a_t)\)</span>，最简单的办法就是<strong>使用蒙特卡洛采样近似</strong>，这也是REINFORCE 最重要的点。</p><p>要估计 <span class="math inline">\(q_\pi(s_t,a_t)\)</span>，我们就从<span class="math inline">\((s,a)\)</span> 出发得到一个完整Episode，使用累计奖励回报 <span class="math inline">\(g_t\)</span>来近似结果。此外，我们还可以用上时序差分等方法进行估计（例如，之后会介绍的Actor-Critic 方法）。</p><h3 id="策略梯度的物理意义">策略梯度的物理意义</h3><p>为了进一步理解 REINFORCE的优化过程，我们来分析一下策略梯度的物理意义。我们先对更新表达式进一步改写：<span class="math display">\[\begin{aligned}\theta_{t+1} &amp; =\theta_t+\alpha \nabla_\theta \ln \pi\left(a_t \mids_t; \theta_t\right) q_t\left(s_t, a_t\right) \\&amp; =\theta_t+\alpha \underbrace{\left(\frac{q_t\left(s_t,a_t\right)}{\pi\left(a_t \mid s_t; \theta_t\right)}\right)}_{\beta_t}\nabla_\theta \pi\left(a_t \mid s_t; \theta_t\right)\end{aligned}\]</span> 使用 <span class="math inline">\(\beta_t\)</span>简写后，更新表达式变为： <span class="math display">\[\theta_{t+1} =\theta_t+\alpha \beta_t \nabla_\theta \pi\left(a_t \mids_t; \theta_t\right)\]</span> 如果将 <span class="math inline">\(\alpha \beta_t\)</span>认为是<strong>新的更新步长</strong>，则这个式子的目标就变成了<strong>最大化</strong><span class="math inline">\(\pi\left(a_t \mids_t\right)\)</span>！这个时候大家应该也意识到了，什么时候需要最大化动作<span class="math inline">\(a_t\)</span> 在 <spanclass="math inline">\(s_t\)</span>下的发生概率？当且仅当<strong>这个动作是好动作</strong>的时候！</p><p>从微分的视角可以证明：当 <spanclass="math inline">\(\beta_t&gt;0\)</span> 且 <spanclass="math inline">\(|\alpha \beta_t|\)</span> 充分小的时候，一定有<span class="math inline">\(\pi\left(a_t \mid s_t; \theta_{t+1}\right)&gt; \pi\left(a_t \mid s_t; \theta_{t}\right)\)</span>；而当 <spanclass="math inline">\(\beta_t&lt;0\)</span> 且 <spanclass="math inline">\(|\alpha \beta_t|\)</span> 充分小的时候，一定有<span class="math inline">\(\pi\left(a_t \mid s_t; \theta_{t+1}\right)&lt; \pi\left(a_t \mid s_t; \theta_{t}\right)\)</span>。</p><p>事实上，<span class="math inline">\(\beta_t\)</span>可以很好地平衡探索（Exploration）与利用（Exploitation）：</p><ul><li>利用：当分子 <span class="math inline">\(q_t\left(s_t,a_t\right)\)</span> 越大，<span class="math inline">\(\beta_t\)</span>就越大，<span class="math inline">\(\pi\left(a_t \mid s_t;\theta_{t+1}\right)\)</span>也就越大，意味着新的策略会<strong>将更大的概率分配给已经有较大价值的动作</strong>；</li><li>探索：当分母 <span class="math inline">\(\pi\left(a_t \mid s_t;\theta_{t}\right)\)</span> 越小，<spanclass="math inline">\(\beta_t\)</span> 就越大，<spanclass="math inline">\(\pi\left(a_t \mid s_t;\theta_{t+1}\right)\)</span>也就越大，意味着新的策略会<strong>将更大的概率分配给先前选择概率小的动作</strong>；</li></ul><h3 id="reinforce-算法步骤">REINFORCE 算法步骤</h3><p>在实际执行的过程中，我们需要有两次采样，第一次是随机梯度采样的 <spanclass="math inline">\((s,a)\)</span>，第二次是蒙特卡洛采样的 <spanclass="math inline">\(g_t\)</span>。而在采样 <spanclass="math inline">\((s,a)\)</span>的过程中，还有一个分布问题需要解决。</p><p>考虑到 <span class="math inline">\(S\simd_\pi\)</span>，我们需要用稳态分布来获取 <spanclass="math inline">\(s\)</span>，但实际情况下我们不会太在意这一点。而考虑到<span class="math inline">\(A\sim \pi(A\mid,S;\theta)\)</span>，我们需要用策略 <spanclass="math inline">\(\pi(\theta_t)\)</span> 的分布来获取 <spanclass="math inline">\(a\)</span>。所以 Policy Gradient 实际上是一种<strong>On-Policy</strong> 算法，其行为策略就是我要改进的目标策略。</p><blockquote><p>当然也有 Off-Policy的实现形式，但会需要额外的技巧，这将在下一节介绍。</p></blockquote><p>以下是 REINFORCE 的完整算法步骤：</p><ol type="1"><li><p><strong>初始化策略参数</strong>：设定初始参数 <spanclass="math inline">\(\theta\)</span>；</p></li><li><p><strong>蒙特卡洛采样轨迹</strong>：根据当前策略 <spanclass="math inline">\(\pi_\theta(a \mid s)\)</span>与环境交互，生成一个轨迹 <span class="math inline">\(\tau = \{s_0, a_0,r_1, s_1, \dots, s_T\}\)</span>；</p></li><li><p><strong>估计价值和更新策略参数</strong>：对轨迹中的每一步，执行：</p><ul><li><p><strong>Value Updata</strong>：利用累计奖励回报估计动作价值：<span class="math display">\[q_t\left(s_t, a_t\right) = \sum_{k=t+1}^T \gamma^{k-t-1} r_k\]</span></p></li><li><p><strong>PolicyUpdate</strong>：利用策略梯度公式更新参数：</p></li></ul><p><span class="math display">\[\theta_{t+1} =\theta_t + \alpha \nabla_\theta \ln \pi(a_t \mids_t;\theta) \cdot q_t(s_t, a_t)\]</span></p></li><li><p><strong>重复</strong>：在遍历完一条轨迹后，使用最新策略重复步骤 2继续采样，直至策略收敛。</p></li></ol><blockquote><p>看到这里可能大家已经发现了 REINFORCE的一个提升空间：每次采样都需要生成一个完整轨迹，并且需要将整个轨迹遍历完后才能用最新的策略再采样新的轨迹。而中间在Policy Update 的时候，尽管 <span class="math inline">\(\theta_t\)</span>一直在更新，我们使用的还是旧策略采样的旧轨迹。</p><p>显然，我们可以边更新 <span class="math inline">\(\theta_t\)</span>边采样新的数据，这就是下一节将要介绍的基于 TD Learning的策略梯度方法。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL 学习笔记 #07 值函数近似和 DQN 算法</title>
    <link href="/RL-Note-7.html"/>
    <url>/RL-Note-7.html</url>
    
    <content type="html"><![CDATA[<p>前面介绍的 SARSA 和 Q-Learning等算法，实际上是基于<strong>表格形式</strong>（TabularRepresentation）的值函数学习方法。这个表格就是所谓的<strong>Q-Table</strong> —— 可以将 <spanclass="math inline">\(q(s,a)\)</span> 放置到一个二维表格，将 <spanclass="math inline">\(v(s)\)</span> 放置到一个一维表格。尽管这些方法在<strong>小规模、离散状态空间</strong>中能够很好地工作，但随着状态空间的扩展，存储和更新表格的代价变得难以承受，且无法<strong>泛化</strong>到未知状态。</p><p>为了克服这一问题，我们需要引入<strong>值函数近似</strong>（ValueFunction Approximation）方法。举个例子，对于组状态 <spanclass="math inline">\(s_1, \ldots, s_N\)</span>，他们的状态值为 <spanclass="math inline">\(v(s_1),\ldots,v(s_N)\)</span>。当 <spanclass="math inline">\(N\)</span>非常大的时候，我们尝试用一个简单的曲线来拟合他们之间的关系： <spanclass="math display">\[\hat{v}(s, \theta)=a s+b=\underbrace{[s, 1]}_{\phi^\top(s)}\underbrace{\left[\begin{array}{c}a \\b\end{array}\right]}_\theta=\phi^\top(s) \theta\]</span> 其中：</p><ul><li><span class="math inline">\(\hat{v}(s, \theta)\)</span>表示对曲线的估计，这里将其拟合为线性关系；</li><li>为了形式的统一，我们将其写为向量相乘的形式，<spanclass="math inline">\(\theta\)</span>表示<strong>参数向量</strong>，<spanclass="math inline">\(\phi(s)\)</span>表示<strong>特征向量</strong>。</li></ul><p>此外，我们也可以用更高阶的曲线进行拟合： <spanclass="math display">\[\hat{v}(s, \theta)=a s^2+bs+c=\underbrace{[s^2, s, 1]}_{\phi^\top(s)}\underbrace{\left[\begin{array}{c}a \\b \\c\end{array}\right]}_\theta=\phi^\top(s) \theta\]</span> 这个时候 <span class="math inline">\(\hat{v}(s, w)\)</span>对于 <span class="math inline">\(s\)</span> 是非线性的，但对于 <spanclass="math inline">\(\theta\)</span>仍保持<strong>线性</strong>，因为非线性参数被蕴含到特征向量中。这将有助于后续的求解<span class="math inline">\(\theta\)</span> 的过程。</p><h2 id="值函数近似">值函数近似</h2><p>值函数近似的核心思想是<strong>用参数化函数替代表格形式</strong>的值函数，从而将强化学习的问题转化为一个<strong>优化问题</strong>。我们用带有参数的函数<span class="math inline">\(\hat{q}(s, a; \theta)\)</span> 或 <spanclass="math inline">\(\hat{v}(s; \theta)\)</span> 近似动作价值函数 <spanclass="math inline">\(q(s, a)\)</span> 或状态价值函数 <spanclass="math inline">\(v(s)\)</span>，其中 <spanclass="math inline">\(\theta\)</span>是函数的参数集合（例如神经网络的权重）。</p><h3 id="目标函数定义">目标函数定义</h3><p>值函数近似需要定义一个目标函数来衡量当前近似值函数与真实值函数的偏差。最常用的目标函数是均方误差（MeanSquared Error，MSE）形式： <span class="math display">\[J(\theta) = \mathbb{E}\left[\left( v_\pi(S) - \hat{v}(S; \theta)\right)^2 \right],\]</span> 通过最小化 <spanclass="math inline">\(J(\theta)\)</span>，我们可以找到最优参数 <spanclass="math inline">\(\theta^*\)</span>。注意，这里的均值 <spanclass="math inline">\(\mathbb{E}\)</span> 有两种处理方法：</p><ol type="1"><li><strong>平均分布</strong>（UniformDistribution）：平等看待所有状态，乘以 <spanclass="math inline">\(1/|\mathcal{S}|\)</span>。但这样并不好，因为目标状态和接近目标的状态实际上会更重要！</li><li><strong>稳态分布</strong>（StationaryDistribution）：用于描述<strong>长期行为</strong>（long-runbehavior），将一个智能体长期放置在环境中，以策略 <spanclass="math inline">\(\pi\)</span>进行交互，最终可以统计出智能体在每个状态停留的概率 <spanclass="math inline">\(\{d_\pi(s)\}_{s\in \mathcal{S}}\)</span>。</li></ol><p>稳态分布也被称为 Steady-state Distribution 或 LimitingDistribution，在这一分布下，目标函数可以转化为： <spanclass="math display">\[J(\theta) = \mathbb{E}\left[\left( v_\pi(S) - \hat{v}(S; \theta)\right)^2 \right]=\sum_{s \in \mathcal{S}}d_\pi(s)\left(v_\pi(s)-\hat{v}(s; \theta)\right)^2\]</span> 在<strong>有模型</strong>的情况下，<spanclass="math inline">\(d_\pi(s)\)</span>也可以直接用<strong>迭代法</strong>计算出来： <spanclass="math display">\[d_\pi^\top=d_\pi^\top P_\pi\]</span></p><h3 id="优化算法和函数选择">优化算法和函数选择</h3><p>值函数近似中，参数优化的核心方法包括<strong>梯度下降</strong>（GradientDescent）及其变种算法。针对目标函数 <spanclass="math inline">\(J(\theta)\)</span>，通过对 <spanclass="math inline">\(\theta\)</span> 计算梯度，可以更新参数：</p><p><span class="math display">\[\theta_{t+1} = \theta_t - \alpha \nabla_\theta J(\theta_t),\]</span></p><p>其中梯度可以展开为： <span class="math display">\[\begin{aligned}\nabla_\theta J(\theta) &amp; =\nabla_\theta\mathbb{E}\left[\left(v_\pi(S)-\hat{v}(S; \theta)\right)^2\right] \\&amp; =\mathbb{E}\left[\nabla_\theta\left(v_\pi(S)-\hat{v}(S;\theta)\right)^2\right] \\&amp; =2 \mathbb{E}\left[\left(v_\pi(S)-\hat{v}(S;\theta)\right)\left(-\nabla_\theta \hat{v}(S; \theta)\right)\right] \\&amp; =-2 \mathbb{E}\left[\left(v_\pi(S)-\hat{v}(S; \theta)\right)\nabla_\theta \hat{v}(S; \theta)\right]\end{aligned}\]</span>为了消除求期望运算，我们使用<strong>随机梯度下降</strong>，则更新公式变为：<span class="math display">\[\theta_{t+1} = \theta_t + \alpha_t \left(v_\pi(s_t)-\hat{v}(s_t;\theta_t)\right) \nabla_\theta \hat{v}(s_t; \theta_t)\]</span> 但还有一个问题，<spanclass="math inline">\(v_\pi(s_t)\)</span> 是未知的。这里有两种方法：</p><ol type="1"><li><p>基于 <strong>MC Learning</strong> 的方法，使用采样的累计汇报<span class="math inline">\(g_t\)</span> 来代替： <spanclass="math display">\[\theta_{t+1} = \theta_t + \alpha_t \left(g_t-\hat{v}(s_t;\theta_t)\right) \nabla_\theta \hat{v}(s_t; \theta_t)\]</span></p></li><li><p>基于 <strong>TD Learning</strong> 的方法，使用 TD Target 来代替：<span class="math display">\[\theta_{t+1} = \theta_t + \alpha_t \left(r_{t+1} + \gamma\hat{v}(s_{t+1};\theta_t) -\hat{v}(s_t; \theta_t)\right) \nabla_\theta\hat{v}(s_t; \theta_t)\]</span></p></li></ol><p>函数选择方面，我们可以使用以下模型进行值函数的近似：</p><ol type="1"><li><p><strong>线性模型</strong>：线性函数 <spanclass="math inline">\(\hat{v}(s; \theta) = \phi(s)^\top\theta\)</span>，其中 <span class="math inline">\(\phi(s)\)</span>是状态特征向量，<strong>需要人工选取特征</strong>。此时优化算法也称为<strong>TD-Linear</strong>： <span class="math display">\[\theta_{t+1} = \theta_t + \alpha_t \left(r_{t+1} + \gamma\phi^\top(s_{t+1}) \theta_t -\phi^\top(s_{t}) \theta_t\right) \phi(s_t)\]</span></p></li><li><p><strong>非线性模型</strong>：如多层感知机（MLP）或卷积神经网络（CNN），<strong>无需人工选取特征</strong>，适用于处理高维状态空间和复杂特征，但缺乏可解释性。</p></li><li><p><strong>核方法</strong>：如高斯核函数，能够较为自然地处理非线性映射。</p></li></ol><h3 id="td-优化目标分析">TD 优化目标分析</h3><p>前面介绍到最常用的目标函数是均方误差形式的<strong>真实残差</strong>（TrueValue Error）： <span class="math display">\[J_E(\theta) = \mathbb{E}\left[\left( v_\pi(S) - \hat{v}(S; \theta)\right)^2 \right]=\left\| v_\pi - \hat{v}(\theta) \right\|^2_D\]</span> 其中 <span class="math inline">\(\left\| \cdot\right\|\)</span> 表示向量取模，下角标 <spanclass="math inline">\(D\)</span> 表示 <spanclass="math inline">\(\{d_\pi(s)\}_{s\in \mathcal{S}}\)</span>构成的对角矩阵，一般有 <span class="math inline">\(\left\| X\right\|^2_D=X^\top D X\)</span>。</p><hr /><p>实际上，还有另一种<strong>贝尔曼残差</strong>（Bellman Error）写为：<span class="math display">\[J_{BE}(\theta)=\left\|\hat{v}(\theta)-\left(r_\pi+\gamma P_\pi\hat{v}(\theta)\right)\right\|_D^2\doteq\left\|\hat{v}(\theta)-T_\pi(\hat{v}(\theta))\right\|_D^2\]</span> 其中 <span class="math inline">\(T_\pi(x) \doteq r_\pi+\gammaP_\pix\)</span>，这是因为我们希望<strong>拟合的值函数也满足贝尔曼公式</strong>，但实际上这两个值可能不相等，于是我们就去最小化贝尔曼残差。</p><hr /><p>而当我们用 TD Target 代替 <spanclass="math inline">\(v_\pi(s_t)\)</span>时，还要考虑到函数选择的问题。由于我们选择的函数很可能最终无法完全逼近真实值函数（例如线性模型），因此<span class="math inline">\(\hat{v}(\theta)\)</span> 和 <spanclass="math inline">\(T_\pi(\hat{v}(\theta))\)</span>可能永远也不会相等，此时我们会对 <spanclass="math inline">\(T(x)\)</span> 再进行一次投影，使其最终能够等于零：<span class="math display">\[J_{PBE}(\theta)=\left\|\hat{v}(\theta)-MT_\pi(\hat{v}(\theta))\right\|_D^2\]</span> 此时的目标我们称为<strong>投影贝尔曼残差</strong>（ProjectedBellman Error）。</p><h3 id="案例分析">案例分析</h3><p>下面用一个网格世界的案例来分析值函数近似的效果，我们首先给出最优状态值对应的可视化结果：</p><p><img src="/img/blog/RL-Note-7-images/Grid-World.png" alt="网格世界案例的最优状态值及其三维可视化" width=90% /></p><p>我们可以利用经验数据进行迭代估计，如果使用 Tabular方法，则最终结果如下：</p><p><img src="/img/blog/RL-Note-7-images/TD-Table.png" alt="TD-Table最终收敛结果" width=70%/></p><p>可以看到随着使用的经验数据变多，均方误差接近 <spanclass="math inline">\(0\)</span>，可视化效果也十分接近真实值。而如果使用一阶线性模型近似，则最终结果如下：</p><p><img src="/img/blog/RL-Note-7-images/TD-Linear-1.png" alt="一阶TD-Linear最终收敛结果" width=90%/></p><p>可以看到拟合出的趋势接近，但实际值仍有较大误差。如果使用更高阶的模型，则效果将会更加还原：</p><p><img src="/img/blog/RL-Note-7-images/TD-Linear-2.png" alt="二阶TD-Linear最终收敛结果" width=70% /></p><p><img src="/img/blog/RL-Note-7-images/TD-Linear-3.png" alt="三阶TD-Linear最终收敛结果" width=70% /></p><h3 id="sarsa-和-q-learning-的应用">SARSA 和 Q-Learning 的应用</h3><p>前面的介绍都是以状态值为主，但我们知道在实际应用中，动作价值将更有助于找到最优策略。这里我们将值函数近似与SARSA 和 Q-Learning 算法结合，实现对高维状态空间的高效学习。</p><ol type="1"><li><p><strong>近似 SARSA</strong>：使用 <spanclass="math inline">\(\hat{q}(s, a; \theta)\)</span> 替代表格形式的<span class="math inline">\(q(s, a)\)</span>，更新公式为： <spanclass="math display">\[\theta_{t+1} = \theta_t + \alpha_t \left( r_{t+1} + \gamma\hat{q}(s_{t+1}, a_{t+1}; \theta_t) - \hat{q}(s_t, a_t; \theta_t)\right) \nabla_\theta \hat{q}(s_t, a_t; \theta_t).\]</span></p></li><li><p><strong>近似 Q-Learning</strong>：更新公式为： <spanclass="math display">\[\theta_{t+1} = \theta_t + \alpha_t \left( r_{t+1} + \gamma \max_{a \inA(s_{t+1})} \hat{q}(s_{t+1}, a; \theta_t) - \hat{q}(s_t, a_t; \theta_t)\right) \nabla_\theta \hat{q}(s_t, a_t; \theta_t).\]</span></p></li></ol><p>与 Tabular 形式的 SARSA 和 Q-Learning 区别在于，我们现在不直接更新<span class="math inline">\(q_t(s,a)\)</span>，而是更新模型参数 <spanclass="math inline">\(\theta_t\)</span>。</p><p>在迭代收敛后，也就完成了策略评估（PE），之后的策略改进（PI）也和之前类似，只不过在选择最优动作时<span class="math inline">\(a=\arg \max_{a \in A(s_t)} \hat{q}(s_t, a;\theta_t)\)</span> 不能直接索引，而是要代入 <spanclass="math inline">\(\hat{q}\)</span><strong>算出函数值再比较</strong>。</p><h2 id="deep-q-network-dqn">Deep Q-Network | DQN</h2><p>值函数近似的进一步发展是<strong>深度 Q 网络</strong>（DeepQ-Network，DQN），它将深度学习引入到 Q-Learning中，使用深度神经网络近似动作价值函数，也被称为 <strong>DeepQ-Learning</strong>。</p><p>DQN 的更新公式与近似 Q-Learning 完全一致，只不过 <spanclass="math inline">\(\hat{q}(s, a; \theta)\)</span>使用了深度神经网络：</p><p><span class="math display">\[\theta_{t+1} = \theta_t + \alpha_t \left( r_{t+1} + \gamma \max_{a \inA(s_{t+1})} \hat{q}(s_{t+1}, a; \theta_t) - \hat{q}(s_t, a_t; \theta_t)\right) \nabla_\theta \hat{q}(s_t, a_t; \theta_t).\]</span></p><p>其优化目标自然就是<strong>贝尔曼最优残差</strong>（Bellman OptimalityError）： <span class="math display">\[J_{BOE}(\theta)=\mathbb{E}\left[\left( R+\gamma \max_{a \in A(S&#39;)}\hat{q}(S&#39;, a; \theta) - \hat{q}(S,A;\theta) \right)^2 \right]\]</span></p><h3 id="目标网络-target-network">目标网络 | Target Network</h3><p>有了这个目标后，我们自然想到用梯度下降来求解。对于 <spanclass="math inline">\(\hat{q}(S,A;\theta)\)</span>很容易就能求出其梯度，但对于 <span class="math inline">\(\max\hat{q}(S&#39;, a; \theta)\)</span> 则必须用上特殊的技巧。</p><p>在 DQN 中，我们令： <span class="math display">\[y \doteq R+\gamma \max_{a \in A(S&#39;)} \hat{q}(S&#39;, a; \theta)\]</span> 在优化过程中，通过将 <span class="math inline">\(y\)</span>固定为一个常量，就可以免去 <span class="math inline">\(\max\)</span>项的优化。具体而言，我们引入两个网络：</p><ul><li><strong>主网络</strong>（Main network）：<spanclass="math inline">\(\hat{q}(s,a;\theta)\)</span>，持续优化的主网络参数，每一步需要计算<span class="math inline">\(\nabla \hat{q}(s,a;\theta)\)</span>进行梯度下降；</li><li><strong>目标网络</strong>（Target network）：<spanclass="math inline">\(\hat{q}(s,a;\theta_T)\)</span>，是主网络参数的一个<strong>延迟副本</strong>，在优化的时候<strong>每隔固定步数将主网络参数复制到目标网络中</strong>，而非每一步都更新。</li></ul><p>这样一来，我们就避免了复杂的求梯度运算，且可以让目标值变得更加稳定、单一，从而避免训练过程中振荡或发散。</p><h3 id="经验回放-experience-replay">经验回放 | Experience Replay</h3><p>DQN 还引入了经验回放（ExperienceReplay）机制，用来<strong>打破数据之间的时序相关性</strong>并提高<strong>样本效率</strong>。经验回放的主要步骤如下：</p><ol type="1"><li><strong>存储经验</strong>：将每一步 <spanclass="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span>存入<strong>回放缓冲区</strong>（Replay Buffer）。</li><li><strong>随机采样</strong>：从缓冲区中随机抽取一个小批量（mini-batch）的经验样本。这种采样方式可以<strong>去除经验之间的时序相关性</strong>，近似独立同分布（i.i.d.）。</li><li><strong>训练网络</strong>：基于抽取的样本计算损失函数，并更新网络参数。</li></ol><p>那么，接下来有几个问题还需要回答：为什么经验回放对于 DQN来说是必须的？为什么采样必须服从<strong>均匀分布</strong>（UniformDistribution），即每个样本采样概率相等？</p><ul><li><p>在目标函数 <span class="math inline">\(J_{BOE}(\theta)\)</span>中，我们假设 <span class="math inline">\((S,A)\sim d\)</span>，其中<span class="math inline">\(d\)</span> 是一个分布，<spanclass="math inline">\((S,A)\)</span>整体视作一个随机变量。在没有明确的先验知识时，通常假设 <spanclass="math inline">\(d\)</span>是均匀分布，即所有状态-动作对同等重要。</p></li><li><p>但在采集数据时，智能体是按照时序顺序生成数据的，导致样本实际分布可能偏离均匀分布。因此，均匀随机采样是一个有效的<strong>近似手段</strong>，能够尽可能还原目标函数的分布假设。</p></li></ul><p>第二个问题，经验回放是如何提高样本效率的？为什么现在需要样本效率了？</p><ul><li>Replay Buffer中的经验允许被反复采样，多次利用。此外，某些关键状态（例如稀疏奖励问题中接近奖励的状态）可能在直接采样的过程中较少出现，经验回放通过保留这些状态，确保训练时样本的多样性。甚至我们可以使用<strong>优先经验回放</strong>（PrioritizedExperience Replay），经验的优先级根据其 TD Error动态更新，确保关键状态的学习次数。</li><li>深度神经网络通常需要大量的数据来稳定训练。如果只依赖实时采样生成的数据，可能无法充分训练神经网络。</li></ul><p>第三个问题，为什么之前表格形式的 Q-Learning 不需要经验回放？</p><ul><li>Deep Q-Learning 的目标是最小化一个标量目标函数 <spanclass="math inline">\(\mathbb{E}_{*,S,A}\)</span>，因此必然涉及到 <spanclass="math inline">\((S,A)\)</span>的分布，也就需要假设均匀分布，进而需要随机采样。</li><li>而基于表格的 Q-Learning 的目标是求解贝尔曼最优公式 —— 对于每个 <spanclass="math inline">\((s,a)\)</span>都要满足的公式，因此<strong>不会涉及到分布问题</strong>。当然，由于Q-Learning 本身是 off-policy 的，因此想用经验回放也完全没问题！</li></ul><h3 id="具体步骤">具体步骤</h3><p>以下是 DQN 的具体步骤：</p><ol type="1"><li><p>初始化主网络参数 <span class="math inline">\(\theta\)</span>和目标网络参数 <span class="math inline">\(\theta_{T} =\theta\)</span>。</p></li><li><p>初始化回放缓冲区 <spanclass="math inline">\(\mathcal{B}=\{\}\)</span>。</p></li><li><p>重复以下步骤（直到训练完成）：</p><ul><li><p>与环境交互（On-Policy / Off-Policy 都行），得到经验元组 <spanclass="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span>并存入缓冲区；</p></li><li><p>从缓冲区 <span class="math inline">\(\mathcal{B}\)</span>随机采样一个小批量经验；</p></li><li><p>计算目标值（通过目标网络参数）： <span class="math display">\[y_T = r_{t+1} + \gamma \max_{a&#39;} \hat{q}(s_{t+1}, a&#39;; \theta_T)\]</span></p></li><li><p>最小化损失函数： <span class="math display">\[J(\theta) = \mathbb{E} \left[ \left( y_T - \hat{q}(s_t, a_t; \theta)\right)^2 \right]\]</span></p></li><li><p>用梯度下降（反向传播）更新主网络参数 <spanclass="math inline">\(\theta\)</span>；</p></li><li><p>每隔固定步数 <span class="math inline">\(C\)</span>将主网络参数复制到目标网络参数 <spanclass="math inline">\(\theta_T\)</span>。</p></li></ul></li><li><p>策略更新：</p><ul><li>如果是 On-Policy，需要在更新完 <spanclass="math inline">\(\theta\)</span> 根据 <spanclass="math inline">\(\hat{q}\)</span> 后更新 <spanclass="math inline">\(\hat{\pi}\)</span>，再用于和环境交互生成经验；</li><li>如果是 Off-Policy，则只需要在训练完成后，用收敛的 <spanclass="math inline">\(\hat{q}\)</span> 计算出最终 <spanclass="math inline">\(\hat{\pi}\)</span> 即可。</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL 学习笔记 #06 时序差分学习算法</title>
    <link href="/RL-Note-6.html"/>
    <url>/RL-Note-6.html</url>
    
    <content type="html"><![CDATA[<p>前面我们介绍了首个无模型的强化学习算法 ——蒙特卡洛学习，这是一种非增量（Non-Incremental）的方法，尽管它通过多次采样逐步逼近价值函数，但更新时需要<strong>完整的采样回合</strong>。本节将介绍一种重要的<strong>增量式</strong>（Incremental）无模型强化学习方法——<strong>时序差分学习</strong>（TemporalDifference Learning），其结合了蒙特卡洛方法和动态规划的优点：</p><ol type="1"><li><strong>类似动态规划</strong>：TD学习不需要完整的回报序列，只需利用<strong>当前状态和下一状态</strong>的信息。</li><li><strong>类似蒙特卡洛方法</strong>：TD学习能够通过与环境交互的数据进行更新，而<strong>不需要已知的环境模型</strong>。</li></ol><p>这里我们首先用上一节介绍的 RM 算法引入，考虑如下的估计问题： <spanclass="math display">\[\theta=\mathbb{E}[R+\gamma v(X)]\]</span> 其中，<span class="math inline">\(R, X\)</span>为随机变量，<span class="math inline">\(\gamma\)</span> 为常数，<spanclass="math inline">\(v(\cdot)\)</span>为一个未知函数。假设我们获取了采样序列 <spanclass="math inline">\(\{x\}\)</span> 和 <spanclass="math inline">\(\{r\}\)</span>，则有： <spanclass="math display">\[\begin{aligned}g(\theta) &amp; =\theta-\mathbb{E}[R+\gamma v(X)] \\\tilde{g}(\theta, \eta) &amp; =\theta-[r+\gamma v(x)] \\&amp; =(\theta-\mathbb{E}[R+\gamma v(X)])+(\mathbb{E}[R+\gammav(X)]-[r+\gamma v(x)]) \\&amp; \doteq g(\theta)+\eta\end{aligned}\]</span> 于是，我们将估计问题转化为方程 <spanclass="math inline">\(g(\theta)=0\)</span> 的根的求解，最后 RM算法可以表示为： <span class="math display">\[\theta_{n+1}=\theta_n-\alpha_n \tilde{g}\left(\theta_n,\eta_n\right)=\theta_n-\alpha_n\left[\theta_n-\left(r_n+\gammav\left(x_n\right)\right)\right]\]</span> 上述式子其实已经蕴含了 TD Learning的经典形式，接下来我们先介绍用于估计状态值的 TD 算法。</p><h2 id="状态价值-td-learning">状态价值 TD Learning</h2><p>给定策略 <spanclass="math inline">\(\pi\)</span>，我们希望能估计状态价值 <spanclass="math inline">\(v_\pi(s)\)</span>，完成策略评估（PE）。增量更新的特点在于<strong>动态调整</strong>其估计值<span class="math inline">\(v_t(s)\)</span>，最终逐步接近真实值 <spanclass="math inline">\(v_\pi(s)\)</span>。</p><p>由于无模型，我们仅依赖策略 <span class="math inline">\(\pi\)</span>产生的经验数据 <span class="math inline">\(\{(s_t, r_{t+1},s_{t+1})\}\)</span>。其更新公式如下： <span class="math display">\[\left\{\begin{aligned}v_{t+1}\left(s_t\right)&amp;=v_t\left(s_t\right)-\alpha_t\left(s_t\right)\left[v_t\left(s_t\right)-\left[r_{t+1}+\gammav_t\left(s_{t+1}\right)\right]\right], \\v_{t+1}(s) &amp;=v_t(s), \quad \forall s \neq s_t\end{aligned}\right.\]</span> 其中：</p><ul><li>在 <span class="math inline">\(t\)</span> 时刻，我们对当前状态 <spanclass="math inline">\(s_t\)</span> 对应的 <spanclass="math inline">\(v_t(s_t)\)</span>进行了修正，而其他状态对应的状态值则保持原样；</li><li>修正项中，<spanclass="math inline">\(\alpha_t\left(s_t\right)\)</span>表示学习率，控制更新步长；</li><li>第一个大括号表示 <strong>TD Error</strong> <spanclass="math inline">\(\delta_t\)</span>，是估计值 <spanclass="math inline">\(v_t(s_t)\)</span> 与目标值的差距；</li><li>第二个大括号表示 <strong>TD Target</strong> <spanclass="math inline">\(\bar{v}_t\)</span>，我们希望 <spanclass="math inline">\(v_t(s_t)\)</span> 能朝着这个目标值去修改。</li></ul><p>为什么能控制修改方向呢？我们先对更新公式简单变形，两边都减去 <spanclass="math inline">\(\bar{v}_t\)</span>： <span class="math display">\[v_{t+1}\left(s_t\right)-\bar{v}_t=v_t\left(s_t\right)-\bar{v}_t-\alpha_t\left(s_t\right)\left[v_t\left(s_t\right)-\bar{v}_t\right]\]</span> 合并得到： <span class="math display">\[v_{t+1}\left(s_t\right)-\bar{v}_t=\left[1-\alpha_t\left(s_t\right)\right]\left[v_t\left(s_t\right)-\bar{v}_t\right]\]</span> 各项取绝对值： <span class="math display">\[\left|v_{t+1}\left(s_t\right)-\bar{v}_t\right|=\left|1-\alpha_t\left(s_t\right)\right|\left|v_t\left(s_t\right)-\bar{v}_t\right|\]</span> 考虑到 <spanclass="math inline">\(\alpha_t\left(s_t\right)\)</span>是一个小的正数，我们有： <span class="math display">\[0&lt;1-\alpha_t\left(s_t\right)&lt;1\]</span></p><p>进而得到：</p><p><span class="math display">\[\left|v_{t+1}\left(s_t\right)-\bar{v}_t\right|\leq\left|v_t\left(s_t\right)-\bar{v}_t\right|\]</span> 显然，在一次更新后估计值 <spanclass="math inline">\(v_t(s_t)\)</span>与目标值的差距变小了！现在另一个问题是，为什么我们需要将 TD Target定义为这种形式呢？考虑： <span class="math display">\[\delta_t=v\left(s_t\right)-\left[r_{t+1}+\gammav\left(s_{t+1}\right)\right]\]</span> 一方面这个形式<strong>来自 RM 算法的递推式</strong>（当前估计-带噪样本），另一方面这个式子也可以认为是<strong>两个相邻时间步的差</strong>——时序差分算法名字的由来。它反映的是<span class="math inline">\(v_t\)</span> 和 <spanclass="math inline">\(v_\pi\)</span> 的误差，当 <spanclass="math inline">\(\delta_t=0\)</span>时，这个式子就收敛到状态值的递归定义。</p><p>因此，为了减小误差、改进估计的准确度，每当一个新的<strong>单步经验数据</strong><span class="math inline">\((s_t, r_{t+1}, s_{t+1})\)</span>产生时，我们都可以用这个公式进行增量更新。</p><h3 id="rm-算法求解贝尔曼公式">RM 算法求解贝尔曼公式</h3><p>TD Learning算法在数学上究竟在干什么呢？实际上其目标是求解无模型时策略 <spanclass="math inline">\(\pi\)</span>的贝尔曼公式。这里我们将从理论上更深入探讨 RM算法如何用于这一目标。首先引入贝尔曼公式：</p><p><span class="math display">\[v_\pi(s)=\mathbb{E}[R+\gamma G \mid S=s], \quad s \in \mathcal{S}\]</span></p><p>其中 <span class="math inline">\(G\)</span>为累计奖励回报，由于：</p><p><span class="math display">\[\mathbb{E}[G \mid S=s]=\sum_a \pi(a \mid s) \sum_{s^{\prime}}p\left(s^{\prime} \mid s, a\right)v_\pi\left(s^{\prime}\right)=\mathbb{E}\left[v_\pi\left(S^{\prime}\right)\mid S=s\right]\]</span></p><p>其中 <span class="math inline">\(S^{\prime}\)</span>是下一个状态，因此我们可以改写定义得到：</p><p><span class="math display">\[v_\pi(s)=\mathbb{E}\left[R+\gamma v_\pi\left(S^{\prime}\right) \midS=s\right], \quad s \in \mathcal{S} .\]</span></p><p>接下来我们继续用 RM 算法来求解上式表示的估计问题，首先定义： <spanclass="math display">\[g(v(s))=v(s)-\mathbb{E}\left[R+\gamma v_\pi\left(S^{\prime}\right) \mids\right]\]</span></p><p>问题转化为求解 <spanclass="math inline">\(g(v(s))=0\)</span>。由于环境的概率模型未知，我们通过采样数据<span class="math inline">\({(s, r, s&#39;)}\)</span>，定义一个噪声版<span class="math inline">\(g\)</span>：</p><p><span class="math display">\[\begin{aligned}\tilde{g}(v(s)) &amp; =v(s)-\left[r+\gammav_\pi\left(s^{\prime}\right)\right] \\&amp; =\underbrace{\left(v(s)-\mathbb{E}\left[R+\gammav_\pi\left(S^{\prime}\right) \mids\right]\right)}_{g(v(s))}+\underbrace{\left(\mathbb{E}\left[R+\gammav_\pi\left(S^{\prime}\right) \mid s\right]-\left[r+\gammav_\pi\left(s^{\prime}\right)\right]\right)}_\eta .\end{aligned}\]</span> 套用 RM 算法，更新公式为： <span class="math display">\[v_{t+1}(s) = v_t(s) - \alpha_t \tilde{g}(v_t(s)) = v_t(s) - \alpha_t\left( v_t(s)-\left[r_k +\gammav_\pi\left(s^{\prime}_k\right)\right]\right),\quad k=1,2,3\]</span> 此公式与我们之前的 TD更新形式非常相似，但请注意，这里是需要采样一系列 <spanclass="math inline">\(\{(s, r_k, s&#39;_k)\}\)</span>进行迭代更新的。换句话说，我们得从 <spanclass="math inline">\(s\)</span> 出发计算多次来估计 <spanclass="math inline">\(v_\pi(s)\)</span>。这就有两点关键的实际问题需要解决：</p><ol type="1"><li><strong>数据采样的形式</strong>：如何用一个经验轨迹更新多个状态的估计值？</li><li><strong>估计目标的替代</strong>：实际中 <spanclass="math inline">\(v_\pi(s&#39;)\)</span> 无法直接获取。</li></ol><p>TD Learning 的解决方案如下：</p><ol type="1"><li><strong>采样形式的改进</strong>：改用轨迹数据，在序列 <spanclass="math inline">\(\{(s_t, r_{t+1}, s_{t+1})\}\)</span>中，仅当访问某状态 <span class="math inline">\(s\)</span>时更新其估计值，而<strong>未访问到的状态保持不变</strong>。这样，一条轨迹可以同时用于多个状态的更新。</li><li><strong>估计目标的替代</strong>：用当前的估计值 <spanclass="math inline">\(v_t(s&#39;)\)</span> 替代真实的 <spanclass="math inline">\(v_\pi(s&#39;)\)</span>，虽然这会引入偏差，但随着更新的累积，估计值会逐步收敛到真实值。</li></ol><p>通过以上调整，RM 算法可以高效求解无模型时的贝尔曼公式。</p><h3 id="收敛性分析">收敛性分析</h3><p>通过 RM 算法框架，可以严格证明 TD算法的收敛性。其核心定理为，若满足以下学习率条件：</p><ol type="1"><li><span class="math inline">\(\sum_{t=1}^\infty \alpha_t(s) =\infty\)</span>，注意这里要对每个状态都成立，即<strong>每个状态都需要被访问无数次</strong>——实际中只需要较多次即可；</li><li><span class="math inline">\(\sum_{t=1}^\infty \alpha_t^2 (s) &lt;\infty\)</span>，即学习率逐渐减小——实际中学习率通常设为一个小的常数（如<spanclass="math inline">\(0.01\)</span>），防止最后经验失去作用，虽然严格来说不满足条件，但实践效果良好。</li></ol><p>则 <span class="math inline">\(v_t(s)\)</span> 收敛到真实值 <spanclass="math inline">\(v_\pi(s)\)</span>，得到最终的策略评估（PE）。</p><h3 id="td-learning-与-mc-learning-的对比">TD Learning 与 MC Learning的对比</h3><p>我们已经了解了时序差分学习和蒙特卡洛学习两种无模型算法的基本性质，以下是两者的详细对比：</p><table><thead><tr class="header"><th style="text-align: center;">特性</th><th style="text-align: center;">TD/Sarsa Learning</th><th style="text-align: center;">MC Learning</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><strong>更新时机</strong></td><td style="text-align: center;">在线（Online）逐状态更新</td><td style="text-align: center;">离线（Offline）Episode 结束才更新</td></tr><tr class="even"><td style="text-align: center;"><strong>任务类型</strong></td><td style="text-align: center;">Continuing 无终止任务、Episode 任务</td><td style="text-align: center;">仅 Episode 任务</td></tr><tr class="odd"><td style="text-align: center;"><strong>更新方式</strong></td><td style="text-align: center;">Bootstrapping 从猜测出发修正</td><td style="text-align: center;">直接用采样结果估计</td></tr><tr class="even"><td style="text-align: center;"><strong>估计方差</strong></td><td style="text-align: center;">较小，因为单步采样变量少</td><td style="text-align: center;">较大，整个 Episode 不确定因素多</td></tr><tr class="odd"><td style="text-align: center;"><strong>估计偏差</strong></td><td style="text-align: center;">有偏，来自于初始猜测，但会收敛</td><td style="text-align: center;">无偏，直接求期望</td></tr></tbody></table><h2 id="动作价值-td-learning">动作价值 TD Learning</h2><p>基础的 TD Learning 仍存在一些局限性：</p><ul><li>无法直接估计动作价值函数 <spanclass="math inline">\(q(s,a)\)</span>，因为 <spanclass="math inline">\(r\)</span> 仍具有不确定性；</li><li>进而无法直接找到最优策略 <spanclass="math inline">\(\pi^*\)</span>，通常我们会贪心选取动作价值最高者。</li></ul><p>如果能将 TD Learning从状态值函数扩展到动作价值函数，就能在策略评估（PE）后进行<strong>策略改进</strong>（PI），从而找到最优策略。本小节将介绍的<strong>SARSA</strong>是一种估计动作价值的时序差分算法，其名称来源于更新公式中的五元组 <spanclass="math inline">\((S_t, A_t, R_{t+1}, S_{t+1},A_{t+1})\)</span>。</p><h3 id="sarsa-算法">SARSA 算法</h3><p>由于无模型，我们仅依赖策略 <span class="math inline">\(\pi\)</span>产生的经验数据 <span class="math inline">\(\{(s_t, a_t, r_{t+1},s_{t+1}, a_{t+1})\}\)</span>。其更新公式如下： <spanclass="math display">\[\left\{\begin{aligned}q_{t+1}\left(s_t, a_t\right) &amp;=q_t\left(s_t,a_t\right)-\alpha_t\left(s_t, a_t\right)\left[q_t\left(s_t,a_t\right)-\left[r_{t+1}+\gamma q_t\left(s_{t+1},a_{t+1}\right)\right]\right], \\q_{t+1}(s, a) &amp;=q_t(s, a), \quad \forall(s, a) \neq\left(s_t,a_t\right),\end{aligned}\right.\]</span> 其中：</p><ul><li>在 <span class="math inline">\(t\)</span> 时刻，我们对当前状态 <spanclass="math inline">\(s_t\)</span> 和动作 <spanclass="math inline">\(a_t\)</span> 对应的 <spanclass="math inline">\(q_t(s_t, a_t)\)</span>进行了修正，而其他状态对应的状态值则保持原样；</li><li>算法的整体形式和前面介绍的状态值 TD Learning 是完全一样的，包含 TDError 和 TD Target。</li></ul><p>同理，SARSA 在数学上也是求解了一个贝尔曼公式： <spanclass="math display">\[q_\pi(s,a)=\mathbb{E}\left[R+\gamma q_\pi\left(S^{\prime},A^{\prime}\right) \mid S=s, A=a\right], \quad \forall s, a .\]</span>其收敛性条件和上一小节所述的也完全一样，这里略去证明过程。我们直接来讨论SARSA的重要优化：如何与策略改进结合起来？其<strong>算法步骤</strong>可以概括如下：</p><ol type="1"><li>初始化：初始化动作价值函数 <span class="math inline">\(q(s,a)\)</span>（通常为零或随机值），初始化初始状态 <spanclass="math inline">\(s\)</span> 和动作 <spanclass="math inline">\(a\)</span>；</li><li><strong>执行策略</strong> <spanclass="math inline">\(\pi\)</span>：在环境中执行动作 <spanclass="math inline">\(a\)</span>，观察到即时奖励 <spanclass="math inline">\(r\)</span> 和下一状态 <spanclass="math inline">\(s^{\prime}\)</span>；</li><li><strong>策略评估</strong>：根据当前策略选择下一动作 <spanclass="math inline">\(a^{\prime}\)</span>，再根据经验数据更新动作价值函数；</li><li><strong>策略改进</strong>：更新策略，通常选择 <spanclass="math inline">\(\varepsilon\)</span>-贪心策略来平衡探索和利用；</li><li>重复：继续采样，直到到达终止状态或者策略收敛至 <spanclass="math inline">\(\pi^*\)</span>。</li></ol><h3 id="expected-sarsa-算法">Expected SARSA 算法</h3><p>SARSA 的更新公式基于采样的 <spanclass="math inline">\(q_t\left(s_{t+1}, a_{t+1}\right)\)</span>，而Expected SARSA 则<strong>改用期望更新</strong>： <spanclass="math display">\[\left\{\begin{aligned}q_{t+1}(s_t, a_t) &amp;= q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - \left( r_{t+1} + \gamma \mathbb{E}\left[q_t\left(s_{t+1}, A\right) \right] \right) \right], \\q_{t+1}(s, a) &amp;= q_t(s, a), \quad \forall (s, a) \neq (s_t, a_t),\end{aligned}\right.\]</span></p><p>其中，<span class="math inline">\(\mathbb{E}\left[q_t(s_{t+1}, A)\right]\)</span> 是下一状态 <span class="math inline">\(s_{t+1}\)</span>下，根据策略 <span class="math inline">\(\pi\)</span>所有可能动作的加权期望值，展开即为： <span class="math display">\[\mathbb{E}\left[q_t(s_{t+1}, A) \right]=\sum_a \pi_t(a\mid s_{t+1})q_t(s_{t+1}, a) \doteq v_t(s_{t+1})\]</span></p><p>Expected SARSA 的关键是，其 <strong>TD Target</strong>由随机采样的一个动作价值转变为了 <spanclass="math inline">\(s_{t+1}\)</span> 的状态值。相比SARSA，这里进行期望估计的计算量更大，但是也减少了对单一随机动作的依赖，从而<strong>降低了方差</strong>，适用于某些需要<strong>更稳定更新</strong>的任务。</p><h3 id="n-step-sarsa-算法">N-Step SARSA 算法</h3><p>N-Step SARSA 是 SARSA的延伸，通过引入<strong>多个步骤的累计回报</strong>来改进更新，从而平衡偏差和方差。可以认为这是SARSA 和蒙特卡洛方法的平衡，其更新公式如下： <spanclass="math display">\[\left\{\begin{aligned}q_{t+1}(s_t, a_t) &amp;= q_t(s_t, a_t) - \alpha_t(s_t, a_t) \left[q_t(s_t, a_t) - G_t^{(N)} \right], \\q_{t+1}(s, a) &amp;= q_t(s, a), \quad \forall (s, a) \neq (s_t, a_t),\end{aligned}\right.\]</span></p><p>其中，TD Target <span class="math inline">\(G_t^{(N)}\)</span> 为<span class="math inline">\(N\)</span> 步的累计回报，定义为：</p><p><span class="math display">\[G_t^{(N)} = \sum_{k=0}^{N-1} \gamma^k r_{t+k+1} + \gamma^N q(s_{t+N},a_{t+N}).\]</span></p><p>在 <span class="math inline">\(N\)</span> 步过程中：</p><ul><li>前 <span class="math inline">\(N\)</span> 步的奖励 <spanclass="math inline">\(r_{t+k+1}\)</span> 被折扣为 <spanclass="math inline">\(\gamma^k r_{t+k+1}\)</span>；</li><li>最后一步的估计值 <span class="math inline">\(q(s_{t+N},a_{t+N})\)</span> 则被用于最终的回报估计。</li></ul><p>N-Step SARSA 的优势在于，它能够利用多步经验数据 <spanclass="math inline">\(\{(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}, \cdots,r_{t+N}, s_{t+N},a_{t+N})\}\)</span>，从而提高估计的准确性。在实际中，<spanclass="math inline">\(N\)</span> 的选择会影响算法的性能：</p><ul><li>当 <span class="math inline">\(N = 1\)</span> 时，N-Step SARSA退化为<strong>标准的单步 SARSA</strong>；</li><li>当 <span class="math inline">\(N \to \infty\)</span> 时，N-StepSARSA 逐渐<strong>接近蒙特卡洛方法</strong>。</li></ul><p>通过调整 <spanclass="math inline">\(N\)</span>，可以在偏差和方差之间找到合适的平衡。</p><h2 id="最优动作价值-td-learning">最优动作价值 TD Learning</h2><p>SARSA通过估计动作价值解决了策略改进（PI）的问题，但还有一种思路，就是直接学习最优动作价值函数<span class="math inline">\(q^*(s, a)\)</span> 来推导出最优策略 <spanclass="math inline">\(\pi^*\)</span>。</p><p>这需要对动作价值函数进行<strong>无策略依赖</strong>的迭代更新。<strong>Q-Learning</strong>就是解决这一问题的经典算法。</p><h3 id="q-learning-算法">Q-Learning 算法</h3><p>Q-Learning 不依赖于当前的行为策略 <spanclass="math inline">\(\pi\)</span>，而是通过<strong>选择最大动作价值来进行更新</strong>，从而逐步逼近最优策略。</p><p>其更新公式如下：</p><p><span class="math display">\[\left\{\begin{aligned}q_{t+1}(s_t, a_t) &amp;= q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[q_t(s_t, a_t) - \left( r_{t+1} + \gamma \max_{a&#39; \in A} q_t(s_{t+1},a&#39;) \right) \right], \\q_{t+1}(s, a) &amp;= q_t(s, a), \quad \forall (s, a) \neq (s_t, a_t).\end{aligned}\right.\]</span></p><p>其中，<span class="math inline">\(\max_{a&#39;\in A} q_t(s_{t+1},a&#39;)\)</span> 表示在下一状态 <spanclass="math inline">\(s_{t+1}\)</span> 下，所有可能动作 <spanclass="math inline">\(a&#39;\)</span> 中的最大动作价值。这个选择是Q-Learning 与 SARSA 的关键区别之一，<strong>Q-Learning总是选择最大值，而不依赖于当前策略</strong>。</p><p>在数学上，Q-Learning求解了如下贝尔曼<strong>最优</strong>公式（不同于之前的所有方法）：<span class="math display">\[q(s, a)=\mathbb{E}\left[R_{t+1}+\gamma \max _a q\left(S_{t+1}, a\right)\mid S_t=s, A_t=a\right], \quad \forall s, a .\]</span></p><p>Q-Learning算法的核心是通过对每一个状态-动作对的更新，逐渐逼近最优动作价值函数<span class="math inline">\(q^*(s, a)\)</span>。最终通过贪心策略 <spanclass="math inline">\(\pi^*(s) = \arg \max_a q^*(s,a)\)</span>，得到最优策略。</p><p>至此大家可能已经迫不及待想要查看 Q-Learning的具体步骤了，但此时我们还需要引入一个重要概念。</p><h3 id="off-policy-vs.-on-policy">Off-Policy vs. On-Policy</h3><p>我们在之前的学习中已经提到过两种策略：</p><ul><li><strong>行为策略</strong>（BehaviorPolicy）：用于与环境交互生成经验数据。</li><li><strong>目标策略</strong>（TargetPolicy）：我们希望最终达到的策略，也就是我们要不断更新直到收敛到最优策略。</li></ul><p>根据策略之间的关系，可以将强化学习算法分为 <strong>On-Policy</strong>和 <strong>Off-Policy</strong> 两大类。</p><ul><li><strong>On-Policy</strong>：在学习过程中，<strong>使用的行为策略与目标策略是相同的</strong>。也就是说，行为策略直接与目标策略同步，所有的经验数据都是根据当前的目标策略生成的。</li><li><strong>Off-Policy</strong>：在学习过程中，<strong>使用的行为策略和目标策略不需要相同</strong>。也就是说，算法可以通过一个<strong>探索性较强的行为策略</strong>和环境交互得到大量的经验，再独立地去改进另一个目标策略。并且积累的经验数据也可以不断<strong>回放</strong>（Replay）。</li></ul><p>在本课程学习过的内容中，SARSA 和 MC Learning 属于经典的 On-Policy算法，而 Q-Learning 则属于 Off-Policy 算法：</p><ul><li><p>SARSA的更新公式中，动作价值的更新是直接基于<strong>当前的行为策略</strong><span class="math inline">\(\pi\)</span> 进行的： <spanclass="math display">\[q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t,a_t) - \left( r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1}) \right) \right],\]</span> 其使用的经验数据 <span class="math inline">\(\{(s_t, a_t,r_{t+1}, s_{t+1}, a_{t+1})\}\)</span> 中，<spanclass="math inline">\(s_t\)</span> 和 <spanclass="math inline">\(a_t\)</span> 属于条件，<spanclass="math inline">\(r_{t+1}\)</span> 和 <spanclass="math inline">\(s_{t+1}\)</span> 由环境决定也与策略无关，但是<span class="math inline">\(a_{t+1}\)</span> 则完全依赖策略 <spanclass="math inline">\(\pi_t(s_{t+1})\)</span>。</p></li><li><p>MC Learning的更新公式中，轨迹的采样也是直接基于<strong>当前的行为策略</strong><span class="math inline">\(\pi\)</span> 进行的： <spanclass="math display">\[q_\pi(s,a) = \mathbb{E}_\pi\left[G_t \mid S_t = s, A_t = a\right]\approx \frac{1}{N} \sum_{i=1}^N g^{(i)}(s, a)\]</span></p></li><li><p>Q-Learning 的更新公式中，<span class="math inline">\(\maxq_t(s_{t+1}, a&#39;)\)</span>代表的是对<strong>目标策略</strong>的估计，与当前的采样所用的行为策略完全无关：<span class="math display">\[q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t,a_t) - \left( r_{t+1} + \gamma \max_{a&#39; \in A} q_t(s_{t+1}, a&#39;)\right) \right],\]</span> 其使用的经验数据 <span class="math inline">\(\{(s_t, a_t,r_{t+1}, s_{t+1}\}\)</span>中，都与策略无关。此时行为策略可以是任意的（例如 <spanclass="math inline">\(\varepsilon\)</span>-greedy策略），甚至越灵活越好。</p></li></ul><h3 id="q-learning-算法步骤">Q-Learning 算法步骤</h3><p>既然 Q-Learning的行为策略可以是任意的，那么我们也可以强行让行为策略和目标策略一致，就会得到一个<strong>On-Policy</strong> 版本的步骤（与 SARSA 几乎一致）：</p><ol type="1"><li>初始化：初始化动作价值函数 <span class="math inline">\(q(s,a)\)</span>，初始化初始状态 <span class="math inline">\(s\)</span>和动作 <span class="math inline">\(a\)</span>；</li><li>执行策略 <span class="math inline">\(\pi\)</span>：在环境中执行动作<span class="math inline">\(a\)</span>，观察到即时奖励 <spanclass="math inline">\(r\)</span> 和下一状态 <spanclass="math inline">\(s^{\prime}\)</span>；</li><li>策略评估：<strong>无需选择下一动作</strong>，直接用 <spanclass="math inline">\(\max q(s^{\prime}, a)\)</span>更新动作价值函数；</li><li>策略改进：更新策略，通常选择 <spanclass="math inline">\(\varepsilon\)</span>-贪心策略来平衡探索和利用；</li><li>重复：继续采样，<strong>直到到动作价值收敛</strong>后，求出最优策略<span class="math inline">\(\pi^*\)</span>。</li></ol><hr /><p>反之，如果行为策略独立，则 <strong>Off-Policy</strong>版本的步骤如下：</p><ol type="1"><li>初始化：初始化动作价值函数 <span class="math inline">\(q(s,a)\)</span>，初始化初始状态 <span class="math inline">\(s\)</span>和动作 <span class="math inline">\(a\)</span>；</li><li>执行策略 <spanclass="math inline">\(\pi_b\)</span>：根据行为策略随机采样一组经验数据<spanclass="math inline">\(\{s_0,a_0,r_1,s_1,a_1,r_2,\cdots\}\)</span>；</li><li>策略评估：对于经验数据中的<strong>每个时刻</strong>，直接用 <spanclass="math inline">\(\max q(s^{\prime}, a)\)</span>更新动作价值函数；</li><li>策略改进 <spanclass="math inline">\(\pi_t\)</span>：更新策略，<strong>直接用贪心策略更新</strong>，因为此时目标策略不用于采样，无需探索性；</li><li>重复：继续采样，直到到动作价值收敛后，求出最优策略 <spanclass="math inline">\(\pi^*\)</span>。</li></ol><h2 id="td-learning-的统一视角">TD Learning 的统一视角</h2><p>本节中介绍的算法都可以用一个公式概括： <span class="math display">\[q_{t+1}\left(s_t, a_t\right)=q_t\left(s_t, a_t\right)-\alpha_t\left(s_t,a_t\right)\left[q_t\left(s_t, a_t\right)-\bar{q}_t\right],\]</span> 其中 <span class="math inline">\(\bar{q}_t\)</span> 表示 TDTarget，根据取值的不同有如下变种：</p><table><thead><tr class="header"><th style="text-align: center;">算法</th><th style="text-align: center;"><spanclass="math inline">\(\bar{q}_t\)</span> 形式</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">SARSA</td><td style="text-align: center;"><spanclass="math inline">\(\bar{q}_t=r_{t+1}+\gamma q_t\left(s_{t+1},a_{t+1}\right)\)</span></td></tr><tr class="even"><td style="text-align: center;">N-Step SARSA</td><td style="text-align: center;"><spanclass="math inline">\(\bar{q}_t=r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^nq_t\left(s_{t+n}, a_{t+n}\right)\)</span></td></tr><tr class="odd"><td style="text-align: center;">Expected SARSA</td><td style="text-align: center;"><spanclass="math inline">\(\bar{q}_t=r_{t+1}+\gamma \sum_a \pi_t\left(a \mids_{t+1}\right) q_t\left(s_{t+1}, a\right)\)</span></td></tr><tr class="even"><td style="text-align: center;">Q-Learning</td><td style="text-align: center;"><spanclass="math inline">\(\bar{q}_t=r_{t+1}+\gamma \max _a q_t\left(s_{t+1},a\right)\)</span></td></tr><tr class="odd"><td style="text-align: center;">MC Learning</td><td style="text-align: center;"><spanclass="math inline">\(\bar{q}_t=r_{t+1}+\gammar_{t+2}+\cdots\)</span></td></tr></tbody></table><p>同理，他们所解决的数学问题也可以统一表达：</p><table><thead><tr class="header"><th style="text-align: center;">算法</th><th style="text-align: center;">数学问题</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">SARSA</td><td style="text-align: center;"><span class="math inline">\(\mathrm{BE}:q_\pi(s, a)=\mathbb{E}\left[R_{t+1}+\gamma q_\pi\left(S_{t+1},A_{t+1}\right) \mid S_t=s, A_t=a\right]\)</span></td></tr><tr class="even"><td style="text-align: center;">N-Step SARSA</td><td style="text-align: center;"><span class="math inline">\(\mathrm{BE}:q_\pi(s, a)=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^nq_\pi\left(s_{t+n}, a_{t+n}\right) \mid S_t=s,A_t=a\right]\)</span></td></tr><tr class="odd"><td style="text-align: center;">Expected SARSA</td><td style="text-align: center;"><span class="math inline">\(\mathrm{BE}:q_\pi(s, a)=\mathbb{E}\left[R_{t+1}+\gamma\mathbb{E}_{A_{t+1}}\left[q_\pi\left(S_{t+1}, A_{t+1}\right)\right] \midS_t=s, A_t=a\right]\)</span></td></tr><tr class="even"><td style="text-align: center;">Q-Learning</td><td style="text-align: center;"><spanclass="math inline">\(\textcolor{red}{\mathrm{BOE}}: q(s,a)=\mathbb{E}\left[R_{t+1}+\max _a q\left(S_{t+1}, a\right) \mid S_t=s,A_t=a\right]\)</span></td></tr><tr class="odd"><td style="text-align: center;">MC Learning</td><td style="text-align: center;"><span class="math inline">\(\mathrm{BE}:q_\pi(s, a)=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\ldots \mid S_t=s,A_t=a\right]\)</span></td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL 学习笔记 #05 随机近似与随机梯度下降</title>
    <link href="/RL-Note-5.html"/>
    <url>/RL-Note-5.html</url>
    
    <content type="html"><![CDATA[<p>在上一节中，我们介绍了首个无模型的强化学习算法 ——蒙特卡洛学习（MCLearning）。在进一步讨论其他无模型算法之前，需要了解一个重要的理论背景：<strong>随机近似理论（StochasticApproximation）</strong>。</p><h2 id="随机近似-stochastic-approximation">随机近似 | StochasticApproximation</h2><p>在强化学习中，许多算法需要估计某个随机过程的期望值。例如，给定一个随机变量<span class="math inline">\(X_1, X_2, \dots,X_n\)</span>，我们希望估计其均值 <span class="math inline">\(\mu =\mathbb{E}[X]\)</span>。一个简单的估计方法是<strong>均值估计</strong>：<span class="math display">\[\hat{\mu}_n = \frac{1}{n} \sum_{i=1}^n X_i\]</span>但是在实践中，等待足够多的数据点进行计算往往非常耗时。为了高效地更新估计值，我们引入<strong>增量式更新</strong>（IncrementalUpdate）的思想，即通过不断使用新数据来逐步逼近 <spanclass="math inline">\(\mu\)</span>。这样的更新方式不需要等待所有数据点收集完毕，适用于实时更新。</p><p>具体而言，假设： <span class="math display">\[\theta_{n+1}=\frac{1}{n} \sum_{i=1}^n x_i, \quad n=1,2, \ldots\]</span></p><p>进而有：</p><p><span class="math display">\[\theta_n=\frac{1}{n-1} \sum_{i=1}^{n-1} x_i, \quad n=2,3, \ldots\]</span></p><p>于是，<span class="math inline">\(\theta_{n+1}\)</span> 可以用 <spanclass="math inline">\(\theta_n\)</span> 表示出来：</p><p><span class="math display">\[\begin{aligned}\theta_{n+1}=\frac{1}{n} \sum_{i=1}^n x_i &amp;=\frac{1}{n}\left(\sum_{i=1}^{n-1} x_i+x_n\right) \\&amp; =\frac{1}{n}\left((n-1)\theta_n+x_n\right)=\theta_n-\frac{1}{n}\left(\theta_n-x_n\right)\end{aligned}\]</span> 这个递归公式表明，<spanclass="math inline">\(\theta_{n+1}\)</span> 只依赖于当前的估计值 <spanclass="math inline">\(\theta_n\)</span> 和新样本 <spanclass="math inline">\(x_n\)</span>，每一步的更新只需少量的计算资源，从而可以在实时环境中进行增量更新。</p><p><strong>随机近似理论</strong>扩展了这一思想，通过一般化的递归更新公式处理更复杂的估计问题：<span class="math display">\[\theta_{n+1} = \theta_n - \alpha_n h(\theta_n, X_{n+1})\]</span> 其中：</p><ul><li><span class="math inline">\(\theta_n\)</span> 是当前估计值，<spanclass="math inline">\(\theta_{n+1}\)</span> 是下一步的估计；</li><li><span class="math inline">\(\alpha_n\)</span>是步长（学习率），控制每次更新的幅度；</li><li><span class="math inline">\(h(\theta_n, X_{n+1})\)</span>是一个与当前估计值 <span class="math inline">\(\theta_n\)</span>和新样本 <span class="math inline">\(X_{n+1}\)</span>相关的函数，通常代表误差。</li></ul><p>这一理论为强化学习中许多算法（如值函数估计）提供了理论支撑。</p><h2 id="robbins-monro-算法">Robbins-Monro 算法</h2><p>在随机近似理论中，最经典的方法之一就是 <strong>Robbins-Monro算法</strong>，它常用于估计期望或者求解方程的根，且通常用于具有噪声的场景。</p><h3 id="案例-1解方程">案例 1：解方程</h3><p>例如，在神经网络中，我们的目标是最小化损失函数 <spanclass="math inline">\(J(\theta)\)</span>，此时优化问题可以转化为求 <spanclass="math inline">\(\nabla_\theta J(\theta)=0\)</span>的解。但由于模型的复杂性，我们无法直接求出其梯度的表达式，这个时候通常会用RM 算法来逼近目标。</p><p>考虑求解目标方程 <span class="math inline">\(g(\theta)=0\)</span>的根，此时具体的更新公式是： <span class="math display">\[\theta_{n+1} = \theta_n - a_n \tilde{g}(\theta_n, \eta_n)\]</span> 其中：</p><ul><li><span class="math inline">\(\theta_n\)</span> 是第 <spanclass="math inline">\(n\)</span> 次的估计值，算法的目标是通过对观测值 $(_n, _n)$ 的递归更新，逐步逼近方程 $g() = 0 $ 的根；</li><li><span class="math inline">\(\tilde{g}(\theta_n, \eta_n) =g(\theta_n) + \eta_n\)</span> 是<strong>带噪声的观测值</strong>，<spanclass="math inline">\(\eta_n\)</span> 是噪声项，表示某个函数 <spanclass="math inline">\(g(\theta_n)\)</span>的值由于噪声的存在，最终呈现为一个带有误差的随机变量；</li><li><span class="math inline">\(a_n\)</span>是步长参数，控制每次更新的幅度，需要满足一定条件来确保收敛性（后述）。</li></ul><h3 id="案例-2求期望">案例 2：求期望</h3><p>又例如，在求解期望 <span class="math inline">\(\mu =\mathbb{E}[X]\)</span> 时，RM 算法通过以下更新规则逐步逼近： <spanclass="math display">\[\hat{\mu}_{n+1} = \hat{\mu}_n-a_n (\hat{\mu}_n - X_{n+1})\]</span> 其中：</p><ul><li><span class="math inline">\(\hat{\mu}_n\)</span>是当前的期望估计值，算法的目标是通过不断更新 <spanclass="math inline">\(\hat{\mu}_n\)</span> 来逼近真实的期望值 <spanclass="math inline">\(\mu = \mathbb{E}[X]\)</span>；</li><li><span class="math inline">\(X_{n+1}\)</span>是观测到的随机变量样本值，带有<strong>观测噪声</strong> <spanclass="math inline">\(\eta_n\)</span>，这里也可以理解为 <spanclass="math inline">\(\tilde{g}(\mu_n, \eta_n)\)</span>；</li><li><span class="math inline">\(a_n\)</span> 是步长参数，需要满足 <spanclass="math inline">\(\alpha_n &gt; 0\)</span> 且 <spanclass="math inline">\(\sum_n \alpha_n = \infty\)</span> 和 <spanclass="math inline">\(\sum_n \alpha_n^2 &lt;\infty\)</span>，以确保<strong>收敛性</strong>。</li></ul><p>在强化学习中，RM算法可以应用于状态值函数或动作值函数的估计。例如，使用蒙特卡洛方法估计状态值函数时，可以通过以下更新规则来逼近目标值：<span class="math display">\[v(s) := v(s) - \alpha \left[ v(s) - G_t \right]\]</span> 其中，<span class="math inline">\(G_t\)</span> 是从状态 <spanclass="math inline">\(s\)</span>出发到终止状态的累计回报，也是和下一节要介绍的时序差分方法的主要区别。</p><h3 id="收敛性分析">收敛性分析</h3><p>收敛性分析是 RM算法的核心问题之一。我们希望确保在有限的步骤内，更新的估计值能够收敛到目标值。为了保证收敛性，通常需要满足以下几个条件：</p><ol type="1"><li><strong>步长条件</strong>：步长 <spanclass="math inline">\(\alpha_n\)</span> 必须满足两个条件：<ul><li><span class="math inline">\(\sum_n \alpha_n^2 &lt;\infty\)</span>，意味着 <span class="math inline">\(a_n\)</span>最终会<strong>收敛</strong>到 <spanclass="math inline">\(0\)</span>，防止步长过大导致震荡和不收敛。</li><li><span class="math inline">\(\sum_n \alpha_n =\infty\)</span>，意味着 <span class="math inline">\(a_n\)</span>最终收敛到 <span class="math inline">\(0\)</span>的速度<strong>不能太快</strong>，确保随着迭代次数增加，算法能够不断更新。</li><li>常见的一个设置是：<spanclass="math inline">\(a_n=\frac{1}{n}\)</span>，通过数论可以证明满足上述条件。实际使用中我们会在一定步骤后固定<span class="math inline">\(a_n\)</span>为一个小数，防止后续新增数据点失去作用。</li></ul></li><li><strong>梯度存在性和有界性</strong>：对所有的 <spanclass="math inline">\(\theta\)</span>，有 <span class="math inline">\(0&lt; c_1 \leq \nabla_\theta g(\theta) \leq c_2\)</span>。<ul><li>要求 <span class="math inline">\(g(\theta)\)</span>曲线的导数是正数，即 <span class="math inline">\(g(\theta)\)</span>递增，从而解 <span class="math inline">\(\theta^*\)</span>存在且唯一。而梯度有界则避免了算法的发散。</li><li>联系前面提及的 <span class="math inline">\(\nabla_\thetaJ(\theta)=0\)</span>，可以发现这里其实是对 <spanclass="math inline">\(J(\theta)\)</span> 的二阶导 Hessian矩阵提出的要求。Hessian 矩阵为正对应了 <spanclass="math inline">\(J(\theta)\)</span>的<strong>凸函数</strong>性质（Convexity）。</li></ul></li><li><strong>无偏可控误差</strong>：误差需要满足 $= 0 $ 且 <spanclass="math inline">\(\mathbb{E}\left[\eta_n^2 \mid \mathcal{H}_n\right]&lt; \infty\)</span>，<span class="math inline">\(\mathcal{H}_n\)</span>表示历史信息。<ul><li>如果误差序列 <span class="math inline">\(\{\eta_n\}\)</span>是独立同分布的（iid），这就自然满足了上述两个条件：由于独立性，<spanclass="math inline">\(\eta_n\)</span> 不依赖于历史信息 <spanclass="math inline">\(\mathcal{H}_n\)</span>，因此 <spanclass="math inline">\(\mathbb{E}[\eta_n \mid \mathcal{H}_n] =\mathbb{E}[\eta_n] =0\)</span>。如果采样的分布具有有限的二阶矩（即有限的方差），那么 <spanclass="math inline">\(\mathbb{E}[\eta_n^2 \mid \mathcal{H}_n] =\mathbb{E}[\eta_n^2] &lt; \infty\)</span> 也成立。</li><li>并且 <span class="math inline">\(\eta_n\)</span>不要求是高斯噪音——虽然正态噪声是最常见的一种情况，但它不是唯一可能的分布。</li></ul></li></ol><p>根据这些条件，可以证明 Robbins-Monro算法在适当的步长设置下会收敛于一个不动点，即目标值。</p><h2 id="随机梯度下降-sgd">随机梯度下降 | SGD</h2><p>在强化学习和深度学习中，<strong>随机梯度下降（Stochastic GradientDescent，SGD）</strong>是一种广泛使用的优化方法，它基于随机近似理论（实际上是 RM算法的一个特殊情况），可以有效地逼近函数的最优解。</p><p>SGD 要解决的典型优化问题是： <span class="math display">\[\min_\theta J(\theta)=\mathbb{E}_X[f(\theta,X)]\]</span></p><p>其中：</p><ul><li><span class="math inline">\(X\)</span>是随机变量，其概率分布已知但不可直接计算；</li><li><span class="math inline">\(f(\theta, X)\)</span> 是给定参数 <spanclass="math inline">\(\theta\)</span> 时的目标函数值。</li></ul><p>由于直接计算 <span class="math inline">\(\mathbb{E}[f(\theta,X)]\)</span>通常不可行，我们需要<strong>借助样本进行近似</strong>。常见的三种方法如下：</p><ol type="1"><li><p>梯度下降（Gradient Descent，GD）</p><ul><li><p>梯度下降是一种经典的优化方法，基于目标函数的梯度更新参数： <spanclass="math display">\[\theta_{n+1} = \theta_n - \alpha_n \nabla_\theta \mathbb{E}[f(\theta_n,X)] = \theta_n - \alpha_n \mathbb{E}[\nabla_\theta f(\theta_n, X)]\]</span></p></li><li><p>其中：<span class="math inline">\(\alpha_n\)</span>是步长（学习率），<span class="math inline">\(\nabla_\theta\mathbb{E}[f(\theta_n, X)]\)</span> 是目标函数 <spanclass="math inline">\(J(\theta)\)</span> 的梯度。</p></li><li><p>缺点：没有模型，精确的期望值难以获得——只能用数据去估计。</p></li></ul></li><li><p>批量梯度下降（Batch Gradient Descent，BGD）</p><ul><li>为解决期望计算的难题，引入有限样本近似。通过采样 <spanclass="math inline">\(n\)</span> 个数据点 <spanclass="math inline">\({x_1, x_2, \dots,x_n}\)</span>，我们将期望用样本均值代替：</li></ul><p><span class="math display">\[\mathbb{E}[\nabla_\theta f(\theta_n, X)] \approx \frac{1}{n}\sum_{i=1}^n \nabla_\theta f(\theta_n, x_i)\]</span></p><ul><li>更新公式变为：</li></ul><p><span class="math display">\[\theta_{n+1} = \theta_n - \alpha_n \frac{1}{n} \sum_{i=1}^n\nabla_\theta f(\theta_n, x_i)\]</span></p><ul><li>缺点：每次迭代需要对 <span class="math inline">\(n\)</span>个数据点计算梯度，计算代价仍然过高，尤其在数据量大时难以适用。</li></ul></li><li><p>随机梯度下降（Stochastic Gradient Descent，SGD）</p><ul><li><p>随机梯度下降则通过<strong>仅用一个样本来近似梯度</strong>： <spanclass="math display">\[\theta_{n+1} = \theta_n - \alpha_n  \nabla_\theta f(\theta_n, x_i)\]</span></p></li><li><p>这里的 <span class="math inline">\(x_i\)</span>是从分布中随机采样的单个样本，此时计算开销显著降低。</p></li><li><p>可以看出，SGD 实际上是 BGD 在 <span class="math inline">\(n =1\)</span>时的特例，且与之前提到的<strong>增量式更新</strong>方法密切相关。</p></li></ul></li></ol><h3 id="收敛性分析-1">收敛性分析</h3><p>在 SGD 中，我们用一个随机梯度（stochasticgradient）代替了真实梯度（truegradient），这种随机性引入了更新误差。具体地，将随机梯度分解为两个部分：<span class="math display">\[\nabla_\theta f\left(\theta_n, x_n\right)=\mathbb{E}\left[\nabla_\thetaf(\theta, X)\right]+\underbrace{\nabla_\theta f\left(\theta_n,x_n\right)-\mathbb{E}\left[\nabla_\theta f(\theta, X)\right]}_\eta .\]</span> 其中：</p><ul><li><span class="math inline">\(\mathbb{E}[\nabla_\theta f(\theta,X)]\)</span> 是真实梯度；</li><li><span class="math inline">\(\eta_n\)</span>是噪声项，反映了随机梯度的偏差。</li></ul><p>那么是否当 <span class="math inline">\(n \rightarrow \infty\)</span>时有 <span class="math inline">\(\theta_n \rightarrow\theta^*\)</span>？通过分析 SGD 可以发现，它实际上是一个<strong>特殊的Robbins-Monro 算法</strong>。证明的思路如下：</p><ol type="1"><li>目标优化问题可以转化为梯度方程 <span class="math inline">\(g(\theta)= 0\)</span> 的求解问题；</li><li>虽然我们没法计算出 <spanclass="math inline">\(\mathbb{E}[\nabla_\theta f(\theta_n,X)]\)</span>，但可以测量出 <span class="math inline">\(\nabla_\thetaf(\theta_n, x_n)\)</span>，也就是带噪音的 <spanclass="math inline">\(\tilde{g}(\theta_n, \eta_n)\)</span>。</li></ol><p>最终可以得到： <span class="math display">\[\theta_{n+1} = \theta_n - a_n \tilde{g}(\theta_n, \eta_n)= \theta_n -a_n \nabla_\theta f\left(\theta_n, x_n\right)\]</span></p><p>因此 SGD 的更新公式与 RM 算法完全一致，因此 SGD 的收敛性可以通过 RM的分析条件加以修改得到：</p><ul><li><strong>步长参数</strong>：<span class="math inline">\(\sum_n\alpha_n = \infty\)</span> 且 <span class="math inline">\(\sum_n\alpha_n^2 &lt; \infty\)</span>。</li><li><strong>目标函数的严格凸性</strong>：<span class="math inline">\(0&lt; c_1 \leq \nabla^2_\theta f(\theta, X)\leq c_2\)</span>。</li><li><strong>独立同分布</strong>：<spanclass="math inline">\(\{x_k\}^{\infty}_{k=1}\)</span> 是 iid. 的。</li></ul><h3 id="随机性与收敛性">随机性与收敛性</h3><p>我们讨论 SGD的收敛性时，关注的核心问题是：<strong>随机梯度的引入是否会显著影响收敛路径和速度？</strong>换句话说，随机性是否会导致 <span class="math inline">\(\theta_k\)</span>在收敛过程中绕了远路？</p><p>为了评估随机性对收敛路径的影响，我们引入<strong>相对误差（relativeerror）</strong> 的概念，即： <span class="math display">\[\delta_n \doteq \frac{\left|\nabla_\theta f\left(\theta_n,x_n\right)-\mathbb{E}\left[\nabla_\theta f\left(\theta_n,X\right)\right]\right|}{\left|\mathbb{E}\left[\nabla_\thetaf\left(\theta_n, X\right)\right]\right|}\]</span> 由于 <span class="math inline">\(\mathbb{E}\left[\nabla_\thetaf\left(\theta^*, X\right)\right]=0\)</span>，得到： <spanclass="math display">\[\delta_n=\frac{\left|\nabla_\theta f\left(\theta_n,x_n\right)-\mathbb{E}\left[\nabla_\theta f\left(\theta_n,X\right)\right]\right|}{\left|\mathbb{E}\left[\nabla_\thetaf\left(\theta_n, X\right)\right]-\mathbb{E}\left[\nabla_\thetaf\left(\theta^*, X\right)\right]\right|}=\frac{\left|\nabla_\thetaf\left(\theta_n, x_n\right)-\mathbb{E}\left[\nabla_\thetaf\left(\theta_n,X\right)\right]\right|}{\left|\mathbb{E}\left[\nabla_\theta^2f\left(\tilde{\theta}_n,X\right)\left(\theta_n-\theta^*\right)\right]\right|}\]</span> 这里分母的变形引入了<strong>中值定理</strong>。当我们假设<span class="math inline">\(f\)</span> 是严格凸的，则有： <spanclass="math display">\[\nabla_\theta^2 f \ge c &gt;0\]</span> 对分母再次变形可得： <span class="math display">\[\begin{aligned}\left|\mathbb{E}\left[\nabla_\theta^2 f\left(\tilde{\theta}_n,X\right)\left(\theta_n-\theta^*\right)\right]\right| &amp;=\left|\mathbb{E}\left[\nabla_\theta^2 f\left(\tilde{\theta}_n,X\right)\right]\left(\theta_n-\theta^*\right)\right| \\&amp; =\left|\mathbb{E}\left[\nabla_\theta^2 f\left(\tilde{\theta}_n,X\right)\right]\right|\left|\left(\theta_n-\theta^*\right)\right| \geqc\left|\theta_n-\theta^*\right|\end{aligned}\]</span> 代回原式可得： <span class="math display">\[\delta_n \leq \frac{|\overbrace{\nabla_\theta f\left(\theta_n,x_n\right)}^{\text {stochastic gradient}}-\overbrace{\mathbb{E}\left[\nabla_\theta f\left(\theta_n,X\right)\right]}^{\text {true gradient}}|}{\underbrace{c\left|\theta_n-\theta^*\right|}_{\text {distance tothe optimal solution }}}\]</span> 其中：</p><ul><li>分子是随机梯度和真实梯度的绝对误差；</li><li>分母是当前估计和目标解的距离，显然：当 <spanclass="math inline">\(\theta_n\)</span> 与 <spanclass="math inline">\(\theta^*\)</span> <strong>距离比较远的时候，SGD呈现出来的行为跟 GD差不多</strong>，因为相对误差较小；当距离较近的时候，才会出现比较大的随机性。</li></ul><h3 id="bgd-vs.-mbgd-vs.-sgd">BGD vs. MBGD vs. SGD</h3><p>当我们获取了一组随机数据 <spanclass="math inline">\(\left\{x_i\right\}_{i=1}^n\)</span>后，其实有以下三种用法：</p><ul><li><p><strong>BGD（Batch GradientDescent）</strong>：每次迭代都使用整个数据集计算梯度，计算开销大，但收敛稳定。<span class="math display">\[\theta_{n+1}=\theta_n-\alpha_n \frac{1}{n} \sum_{i=1}^n \nabla_\thetaf\left(\theta_n, x_i\right)\]</span></p></li><li><p><strong>MBGD（Mini-Batch GradientDescent）</strong>：每次使用数据集的一个小批次计算梯度，权衡了计算效率和收敛稳定性。<span class="math display">\[\theta_{n+1}=\theta_n-\alpha_n \frac{1}{m} \sum_{j \in \mathcal{I}_n}\nabla_\theta f\left(\theta_n, x_j\right)\]</span></p></li><li><p><strong>SGD（Stochastic GradientDescent）</strong>：每次使用一个样本计算梯度，计算开销最小，但收敛较慢，波动较大。<span class="math display">\[\theta_{n+1}=\theta_n-\alpha_n \nabla_\theta f\left(\theta_n, x_n\right)\]</span></p></li></ul><p>在实践中，MBGD 结合了 BGD 和 SGD的优点，成为最常用的梯度下降变种。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL 学习笔记 #04 蒙特卡洛学习算法</title>
    <link href="/RL-Note-4.html"/>
    <url>/RL-Note-4.html</url>
    
    <content type="html"><![CDATA[<p>在上一节中，我们介绍了策略迭代，它依赖于明确的环境模型（model-based）来进行策略评估和改进。然而，在许多现实问题中，环境模型不可用，或者我们无法轻易获得完整的转移概率<span class="math inline">\(T\)</span> 和奖励函数 <spanclass="math inline">\(R\)</span>。这时，<strong>蒙特卡洛学习算法</strong>（MonteCarloLearning）作为一种<strong>无模型</strong>（model-free）方法，提供了一种通过样本进行策略优化的途径。</p><p>蒙特卡洛方法的核心思想是：通过对环境的采样，基于多次模拟的经验回报，<strong>估计</strong>状态值函数或状态-动作值函数（MonteCarlo Estimation）。其依据就是经典的<strong>大数定律</strong>。</p><p>蒙特卡洛学习主要依赖以下几个关键特性：</p><ol type="1"><li><strong>基于样本</strong>：直接使用采样的经验（例如状态序列、奖励序列）进行学习。</li><li><strong>延迟更新</strong>：直到采样结束后，才对策略或值函数进行更新。</li><li><strong>无需环境模型</strong>：完全基于与环境的交互，适用于复杂环境。</li></ol><h2 id="mc-基本算法-mc-basic">MC 基本算法 | MC Basic</h2><p>与策略迭代类似，蒙特卡洛方法需要基于一个固定策略 <spanclass="math inline">\(\pi\)</span>。我们知道策略迭代包含两个主要步骤：</p><ul><li>策略评估（PE）：<spanclass="math inline">\(v_{\pi^{(k)}}=r_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}v_{\pi^{(k)}}\)</span></li><li>策略改进（PI）：<span class="math inline">\(\pi^{(k+1)} = \arg\max_\pi\left(r_\pi+\gamma P_\pi v_{\pi^{(k)}}\right)\)</span></li></ul><p>这里面最核心的地方就是状态值 <spanclass="math inline">\(v_{\pi^{(k)}}\)</span>的求解，此时有两种办法：</p><ol type="1"><li><p>基于模型 <span class="math inline">\(T\)</span> 和 <spanclass="math inline">\(R\)</span>，使用迭代法求解： <spanclass="math display">\[v_{\pi^{(k)}}(s) = \sum_a \pi^{(k)}(a \mid s) \left[ \sum_r p(r \mid s,a) r + \gamma \sum_{s&#39;} p(s&#39; \mid s, a) v_{\pi^{(k)}}(s&#39;)\right]\]</span></p></li><li><p>免模型求解，回归状态值的<strong>原始定义</strong>： <spanclass="math display">\[v_\pi(s) = \mathbb{E}_\pi\left[G_t \mid S_t = s\right]\]</span></p></li></ol><p>换句话说，我们可以从任意状态 <span class="math inline">\(s\)</span>出发，随机采样若干轨迹 <spanclass="math inline">\(\{\tau\}\)</span>，使用这些轨迹的累计回报 <spanclass="math inline">\(\{g_t\}\)</span> 的<strong>平均值</strong>来估计<span class="math inline">\(s\)</span> 的期望累计回报： <spanclass="math display">\[v_\pi(s) = \mathbb{E}_\pi\left[G_t \mid S_t = s\right] \approx\frac{1}{N} \sum_{i=1}^N g^{(i)}(s)\]</span> 同理，也可以估计动作 <span class="math inline">\(a\)</span>的期望累计回报： <span class="math display">\[q_\pi(s,a) = \mathbb{E}_\pi\left[G_t \mid S_t = s, A_t = a\right]\approx \frac{1}{N} \sum_{i=1}^N g^{(i)}(s, a)\]</span>总之就是一句话：<strong>没有模型时，就得有数据</strong>！这里采样到的轨迹在统计学或概率学上会称为样本（Sample），而在强化学习中则称为<strong>经验</strong>（Experience）。</p><h3 id="算法框架">算法框架</h3><p>以下的步骤包含在策略迭代过程中的策略估计（PE）步骤中。这里我们使用蒙特卡洛方法估计动作价值<span class="math inline">\(q_\pi(s,a)\)</span>，因为如果先估计状态值<spanclass="math inline">\(v_\pi(s)\)</span>，在之后的策略改进（PI）步骤中还是需要计算<spanclass="math inline">\(q_\pi(s,a)\)</span>，此时<strong>又得需要模型</strong>！</p><p>具体步骤如下：</p><ol type="1"><li><p><strong>采样</strong>：从待求解状态 <spanclass="math inline">\(s_t\)</span> 和动作 <spanclass="math inline">\(a_t\)</span> 出发，基于策略 <spanclass="math inline">\(\pi\)</span>生成多条完整的轨迹，每条轨迹由状态、动作、奖励序列组成，例如： <spanclass="math display">\[\tau = \{s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}, r_{t+2}, \dots, s_T\}\]</span> 其中 <span class="math inline">\(T\)</span>是终止时刻。</p></li><li><p><strong>回报计算</strong>：对于每个轨迹，计算累计回报（Return）：<span class="math display">\[g_t = \sum_{k=t}^T \gamma^{k-t} r_{k+1}\]</span></p></li><li><p><strong>更新</strong>：通过多次采样得到的回报 <spanclass="math inline">\(\{g_t\}\)</span>，对动作价值函数进行更新： <spanclass="math display">\[q_\pi(s_t,a_t) \leftarrow \frac{\text{累计回报和}}{\text{访问次数}}\]</span></p></li></ol><p>之后在策略改进（PI）步骤中，依旧是贪心选择最大动作价值来更新策略。</p><blockquote><p>有趣的现象——回合（Episode）的长度：</p><ul><li>为了 <span class="math inline">\(G_t\)</span>估计的准确性，我们希望每次采样到的 <spanclass="math inline">\(g_t\)</span> 都尽量精确，因此 Episode也是<strong>越长越好</strong>。</li><li>如果 Episode 长度很短，例如 <spanclass="math inline">\(1\)</span>，即模型只会探索一个步骤，最终只有目标周围一步的状态值会得到准确估计，其他地方均为<span class="math inline">\(0\)</span>，因为无法得到最终奖励。</li><li>推广开来，只有当 Episode长度超过一个状态距离目标状态的最短路径时，该状态值才能得到更新；并且<strong>超过得越多，这个估计状态值就离真实值越接近</strong>。</li></ul></blockquote><h2 id="mc-探索性启动-mc-exploring-starts">MC 探索性启动 | MC ExploringStarts</h2><p>为了估计的准确性，蒙特卡洛算法往往需要采样大量样本（每个状态、每个动作都要采样，如果环境是随机的，则还需要采样<span class="math inline">\(N\)</span> 次求平均），导致 Basic算法的效率过低。</p><p>在实际使用中，我们会用更高效的改进算法。蒙特卡洛方法的一个基础假设是<strong>每个状态和动作都可以被访问（Visit）到</strong>。而MC Basic 实际上是 Initial-visit 方法，即对于一个 Episode 只考虑起点的<spanclass="math inline">\((s_t,a_t)\)</span>。但这样其实是对后续轨迹的浪费！</p><p>一个完整的轨迹上的所有 <span class="math inline">\((s,a)\)</span>的访问实际上都可以被利用，这就是<strong>探索性启动</strong>（ExploringStarts, ES）。当然细化下去还可以分为两种：</p><ul><li>First-visit：对于轨迹上的 <spanclass="math inline">\((s_t,a_t)\)</span>，只考虑首次访问时的 <spanclass="math inline">\(g_t\)</span>。</li><li>Every-visit：对于轨迹上的 <spanclass="math inline">\((s_t,a_t)\)</span>，每次访问时的 <spanclass="math inline">\(g_t\)</span> 都被计算。</li></ul><blockquote><p>Exploring Starts 也有一定的局限性：</p><ul><li>需要对环境有完全的访问权限，才能随机选择任意状态和动作。</li><li>在复杂环境中，不易实现随机探索。</li></ul><p>这也是 Exploring Starts名字的由来——因为有的状态访问不到，所有需要指定起点（Starts）进行探索（Exploring）。</p></blockquote><p>另一个优化的思路是<strong>增量式更新</strong>。在上一节中我们曾介绍过截断策略迭代，这是一种利用不精确估计去减少计算量的方法。同样，在MC 算法中也可以借鉴这种思想。</p><p>即使有了探索性启动，我们也需要采样多个轨迹取平均才能精确估计 <spanclass="math inline">\(G_t\)</span>。但是在这里，我们可以每得到一个 <spanclass="math inline">\(g_t\)</span>就<strong>立即改进策略</strong>（PI）。虽然不能准确得到结果，但是也能将<span class="math inline">\(\pi\)</span>向着优化方向前进一小步，此时下一次采样轨迹就会更加接近最优解，加快收敛速度。</p><p>这一类采用不精确中间结果来加快收敛速度的思想，也被统称为<strong>Generalized Policy Iteration (GPI)</strong>。</p><h2 id="mc-varepsilon-贪婪策略-mc-varepsilon-greedy">MC <spanclass="math inline">\(\varepsilon\)</span>-贪婪策略 | MC <spanclass="math inline">\(\varepsilon\)</span>-Greedy</h2><p>在实际问题中，探索性启动往往也需要消耗额外的资源，因为某些策略 <spanclass="math inline">\(\pi\)</span>可能永远不会访问某些状态或执行某些动作。之前我们提到的贪心策略（确定性策略）正是如此，对于一种状态只考虑一个最优的动作，就导致许多动作无法被访问到。</p><p>为此，我们引入<strong>软策略</strong>（SoftPolicy）的概念：在<strong>每个状态有概率选取不同动作</strong>的随机性策略。由于随机性的存在，只要采样的Episode 足够长，所有的 <span class="math inline">\((s,a)\)</span>都会被访问到。这样我们就能去掉 Exploring Starts 的条件。</p><p>最常见的软策略是 <spanclass="math inline">\(\varepsilon\)</span>-贪婪策略（<spanclass="math inline">\(\varepsilon\)</span>-Greedy Policy）： <spanclass="math display">\[\pi(a \mid s)= \begin{cases}1-\frac{\varepsilon}{|A(s)|}(|A(s)|-1),&amp; \text { for the greedy action, } \\ \frac{\varepsilon}{|A(s)|},&amp; \text { for the other }|A(s)|-1 \text { actions. }\end{cases}\]</span> 其中，参数 $<span class="math inline">\(，\)</span>|A(s)| $表示状态 <span class="math inline">\(s\)</span> 的动作数。</p><p>可以看到，最优动作所赋予的概率 <spanclass="math inline">\(1-\frac{\varepsilon}{|A(s)|}(|A(s)|-1)=1-\varepsilon+ \frac{\varepsilon}{|A(s)|} \ge\frac{\varepsilon}{|A(s)|}\)</span>，也就是说我们还是会将最大的概率留给最优动作，只不过略微增加了其他动作被探索到的概率。</p><p><spanclass="math inline">\(\varepsilon\)</span>-贪婪策略反应的是<strong>探索</strong>（exploration）和<strong>利用</strong>（exploitation）的平衡：<spanclass="math inline">\(\varepsilon\)</span>越小就越侧重利用（更贪心），<spanclass="math inline">\(\varepsilon\)</span>越大就越侧重探索（更均衡）。</p><h3 id="对蒙特卡洛学习的改进">对蒙特卡洛学习的改进</h3><p>之前我们在做策略更新（PU）或者策略优化（PI）的时候，都默认进行贪心选择<span class="math inline">\(a = \arg\max_a q(s, a)\)</span>，即：</p><p><span class="math display">\[\pi^{(k+1)}(a \mid s) =\begin{cases}1, &amp; \;\; a = a^*_k, \\0, &amp; \;\; a \ne a^*_k.\end{cases}\]</span> 现在我们将 <spanclass="math inline">\(\varepsilon\)</span>-贪婪策略嵌入到更新规则中：<span class="math display">\[\pi^{(k+1)}(a \mid s) =\begin{cases}1 - \epsilon + \frac{\varepsilon}{|A(s)|}, &amp; \;\; a = a^*_k, \\\frac{\varepsilon}{|A(s)|}, &amp; \;\; a \ne a^*_k.\end{cases}\]</span> 可以证明，随着 <span class="math inline">\(\varepsilon \to0\)</span>时，算法逐渐<strong>收敛于最优策略</strong>。一个常用的方法是随着策略更新，逐渐减小<span class="math inline">\(\varepsilon\)</span>，直到当前策略与 <spanclass="math inline">\(\varepsilon=0\)</span>的贪心策略一致（Consistent）。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL 学习笔记 #03 值迭代和策略迭代</title>
    <link href="/RL-Note-3.html"/>
    <url>/RL-Note-3.html</url>
    
    <content type="html"><![CDATA[<p>在上一节中，我们介绍了贝尔曼最优公式，这里将介绍求解 BOE 的首个model-based 的算法：<strong>值迭代</strong>（ValueIteration）和<strong>策略迭代</strong>（PolicyIteration）。此外，我们还将讨论它们的变种之一：<strong>截断策略迭代</strong>（TruncatedPolicy Iteration）。</p><h2 id="值迭代-value-iteration">值迭代 | Value Iteration</h2><p>上一节中，我们将贝尔曼最优公式整理为递归形式： <spanclass="math display">\[v_*(s) = \max_a \left[ \sum_r p(r \mid s, a) r + \gamma \sum_{s&#39;}p(s&#39; \mid s, a) v_*(s&#39;) \right]\]</span> <strong>值迭代</strong>（ValueIteration）直接利用递归性，通过不断迭代逼近最优状态值函数 <spanclass="math inline">\(v_*(s)\)</span>。整个算法可以概括为： <spanclass="math display">\[v^{(k+1)}=f\left(v^{(k)}\right)=\max _\pi\left(r_\pi+\gamma P_\piv^{(k)}\right), \quad k=1,2,3 \ldots\]</span> 因此这个算法理论上需要包含两个主要步骤：</p><ul><li>策略更新（PU）：<span class="math inline">\(\pi^{(k+1)} = \arg\max_\pi\left(r_\pi+\gamma P_\pi v^{(k)}\right)\)</span></li><li>状态更新（VU）：<spanclass="math inline">\(v^{(k+1)}=r_{\pi^{(k+1)}}+\gamma P_{\pi^{(k+1)}}v^{(k)}\)</span></li></ul><p>而策略 <span class="math inline">\(\pi\)</span>更新的时候由于使用了贪心选择，这两个步骤可以合并简化，<strong>直接使用最优的动作价值去更新状态价值</strong>：<span class="math display">\[v^{(k+1)}(s) = \max_a q^{(k+1)}(s,a)\]</span></p><blockquote><p>值得一提的是，贪心更新策略 <span class="math inline">\(\pi\)</span>也带来了一个有趣的现象：<strong>越靠近目标区域的状态越先变好</strong>。直观上就是因为<span class="math inline">\(\pi\)</span>依赖于其他状态，而当其他状态都不好的时候无从更新，只有接近目标区域的状态有明确的优化方向。</p></blockquote><h3 id="具体步骤">具体步骤</h3><ol type="1"><li><p><strong>初始化值函数</strong>：设定初始值 <spanclass="math inline">\(v^{(0)}(s)\)</span>，通常初始化为全零或任意常数。</p></li><li><p><strong>迭代更新值函数</strong>： 对于每个状态 <spanclass="math inline">\(s\)</span>，根据贝尔曼最优公式更新 <spanclass="math inline">\(v(s)\)</span>： <span class="math display">\[v^{(k+1)}(s) = \max_a \left[ \sum_r p(r \mid s, a) r + \gamma\sum_{s&#39;} p(s&#39; \mid s, a) v^{(k)}(s&#39;) \right]\]</span></p></li><li><p><strong>判断收敛</strong>： 检查值函数更新是否足够小（例如 <spanclass="math inline">\(\lVert v^{(k+1)} - v^{(k)} \rVert_\infty &lt;\epsilon\)</span>），若收敛则停止迭代并输出 <spanclass="math inline">\(v^{(k+1)}\)</span>。</p></li><li><p><strong>推导策略</strong>： 一旦得到收敛的值函数 <spanclass="math inline">\(v_*(s)\)</span>，通过以下方式获得对应的最优策略<span class="math inline">\(\pi^*\)</span>： <spanclass="math display">\[\pi^*(s) = \arg\max_a \left[ \sum_r p(r \mid s, a) r + \gamma\sum_{s&#39;} p(s&#39; \mid s, a) v_*(s&#39;) \right]\]</span></p></li></ol><h3 id="伪代码实现">伪代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">value_iteration</span>(<span class="hljs-params">states, actions, transition_probs, rewards, gamma, epsilon</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    states: list of all states</span><br><span class="hljs-string">    actions: list of all actions</span><br><span class="hljs-string">    transition_probs: function p(s&#x27;, r | s, a)</span><br><span class="hljs-string">    rewards: function r(s, a, s&#x27;)</span><br><span class="hljs-string">    gamma: discount factor</span><br><span class="hljs-string">    epsilon: convergence threshold</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    v = &#123;s: <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states&#125;  <span class="hljs-comment"># Initialize value function</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        delta = <span class="hljs-number">0</span><br>        new_v = v.copy()<br>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states:<br>            new_v[s] = <span class="hljs-built_in">max</span>(<br>                <span class="hljs-built_in">sum</span>(p * (r + gamma * v[s_next]) <span class="hljs-keyword">for</span> p, r, s_next <span class="hljs-keyword">in</span> transition_probs(s, a)) \<br>                <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> actions[s]<span class="hljs-comment"># Calculate q-table</span><br>            ) <span class="hljs-comment"># Update v = max (q)</span><br>            delta = <span class="hljs-built_in">max</span>(delta, <span class="hljs-built_in">abs</span>(new_v[s] - v[s]))<br>        v = new_v<br>        <span class="hljs-keyword">if</span> delta &lt; epsilon:  <span class="hljs-comment"># Check convergence</span><br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-comment"># Derive the optimal policy</span><br>    policy = &#123;s: <span class="hljs-built_in">max</span>(actions, key=<span class="hljs-keyword">lambda</span> a: <span class="hljs-built_in">sum</span>(p * (r + gamma * v[s_next]) \<br>                                                <span class="hljs-keyword">for</span> p, r, s_next <span class="hljs-keyword">in</span> transition_probs(s, a))) \<br>              <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states&#125;<br>    <span class="hljs-keyword">return</span> v, policy<br></code></pre></td></tr></table></figure><h2 id="策略迭代-policy-iteration">策略迭代 | Policy Iteration</h2><p>策略迭代是另一种求解 BOE的方法，核心思想是交替优化策略和状态值函数，直到收敛到最优策略 <spanclass="math inline">\(\pi^*\)</span>。</p><p>与值迭代不同的是，策略迭代先初始化一个<strong>随机策略</strong>，再交替进行以下两个主要步骤：</p><ul><li>策略评估（PE）：<spanclass="math inline">\(v_{\pi^{(k)}}=r_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}v_{\pi^{(k)}}\)</span></li><li>策略改进（PI）：<span class="math inline">\(\pi^{(k+1)} = \arg\max_\pi\left(r_\pi+\gamma P_\pi v_{\pi^{(k)}}\right)\)</span></li></ul><p>这里有四个问题需要回答：</p><ul><li>在 PE 步骤如何求解状态值？<ul><li>在上一节有介绍，已知策略时可以通过矩阵形式直接求解精确解，也可以利用Bootstrap 迭代法。</li><li>注意：如果采用 Bootstrap迭代法，实际上我们是在整个大的策略迭代中又<strong>嵌套</strong>了一个小的迭代！</li></ul></li><li>在 PI 步骤如何确保新策略好于旧策略？<ul><li>可以证明：对于任何 <span class="math inline">\(k\)</span>，有 <spanclass="math inline">\(v_{\pi^{(k+1)}} \gev_{\pi^{(k)}}\)</span>。证明过程略。</li></ul></li><li>这个算法为什么能到达最优策略？<ul><li>可以证明收敛性：<spanclass="math inline">\(\{v_{\pi^{(k)}}\}^{\infty}_k \tov^*\)</span>。证明过程略。</li></ul></li><li>策略迭代和值迭代有什么关系？<ul><li>策略迭代的收敛性证明是基于值迭代的收敛性。</li><li>这两种算法是截断策略迭代（Truncated PolicyIteration）的两个极端表现。</li></ul></li></ul><h3 id="具体步骤-1">具体步骤</h3><ol type="1"><li><p><strong>初始化策略</strong>： 随机初始化一个初始策略 <spanclass="math inline">\(\pi^{(0)}\)</span>。</p></li><li><p><strong>策略评估</strong>： 在当前策略 <spanclass="math inline">\(\pi^{(k)}\)</span>下，通过求解以下线性方程组来获得状态值函数 <spanclass="math inline">\(v_{\pi^{(k)}}(s)\)</span>： <spanclass="math display">\[v_{\pi^{(k)}}(s) = \sum_a \pi^{(k)}(a \mid s) \left[ \sum_r p(r \mid s,a) r + \gamma \sum_{s&#39;} p(s&#39; \mid s, a) v_{\pi^{(k)}}(s&#39;)\right]\]</span></p><ul><li><p>当状态空间较小时，可以直接解线性方程组；</p></li><li><p>对于较大的问题，可以使用 Bootstrap 迭代逼近的方法。</p></li></ul></li><li><p><strong>策略改进</strong>： 基于更新后的值函数 <spanclass="math inline">\(v_{\pi^{(k)}}\)</span>，贪心更新策略： <spanclass="math display">\[\pi^{(k+1)}(s) = \arg\max_a \left[ \sum_r p(r \mid s, a) r + \gamma\sum_{s&#39;} p(s&#39; \mid s, a) v_{\pi^{(k)}}(s&#39;) \right]\]</span></p></li><li><p><strong>检查收敛</strong>： 如果策略不再变化，即 <spanclass="math inline">\(\pi^{(k+1)} =\pi^{(k)}\)</span>，则算法结束，<spanclass="math inline">\(\pi^{(k)}\)</span> 即为最优策略；否则返回步骤2。</p></li></ol><h3 id="伪代码实现-1">伪代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">policy_iteration</span>(<span class="hljs-params">states, actions, transition_probs, rewards, gamma, epsilon</span>):</span><br>    v = &#123;s: <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states&#125;  <span class="hljs-comment"># Initialize value function</span><br>    policy = &#123;s: actions[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states&#125;  <span class="hljs-comment"># Initialize policy</span><br>    <br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-comment"># Policy evaluation</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>            delta = <span class="hljs-number">0</span><br>            new_v = v.copy()<br>            <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states:<br>                a = policy[s]<br>                new_v[s] = <span class="hljs-built_in">sum</span>(p * (r + gamma * v[s_next]) <span class="hljs-keyword">for</span> p, r, s_next <span class="hljs-keyword">in</span> transition_probs(s, a))<br>                delta = <span class="hljs-built_in">max</span>(delta, <span class="hljs-built_in">abs</span>(new_v[s] - v[s]))<br>            v = new_v<br>            <span class="hljs-keyword">if</span> delta &lt; epsilon:  <span class="hljs-comment"># Check convergence</span><br>                <span class="hljs-keyword">break</span><br>        <br>        <span class="hljs-comment"># Policy improvement</span><br>        stable = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states:<br>            best_action = <span class="hljs-built_in">max</span>(actions, key=<span class="hljs-keyword">lambda</span> a: <span class="hljs-built_in">sum</span>(p * (r + gamma * v[s_next]) \<br>                                                         <span class="hljs-keyword">for</span> p, r, s_next <span class="hljs-keyword">in</span> transition_probs(s, a)))<br>            <span class="hljs-keyword">if</span> best_action != policy[s]:<br>                policy[s] = best_action<br>                stable = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">if</span> stable:  <span class="hljs-comment"># Check if policy is stable</span><br>            <span class="hljs-keyword">break</span><br><br>    <span class="hljs-keyword">return</span> v, policy<br></code></pre></td></tr></table></figure><h2 id="截断策略迭代-truncated-policy-iteration">截断策略迭代 |Truncated Policy Iteration</h2><p>截断策略迭代（Truncated PolicyIteration）是值迭代和策略迭代的一般化推广，旨在权衡收敛速度与计算效率。如果将两个算法对齐：</p><table><thead><tr class="header"><th style="text-align: left;">Steps</th><th>Policy Iteration</th><th>Value Iteration</th><th>Comments</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">(1) Policy</td><td><span class="math inline">\(\pi^{(0)}\)</span></td><td>N/A</td><td></td></tr><tr class="even"><td style="text-align: left;">(2) Value</td><td><span class="math inline">\(v_{\pi^{(0)}}=r_{\pi^{(0)}}+\gammaP_{\pi^{(0)}} v_{\pi^{(0)}}\)</span></td><td><spanclass="math inline">\(\textcolor{red}{v^{(0)}:=v_{\pi^{(0)}}}\)</span></td><td>人为赋予 VI 的相同初始值，便于比较</td></tr><tr class="odd"><td style="text-align: left;">(3) Policy</td><td><span class="math inline">\(\pi^{(1)} = \arg\max_\pi\left(r_\pi+\gamma P_\pi v_{\pi^{(0)}}\right)\)</span></td><td><span class="math inline">\(\pi^{(1)} = \arg\max_\pi\left(r_\pi+\gamma P_\pi v^{(0)}\right)\)</span></td><td>这一步的操作和结果一模一样</td></tr><tr class="even"><td style="text-align: left;">(4) Value</td><td><spanclass="math inline">\(\textcolor{blue}{v_{\pi^{(1)}}=r_{\pi^{(1)}}+\gammaP_{\pi^{(1)}} v_{\pi^{(1)}}}\)</span></td><td><spanclass="math inline">\(\textcolor{blue}{v^{(1)}=r_{\pi^{(1)}}+\gammaP_{\pi^{(1)}} v^{(0)}}\)</span></td><td>由 <span class="math inline">\(v_{\pi^{(1)}}\ge v^{(0)}\)</span> 得<span class="math inline">\(v_{\pi^{(1)}}\ge v^{(1)}\)</span></td></tr><tr class="odd"><td style="text-align: left;">(5) Policy</td><td><span class="math inline">\(\pi^{(2)} = \arg\max_\pi\left(r_\pi+\gamma P_\pi v_{\pi^{(1)}}\right)\)</span></td><td><span class="math inline">\(\pi^{(2)} = \arg\max_\pi\left(r_\pi+\gamma P_\pi v^{(1)}\right)\)</span></td><td>接下来差距越变越大</td></tr><tr class="even"><td style="text-align: left;">...</td><td>...</td><td>...</td><td>...</td></tr></tbody></table><p>可以看到，关键的分歧在于 (4) Value步骤时，策略迭代采用的是一个内嵌迭代去<strong>多次更新</strong>了状态值，而值迭代只采用<strong>一次更新</strong>得到了新状态值。</p><p>那么，在这二者之间，是否存在一个<strong>中间结果</strong>？截断策略迭代应运而生，其主要特点是对策略评估（PE）过程进行截断。减少了内嵌迭代的计算开销，相较于策略迭代更高效。</p><p>关于截断策略迭代的收敛性证明，直观上可以用 VI 和 PI两条曲线的夹逼来解释。具体证明过程略去。</p><h3 id="具体步骤-2">具体步骤</h3><ol type="1"><li><p><strong>初始化策略</strong>： 随机初始化一个策略 <spanclass="math inline">\(\pi^{(0)}\)</span>。</p></li><li><p><strong>截断的策略评估</strong>： 在当前策略 <spanclass="math inline">\(\pi^{(k)}\)</span> 下，迭代更新值函数 <spanclass="math inline">\(v^{(k)}(s)\)</span>，但仅进行<strong>有限次</strong>迭代（如<span class="math inline">\(n\)</span> 次），而非完全收敛。</p></li><li><p><strong>策略改进</strong>： 基于截断后的值函数 <spanclass="math inline">\(v^{(k)}(s)\)</span>，更新策略： <spanclass="math display">\[\pi^{(k+1)}(s) = \arg\max_a \left[ \sum_r p(r \mid s, a) r + \gamma\sum_{s&#39;} p(s&#39; \mid s, a) v^{(k)}(s&#39;) \right]\]</span></p></li><li><p><strong>判断收敛</strong>： 如果策略 <spanclass="math inline">\(\pi^{(k+1)} =\pi^{(k)}\)</span>，则停止；否则返回步骤 2。</p></li></ol><h3 id="伪代码实现-2">伪代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">truncated_policy_iteration</span>(<span class="hljs-params">states, actions, transition_probs, rewards, gamma, epsilon, k</span>):</span><br>    v = &#123;s: <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states&#125;  <span class="hljs-comment"># Initialize value function</span><br>    policy = &#123;s: actions[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states&#125;  <span class="hljs-comment"># Initialize policy</span><br><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-comment"># Truncated policy evaluation (k iterations)</span><br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>            new_v = v.copy()<br>            <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states:<br>                a = policy[s]<br>                new_v[s] = <span class="hljs-built_in">sum</span>(p * (r + gamma * v[s_next]) <span class="hljs-keyword">for</span> p, r, s_next <span class="hljs-keyword">in</span> transition_probs(s, a))<br>            v = new_v<br><br>        <span class="hljs-comment"># Policy improvement</span><br>        stable = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states:<br>            best_action = <span class="hljs-built_in">max</span>(actions, key=<span class="hljs-keyword">lambda</span> a: <span class="hljs-built_in">sum</span>(p * (r + gamma * v[s_next]) \<br>                                                         <span class="hljs-keyword">for</span> p, r, s_next <span class="hljs-keyword">in</span> transition_probs(s, a)))<br>            <span class="hljs-keyword">if</span> best_action != policy[s]:<br>                policy[s] = best_action<br>                stable = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">if</span> stable:  <span class="hljs-comment"># Check if policy is stable</span><br>            <span class="hljs-keyword">break</span><br><br>    <span class="hljs-keyword">return</span> v, policy<br></code></pre></td></tr></table></figure><h2 id="算法比较">算法比较</h2><table><thead><tr class="header"><th><strong>算法</strong></th><th><strong>优点</strong></th><th><strong>缺点</strong></th><th><strong>适用场景</strong></th></tr></thead><tbody><tr class="odd"><td>值迭代</td><td>通过单个过程即可直接逼近最优值函数，简单、直接，适用于小规模问题</td><td>收敛速度较慢，尤其是当状态空间较大或 <spanclass="math inline">\(\gamma\)</span> 接近 1时，每次迭代的更新量可能变得很小</td><td>状态空间较小、对精度要求较高</td></tr><tr class="even"><td>策略迭代</td><td>收敛速度快，通常只需几轮策略更新即可收敛</td><td>每轮策略评估计算量大，尤其是在状态空间很大时，可能成为瓶颈</td><td>中小规模问题、对速度要求较高</td></tr><tr class="odd"><td>截断策略迭代</td><td>权衡了值迭代和策略迭代的优缺点，减少了策略评估的计算开销，相较于策略迭代更高效</td><td>收敛速度可能略低于完整的策略迭代</td><td>大规模问题、需要减少计算开销</td></tr></tbody></table><p>以上三种算法是强化学习中的经典方法，为后续的 Model-Free算法奠定了重要的基础。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL 学习笔记 #02 贝尔曼公式</title>
    <link href="/RL-Note-2.html"/>
    <url>/RL-Note-2.html</url>
    
    <content type="html"><![CDATA[<p>在上一节中，我们介绍了强化学习的基本概念，其中提到了用于评估「<strong>某状态的长期价值</strong>」的<strong>价值函数</strong>（ValueFunction）。</p><p>在本节中，我们将其定义为<strong>状态值</strong>（StateValue）：从某一状态出发，沿着某个策略所能获得的奖励回报的<strong>期望值</strong>（ExpectedReturn）。之所以是期望值，是因为该策略对应的轨迹可能有多条（因为一个状态下可能会采取不同动作，或者采取动作后可能会转移到多个不同状态）。而最终状态值越高，说明对应的策略越好。</p><h2 id="状态值函数-state-value-function">状态值函数 | State ValueFunction</h2><p>如上所述，状态值 <span class="math inline">\(v\)</span> 与状态 <spanclass="math inline">\(s\)</span> 和策略 <spanclass="math inline">\(\pi\)</span> 存在对应关系，因此可以表示为 <spanclass="math inline">\(v_\pi(s)\)</span>，也就是<strong>状态值函数</strong>。具体而言：<span class="math display">\[\begin{aligned}    v_\pi(s) &amp;= \mathbb{E}_\pi\left[G_t \mid S_t = s\right]\\    &amp;= \mathbb{E}_\pi \left[\sum_{k=0}^\infty \gamma^k r_{t+k} \midS_t = s \right]\\\end{aligned}\]</span> 其中：</p><ul><li><span class="math inline">\(G_t\)</span> 是累计奖励回报，<spanclass="math inline">\(S_t\)</span> 是第 <spanclass="math inline">\(t\)</span> 步开始前的状态；</li><li><span class="math inline">\(\gamma \in [0, 1]\)</span>是折扣因子，用于衡量未来奖励的衰减程度；</li><li><span class="math inline">\(r_t\)</span> 是第 <spanclass="math inline">\(t\)</span> 步的即时奖励。</li></ul><p>假设一个简单的策略轨迹如下：<span class="math inline">\(s_1 \to s_2\to s_3 \to s_4 \to s_1\)</span>，则对于状态 <spanclass="math inline">\(s_i\)</span> 的状态值 <spanclass="math inline">\(v_i\)</span>，可以通过递推公式表示为： <spanclass="math display">\[\begin{aligned}    v(s_i) &amp;= r_i + \gamma r_{i+1} + \gamma^2 r_{i+2} + \cdots \\    &amp;= r_i + \gamma \left( r_{i+1} + \gamma r_{i+2} + \cdots \right)\\    &amp;= r_i + \gamma v(s_{i+1})\\\end{aligned}\]</span></p><p>可以看出，状态之间的 Return可以是<strong>相互依赖</strong>的，这种现象也被称为<strong>自举</strong>（Bootstrap）。如果将一组式子列出来，可以得到：<span class="math display">\[\underbrace{\left[\begin{array}{c}v_1 \\v_2 \\v_3 \\v_4\end{array}\right]}_{\mathbf{v}}=\left[\begin{array}{l}r_1 \\r_2 \\r_3 \\r_4\end{array}\right]+\left[\begin{array}{l}\gamma v_2 \\\gamma v_3 \\\gamma v_4 \\\gamma v_1\end{array}\right]=\underbrace{\left[\begin{array}{c}r_1 \\r_2 \\r_3 \\r_4\end{array}\right]}_{\mathbf{r}}+\gamma\underbrace{\left[\begin{array}{cccc}0 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1 \\1 &amp; 0 &amp; 0 &amp; 0\end{array}\right]}_{\mathbf{P}} \underbrace{\left[\begin{array}{c}v_1 \\v_2 \\v_3 \\v_4\end{array}\right]}_{\mathbf{v}}\]</span> 即： <span class="math display">\[\mathbf{v}=\mathbf{r}+\gamma \mathbf{P}\mathbf{v}\]</span> 其中，<span class="math inline">\(\mathbf{P}\)</span>是状态转移概率矩阵，<span class="math inline">\(\mathbf{P}_{ij}\)</span>表示状态 <span class="math inline">\(s_i\)</span> 转移到状态 <spanclass="math inline">\(s_j\)</span> 的概率（策略 <spanclass="math inline">\(\pi\)</span> 的选择已蕴含在 <spanclass="math inline">\(\mathbf{P}\)</span> 中）。</p><p>这个式子就是贝尔曼公式的<strong>矩阵向量形式</strong>，只需变形即可求解：<span class="math display">\[\mathbf{v} = (\mathbf{I} - \gamma \mathbf{P})^{-1} \mathbf{r}\]</span></p><h2 id="贝尔曼公式-bellman-equation">贝尔曼公式 | Bellman Equation</h2><p>向量形式是贝尔曼公式的具体表现，它描述了所有状态值之间的<strong>递归关系</strong>。</p><p>这里我们正式介绍，对于任意策略 <spanclass="math inline">\(\pi\)</span>，状态值函数满足： <spanclass="math display">\[\begin{aligned}    v_\pi(s) &amp;= \mathbb{E}_\pi\left[G_t \mid S_t = s\right]\\    &amp;= \mathbb{E}_\pi\left[R_{t+1}+\gamma G_{t+1} \mid S_t =s\right]\\    &amp;= \mathbb{E}_\pi\left[R_{t+1} \mid S_t = s\right] + \gamma\mathbb{E}_\pi\left[G_{t+1} \mid S_t = s\right]\\\end{aligned}\]</span> 其中，第一项表示<strong>即时奖励</strong>的期望： <spanclass="math display">\[\begin{aligned}\mathbb{E}\left[R_{t+1} \mid S_t=s\right] &amp; =\sum_a \pi(a \mid s)\mathbb{E}\left[R_{t+1} \mid S_t=s, A_t=a\right] \\&amp; =\sum_a \pi(a \mid s) \sum_r p(r \mid s, a) r\end{aligned}\]</span>第二项表示<strong>未来奖励</strong>的期望（利用马尔可夫性质）： <spanclass="math display">\[\begin{aligned}\mathbb{E}\left[G_{t+1} \mid S_t=s\right] &amp; =\sum_{s^{\prime}}\mathbb{E}\left[G_{t+1} \mid S_t=s, S_{t+1}=s^{\prime}\right]p\left(s^{\prime} \mid s\right) \\&amp; =\sum_{s^{\prime}} \mathbb{E}\left[G_{t+1} \midS_{t+1}=s^{\prime}\right] p\left(s^{\prime} \mid s\right) \\&amp; =\sum_{s^{\prime}} v_\pi\left(s^{\prime}\right) p\left(s^{\prime}\mid s\right) \\&amp; =\sum_{s^{\prime}} v_\pi\left(s^{\prime}\right) \sum_ap\left(s^{\prime} \mid s, a\right) \pi(a \mid s)\\&amp; = \sum_a \pi(a \mid s) \sum_{s^{\prime}} p\left(s^{\prime} \mid s,a\right) v_\pi\left(s^{\prime}\right)\end{aligned}\]</span> 整合后可得<strong>一般形式</strong>的贝尔曼公式： <spanclass="math display">\[v_\pi(s)=\sum_a \pi(a \mid s) \left[ \sum_r p(r \mid s, a) r + \gamma\sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right)v_\pi\left(s^{\prime}\right) \right]\]</span> 其中：</p><ul><li><span class="math inline">\(v_\pi(s)\)</span> 和 <spanclass="math inline">\(v_\pi\left(s^{\prime}\right)\)</span>是待求解对象，也就是 Bootstrap 的目标；</li><li><span class="math inline">\(p(r \mid s, a)\)</span> 和 <spanclass="math inline">\(p\left(s^{\prime} \mid s, a\right)\)</span>代表了<strong>模型</strong>（model-based），这里我们假设模型已知——之后还会介绍无模型（model-free）算法。</li></ul><p>上式可以直观地理解为：<strong>状态值是当前状态下，所有可采取的动作，带来的即时奖励和未来状态值的折现和</strong>。</p><h3 id="策略评估-policy-evaluation">策略评估 | Policy Evaluation</h3><p>上述贝尔曼公式不仅仅是一个单独的式子，而是对所有 <spanclass="math inline">\(\forall s \in S\)</span>都成立的。策略评估（PolicyEvaluation）就是通过<strong>一组</strong>贝尔曼公式求解状态值函数的过程。只有通过评价策略的好坏，才能进一步地改进策略，最终找到最优策略。</p><p>第一种方法是，通过矩阵形式 <span class="math inline">\(\mathbf{v} =(\mathbf{I} - \gamma \mathbf{P})^{-1} \mathbf{r}\)</span>直接求解精确解。但对于高维状态空间，求逆矩阵的计算量过大，这种方法通常不可行。</p><p>第二种方法是，基于 Bootstrap特性，可以采用迭代法（称为<strong>贝尔曼期望方程迭代</strong>）：</p><ol type="1"><li><p>初始化状态值函数 <span class="math inline">\(v_\pi(s)\)</span>为一个猜测值 <span class="math inline">\(v_\pi^{(0)}(s)\)</span>；</p></li><li><p>不断更新： <span class="math display">\[v_\pi^{(k+1)}(s) = r_\pi(s) + \gamma P_\pi \cdotv_\pi^{(k)}\left(s\right)\]</span></p></li><li><p>直到 <span class="math inline">\(v_\pi\)</span> 收敛，即 <spanclass="math inline">\(\lVert v_\pi^{(k+1)} - v_\pi^{(k)} \rVert &lt;\epsilon\)</span>。</p></li></ol><h3 id="动作价值-action-value">动作价值 | Action Value</h3><p>除了状态值函数 <spanclass="math inline">\(v_\pi(s)\)</span>，另一个非常重要的概念是<strong>动作价值函数</strong>（ActionValue Function），记为 <span class="math inline">\(q_\pi(s,a)\)</span>。它不仅仅考虑某个状态的长期价值，还具体评估在<strong>该状态下选择某一动作</strong>的长期价值。</p><p>状态价值虽然建立起了策略的评估体系，但是策略关注的根本问题是「当前状态下需要选择哪个动作」。这个时候动作价值<strong>将动作选择与策略评估进一步结合</strong>，其是构建策略迭代、Q-learning等核心算法的基础。</p><p>这里我们正式介绍，在策略 <span class="math inline">\(\pi\)</span>下，从状态 <span class="math inline">\(s\)</span> 开始选择动作 <spanclass="math inline">\(a\)</span> 后，动作价值函数满足： <spanclass="math display">\[\begin{aligned}    q_\pi(s, a) &amp;= \mathbb{E}_\pi\left[G_t \mid S_t = s, A_t =a\right]\\    &amp;= \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k r_{t+k} \midS_t = s, A_t = a \right]\\\end{aligned}\]</span> 基于上式，我们可以写出动作价值函数与状态值函数的数学关系：<span class="math display">\[\begin{aligned}    v_\pi(s) &amp;= \mathbb{E}_\pi\left[G_t \mid S_t = s\right]\\    &amp;=\sum_a \mathbb{E}_\pi\left[G_t \mid S_t = s, A_t =a\right]\pi(a \mid s) \\    &amp;= \sum_a \pi(a \mid s) q_\pi(s, a)\\\end{aligned}\]</span> 这意味着状态值是动作价值的加权平均，而权重由策略 <spanclass="math inline">\(\pi(a \mid s)\)</span> 决定。</p><p>此外，观察和比较状态值函数的一般化贝尔曼公式，也可以进一步写出：<span class="math display">\[q_\pi(s, a) =\sum_r p(r \mid s, a) r + \gamma \sum_{s^{\prime}}p\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right)\]</span> 这些式子告诉我们：</p><ul><li>如果知道<strong>模型和当前策略的所有状态值</strong>，就能计算出所有的动作价值。</li><li>如果知道<strong>当前状态的所有动作价值</strong>，就能计算出当前策略的状态值。</li></ul><blockquote><p>注意：当前策略未必能涵盖所有动作，只不过计算时 <spanclass="math inline">\(\pi(a \mid s)=0\)</span>而被忽略了。而未被涵盖的动作价值其实不一定为 <spanclass="math inline">\(0\)</span>，甚至可能更高——这决定了策略优化的方向（但<strong>不一定直接就是最优策略，因为此时其他状态值可能还未收敛</strong>）。</p><p>除了先计算状态值再计算动作价值，也可以通过 Model-free的方法，用数据来估计。</p></blockquote><h2 id="贝尔曼最优公式-boe">贝尔曼最优公式 | BOE</h2><p>每一个贝尔曼公式都对应了一个策略，而贝尔曼最优公式（BellmanOptimalityEquation）作为一个特殊的贝尔曼公式，则对应<strong>最优策略</strong>（OptimalPolicy）和<strong>最优状态值</strong>（Optimal StateValue）——沿着最优策略能够得到最大状态值。</p><p>对于两个策略 <span class="math inline">\(\pi_1\)</span> 和 <spanclass="math inline">\(\pi_2\)</span>，若满足：<spanclass="math inline">\(v_{\pi_1}(s) \ge v_{\pi_2}(s),\;\; \forall s \inS\)</span>，则认为 <span class="math inline">\(\pi_1\)</span> 优于 <spanclass="math inline">\(\pi_2\)</span>。</p><p>进一步地，当一个策略 <span class="math inline">\(\pi^*\)</span>满足：<span class="math inline">\(v_{\pi^*}(s) \ge v_{\pi}(s),\;\;\forall s \in S \land \forall \pi \ne \pi^*\)</span>，则称 <spanclass="math inline">\(\pi^*\)</span> 为最优策略。</p><p>接下来，我们就需要回答一系列问题：</p><ul><li>最优策略 <span class="math inline">\(\pi^*\)</span> 是否存在？</li><li>最优策略 <span class="math inline">\(\pi^*\)</span> 是否唯一？</li><li>最优策略 <span class="math inline">\(\pi^*\)</span>是确定性的还是随机性的？</li><li>最优策略 <span class="math inline">\(\pi^*\)</span> 如何得到？</li></ul><p>为了回答这些问题，我们先引入贝尔曼最优公式的一般形式： <spanclass="math display">\[\begin{aligned}    v(s) &amp;= \max_\pi \sum_a \pi(a \mid s) \left[ \sum_r p(r \mid s,a) r + \gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right)v\left(s^{\prime}\right) \right]\\    &amp;= \max_\pi\sum_a \pi(a \mid s) q(s, a)\\\end{aligned}\]</span> 其中：</p><ul><li>已知模型 <span class="math inline">\(p(r \mid s, a)\)</span> 和<span class="math inline">\(p\left(s^{\prime} \mid s,a\right)\)</span>，已知环境参数 <spanclass="math inline">\(r\)</span>，已知常量 <spanclass="math inline">\(\gamma\)</span>；</li><li>待求解状态值函数 <span class="math inline">\(v(s)\)</span> 和 <spanclass="math inline">\(v(s^{\prime})\)</span>，待求解策略 <spanclass="math inline">\(\pi(s)\)</span>。</li></ul><blockquote><p>当然，我们也可以有矩阵向量形式，但这里暂时不需要： <spanclass="math display">\[\mathbf{v}= \max_\pi (\mathbf{r}_\pi+\gamma \mathbf{P}_\pi\mathbf{v})\]</span></p></blockquote><h3 id="boe-的化简过程">BOE 的化简过程</h3><p>此时等式右侧是一个最优化问题，但有两个未知量需要求解。由于 <spanclass="math inline">\(\pi\)</span> 是最优化问题的变量，可以先确定 <spanclass="math inline">\(\pi(a \mid s)\)</span> 再去求解剩下的未知量。</p><p>对于一次多项式而言，我们希望将最大的 <span class="math inline">\(q(s,a)\)</span> 分配最大权重。显然，当 <span class="math inline">\(\pi(a\mid s)\)</span> 是<strong>确定性策略</strong>时，即 <spanclass="math inline">\(\pi(a \mid s) = 1\)</span> 仅针对最优动作 <spanclass="math inline">\(a^* = \arg\max_a q(s, a)\)</span>时，能实现最大化。于是，最优策略满足： <span class="math display">\[\pi^*(a \mid s) =\begin{cases}1 &amp; \text{if } a = a^*, \\0 &amp; \text{otherwise}.\end{cases}\]</span> 此时我们有状态值： <span class="math display">\[v(s)= \max_\pi\sum_a \pi(a \mid s) q(s, a)=\max_{a\in A(s)} q(s, a)\]</span></p><p>通过重新整理，贝尔曼最优公式可表示为： <span class="math display">\[v_*(s) = \max_a \left[ \sum_r p(r \mid s, a) r + \gamma \sum_{s&#39;}p(s&#39; \mid s, a) v_*(s&#39;) \right]\]</span> 这是一个<strong>递归定义</strong>，其中右侧仅依赖于 <spanclass="math inline">\(v_*\)</span> 本身，可以写作： <spanclass="math display">\[v_* = f(v_*)\]</span> 其中，<span class="math inline">\(f(v)\)</span>是一个映射，<strong>将当前估计的值函数更新为更接近实际的值函数</strong>。</p><h3 id="不动点与收敛性contraction-mapping">不动点与收敛性：ContractionMapping</h3><p>我们需要解的是 <span class="math inline">\(v = f(v)\)</span>的<strong>不动点</strong>（Fixed Point），即在 <spanclass="math inline">\(f\)</span> 的作用下<strong>不再发生变化</strong>的<spanclass="math inline">\(v\)</span>。在强化学习中，值函数的迭代计算会逐渐逼近这个不动点。</p><p>如果 <span class="math inline">\(f(v)\)</span>是一个<strong>收缩映射</strong>（Contraction Mapping），即满足以下性质：<span class="math display">\[\lVert f(v_1) - f(v_2) \rVert \leq \gamma \lVert v_1 - v_2 \rVert, \quad\gamma \in [0, 1)\]</span> 则可以保证了值函数迭代的收敛性。</p><blockquote><p>收缩映射规定参数 <span class="math inline">\(\gamma\)</span>是严格小于 <span class="math inline">\(1\)</span>的数字，这样才能确保在映射过程中 <spanclass="math inline">\(v_1\)</span> 和 <spanclass="math inline">\(v_2\)</span> 相互靠近。</p><p>例如：对于 <span class="math inline">\(x=f(x)=0.5x,\;\; x \in\mathbb{R}\)</span>，我们可以很容易的发现 <spanclass="math inline">\(x=0\)</span> 是一个不动点。此外，<spanclass="math inline">\(f(x)=0.5x\)</span> 是一个收缩映射，因为： <spanclass="math display">\[\lVert 0.5x_1 - 0.5x_2 \rVert =  0.5 \lVert x_1 - x_2 \rVert  \leq\gamma \lVert x_1 - x_2 \rVert, \;\; \text{for any } \gamma \in [0.5, 1)\]</span> 又例如：对于 <span class="math inline">\(x=f(x)=Ax,\;\;x \in\mathbb{R}, \;\; A \in \mathbb{R}^{n\times n}\)</span>，其中 <spanclass="math inline">\(\lVert A \rVert \le \gamma &lt;1\)</span>，同样也可以证明 <span class="math inline">\(x=0\)</span>是一个不动点，且 <span class="math inline">\(f(x)\)</span>是一个收缩映射。</p></blockquote><p>这里再介绍一下<strong>收缩映射定理</strong>（Contraction MappingTheorem）。对于一个形如 <span class="math inline">\(x=f(x)\)</span>的等式，如果 <span class="math inline">\(f\)</span>是一个收缩映射，则：</p><ul><li><strong>有且仅有</strong>一个不动点 <spanclass="math inline">\(x^*\)</span> 使得 <spanclass="math inline">\(f(x^*)=x^*\)</span>；</li><li>可以通过迭代式算法 <spanclass="math inline">\(x_{k+1}=f(x_k)\)</span> 求解不动点：当 <spanclass="math inline">\(k\to \infty\)</span> 时，<spanclass="math inline">\(x_k \tox^*\)</span>，且<strong>收敛速率呈指数倍</strong>。</li></ul><h3 id="证明-fx-是收缩映射-选读">证明 <spanclass="math inline">\(f(x)\)</span> 是收缩映射 [选读]</h3><p>为了证明 <span class="math inline">\(f(v)\)</span>是一个收缩映射，我们先对任意状态 <spanclass="math inline">\(s\)</span>，计算 <spanclass="math inline">\(f(v_1)(s) - f(v_2)(s)\)</span> 的绝对值： <spanclass="math display">\[\begin{aligned}\left| f(v_1)(s) - f(v_2)(s) \right|&amp;= \left| \max_a \left[ \sum_r p(r \mid s, a) r + \gamma\sum_{s&#39;} p(s&#39; \mid s, a) v_1(s&#39;) \right] \right. \\&amp; \quad \left. - \max_a \left[ \sum_r p(r \mid s, a) r + \gamma\sum_{s&#39;} p(s&#39; \mid s, a) v_2(s&#39;) \right] \right|\end{aligned}\]</span>由于最大值运算是一个非扩张运算（Non-Expansive），对于任意两个实数序列<span class="math inline">\(x_i\)</span> 和 <spanclass="math inline">\(y_i\)</span>，有： <span class="math display">\[\left| \max_i x_i - \max_i y_i \right| \leq \max_i \left| x_i - y_i\right|\]</span> 因此，可以将上式改写为： <span class="math display">\[\left| f(v_1)(s) - f(v_2)(s) \right| \leq \max_a \left| \gamma\sum_{s&#39;} p(s&#39; \mid s, a) v_1(s&#39;) - \gamma \sum_{s&#39;}p(s&#39; \mid s, a) v_2(s&#39;) \right|\]</span> 将 <span class="math inline">\(\gamma\)</span>提取出来，利用状态转移概率 <span class="math inline">\(p(s&#39; \mid s,a)\)</span> 的线性性： <span class="math display">\[\left| f(v_1)(s) - f(v_2)(s) \right|\leq \gamma \max_a \sum_{s&#39;} p(s&#39; \mid s, a) \left| v_1(s&#39;)- v_2(s&#39;) \right|\]</span> 状态转移概率 <span class="math inline">\(p(s&#39; \mid s,a)\)</span> 是一个和为 <span class="math inline">\(1\)</span>的分布，因此 <span class="math inline">\(\sum_{s&#39;} p(s&#39; \mid s,a) \left| v_1(s&#39;) - v_2(s&#39;) \right|\)</span> 是 <spanclass="math inline">\(\left| v_1(s&#39;) - v_2(s&#39;) \right|\)</span>的加权平均值，不超过 <span class="math inline">\(\lVert v_1 - v_2\rVert_\infty\)</span>。因此： <span class="math display">\[\left| f(v_1)(s) - f(v_2)(s) \right| \leq \gamma \lVert v_1 - v_2\rVert_\infty\]</span> 取 <span class="math inline">\(\lVert \cdot\rVert_\infty\)</span> 范数的全局最大值后，得到： <spanclass="math display">\[\lVert f(v_1) - f(v_2) \rVert_\infty \leq \gamma \lVert v_1 - v_2\rVert_\infty\]</span> 由于 <span class="math inline">\(\gamma \in [0,1)\)</span>，我们证明了 <span class="math inline">\(f(v)\)</span>是一个收缩映射。根据 <strong>Banach不动点定理</strong>，收缩映射在完备空间上（如 <spanclass="math inline">\(\mathbb{R}^n\)</span> 下的 <spanclass="math inline">\(\lVert \cdot \rVert_\infty\)</span>范数空间）有唯一的不动点。因此，贝尔曼最优公式的解 <spanclass="math inline">\(v_*\)</span><strong>唯一存在</strong>，并可以通过迭代方法（如<strong>值迭代法</strong>）收敛到该解。</p><h3 id="boe-的性质">BOE 的性质</h3><p>现在，我们可以回答前面提到的一系列问题：</p><ul><li>最优策略 <span class="math inline">\(\pi^*\)</span>是否存在？答：存在，且对应最优状态 <spanclass="math inline">\(v_*\)</span>，并且可证明<strong>策略最优性</strong>（此处不赘述）。</li><li>最优策略 <span class="math inline">\(\pi^*\)</span>是否唯一？答：最优状态 <span class="math inline">\(v_*\)</span><strong>唯一存在</strong>，但最优策略 <spanclass="math inline">\(\pi^*\)</span> 可能有<strong>多个</strong>。</li><li>最优策略 <span class="math inline">\(\pi^*\)</span>是确定性的还是随机性的？答：确定性策略，<strong>贪心</strong>选择 <spanclass="math inline">\(a^*\)</span>。</li><li>最优策略 <span class="math inline">\(\pi^*\)</span>如何得到？答：可通过<strong>迭代法</strong>求解，将在下一章介绍。</li></ul><p>这里补充一些影响最优策略的因素：</p><ul><li>当 <span class="math inline">\(\gamma\)</span> 接近 <spanclass="math inline">\(1\)</span>，智能体越关注长期收益，可能会做出当下看来不利的动作，例如进入违禁区；</li><li>当 <span class="math inline">\(\gamma\)</span>降低，智能体则更关注短期奖励，起点附近的状态值会较低，高状态值集中在终点附近；</li><li>当 <span class="math inline">\(\gamma\)</span> 降到 <spanclass="math inline">\(0\)</span>，智能体完全短视，只关注即时奖励，只有终点或边界清晰的地方有决策，其他地方会随机选取动作，从很多状态出发甚至到达不了终点；</li><li>当 <span class="math inline">\(\gamma\)</span> 接近 <spanclass="math inline">\(1\)</span> 但改变部分 <spanclass="math inline">\(r\)</span>，例如增大违禁区的惩罚，也会导致智能体减少不利动作；</li><li>而当所有 <span class="math inline">\(r\)</span> 修改为 <spanclass="math inline">\(ar+b\)</span>时，此时的<strong>最优策略不会变化</strong>，而最优状态值则变为：</li></ul><p><span class="math display">\[v^{\prime}=a v^*+\frac{b}{1-\gamma} \mathbf{1}\]</span></p><p>最后是一个关于最短路径的分析：初学者往往会给 Agent的每个步骤加上基础惩罚（代表能量消耗），来避免出现<strong>绕路</strong>（meaninglessdetour）的现象。但实际上无需这种惩罚 Agent 也会走最短路径，因为还有<span class="math inline">\(\gamma\)</span>作为约束——绕路意味着到达目标状态越晚，而 <spanclass="math inline">\(\gamma\)</span> 带来的折扣就越大！</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL 学习笔记 #01 基本概念</title>
    <link href="/RL-Note-1.html"/>
    <url>/RL-Note-1.html</url>
    
    <content type="html"><![CDATA[<p>该笔记是观看西湖大学赵世钰老师的 <ahref="https://www.bilibili.com/video/BV1sd4y167NS">强化学习的数学原理</a>网课所做的笔记，过程中参考了 <ahref="https://datawhalechina.github.io/easy-rl/#/">ReayRL</a>书籍。部分笔记内容由 ChatGPT 辅助生成。</p><h2 id="强化学习-vs.-监督学习">强化学习 vs. 监督学习</h2><p>强化学习（ReinforcementLearning，RL）的核心在于「学习该如何去做」，其终极目标就在于「<strong>求解最优策略</strong>」。具体来说，RL旨在让智能体（Agent）从某个状态（State）映射到某个行为（Action），以最大化某个奖励信号（RewardSignal）。这种学习方式与监督学习（SupervisedLearning）有显著区别，监督学习假设：</p><ol type="1"><li><strong>输入数据是独立的</strong>：监督学习假设样本之间没有关联性，否则难以有效学习。</li><li><strong>提供正确标签</strong>：每个样本都有明确的正确答案，学习器依赖这些标签来优化预测。</li></ol><p>然而，在强化学习中，这两条假设均不成立。强化学习有以下几个关键特性：</p><ol type="1"><li><strong>序列相关性</strong>：强化学习中的输入是序列数据，样本之间具有强关联性，当前的动作会影响未来的状态和奖励。</li><li><strong>缺乏直接监督</strong>：没有明确的「正确动作」，智能体需要通过尝试和反馈自己发现最优策略（Policy）。</li><li><strong>延迟奖励</strong>：某一时刻的动作可能对未来的奖励有深远影响，智能体需要在较长的时间范围内评估某个动作的效果。</li></ol><p>因此，智能体需要通过<strong>试错搜索</strong>（Trial-and-ErrorSearch）的方式逐步提升表现，具体而言，通过对环境的交互、观察反馈（奖励或惩罚）来优化策略。</p><p>其面临的核心问题之一是如何平衡探索（exploration）和利用（exploitation）：</p><ul><li><strong>利用（Exploit）</strong>：基于已有知识选择奖励最大的动作。</li><li><strong>探索（Explore）</strong>：尝试未知的动作，可能发现更优的策略。</li></ul><p>有效的策略通常在探索与利用之间找到平衡。过度利用可能导致智能体停留在次优解，而过度探索则可能浪费资源。</p><h2 id="序列决策-sequential-decision-making">序列决策 | SequentialDecision Making</h2><p>序列决策（Sequential DecisionMaking）是强化学习的核心思想之一。智能体通过与环境的连续交互，基于状态、动作及奖励的反馈，逐步优化决策策略，以最大化累计奖励。</p><h3 id="核心要素">核心要素</h3><ol type="1"><li><p>状态 <spanclass="math inline">\(s\)</span>（State）：智能体从环境中感知到的状态信息，用于决策。</p></li><li><p>动作 <spanclass="math inline">\(a\)</span>（Action）：智能体基于当前状态选择的输出，直接影响后续<strong>状态转移</strong>（StateTransition）。</p></li><li><p>策略 <spanclass="math inline">\(\pi\)</span>（Policy）：将状态映射到动作的规则，定义了智能体在每个状态下的动作。</p><ul><li>确定性策略（Deterministic Policy）：<spanclass="math inline">\(\pi(s)\)</span> 直接输出在状态 <spanclass="math inline">\(s\)</span> 下的最优动作。</li><li>随机性策略（Stochastic Policy）：<spanclass="math inline">\(\pi(a|s)\)</span> 表示在状态 <spanclass="math inline">\(s\)</span> 下采取动作 <spanclass="math inline">\(a\)</span> 的概率。</li></ul></li><li><p>奖励 <spanclass="math inline">\(r\)</span>（Reward）：一个<strong>标量</strong>反馈信号（scalarfeedback signal），代表某一步动作的好坏。</p><ul><li>奖励值直接依赖于<strong>当前的状态和采取的动作</strong> <spanclass="math inline">\(r(s_i,a_i)\)</span>，而非下一个状态 <spanclass="math inline">\(s_{i+1}\)</span>。</li></ul></li><li><p>轨迹 <spanclass="math inline">\(\tau\)</span>（Trajectory）：智能体与环境交互所经历的状态、动作和奖励序列：<span class="math display">\[\tau=\left(s_0, a_0, r_0, s_1, a_1, r_1, \ldots\right)\]</span></p><ul><li>回合（Episode）/尝试（Trail）：从初始状态到<strong>终止状态</strong>（terminalstate）的一次完整交互。</li><li>预演（Rollout）：<strong>模拟</strong>智能体在策略 <spanclass="math inline">\(\pi\)</span>下与环境交互，生成轨迹以评估策略效果。</li></ul></li><li><p>回报 <spanclass="math inline">\(G\)</span>（Return）：从当前状态到终止状态的累计奖励。</p><ul><li><p>折扣回报（Discounted Return）：考虑折扣因子 <spanclass="math inline">\(\gamma\)</span>的回报，越遥远的奖励越少，避免回报<strong>发散</strong>。</p></li><li><p>强化学习的目标是最大化<strong>期望累计奖励</strong>（ExpectedCumulative Reward）： <span class="math display">\[G_t = \sum_{k=0}^\infty \gamma^k r_{t+k}\]</span></p></li></ul></li></ol><blockquote><p>关于终止状态 / 目标状态有两种定义：</p><ol type="1"><li>认为目标状态是一种<strong>吸收态</strong>（absorbing state），一旦Agent 到达该状态将会永远停留，并且之后的奖励 <spanclass="math inline">\(r=0\)</span>。</li><li>认为目标状态是一个普通的状态，并且具有策略，Agent可以选择停留在该状态并持续获取 <spanclass="math inline">\(r&gt;0\)</span>，从而不再离开。该定义更加一般化，但在学习策略的过程中需要更多的搜索。</li></ol></blockquote><h3 id="马尔可夫决策过程-mdp">马尔可夫决策过程 | MDP</h3><p>马尔可夫决策过程（Markov DecisionProcess，MDP）提供了数学框架，用于描述强化学习中的环境动态。一个 MDP通常由以下五元组定义：</p><ol type="1"><li><p><span class="math inline">\(S\)</span>（StateSpace）：包含所有可能的状态集合（仅在完全可观测的环境下）。</p></li><li><p><span class="math inline">\(A\)</span>（ActionSpace）：包含所有可能的动作集合，<strong>依赖于状态空间</strong> <spanclass="math inline">\(A(s)\)</span>。</p><ul><li>离散动作空间（Discrete ActionSpace）：有限数量的动作集合，例如上下左右移动。</li><li>连续动作空间（Continuous ActionSpace）：动作为实值向量，例如机器人关节角度。</li></ul></li><li><p><span class="math inline">\(T\)</span>（TransitionFunction）：状态转移概率 <span class="math inline">\(p(s&#39; \mid s,a)\)</span>，表示在状态 <span class="math inline">\(s\)</span> 执行动作<span class="math inline">\(a\)</span> 后转移到状态 <spanclass="math inline">\(s&#39;\)</span> 的概率。</p><ul><li>如果概率均为 <spanclass="math inline">\(1\)</span>，即确定性策略，对应的状态转移过程可用<strong>表格法</strong>（TabularMethod）表示。</li><li><strong>马尔可夫性质</strong>（Markov Property）：Memoryless历史无关，即下一状态 <span class="math inline">\(s&#39;\)</span>只与当前状态 <span class="math inline">\(s\)</span> 和动作 <spanclass="math inline">\(a\)</span> 有关。</li></ul></li><li><p><span class="math inline">\(R\)</span>（RewardFunction）：即时奖励 <span class="math inline">\(R(s,a)\)</span>，表示在状态 <span class="math inline">\(s\)</span>下执行动作 <span class="math inline">\(a\)</span>所获得的奖励。</p></li><li><p><span class="math inline">\(\gamma\)</span>（DiscountFactor）：折扣因子 <span class="math inline">\(\gamma \in [0,1]\)</span>，用于平衡当前奖励和未来奖励的重要性。</p><ul><li><span class="math inline">\(\gamma\)</span> 越接近1，智能体越关注长期收益；<span class="math inline">\(\gamma\)</span>较小，则更关注短期奖励。</li></ul></li></ol><blockquote><p>奖励函数（Reward Function）vs. 价值函数（ValueFunction）：奖励函数是即时反馈，而价值函数衡量长期反馈。</p><p>价值函数用于评估「<strong>某状态的长期价值</strong>」，即从该状态开始，智能体期望获得的累计奖励：<span class="math display">\[V_\pi(s) = \mathbb{E}_\pi\left[G_t \mid S_t = s\right]\]</span> 更进一步，价值函数还分为状态价值函数 <spanclass="math inline">\(V(s)\)</span> 和动作价值函数 <spanclass="math inline">\(Q(s,a)\)</span>，后者表示「<strong>某状态下执行某动作的长期价值</strong>」。</p></blockquote><h3 id="部分可观测马尔可夫决策过程-pomdp">部分可观测马尔可夫决策过程 |POMDP</h3><p>当环境是部分可观测的（智能体无法直接感知完整状态），则使用部分可观测马尔可夫决策过程（PartiallyObservable Markov Decision Process，POMDP）建模。POMDP可以用七元组表示：</p><ol type="1"><li><span class="math inline">\(S\)</span>（StateSpace）：隐变量集合，真实状态不可直接观测。</li><li><span class="math inline">\(A\)</span>（ActionSpace）：动作集合。</li><li><span class="math inline">\(T\)</span>（TransitionFunction）：状态转移概率 <span class="math inline">\(p(s&#39; \mid s,a)\)</span>。</li><li><span class="math inline">\(R\)</span>（RewardFunction）：即时奖励函数。</li><li><strong><span class="math inline">\(\Omega\)</span>（ObservationSpace）</strong>：观测空间，智能体可以感知到的有限信息。</li><li><strong><span class="math inline">\(O\)</span>（ObservationFunction）</strong>：观测概率 <span class="math inline">\(O(o \mid s,a)\)</span>，表示执行动作 <span class="math inline">\(a\)</span>后在状态 <span class="math inline">\(s\)</span> 下得到观测 <spanclass="math inline">\(o\)</span> 的概率。</li><li><span class="math inline">\(\gamma\)</span>（DiscountFactor）：折扣因子。</li></ol><p>POMDP 的关键在于智能体需要维护一个<strong>信念状态（beliefstate）</strong>，即关于真实状态的概率分布，用来进行决策。</p><h2 id="经典问题">经典问题</h2><p>强化学习的研究中，有一些经典问题常被讨论。这里简单提及，之后的章节中也会详细介绍。</p><h3 id="value-based-和-policy-based-方法">Value-based 和 Policy-based方法</h3><p>在强化学习中，智能体的目标是选择最优动作以获得最大累计奖励。实现这一目标有两种主要途径：基于价值的方法（Value-based）和基于策略的方法（Policy-based）。</p><p>基于价值的方法侧重于学习价值函数，例如状态价值函数 <spanclass="math inline">\(V(s)\)</span> 或动作价值函数 <spanclass="math inline">\(Q(s,a)\)</span>。智能体的策略由价值函数<strong>隐式推导</strong>出，例如通过选择当前<span class="math inline">\(Q(s, a)\)</span>最大的动作来决策。这类方法的典型代表是 Q-learning 和 SARSA，它们通过更新<span class="math inline">\(Q\)</span>值来逼近最优策略。然而，基于价值的方法通常更适合<strong>离散动作空间</strong>，对连续动作空间的适应性较差。</p><p>相比之下，基于策略的方法直接优化策略 <spanclass="math inline">\(\pi(a|s)\)</span>，即从状态到动作的映射。策略可以是随机性的，也可以是确定性的。这种方法不依赖价值函数，能够更自然地处理复杂环境中的<strong>连续动作空间</strong>。例如，策略梯度法（PolicyGradient）是一种直接优化策略参数的算法，通过采样轨迹并使用梯度上升法来最大化累计奖励。</p><p>两种方法各有优势，基于价值的方法训练过程通常更稳定，而基于策略的方法适合高维、连续动作空间。将两者结合的Actor-Critic 方法通过同时学习策略和价值函数实现了取长补短：Actor更新策略，Critic 用价值函数评估策略优劣，为策略改进提供指导。</p><h3 id="model-based-和-model-free-方法">Model-based 和 Model-free方法</h3><p>另一个重要的问题是智能体是否需要显式地建模环境的状态转移关系。若此时对环境的状态转移<span class="math inline">\(P(s&#39;|s, a)\)</span>有较好的建模条件，则称为<strong>有模型</strong>（model-based）方法。这种方法通过学习一个状态转移模型来模拟环境，从而支持规划（planning）或预演（rollout）。例如，在值迭代和策略迭代算法中，已知环境模型时可以直接求解最优策略。</p><p>相反，<strong>免模型</strong>（model-free）方法不依赖状态转移模型，而是直接与环境交互来学习。此类方法使用采样经验来更新价值函数（如MC Learning）或直接优化策略（如 PolicyGradient）。虽然免模型方法更加通用，但通常需要更多的交互样本，并且学习效率低于有模型方法。</p><p>两者的选择取决于问题特点。有模型方法对环境建模依赖较高，但能在数据稀缺时高效决策；免模型方法则适合复杂、未知的环境，但需要大量样本支持。</p><h3 id="online-和-offline-算法">Online 和 Offline 算法</h3><p>根据与环境交互的方式，强化学习算法可以进一步分为<strong>在线算法（OnlineAlgorithm）</strong> 和 <strong>离线算法（OfflineAlgorithm）</strong>。</p><p>Online强调实时学习，智能体在与环境交互的过程中持续更新策略或价值函数。其核心特点是<strong>每次与环境的交互都会对学习过程产生直接影响</strong>。例如，Q-learning和 Actor-Critic方法均为典型的在线算法。在某些环境条件可能随时间变化的场景中，在线算法能够实时调整策略以保持最优性能。根据<strong>行为策略和目标策略是否相同</strong>，Online算法又分为 On-Policy 算法和 Off-Policy 算法：</p><ul><li>On-policy RL 算法包括：MC Learning、SARSA、REINFORCE、A3C、PPO等；</li><li>Off-policy RL 算法包括：Q-learning、DQN、DDPG、SAC 等。</li></ul><p>Offline 也被称为<strong>批量强化学习</strong>（Batch ReinforcementLearning）。它通过从<strong>预先收集的交互数据</strong>（通常称为「经验池」）中学习策略，而无需与环境直接交互。在训练阶段，离线算法不会主动探索新数据，所有的优化都基于已有数据集。离线算法的一个核心挑战是<strong>分布不匹配问题</strong>（distributionalmismatch），即数据分布和真实环境分布可能不同，从而导致策略的泛化性能下降。其与<strong>模仿学习</strong>（ImitationLearning）的区别在于，IL数据中不包括奖励，并且需要假设其专家数据近似最优策略数据。</p><p>Online 和 Offline算法并不是完全独立的，很多强化学习算法可以结合两者的优点。例如，<strong>经验回放</strong>（ExperienceReplay）是一种常用的机制，它在在线学习的过程中收集数据并存储到经验池中，随后通过离线回放这些数据来提高学习效率。</p><h3 id="exploit-和-explore-的平衡">Exploit 和 Explore 的平衡</h3><p>探索与利用的平衡是强化学习的核心难题之一。探索（Exploration）指智能体尝试未知的动作，以期发现更优的策略；利用（Exploitation）则指智能体基于现有知识选择最优动作，从而最大化即时奖励。二者看似对立，但必须平衡，才能在有限的尝试次数内获得最大的累计奖励。</p><p>我们可以通过 <strong>K 臂老虎机问题</strong>（K-armed Bandit）直观理解：在这个理论模型中，智能体面临 <spanclass="math inline">\(K\)</span>个选择，每个选择的奖励分布未知，目标是通过策略最大化奖励。</p><ul><li>如果智能体完全依赖Exploration（如随机选择每个动作），它可能很好地估计各动作的期望奖励，但无法充分利用最优动作。</li><li>如果智能体完全依赖Exploitation（如总是选择当前奖励最大的动作），它可能快速陷入次优解。</li></ul><p>为解决这一问题，常用方法例如 <spanclass="math inline">\(\varepsilon\)</span>-贪婪策略（<spanclass="math inline">\(\epsilon\)</span>-greedy），以一定概率 <spanclass="math inline">\(\epsilon\)</span> 随机探索，而以 <spanclass="math inline">\(1-\epsilon\)</span> 的概率选择当前最优动作。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常用 Prompt 合集</title>
    <link href="/Awesome-Prompts.html"/>
    <url>/Awesome-Prompts.html</url>
    
    <content type="html"><![CDATA[<p>记录一些常用的 Prompt，包括：交互式论文撰写、科研论文的润色、文生图Prompt 优化等。</p><p>本文提供的 Prompt参考了部分公开文档，并结合了一些自己日常使用的经验。</p><h2 id="交互式论文撰写">交互式论文撰写</h2><p>现有的 LLMs已经拥有足够强大的多语言能力，以下内容直接用中文进行输入即可，推荐使用GPT-4o、DeepSeek-V3、Claude 3.5 Sonnet等模型，不建议使用深度思考模型。</p><p>交互式论文撰写的前提是作者<strong>已经有了大概思路</strong>，只是难以将其转化为实际文本的时候。此时通过与模型交互，既能快速产出内容，也能激发更多灵感。</p><h3 id="从头开始写">从头开始写</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">请你作为学术论文写作助手，帮我撰写一篇英文学术论文的 Abstract。现在我会给你提供一些关键点，这些内容是我的头脑风暴过程，较为混乱，请你不要直接翻译，而是将其当作一次讨论的纪要，参考、总结并进行创作。这个段落不需要很长，你需要使用简练、清晰的语言。<br><br>关键点：<br></code></pre></td></tr></table></figure><p>注意事项：</p><ul><li>开头必须强调英文学术论文，否则模型可能用中文输出</li><li>关键点尽量按照你希望呈现的顺序，用中文罗列出来即可，越详细越好</li></ul><h3 id="续写已有段落">续写已有段落</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs text">我正在撰写一篇英文学术论文的 Introduction，以下是我写完的部分：<br><br>TODO<br><br>现在，我准备继续往下写一个英文段落，请你帮我进行创作。现在我会给你提供一些关键点，这些内容是我的头脑风暴过程，较为混乱，请你不要直接翻译，而是将其当作一次讨论的纪要，参考、总结并进行创作。这个段落不需要很长，你需要使用简练、清晰的语言。<br><br>关键点：<br></code></pre></td></tr></table></figure><p>注意事项：</p><ul><li>已有的英文段落可以直接贴上去，有助于让续写风格更一致</li></ul><h3 id="批注式修改">批注式修改</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">你写的很好，我在上面增加了一些批注，请你按照这个批注再优化一下：<br></code></pre></td></tr></table></figure><p>注意事项：</p><ul><li>批注式修改需要紧接着上一轮对话使用，上一轮可以是：生成、续写、修改；</li><li>直接在上一轮输出的英文句子后用中文括号批注即可，例如：abcd（这句话可以再简练一些）；</li><li>也可以自己修改一些英文句子，甚至直接用中文改上去，再让其优化，例如：ab然后 cd（改得连贯些）；</li><li>括号里的中文批注可以虚（简练点、换一种说法）也可以实（这两句话合并、删去xxx）；</li><li>如果上一轮对话已经找不到了怎么办？多加一些 Context 即可。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">我正在撰写一篇英文学术论文的 Introduction，以下是我撰写de一个初稿，以及一些修改意见和批注，请你按照这个批注再优化一下。这个段落不需要很长，你需要使用简练、清晰的语言。<br></code></pre></td></tr></table></figure><h3 id="优化缩写">优化、缩写</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">我正在撰写一篇英文学术论文，这是我已经写好的段落，请你帮我修改一下，使其在保持原意的前提下更简练清晰，不能有语法错误：<br></code></pre></td></tr></table></figure><p>注意事项：</p><ul><li>建议每段话写完之后都新开一个窗口让其优化一下，优化完之后再新开一个窗口续写下一段</li></ul><h3 id="实验与分析">实验与分析</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs text">我正在撰写一篇英文学术论文的实验分析部分，以下是我的实验结果表格：<br><br>TODO<br><br>现在，我准备写一个段落用于分析结果，请你帮我进行创作。现在我会给你提供一些关键点，这些内容是我的头脑风暴过程，较为混乱，请你不要直接翻译，而是将其当作一次讨论的纪要，参考、总结并进行创作。这个段落不需要很长，你需要使用简练、清晰的语言。<br><br>关键点：<br><br>- 你要在开头用 \subsecition&#123;&#125; 给这个分析实验起一个标题，这个标题需要简短、清晰，能直接体现出这个实验想说明的结论，才能吸引读者的兴趣。<br></code></pre></td></tr></table></figure><p>注意事项：</p><ul><li>实验分析的标题很重要，如果不会想的话可以带上第一个关键点</li><li>如果没有实验结果表格，只有图片呢？只需要将图片的 Caption输入，并且描述图片的大致内容，就能起到类似的效果</li></ul><h2 id="文本优化">文本优化</h2><h3 id="英文论文润色">英文论文润色</h3><p>直接改写：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">As an academic essay writing assistant, paraphrase the text using more academic and scientific language. Use a neutral tone and avoid repetitions of words and phrases. Don&#x27;t use too advanced words and make sure it&#x27;s easy to read. Ignore the grammar defects caused by Latex syntax.<br><br>Paragraph: <br></code></pre></td></tr></table></figure><p>改写论文使其变得更连贯：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">As an academic essay writing assistant, rewrite this paragraph to meet the academic style and easy to read. Use a neutral tone and avoid repetitions of words and phrases. You should improve the clarity, concision and overall readability, make it more coherent and progressive. When necessary, rewrite the whole sentence. Ignore the grammar defects caused by Latex syntax.<br><br>Paragraph: <br></code></pre></td></tr></table></figure><p>改写论文使其变得更简练：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">As an academic essay writing assistant, simplify this paragraph to meet the academic style. Don&#x27;t use too many advanced and repetitive words and improve the concision and readability. When necessary, rewrite the whole sentence. Ignore the grammar defects caused by Latex syntax.<br><br>Paragraph: <br></code></pre></td></tr></table></figure><p>GPT-4 可以试着用这个，双栏展示修改结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">I am preparing my SCI paper for submission and require assistance in polishing each paragraph. Could you please refine my writing for academic rigor? I need you to correct any grammatical errors, improve sentence structure for academic suitability, and make the text more formal where necessary. When necessary, rewrite the whole sentence. Ignore the grammar defects caused by Latex syntax.<br><br>For each paragraph we need to improve, you need to put all the modified sentence in a Markdown table, each column contains the following: Full original sentence; Highlight the revised part of the sentence; using Chinese explain why made these changes. If you understand, please reply: yes, let&#x27;s get started.<br></code></pre></td></tr></table></figure><h3 id="检查英文语法">检查英文语法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">As an academic essay writing assistant, please check the grammar and spelling for, and if you find no errors, please tell me the paragraph is fine.  Do not try to polish the text. Please list your corrected sentences and **highlight** the mistakes you fixed. Ignore the grammar defects caused by Latex syntax.<br><br>Paragraph: <br></code></pre></td></tr></table></figure><h3 id="学术中翻英">学术中翻英</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">As an scientific Chinese-English translator, please translate my upcoming Chinese content into elegant, refined, and academic English. You should use rhetorical knowledge and experience about effective writing techniques to reply. <br><br>Paragraph: <br></code></pre></td></tr></table></figure><h3 id="学术英翻中">学术英翻中</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs text">作为一名学术论文的英翻中翻译，请将我即将发表的英文内容翻译成准确、优雅的学术中文论文。您应该运用中文修辞学知识和有效写作技巧的经验来回答。你需要注意文本的语法和整体可读性，同时分解长句，减少重复。<br><br>请你将英文内容中的 LaTeX 语法修改为纯文本形式。例如：<br>- 图表的引用（\ref&#123;xxx&#125;）可以改为「如图 1」、「如表 1 所示」。<br>- 文献的引用（\citep&#123;xxx&#125;）可以改为 [1]、[2]，如果一处地方有多篇文献，可以用 [1,2]。<br>- 章节的引用则根据实际情况判断，例如「章节 1」、「附录 1」。<br>- 对于加粗、斜体等特殊字体，你需要去除这些格式，保留文字内容即可，不需要用 Markdown 语法。<br>- 对于插入的 \begin&#123;figure&#125;、\begin&#123;table&#125; 内容，你只需将其 \caption&#123;&#125; 翻译出来，放在合适的行间占位即可。你可以改为「图1：xxxxx」。<br><br>请翻译以下文本：<br></code></pre></td></tr></table></figure><h3 id="中文论文润色">中文论文润色</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">作为一名中文学术论文写作改进助理，你的任务是改进所提供文本的拼写、语法、清晰、简洁和整体可读性，同时分解长句，减少重复。<br><br>请润色以下文本：<br></code></pre></td></tr></table></figure><h2 id="拒稿意见撰写">拒稿意见撰写</h2><p>除了用于审稿，还可以用于在提交之前检查自己的论文。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs text">Please help me write a rejection review for a paper in the field of artificial intelligence. The review should be structured as follows:<br><br>## Summary<br>Provide a brief overview of the paper, including its research topic, methods, and main findings.<br><br>## Strengths<br><br>Highlight any positive aspects of the paper, such as novelty of the research question, appropriateness of methodology, or potential significance of the work.<br><br>## Weaknesses<br>Identify the paper&#x27;s major shortcomings. These could include issues such as flaws in the theoretical framework, insufficient or inappropriate experimental design, weaknesses in data analysis, lack of clarity in presentation, or unconvincing conclusions. Be specific about what aspects of the paper are problematic and why they detract from its overall quality.<br><br>## Suggestions for Improvement<br>Offer constructive feedback on how the author(s) can improve the paper. These suggestions should address the key weaknesses and provide guidance on how to refine the research design, strengthen data analysis, or enhance the discussion of results. Be detailed enough to give the authors a clear path for improvement.<br><br>The review should be professional, objective, and concise, focusing on clear rejection reasons while offering helpful guidance for future revisions.<br></code></pre></td></tr></table></figure><h2 id="github-issue-回复撰写">GitHub Issue 回复撰写</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs text">我的 GitHub 仓库收到了一条 Issue，内容如下：<br><br><br><br>请你帮我撰写一个有礼貌的英文回复，可以带上l Emoji 符号。你可以按照如下思路进行回复：<br><br>- 首先感谢他对我们工作的关注和认可；<br>- 关于他的问题<br></code></pre></td></tr></table></figure><h2 id="kimi-速读论文">Kimi 速读论文</h2><p>可用于 KimiChat、GPT-4o，效果都不错。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs text">请你作为一个经验丰富的人工智能专业博士研究生，详细阅读这一篇论文，并用1000-1500字左右的篇幅对论文进行深度解读。你需要仔细思考并回答以下问题：<br><br>## 1. 一句话概述：<br><br>## 2. Abstract: 论文试图解决什么问题？有什么贡献？<br><br>## 3. Introduction: 论文的动机是什么？请仔细梳理整个故事逻辑。<br><br>## 4. Method: 解决方案是什么？请仔细梳理论文存在的每个步骤、公式、策略。<br><br>## 5. Experiment: 主实验是什么？结果如何？还有哪些分析实验？结果如何？<br><br>在回答格式上，请注意使用Markdown格式，适当加入列表、加粗等排版元素。使用二级标题对应以上五个问题，清晰划分不同部分。在回答后三个问题的过程中，你可以引用论文中的细节内容、关键数据和实验结果，帮助我清楚地理解论文的创新性贡献。引用原文时请使用blockquote的引用格式。使用中文回复，学术名词可以用英文补充。<br></code></pre></td></tr></table></figure><h2 id="专业-prompt-生成">专业 Prompt 生成</h2><p>适合任务相对复杂，而自己经验或专业知识比较少，导致<strong>无法准确描述需求</strong>的情况；或者你想要ChatGPT 生成一个<strong>更个人化、定制化</strong>的回答时。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs text">I want you to become my Expert Prompt Creator. Your goal is to help me craft the best possible prompt for my needs. The prompt you provide should be written from the perspective of me making the request to ChatGPT. Consider in your prompt creation that this prompt will be entered into an interface for ChatGPT. The process is as follows:<br>1. You will generate the following sections:<br><br>Prompt:<br>&#123;provide the best possible prompt according to my request&#125;<br><br>Critique:<br>&#123;provide a concise paragraph on how to improve the prompt. Be very critical in your response&#125;<br><br>Questions:<br>&#123;ask any questions pertaining to what additional information is needed from me to improve the prompt (max of 3). If the prompt needs more clarification or details in certain areas, ask questions to get more information to include in the prompt&#125;<br><br>2. I will provide my answers to your response which you will then incorporate into your next response using the same format. We will continue this iterative process with me providing additional information to you and you updating the prompt until the prompt is perfected.<br>Remember, the prompt we are creating should be written from the perspective of me making a request to ChatGPT. Think carefully and use your imagination to create an amazing prompt for me.<br><br>You&#x27;re first response should only be a greeting to the user and to ask what the prompt should be about.<br></code></pre></td></tr></table></figure><p>上面那段话直接发给 ChatGPT无需修改，然后等它回复之后再给它一个最初级的 Prompt 让他修改，例如：</p><ul><li>让 ChatGPT 写一篇关于初学者如何学习 Pytorch 的教程</li><li>让 ChatGPT 帮我用 Python 写一个实现计算器程序</li></ul><p>再根据它的回复逐步提出新问题，逐步完善，最终得到一个完整提示。</p><h2 id="文生图-prompt-优化">文生图 Prompt 优化</h2><blockquote><p>一些灵感：<a href="https://wgmimedia.com/best-dalle-prompts/">20 BestDALL·E 3 Use Cases and Prompts - WGMI Media</a></p></blockquote><p>使用方法同上。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs text">I want you to become my Expert Prompt Creator. Your goal is to help me optimize the following image generation prompt by making it more specific and detailed in terms of setting, mood, perspective, lighting, and other key elements that would improve the visual result. Use descriptive adjectives and consider any actions or movement to guide composition. Keep the prompt balanced, avoiding excessive details, and specify the desired style or theme if needed. The prompt you provide should be written from the perspective of me making the request to ChatGPT / DALLE-3. Consider in your prompt creation that this prompt will be entered into an interface for ChatGPT / DALLE-3. The process is as follows:<br>1. You will generate the following sections:<br><br>Prompt:<br>&#123;provide the best possible prompt according to my original prompt&#125;<br><br>Critique:<br>&#123;provide a concise paragraph on how to improve the prompt. Be very critical in your response&#125;<br><br>Questions:<br>&#123;ask any questions pertaining to what additional information is needed from me to improve the prompt (max of 3). If the prompt needs more clarification or details in certain areas, ask questions to get more information to include in the prompt&#125;<br><br>2. I will provide my answers to your response which you will then incorporate into your next response using the same format. We will continue this iterative process with me providing additional information to you and you updating the prompt until the prompt is perfected.<br>Remember, the prompt we are creating should be written from the perspective of me making a request to ChatGPT / DALLE-3. Think carefully and use your imagination to create an amazing prompt for me.<br><br>You&#x27;re first response should only be a greeting to the user and to ask what the prompt should be about.<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>科研笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>OpenAI o1 系列模型背后的技术猜测</title>
    <link href="/OpenAI-o1-Series.html"/>
    <url>/OpenAI-o1-Series.html</url>
    
    <content type="html"><![CDATA[<p>首先是两篇官方博客原文：</p><ul><li><p><ahref="https://openai.com/index/introducing-openai-o1-preview/">IntroducingOpenAI o1 | OpenAI</a></p></li><li><p><ahref="https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/">OpenAIo1-mini | OpenAI</a></p></li></ul><h2 id="openai-o1-背后的技术">OpenAI o1 背后的技术</h2><p>结合以下几个现象，来猜测一下 OpenAI o1 背后的秘密：</p><p>第一，是两个月前发布的 <strong>CriticGPT</strong>，探索了 Text Critic作为反馈或奖励，会优于 Numerical Value 作为奖励的效果。P.S. Google前不久也发了一篇 GenRM，看来也在往这个方向上靠。</p><blockquote><p><a href="https://arxiv.org/abs/2407.00215">LLM Critics Help Catch LLMBugs</a></p></blockquote><p>第二，是模型的<strong>推理速度和价格</strong>，官方博客中的一个演示中，GPT-4o用时 3s，o1-mini 用时 9s，o1-preview 用时15s。而在官网的价格表中，我汇总了下表。如果认为 4o 和 o1的规模相近，4o-mini 和 o1-mini的规模相近，那么<strong>增加的时间和成本只可能是 Test-time Compute带来的</strong>。</p><table><thead><tr class="header"><th style="text-align: center;">模型</th><th style="text-align: center;">输入价格</th><th style="text-align: center;">输出价格</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">gpt-4o-2024-08-06</td><td style="text-align: center;">$2.50 / 1M input tokens</td><td style="text-align: center;">$10.00 / 1M output tokens</td></tr><tr class="even"><td style="text-align: center;">gpt-4o-mini-2024-07-18</td><td style="text-align: center;">$0.150 / 1M input tokens</td><td style="text-align: center;">$0.600 / 1M output tokens</td></tr><tr class="odd"><td style="text-align: center;">o1-preview-2024-09-12</td><td style="text-align: center;">$15.00 / 1M input tokens</td><td style="text-align: center;">$60.00 / 1M output tokens</td></tr><tr class="even"><td style="text-align: center;">o1-mini-2024-09-12</td><td style="text-align: center;">$3.00 / 1M input tokens</td><td style="text-align: center;">$12.00 / 1M output tokens</td></tr></tbody></table><p>第三，是<strong>模型表现</strong>，博客中提到 o1在推理密集的复杂任务上取得了增益，但在文本相关的简单任务没有太多区别。这也印证了第一点，<strong>这种Critic训练策略可能只适合于推理任务</strong>，文本任务可能区分度没有那么大（分不出好坏，不好给奖励），导致没优势。</p><blockquote><p>Similar to o1-preview, o1-mini is preferred to GPT-4o inreasoning-heavy domains, but is not preferred to GPT-4o inlanguage-focused domains.</p><p>与 o1-preview 类似，o1-mini 在偏重推理的领域优于GPT-4o，但在偏重语言的领域则不优于 GPT-4o。</p></blockquote><p><img src="/img/blog/OpenAI-o1-Series-images/fig1.jpg" alt="OpenAI 在各个领域上做了人类偏好的评估，可以看到前两个任务上，o1 vs. 4o 胜率低于 50%" style="zoom: 67%;" /></p><p>第四，是博客中给出的 <strong>Test-time ScalingLaw</strong>。关键是测试时也能 Scaling！这就否定了一些网友关于Reflection 式输出的猜测（总所周知，前几天翻车的 Reflection-70B就是<strong>标注了大量 CoT数据</strong>，让模型在输出时模仿更多的反思步骤，「内置CoT」刷爆了排行榜）。<strong>如果采用这种「简单内置CoT」的方式，是很难自由地控制 Test-time Compute 的！</strong></p><blockquote><p>People have discovered a while ago that prompting the model to “thinkstep by step” boosts performance. But training the model to do this, endto end with trial and error, is far more reliable and — as we’ve seemwith games like Go or Dota — can generate extremely impressiveresults.</p><p>人们早已发现，提示模型"逐步思考"可以提升其表现。但是通过端到端的试错方式来训练模型这样做，会使结果更加可靠，而且——正如我们在围棋或Dota 等游戏中看到的那样——可以产生极其令人印象深刻的结果。</p></blockquote><p><img src="/img/blog/OpenAI-o1-Series-images/fig2.jpg" alt="OpenAI 分别测试了训练和测试时的 Compute 影响" style="zoom: 67%;" /></p><p>最后，是一些其他细节。这里列举出来：</p><ul><li>OpenAI向用户<strong>隐藏了内在思维过程，</strong>目前网页端显示的是总结后的CoT 过程；</li><li>o1 的输入 tokens 计算方式与 GPT-4o相同，<strong>使用相同的分词器</strong>；</li><li>网友测试，模型的思考时间时快时慢，说明其<strong>可以决定何时退出思考</strong>；</li><li>网友测试，模型的思考过程中，经常会有 Hmmm, Wait这类词出现，并且在<strong>发现错误后立刻纠正</strong>；</li><li>网友测试，o1-mini 能够探索更多的思考链条，相较于o1-preview（不保真）；</li><li>网友测试，o1 对提示词更敏感，一些传统的 Prompt策略（如角色扮演、类比举例等）反而会起到副作用，<strong>而简单让其回答效果更好（因为会默认进行思考）</strong>；</li><li>o1 系列的模型在网页端<strong>不支持修改系统提示词</strong>；</li><li>o1系列的模型目前<strong>不支持图像输入、函数调用、结构化输出</strong>，但是据说之后会有。</li></ul><p><img src="/img/blog/OpenAI-o1-Series-images/fig3.jpg" alt="群友的测试结果，笑死，思考真的有用" style="zoom:50%;" /></p><hr /><p>那么，OpenAI o1 背后的技术也就呼之欲出了~</p><ol type="1"><li>新模型是在 GPT-4o 上进行的继续训练，具体的训练策略应该就是流传的<strong>Self-play RL / Q*</strong>。说人话就是：让模型进行在线探索 /MCTS 采样，探索过程中给予不同路径不同的 Critic /奖励，从中优化模型。</li><li>新模型在<strong>推理的时候也用了相同的策略去采样和奖励</strong>，所以会慢几倍！例如，并行采样+ 并行 Critic 反馈 + 投票 Summarize出最终输出，理论上<strong>至少是三倍时间，多倍推理成本</strong>（o1 相比4o 提高了 6 倍，o1-mini 相比 4o-mini 提高了 20倍，也很合理，更弱的模型就采样更多次）</li><li>Critic 反馈的过程中，Action Model 和 Critic Model可能会有多轮交互（Retry），并<strong>最终由 Critic Model 或 SummarizeModel决定是否退出</strong>。因此时间会大于三倍，并且根据问题的复杂度上升！</li></ol><h2 id="相关论文">相关论文</h2><p>附上一些可能的相关论文~ 会持续更新...</p><p><strong>Self-Critic 相关论文：</strong></p><ul><li>标题：LLM Critics Help Catch LLM Bugs<ul><li>链接：<ahref="https://arxiv.org/abs/2407.00215">https://arxiv.org/abs/2407.00215</a></li><li>简介：OpenAI 的CriticGPT，通过训练“批评者”模型来帮助人类更准确地评估由大型语言模型（LLM）生成的代码。这些批评者模型本身也是通过强化学习从人类反馈中训练出来的LLM，能够用自然语言突出代码中的问题。研究发现，CriticGPT在识别代码中自然出现的错误方面，比人类批评者更受青睐，并且在捕获错误方面比人类承包商更有效。</li></ul></li><li>标题：Generative Verifiers: Reward Modeling as Next-Token Prediction<ul><li>链接：<ahref="https://arxiv.org/abs/2408.15240">https://arxiv.org/abs/2408.15240</a><br /></li><li>简介：Google出品，介绍了一种新型的验证器（Verifier），称为生成性验证器（GenerativeVerifiers，简称GenRM），它通过将验证问题转化为下一个词（next-token）预测任务来提高大型语言模型（LLMs）的推理性能。这种方法与传统的基于判别式分类的训练不同，它充分利用了预训练LLMs 的文本生成能力。</li></ul></li></ul><p><strong>MCTS &amp; Self-Improve 相关论文：</strong></p><ul><li>标题：Recursive Introspection: Teaching Language Model Agents How toSelf-Improve<ul><li>链接：<ahref="http://arxiv.org/abs/2407.18219">http://arxiv.org/abs/2407.18219</a></li><li>简介：提出 RISE递归自省，目的是提高模型在测试时连续尝试的响应的最终正确率（Test-timeself-improve）。为此，将连续响应建模为 MDP，用 BoN或者强模型的输出构造训练。</li></ul></li><li>标题：Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers<ul><li>链接：<ahref="http://arxiv.org/abs/2408.06195">http://arxiv.org/abs/2408.06195</a><br /></li><li>简介：提出 rStar，增强 MCTS 的 actionspace，同时用同一个模型验证生成的每个轨迹，类似 Self-Verify。</li></ul></li><li>标题：Q*: Improving Multi-step Reasoning for LLMs with DeliberativePlanning<ul><li>链接：<ahref="http://arxiv.org/abs/2406.14283">http://arxiv.org/abs/2406.14283</a><br /></li><li>简介：学习 Q-value 模型作为预估奖励的启发式函数，指导 LLM搜索并选择推理步骤。</li></ul></li><li>标题：AlphaMath Almost Zero: process Supervision without process<ul><li>链接：<ahref="http://arxiv.org/abs/2405.03553">http://arxiv.org/abs/2405.03553</a><br /></li><li>简介：将 Alphago 自我对弈阶段的思路引入Math，不需要人工标注的解题中间过程，使用 MCTS + 过程奖励估算。</li></ul></li><li>标题：Training Large Language Models for Reasoning through ReverseCurriculum Reinforcement Learning<ul><li>链接：<ahref="http://arxiv.org/abs/2402.05808">http://arxiv.org/abs/2402.05808</a></li><li>简介：ICML 上的一篇论文，提出 R^3训练框架，逆向课程学习，仅使用结果监督的信号来模拟过程监督的效果。</li></ul></li></ul><p><strong>Test-time Scaling Law 相关论文：</strong></p><ul><li>标题：Scaling LLM Test-Time Compute Optimally can be More Effectivethan Scaling Model Parameters<ul><li>链接：<ahref="https://arxiv.org/abs/2408.03314">https://arxiv.org/abs/2408.03314</a><br /></li><li>简介：Google 出品，研究了 LLM 中推理时间计算的 ScalingLaw，回答了以下问题：如果允许 LLM使用固定量的推理时计算量，那么它能在多大程度上提高其在具有挑战性的任务上的性能？</li></ul></li><li>标题：An Empirical Analysis of Compute-Optimal Inference forProblem-Solving with Language Models<ul><li>链接：<ahref="https://arxiv.org/abs/2408.00724">https://arxiv.org/abs/2408.00724</a><br /></li><li>简介：探讨了在有限计算资源下，如何配置 LLMs以实现最优的推理性能。实验表明，使用 REBASE 算法的较小语言模型（如Llemma-7B）在计算资源减半的情况下，能够达到与较大模型（如Llemma-34B）相当的准确性。</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>前沿热点</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLMs</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>手撕经典算法 #4 经典函数篇</title>
    <link href="/Manual-Coding-4.html"/>
    <url>/Manual-Coding-4.html</url>
    
    <content type="html"><![CDATA[<p>本文对深度学习中经典的函数进行了简单的实现和注释。包括：</p><ul><li>损失函数（MSE、CE、BCE、KL、Focal）</li><li>激活函数（Sigmoid、Tanh、ReLU、LeakyReLU、ELU、Swish、GeLU、SwiGLU、Softmax）</li><li>指标计算（PPL、ROUGE、BLEU）</li></ul><h2 id="损失函数">损失函数</h2><h3 id="mse-loss">MSE Loss</h3><p>均方误差（Mean SquaredError，MSE）衡量预测值与真实值的平方差均值，是<strong>回归任务</strong>中最常用的损失函数：</p><p><span class="math display">\[L = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2\]</span></p><p>其中 <span class="math inline">\(y_i\)</span> 为真实值，<spanclass="math inline">\(\hat{y}_i\)</span> 为预测值。其梯度计算为 <spanclass="math inline">\(\frac{\partial L}{\partial \hat{y}_i} =\frac{2}{n}(\hat{y}_i -y_i)\)</span>，具有凸函数的良好优化特性，可导且处处平滑，适合梯度下降。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mse_loss</span>(<span class="hljs-params">y_true, y_pred</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算均方误差损失</span><br><span class="hljs-string">    :param y_true: 真实值数组，形状 (n_samples, )</span><br><span class="hljs-string">    :param y_pred: 预测值数组，形状 (n_samples, )</span><br><span class="hljs-string">    :return: 标量损失值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    squared_error = (y_true - y_pred) ** <span class="hljs-number">2</span><br>    <span class="hljs-keyword">return</span> np.mean(squared_error)<br><br><span class="hljs-comment"># 示例用法</span><br>y_true = np.array([<span class="hljs-number">2.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>])<br>y_pred = np.array([<span class="hljs-number">1.5</span>, <span class="hljs-number">3.8</span>, <span class="hljs-number">4.9</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;MSE Loss: <span class="hljs-subst">&#123;mse_loss(y_true, y_pred):<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)  <span class="hljs-comment"># 输出 MSE Loss: 0.0867</span><br></code></pre></td></tr></table></figure><h3 id="ce-loss">CE Loss</h3><p>交叉熵（CrossEntropy）衡量两个概率分布间的差异，常用于<strong>多分类任务</strong>。给定真实分布<span class="math inline">\(P\)</span> 和预测分布 <spanclass="math inline">\(Q\)</span>：</p><p><span class="math display">\[H(P, Q) = -\sum_{i=1}^N P(x_i) \log Q(x_i)\]</span></p><p>在分类任务中，真实标签常采用 <strong>one-hot编码</strong>，公式简化为：</p><p><span class="math display">\[L = -\frac{1}{N}\sum_{i=1}^N \sum_{i=1}^C y_i \log \hat{y}_i\]</span></p><p>其中 <span class="math inline">\(C\)</span> 为类别总数，<spanclass="math inline">\(\hat{y}_i\)</span> 需经过 <strong>Softmax归一化</strong>。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cross_entropy</span>(<span class="hljs-params">y_true, y_pred</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算交叉熵损失（需配合 Softmax 使用），带数值稳定处理</span><br><span class="hljs-string">    :param y_true: one-hot 编码的真实标签，形状 (n_samples, n_classes)</span><br><span class="hljs-string">    :param y_pred: 模型输出的 logits，形状 (n_samples, n_classes)</span><br><span class="hljs-string">    :return: 标量损失值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 数值稳定处理：减去最大值防止指数爆炸</span><br>    exps = np.exp(y_pred - np.<span class="hljs-built_in">max</span>(y_pred, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>))<br>    softmax_output = exps / np.<span class="hljs-built_in">sum</span>(exps, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br>    <br>    <span class="hljs-comment"># 避免 log(0) 导致数值问题</span><br>    epsilon = <span class="hljs-number">1e-7</span><br>    clipped = np.clip(softmax_output, epsilon, <span class="hljs-number">1</span> - epsilon)<br>    <br>    <span class="hljs-comment"># 只计算真实类别对应的损失</span><br>    n_samples = y_true.shape[<span class="hljs-number">0</span>]<br>    log_likelihood = -np.log(clipped[<span class="hljs-built_in">range</span>(n_samples), y_true.argmax(axis=<span class="hljs-number">1</span>)])<br>    <span class="hljs-keyword">return</span> np.mean(log_likelihood)<br><br><span class="hljs-comment"># 示例用法（三分类问题）</span><br>y_true = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]])  <span class="hljs-comment"># one-hot 编码</span><br>y_pred = np.array([[<span class="hljs-number">2.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.5</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">0.2</span>]])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;CrossEntropy Loss: <span class="hljs-subst">&#123;cross_entropy(y_true, y_pred):<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)  <span class="hljs-comment"># 输出 0.3184</span><br></code></pre></td></tr></table></figure><h3 id="bce-loss">BCE Loss</h3><p>二元交叉熵（Binary CrossEntropy）处理的是<strong>二分类问题</strong>，其归一化的方式从 Softmax替换为Sigmoid，并且<strong>每个类别的概率独立计算</strong>（不像交叉熵仅计算真实类别的损失）：<span class="math display">\[L =  -\frac{1}{N}\sum_{i=1}^N \left[ y_i \cdot \log(\sigma(x_i)) +(1-y_i) \cdot \log(1-\sigma(x_i)) \right]\]</span> 此外，BCE也可以用于<strong>多分类多标签</strong>任务，此时需要将每个类别看作为 0或 1 的二分类问题。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">binary_cross_entropy</span>(<span class="hljs-params">y_true, y_pred</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算二元交叉熵损失（需配合 Sigmoid 使用）</span><br><span class="hljs-string">    :param y_true: 二分类的真实标签（0 或 1），形状 (n_samples, n_classes)</span><br><span class="hljs-string">    :param y_pred: 模型输出的 logits，形状 (n_samples, n_classes)</span><br><span class="hljs-string">    :return: 标量损失值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 应用 Sigmoid 将 logits 转换为概率</span><br>    sigmoid_output = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pred))<br>    <br>    <span class="hljs-comment"># 避免 log(0) 导致的数值问题</span><br>    epsilon = <span class="hljs-number">1e-7</span><br>    clipped = np.clip(sigmoid_output, epsilon, <span class="hljs-number">1</span> - epsilon)<br>    <br>    <span class="hljs-comment"># 计算每个样本每个类别的损失</span><br>    loss_per_element = - (y_true * np.log(clipped) + (<span class="hljs-number">1</span> - y_true) * np.log(<span class="hljs-number">1</span> - clipped))<br>    <br>    <span class="hljs-comment"># 对所有元素取平均损失</span><br>    <span class="hljs-keyword">return</span> np.mean(loss_per_element)<br><br><span class="hljs-comment"># 示例用法（两个样本，三分类多标签问题）</span><br>y_true = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])  <span class="hljs-comment"># 多标签真实值</span><br>y_pred = np.array([[<span class="hljs-number">2.0</span>, <span class="hljs-number">1.0</span>, -<span class="hljs-number">1.0</span>], [<span class="hljs-number">0.5</span>, <span class="hljs-number">3.0</span>, -<span class="hljs-number">0.5</span>]])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;BCE Loss: <span class="hljs-subst">&#123;binary_cross_entropy(y_true, y_pred):<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)  <span class="hljs-comment"># 输出 0.1955</span><br></code></pre></td></tr></table></figure><h3 id="kl-散度">KL 散度</h3><p>KL 散度（Kullback-Leibler Divergence）衡量两个概率分布 <spanclass="math inline">\(P\)</span> 和 <spanclass="math inline">\(Q\)</span> 的差异程度：</p><p><span class="math display">\[D_{KL}(P \parallel Q) = \sum_{i=1}^N P(x_i) \log \frac{P(x_i)}{Q(x_i)}\]</span></p><p>其性质包括：</p><ul><li>非对称性：<span class="math inline">\(D_{KL}(P \parallel Q) \neqD_{KL}(Q \parallel P)\)</span></li><li>非负性：<span class="math inline">\(D_{KL} \geq 0\)</span>，当且仅当P=Q 时等于 0</li><li>与交叉熵的关系：<span class="math inline">\(D_{KL}(P \parallel Q) =H(P, Q) - H(P)\)</span></li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">kl_divergence</span>(<span class="hljs-params">p, q</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算两个离散分布的KL散度</span><br><span class="hljs-string">    :param p: 真实概率分布，形状 (n_classes, )</span><br><span class="hljs-string">    :param q: 预测概率分布，形状 (n_classes, )</span><br><span class="hljs-string">    :return: 标量散度值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 过滤零元素避免数值问题</span><br>    mask = (p != <span class="hljs-number">0</span>)<br>    p = p[mask]<br>    q = q[mask]<br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(p * np.log(p / q))<br><br><span class="hljs-comment"># 示例用法（概率分布差异对比）</span><br>P = np.array([<span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>])<br>Q1 = np.array([<span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>])<br>Q2 = np.array([<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;KL(P||Q1): <span class="hljs-subst">&#123;kl_divergence(P, Q1):<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)  <span class="hljs-comment"># 输出 0.0000</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;KL(P||Q2): <span class="hljs-subst">&#123;kl_divergence(P, Q2):<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)  <span class="hljs-comment"># 输出 0.0204</span><br></code></pre></td></tr></table></figure><h3 id="focal-损失">Focal 损失</h3><p>通过调节难易样本权重解决数据倾斜问题，适用于<strong>长尾分布场景的二分类问题</strong>：</p><p><span class="math display">\[FL = -\alpha_t (1-p_t)^\gamma \log(p_t)\]</span></p><p>其中：</p><ul><li><p><span class="math inline">\(p_t\)</span>是模型对真实类别的预测概率： <span class="math display">\[p_t =  \begin{cases}    p &amp; \text{正样本} \\    1-p &amp; \text{负样本}  \end{cases}\]</span></p></li><li><p><span class="math inline">\(\alpha \in[0,1]\)</span>：类别平衡因子，通常为稀有类别分配更高权重（如 <spanclass="math inline">\(\alpha=0.25\)</span>）。</p></li><li><p><span class="math inline">\(\gamma \geq 0\)</span>：困难样本聚焦参数，调整难易样本的权重比例（通常 $ $ ）。</p></li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">focal_loss</span>(<span class="hljs-params">y_true, y_pred, alpha=<span class="hljs-number">0.25</span>, gamma=<span class="hljs-number">2.0</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算二分类Focal Loss</span><br><span class="hljs-string">    :param y_true: 真实标签 (n_samples, )</span><br><span class="hljs-string">    :param y_pred: 预测概率 (n_samples, )</span><br><span class="hljs-string">    :param alpha: 类别平衡因子</span><br><span class="hljs-string">    :param gamma: 困难样本聚焦参数</span><br><span class="hljs-string">    :return: 标量损失值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    epsilon = <span class="hljs-number">1e-7</span><br>    y_pred = np.clip(y_pred, epsilon, <span class="hljs-number">1</span> - epsilon)<br>    p_t = y_true * y_pred + (<span class="hljs-number">1</span> - y_true) * (<span class="hljs-number">1</span> - y_pred)<br>    alpha_factor = y_true * alpha + (<span class="hljs-number">1</span> - y_true) * (<span class="hljs-number">1</span> - alpha)<br>    loss = -alpha_factor * (<span class="hljs-number">1</span> - p_t) ** gamma * np.log(p_t)<br>    <span class="hljs-keyword">return</span> np.mean(loss)<br><br><span class="hljs-comment"># 示例用法（处理文本分类中的长尾分布）</span><br>y_true = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>])  <span class="hljs-comment"># 多数类别为1</span><br>y_pred = np.array([<span class="hljs-number">0.9</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.1</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Focal Loss: <span class="hljs-subst">&#123;focal_loss(y_true, y_pred):<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)  <span class="hljs-comment"># 输出约0.032</span><br></code></pre></td></tr></table></figure><h2 id="激活函数">激活函数</h2><h3 id="sigmoid">Sigmoid</h3><p>输出区间 (0,1)，符合概率分布特性，常用于二分类输出层。表达式为：<span class="math display">\[\sigma(x) = \frac{1}{1+e^{-x}}\]</span> 导数： <span class="math display">\[\sigma&#39;(x) = \sigma(x)(1-\sigma(x))\]</span> 缺点：</p><ul><li>输入较大或较小时候梯度接近于0，容易导致<strong>梯度消失</strong>（且导数最大值为0.25，更新效率不高）；</li><li>函数输出<strong>不是以 0为中心的</strong>，梯度可能就会向特定方向移动，从而降低权重更新的效率；</li><li>执行指数运算，计算机运行得较慢，比较消耗计算资源。</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">x</span>):</span><br>    x = np.clip(x, -<span class="hljs-number">50</span>, <span class="hljs-number">50</span>)  <span class="hljs-comment"># 防止数值溢出</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br></code></pre></td></tr></table></figure><h3 id="tanh">Tanh</h3><p>输出区间 <spanclass="math inline">\((-1,1)\)</span>，以零为重心，缓解梯度偏移问题，表达式为：<span class="math display">\[\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\]</span> 导数： <span class="math display">\[\tanh&#39;(x) = 1 - \tanh^2(x)\]</span> 缺点：</p><ul><li>仍然存在<strong>梯度饱和</strong>的问题：但 <spanclass="math inline">\(x\)</span> 进入饱和区（saturationregion）时，其<strong>导数值趋近于零</strong>，最终无法有效更新网络参数；</li><li>依然进行的是指数运算，比较消耗计算资源。</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tanh</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> np.tanh(x)  <span class="hljs-comment"># 内置优化实现比手动计算更稳定</span><br></code></pre></td></tr></table></figure><h3 id="relu">ReLU</h3><p>表达式为： <span class="math display">\[\text{ReLU}(x) = \max(0, x)\]</span> 导数： <span class="math display">\[\text{ReLU}&#39;(x) = \begin{cases}1 &amp; x &gt; 0 \\0 &amp; x \leq 0\end{cases}\]</span> 优点：</p><ul><li>ReLU解决了<strong>梯度消失</strong>的问题，当输入值为正时，神经元不会饱和（梯度始终为1）；</li><li>由于 ReLU 线性、非饱和的性质，在 SGD 中能够快速收敛；</li><li>计算复杂度低，不需要进行指数运算。</li></ul><p>缺点：</p><ul><li>Dead ReLU问题：负区间梯度归零，不再对任何数据有所响应，导致相应参数永远不会被更新；</li><li>与 Sigmoid 一样，其输出不是以 0 为中心的。</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">relu</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> np.maximum(<span class="hljs-number">0</span>, x)  <span class="hljs-comment"># 简洁的向量化实现</span><br></code></pre></td></tr></table></figure><h3 id="leaky-relu">Leaky ReLU</h3><p>缓解神经元死亡问题（负区间保留小梯度 <spanclass="math inline">\(\alpha\)</span>，常见取值为 <spanclass="math inline">\(0.01-0.3\)</span>），表达式为： <spanclass="math display">\[\text{LeakyReLU}(x) = \begin{cases}x &amp; x &gt; 0 \\\alpha x &amp; x \leq 0\end{cases} \quad (\alpha \in (0,1))\]</span></p><p>导数： <span class="math display">\[\text{LeakyReLU}&#39;(x) = \begin{cases}1 &amp; x &gt; 0 \\\alpha &amp; x \leq 0\end{cases}\]</span> 代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">leaky_relu</span>(<span class="hljs-params">x, alpha=<span class="hljs-number">0.01</span></span>):</span><br>    <span class="hljs-keyword">return</span> np.where(x &gt; <span class="hljs-number">0</span>, x, alpha * x)  <span class="hljs-comment"># 条件表达式实现分支逻辑</span><br></code></pre></td></tr></table></figure><h3 id="elu">ELU</h3><p>ELU 采用比 ReLU 更平滑的过渡，保持负区间微小梯度（<spanclass="math inline">\(\alpha\)</span>控制信息保留程度）解决神经元死亡问题。最重要的是，可以控制<strong>激活函数的输出均值接近于零</strong>（假设输入分布为标准正态输入），使正常梯度更接近于单位自然梯度，从而加快学习速度。表达式为：<span class="math display">\[\text{ELU}(x) = \begin{cases}x &amp; x &gt; 0 \\\alpha(e^x - 1) &amp; x \leq 0\end{cases}\]</span></p><p>导数： <span class="math display">\[\text{ELU}&#39;(x) = \begin{cases}1 &amp; x &gt; 0 \\\text{ELU}(x) + \alpha &amp; x \leq 0\end{cases}\]</span> 缺点：</p><ul><li>ELU 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息；</li><li>计算的时需要计算指数，计算效率低。</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">elu</span>(<span class="hljs-params">x, alpha=<span class="hljs-number">1.0</span></span>):</span><br>    <span class="hljs-keyword">return</span> np.where(x &gt; <span class="hljs-number">0</span>, x, alpha * (np.exp(x) - <span class="hljs-number">1</span>))  <span class="hljs-comment"># 负区间指数计算</span><br></code></pre></td></tr></table></figure><h3 id="swish">Swish</h3><p>Google Brain (2017)提出的自门控的智能激活，结合了<strong>线性与非线性</strong>特性的激活函数：<span class="math display">\[\text{Swish}(x) = x \cdot \sigma(\beta x)\]</span> 导数： <span class="math display">\[\text{Swish}&#39;(x) = \text{Swish}(x) + \sigma(\beta x)(1 -\text{Swish}(x))\]</span> 优点： - 自适应门控机制（通过 sigmoid 调整 <spanclass="math inline">\(\beta\)</span>，Swish 可以模拟不同形状）； -处处平滑可微，在全体实数域上连续可导（优于 ReLU 系列，<spanclass="math inline">\(x=0\)</span> 处不可导）； - 在深层网络中表现优于ReLU（Google 实验证明）。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">swish</span>(<span class="hljs-params">x, beta=<span class="hljs-number">1.0</span></span>):</span><br>    <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-beta * x)))<br></code></pre></td></tr></table></figure><h3 id="gelu">GeLU</h3><p>高斯误差线性单元（Gaussian Error LinearUnit）是一种结合了高斯分布特性的激活函数，旨在通过<strong>概率建模</strong>的方式平滑地调整神经元的激活状态，<strong>被BERT、GPT 采用</strong>。其数学表达式为：</p><p><span class="math display">\[\text{GeLU}(x) = x \cdot \Phi(x)\]</span></p><p>其中，<span class="math inline">\(\Phi(x)\)</span>是标准高斯分布的累积分布函数（CDF）。为了高效计算，常采用近似公式：<span class="math display">\[\text{GeLU}(x) \approx 0.5x\left(1 + \tanh\left(\sqrt{\frac{2}{\pi}}(x +0.044715x^3)\right)\right)\]</span></p><p>优点：</p><ul><li>跟 Swish 长得非常像，都是平滑版的ReLU（保留非线性同时可微分）；</li><li>通过概率权重调整激活强度，避免 ReLU的神经元死亡现象（负值完全被抑制）。</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gelu</span>(<span class="hljs-params">x</span>):</span><br><span class="hljs-comment"># 使用近似公式</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * x * (<span class="hljs-number">1</span> + np.tanh(np.sqrt(<span class="hljs-number">2</span>/np.pi) * (x + <span class="hljs-number">0.044715</span> * x**<span class="hljs-number">3</span>)))<br></code></pre></td></tr></table></figure><h3 id="swiglu">SwiGLU</h3><p>SwiGLU 是一种结合了 Swish激活函数和<strong>门控线性单元</strong>（GLU）的复合激活函数，近年来被广泛应用于大型语言模型（LLM）<strong>如LLaMA、PaLM 等</strong>。</p><p>GLU 的原始形式为： <span class="math display">\[\text{GLU}(x) = \sigma(W x + b) \otimes (Vx)\]</span> 其中 <span class="math inline">\(\otimes\)</span>是逐元素乘法，<span class="math inline">\(\sigma\)</span> 是 Sigmoid函数，用于门控信息流。而 SwiGLU 将 GLU 中的 Sigmoid 替换为 Swish： <spanclass="math display">\[\text{SwiGLU}(x) = \text{Swish}(W x + b) \otimes (Vx)\]</span></p><p>在实际实现中，SwiGLU 通常被整合到前馈网络（FFN）中，传统的 FFN可以记为： <span class="math display">\[\text{FFN}_\text{ReLU} = (\text{ReLU}(W_1 x+b))W_2 + c\]</span> 而 SwiGLU 通过 <span class="math inline">\(W_1x\)</span>完成升维操作的同时，还会用 <span class="math inline">\(Vx\)</span>完成门控操作，最后再用 <span class="math inline">\(W_2\)</span> 降维：<span class="math display">\[\text{FFN}_\text{SwiGLU} = (\text{Swish}(W_1 x + b) \otimes  (Vx))W_2 +c\]</span> 优点：</p><ul><li>Swish的连续梯度缓解了梯度消失问题，而门控机制进一步平衡了信息流，使深层网络训练更稳定；</li><li>实验表明其计算效率优于 GeLU，且下游任务表现更好。</li></ul><p>Llama 中的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LlamaMLP</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">        self,</span></span><br><span class="hljs-params"><span class="hljs-function">        hidden_size: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># 4096</span></span></span><br><span class="hljs-params"><span class="hljs-function">        intermediate_size: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># 11008</span></span></span><br><span class="hljs-params"><span class="hljs-function">    </span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=<span class="hljs-literal">True</span>)<br>        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=<span class="hljs-literal">False</span>)<br>        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-keyword">return</span> self.down_proj(self.gate_proj(x) * self.up_proj(x))<br></code></pre></td></tr></table></figure><h3 id="softmax">Softmax</h3><p>基本表达式为： <span class="math display">\[\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}\]</span> 在实际使用中，为了消除指数爆炸风险（原始值超过 20时可能发生浮点溢出），通常会等价变形为： <span class="math display">\[\text{Softmax}(x_i) = \frac{e^{x_i - \text{max}(x)}}{\sum_{j=1}^n e^{x_j- \text{max}(x)}}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">softmax</span>(<span class="hljs-params">x, eps=<span class="hljs-number">1e-8</span></span>):</span><br>    <span class="hljs-comment"># x 的形状为 (n_samples, n_classes)</span><br>    exps = np.exp(x - np.<span class="hljs-built_in">max</span>(x, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>))<br>    <span class="hljs-keyword">return</span> exps / np.<span class="hljs-built_in">sum</span>(exps, axis=-<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h2 id="指标计算">指标计算</h2><h3 id="ppl">PPL</h3><p>困惑度（Perplexity）是语言模型的核心评估指标，用于衡量模型对测试数据的预测能力。其本质是交叉熵的指数形式，可理解为模型在预测时面临的平均「选择困境」。<span class="math display">\[PPL = \exp\left(-\frac{1}{N}\sum_{i=1}^N \log p(w_i|w_{&lt;i})\right)\]</span></p><p>其中 <span class="math inline">\(N\)</span> 为测试集词数，<spanclass="math inline">\(p(w_i|w_{&lt;i})\)</span>是模型预测当前词的概率。PPL 值越低，说明模型预测越准确。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_ppl</span>(<span class="hljs-params">log_probs</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算困惑度</span><br><span class="hljs-string">    :param log_probs: 模型输出的对数概率序列</span><br><span class="hljs-string">    :return: PPL值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> np.exp(-np.mean(log_probs))<br><br><span class="hljs-comment"># 示例：假设模型对5个词的预测对数概率分别为-0.2, -0.3, -0.1, -0.4, -0.2</span><br>log_probs = [-<span class="hljs-number">0.2</span>, -<span class="hljs-number">0.3</span>, -<span class="hljs-number">0.1</span>, -<span class="hljs-number">0.4</span>, -<span class="hljs-number">0.2</span>]<br><span class="hljs-built_in">print</span>(calculate_ppl(log_probs))  <span class="hljs-comment"># 输出：1.284</span><br></code></pre></td></tr></table></figure><h3 id="rouge指标">ROUGE指标</h3><p>自动摘要任务的黄金标准，主要变体：</p><ul><li><strong>ROUGE-N</strong>：基于 n-gram 重叠的召回率</li><li><strong>ROUGE-L</strong>：基于最长公共子序列（LCS）</li></ul><p><span class="math display">\[ROUGE{\text-L} =\frac{(1+\beta^2)R_{\text{lcs}}P_{\text{lcs}}}{R_{\text{lcs}}+\beta^2P_{\text{lcs}}}\]</span></p><p>其中 <span class="math inline">\(\beta\)</span>控制召回率权重，通常设为 1.2。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rouge_l</span>(<span class="hljs-params">reference, candidate</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算 ROUGE-L 分数</span><br><span class="hljs-string">    :param reference: 参考摘要（词列表）</span><br><span class="hljs-string">    :param candidate: 生成摘要（词列表）</span><br><span class="hljs-string">    :return: F1分数</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># LCS 长度计算（动态规划实现）</span><br>    m, n = <span class="hljs-built_in">len</span>(reference), <span class="hljs-built_in">len</span>(candidate)<br>    dp = [[<span class="hljs-number">0</span>]*(n+<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m+<span class="hljs-number">1</span>)]<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, m+<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n+<span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">if</span> reference[i-<span class="hljs-number">1</span>] == candidate[j-<span class="hljs-number">1</span>]:<br>                dp[i][j] = dp[i-<span class="hljs-number">1</span>][j-<span class="hljs-number">1</span>] + <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                dp[i][j] = <span class="hljs-built_in">max</span>(dp[i-<span class="hljs-number">1</span>][j], dp[i][j-<span class="hljs-number">1</span>])<br>    lcs = dp[m][n]<br>    <br>    precision = lcs / <span class="hljs-built_in">len</span>(candidate)<br>    recall = lcs / <span class="hljs-built_in">len</span>(reference)<br>    beta = <span class="hljs-number">1.2</span><br>    <span class="hljs-keyword">return</span> ( (<span class="hljs-number">1</span> + beta**<span class="hljs-number">2</span>) * precision * recall ) / ( recall + beta**<span class="hljs-number">2</span> * precision )<br></code></pre></td></tr></table></figure><h3 id="bleu分数">BLEU分数</h3><p>机器翻译经典指标，基于修正 n-gram 精度和长度惩罚：</p><p><span class="math display">\[BLEU = BP \cdot \exp\left(\sum_{n=1}^N w_n \log p_n\right)\]</span></p><p>其中 BP（Brevity Penalty）惩罚过短输出：</p><p><span class="math display">\[BP = \begin{cases}1 &amp; \text{if } c &gt; r \\e^{(1-r/c)} &amp; \text{otherwise}\end{cases}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bleu</span>(<span class="hljs-params">references, candidate, weights=[<span class="hljs-number">0.25</span>]*<span class="hljs-number">4</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算 BLEU 分数</span><br><span class="hljs-string">    :param references: 多个参考译文列表</span><br><span class="hljs-string">    :param candidate: 候选译文（词列表）</span><br><span class="hljs-string">    :param weights: n-gram 权重（默认 4-gram 平均）</span><br><span class="hljs-string">    :return: BLEU 分数（0-1）</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 计算各n-gram精度</span><br>    p_n = []<br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(weights)+<span class="hljs-number">1</span>):<br>        candidate_grams = [<span class="hljs-built_in">tuple</span>(candidate[i:i+n]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(candidate)-n+<span class="hljs-number">1</span>)]<br>        refs_grams = [ [<span class="hljs-built_in">tuple</span>(ref[i:i+n]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(ref)-n+<span class="hljs-number">1</span>)] <span class="hljs-keyword">for</span> ref <span class="hljs-keyword">in</span> references]<br>        <br>        count_clip = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> gram <span class="hljs-keyword">in</span> <span class="hljs-built_in">set</span>(candidate_grams):<br>            max_ref_count = <span class="hljs-built_in">max</span>([ref.count(gram) <span class="hljs-keyword">for</span> ref <span class="hljs-keyword">in</span> refs_grams])<br>            count_clip += <span class="hljs-built_in">min</span>(candidate_grams.count(gram), max_ref_count)<br>            <br>        p_n.append(count_clip / <span class="hljs-built_in">len</span>(candidate_grams) <span class="hljs-keyword">if</span> candidate_grams <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-comment"># 计算长度惩罚</span><br>    c = <span class="hljs-built_in">len</span>(candidate)<br>    r = <span class="hljs-built_in">min</span>(<span class="hljs-built_in">len</span>(ref) <span class="hljs-keyword">for</span> ref <span class="hljs-keyword">in</span> references)<br>    bp = <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> c &gt; r <span class="hljs-keyword">else</span> math.exp(<span class="hljs-number">1</span> - r/c)<br>    <br>    <span class="hljs-comment"># 综合得分</span><br>    score = <span class="hljs-built_in">sum</span>(w * math.log(p) <span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> w, p <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(weights, p_n))<br>    <span class="hljs-keyword">return</span> bp * math.exp(score)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>手撕经典算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>手撕经典算法 #3 Transformer篇</title>
    <link href="/Manual-Coding-3.html"/>
    <url>/Manual-Coding-3.html</url>
    
    <content type="html"><![CDATA[<p>本文在前两章的基础上，对 Transformer模型进行了简单的实现和注释。包括：</p><ul><li>Embedding 层</li><li>Encoder 层</li><li>Decoder 层</li><li>堆叠 Encoder</li><li>堆叠 Decoder</li><li>完整 Transformer</li></ul><h2 id="embedding-层">Embedding 层</h2><p>Transformer 模型的基础组件之一是嵌入层。Token Embedding将输入的单词或标记转换为向量表示，Positional Embedding则为输入的每个位置添加位置信息，以便模型理解序列的顺序。</p><h3 id="token-embedding">Token Embedding</h3><p>以下是 Token Embedding 的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TokenEmbedding</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, vocab_size, hidden_size</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.embedding = nn.Embedding(vocab_size, hidden_size)  <span class="hljs-comment"># 嵌入层</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-comment"># x 形状: (batch_size, seq_len)</span><br>        embedded = self.embedding(x)  <span class="hljs-comment"># 嵌入后的形状: (batch_size, seq_len, hidden_size)</span><br>        <span class="hljs-keyword">return</span> embedded<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_token_embedding</span>():</span><br>    vocab_size = <span class="hljs-number">10000</span>  <span class="hljs-comment"># 词汇表大小</span><br>    hidden_size = <span class="hljs-number">512</span>  <span class="hljs-comment"># 嵌入维度</span><br>    batch_size = <span class="hljs-number">2</span><br>    seq_len = <span class="hljs-number">4</span><br>    <br>    <span class="hljs-comment"># 随机生成输入数据 (batch_size, seq_len)</span><br>    x = torch.randint(<span class="hljs-number">0</span>, vocab_size, (batch_size, seq_len))<br>    <br>    <span class="hljs-comment"># 创建 TokenEmbedding 模块</span><br>    token_embedding = TokenEmbedding(vocab_size, hidden_size)<br>    <br>    <span class="hljs-comment"># 计算嵌入输出</span><br>    output = token_embedding(x)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, x.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    test_token_embedding()<br></code></pre></td></tr></table></figure><h3 id="position-embedding">Position Embedding</h3><p>在 Transformer模型中，位置编码被添加到输入的嵌入表示中。位置编码的计算通常<strong>基于固定函数，例如正弦和余弦函数</strong>。这些函数确保不同位置的编码是不同的，同时保持一定的<strong>周期性和对称性</strong>。</p><p>具体地，位置编码矩阵 <span class="math inline">\(\mathbf{PE}\)</span>的每个元素由以下公式计算：</p><p><span class="math display">\[\mathbf{PE}_{(pos, 2i)} =\sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\]</span> <span class="math display">\[\mathbf{PE}_{(pos, 2i+1)} =\cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\]</span></p><p>其中： - <span class="math inline">\(pos\)</span> 表示位置索引。 -<span class="math inline">\(i\)</span> 表示维度索引。 - <spanclass="math inline">\(d_{\text{model}}\)</span> 表示嵌入维度的大小。</p><p>这些公式确保了不同位置的编码是独特的，并且具有不同频率的正弦和余弦成分。以下是Positional Embedding 的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PositionalEmbedding</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, max_len, hidden_size</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.hidden_size = hidden_size<br>        <br>        <span class="hljs-comment"># 创建位置编码表，大小为 (max_len, hidden_size)</span><br>        <span class="hljs-comment"># position: (max_len, 1)，表示序列中的位置索引，例如 [[0.], [1.], [2.], ...]</span><br>        position = torch.arange(<span class="hljs-number">0</span>, max_len).unsqueeze(<span class="hljs-number">1</span>).<span class="hljs-built_in">float</span>()<br>        <br>        <span class="hljs-comment"># div_term: (hidden_size / 2)，用于计算位置编码的分母</span><br>        div_term = torch.exp(torch.arange(<span class="hljs-number">0</span>, hidden_size, <span class="hljs-number">2</span>).<span class="hljs-built_in">float</span>() * (-math.log(<span class="hljs-number">10000.0</span>) / hidden_size))<br>        <br>        <span class="hljs-comment"># 初始化位置编码矩阵 pe 为零矩阵，大小为 (max_len, hidden_size)</span><br>        pe = torch.zeros(max_len, hidden_size)<br>        <br>        <span class="hljs-comment"># 计算位置编码矩阵，广播机制将 dive_term 扩展为 (1, hidden_size )</span><br>        <span class="hljs-comment"># 偶数索引列使用 sin 函数</span><br>        pe[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>] = torch.sin(position * div_term)<br>        <span class="hljs-comment"># 奇数索引列使用 cos 函数</span><br>        pe[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = torch.cos(position * div_term)<br>        <br>        <span class="hljs-comment"># 将位置编码矩阵注册为 buffer，模型训练时不会更新它</span><br>        self.register_buffer(<span class="hljs-string">&#x27;pe&#x27;</span>, pe)<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-comment"># x 的形状: (batch_size, seq_len, hidden_size)</span><br>        seq_len = x.size(<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-comment"># 将位置编码加到输入张量上</span><br>        <span class="hljs-comment"># self.pe[:seq_len, :] 的形状为 (seq_len, hidden_size)</span><br>        <span class="hljs-comment"># unsqueeze(0) 使其形状变为 (1, seq_len, hidden_size)，便于与输入张量相加</span><br>        x = x + self.pe[:seq_len, :].unsqueeze(<span class="hljs-number">0</span>)<br>        <br>        <span class="hljs-comment"># 返回加上位置编码后的张量</span><br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 测试 PositionalEmbedding 的函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_positional_embedding</span>():</span><br>    max_len = <span class="hljs-number">5000</span>  <span class="hljs-comment"># 最大序列长度</span><br>    hidden_size = <span class="hljs-number">512</span>  <span class="hljs-comment"># 嵌入维度</span><br>    batch_size = <span class="hljs-number">2</span><br>    seq_len = <span class="hljs-number">4</span><br>    <br>    <span class="hljs-comment"># 随机生成输入数据，形状为 (batch_size, seq_len, hidden_size)</span><br>    x = torch.randn(batch_size, seq_len, hidden_size)<br>    <br>    <span class="hljs-comment"># 创建 PositionalEmbedding 模块实例</span><br>    positional_embedding = PositionalEmbedding(max_len, hidden_size)<br>    <br>    <span class="hljs-comment"># 计算位置嵌入输出</span><br>    output = positional_embedding(x)<br>    <br>    <span class="hljs-comment"># 打印输入和输出的形状</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, x.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br><br><span class="hljs-comment"># 如果此模块是主模块，则运行测试函数</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    test_positional_embedding()<br></code></pre></td></tr></table></figure><h3 id="rope-embedding">RoPE Embedding</h3><p>TODO</p><h2 id="encoder-层">Encoder 层</h2><p>Transformer 的 Encoder 由多个子层组成，包括多头注意力机制（Multi-HeadAttention）、前馈神经网络（Feed Forward NeuralNetwork）以及归一化层（Layer Normalization）。</p><p><img src="/img/blog/Manual-Coding-3-images/Transformer.png" alt="Transformer 结构示意图" width=67% /></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">EncoderLayer</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_size, num_heads, ff_size, dropout_prob=<span class="hljs-number">0.1</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.multi_head_attention = MultiHeadAttention(hidden_size, num_heads)  <span class="hljs-comment"># 多头注意力层</span><br>        self.dropout1 = nn.Dropout(dropout_prob)  <span class="hljs-comment"># Dropout 层</span><br>        self.layer_norm1 = nn.LayerNorm(hidden_size)  <span class="hljs-comment"># LayerNorm 层</span><br><br>        self.feed_forward = nn.Sequential(<br>            nn.Linear(hidden_size, ff_size),  <span class="hljs-comment"># 前馈层1</span><br>            nn.ReLU(),  <span class="hljs-comment"># 激活函数</span><br>            nn.Linear(ff_size, hidden_size)  <span class="hljs-comment"># 前馈层2</span><br>        )<br>        self.dropout2 = nn.Dropout(dropout_prob)  <span class="hljs-comment"># Dropout 层</span><br>        self.layer_norm2 = nn.LayerNorm(hidden_size)  <span class="hljs-comment"># LayerNorm 层</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x, attention_mask=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-comment"># 多头注意力子层</span><br>        attn_output = self.multi_head_attention(x, attention_mask)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>        attn_output = self.dropout1(attn_output)  <span class="hljs-comment"># Dropout</span><br>        out1 = self.layer_norm1(x + attn_output)  <span class="hljs-comment"># 残差连接 + LayerNorm</span><br>        <br>        <span class="hljs-comment"># 前馈神经网络子层</span><br>        ff_output = self.feed_forward(out1)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>        ff_output = self.dropout2(ff_output)  <span class="hljs-comment"># Dropout</span><br>        out2 = self.layer_norm2(out1 + ff_output)  <span class="hljs-comment"># 残差连接 + LayerNorm</span><br>        <br>        <span class="hljs-keyword">return</span> out2<br><br><span class="hljs-comment"># 测试 EncoderLayer 的 main 函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    batch_size = <span class="hljs-number">2</span><br>    seq_len = <span class="hljs-number">4</span><br>    hidden_size = <span class="hljs-number">512</span><br>    num_heads = <span class="hljs-number">8</span><br>    ff_size = <span class="hljs-number">2048</span><br>    <br>    <span class="hljs-comment"># 随机生成输入数据 (batch_size, seq_len, hidden_size)</span><br>    x = torch.randn(batch_size, seq_len, hidden_size)<br>    <br>    <span class="hljs-comment"># 创建 EncoderLayer 模块</span><br>    encoder_layer = EncoderLayer(hidden_size, num_heads, ff_size)<br>    <br>    <span class="hljs-comment"># 计算 EncoderLayer 输出</span><br>    output = encoder_layer(x)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, x.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br></code></pre></td></tr></table></figure><h2 id="decoder-层">Decoder 层</h2><p>Transformer 的 Decoder 层和 Encoder 层有类似的结构，但 Decoder层除了包含多头自注意力机制和前馈神经网络，还增加了一个用于编码器-解码器注意力机制的多头注意力子层。这使得Decoder 层能够同时关注当前输出序列的上下文信息和输入序列的编码信息。多个Decoder 层堆叠在一起构成整个 Decoder 模块。</p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DecoderLayer</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_size, num_heads, ff_size, dropout_prob=<span class="hljs-number">0.1</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.self_attention = MultiHeadAttention(hidden_size, num_heads)  <span class="hljs-comment"># 自注意力层</span><br>        self.dropout1 = nn.Dropout(dropout_prob)  <span class="hljs-comment"># Dropout 层</span><br>        self.layer_norm1 = nn.LayerNorm(hidden_size)  <span class="hljs-comment"># LayerNorm 层</span><br><br>        self.encoder_decoder_attention = MultiHeadAttention(hidden_size, num_heads)  <span class="hljs-comment"># 编码器-解码器注意力层</span><br>        self.dropout2 = nn.Dropout(dropout_prob)  <span class="hljs-comment"># Dropout 层</span><br>        self.layer_norm2 = nn.LayerNorm(hidden_size)  <span class="hljs-comment"># LayerNorm 层</span><br><br>        self.feed_forward = nn.Sequential(<br>            nn.Linear(hidden_size, ff_size),  <span class="hljs-comment"># 前馈层1</span><br>            nn.ReLU(),  <span class="hljs-comment"># 激活函数</span><br>            nn.Linear(ff_size, hidden_size)  <span class="hljs-comment"># 前馈层2</span><br>        )<br>        self.dropout3 = nn.Dropout(dropout_prob)  <span class="hljs-comment"># Dropout 层</span><br>        self.layer_norm3 = nn.LayerNorm(hidden_size)  <span class="hljs-comment"># LayerNorm 层</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x, encoder_output, self_attention_mask=<span class="hljs-literal">None</span>, encoder_attention_mask=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-comment"># 自注意力子层</span><br>        self_attn_output = self.self_attention(x, self_attention_mask)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>        self_attn_output = self.dropout1(self_attn_output)  <span class="hljs-comment"># Dropout</span><br>        out1 = self.layer_norm1(x + self_attn_output)  <span class="hljs-comment"># 残差连接 + LayerNorm</span><br>        <br>        <span class="hljs-comment"># 编码器-解码器注意力子层</span><br>        enc_dec_attn_output = self.encoder_decoder_attention(out1, encoder_output, encoder_attention_mask)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>        enc_dec_attn_output = self.dropout2(enc_dec_attn_output)  <span class="hljs-comment"># Dropout</span><br>        out2 = self.layer_norm2(out1 + enc_dec_attn_output)  <span class="hljs-comment"># 残差连接 + LayerNorm</span><br>        <br>        <span class="hljs-comment"># 前馈神经网络子层</span><br>        ff_output = self.feed_forward(out2)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>        ff_output = self.dropout3(ff_output)  <span class="hljs-comment"># Dropout</span><br>        out3 = self.layer_norm3(out2 + ff_output)  <span class="hljs-comment"># 残差连接 + LayerNorm</span><br>        <br>        <span class="hljs-keyword">return</span> out3<br><br><span class="hljs-comment"># 测试 DecoderLayer 的 main 函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    batch_size = <span class="hljs-number">2</span><br>    seq_len = <span class="hljs-number">4</span><br>    hidden_size = <span class="hljs-number">512</span><br>    num_heads = <span class="hljs-number">8</span><br>    ff_size = <span class="hljs-number">2048</span><br>    <br>    <span class="hljs-comment"># 随机生成输入数据 (batch_size, seq_len, hidden_size)</span><br>    x = torch.randn(batch_size, seq_len, hidden_size)<br>    encoder_output = torch.randn(batch_size, seq_len, hidden_size)<br>    <br>    <span class="hljs-comment"># 创建 DecoderLayer 模块</span><br>    decoder_layer = DecoderLayer(hidden_size, num_heads, ff_size)<br>    <br>    <span class="hljs-comment"># 计算 DecoderLayer 输出</span><br>    output = decoder_layer(x, encoder_output)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, x.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Encoder output shape:&quot;</span>, encoder_output.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br></code></pre></td></tr></table></figure><h2 id="堆叠-encoder">堆叠 Encoder</h2><p>多个 Encoder 层堆叠在一起构成整个 Encoder 模块。</p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Encoder</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_size, num_heads, ff_size, num_layers, dropout_prob=<span class="hljs-number">0.1</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.layers = nn.ModuleList([<br>            EncoderLayer(hidden_size, num_heads, ff_size, dropout_prob)<br>            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)<br>        ])  <span class="hljs-comment"># 堆叠多个 EncoderLayer</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x, attention_mask=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:<br>            x = layer(x, attention_mask)  <span class="hljs-comment"># 逐层传递输入</span><br>        <br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 测试 Encoder 的 main 函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    batch_size = <span class="hljs-number">2</span><br>    seq_len = <span class="hljs-number">4</span><br>    hidden_size = <span class="hljs-number">512</span><br>    num_heads = <span class="hljs-number">8</span><br>    ff_size = <span class="hljs-number">2048</span><br>    num_layers = <span class="hljs-number">6</span><br>    <br>    <span class="hljs-comment"># 随机生成输入数据 (batch_size, seq_len, hidden_size)</span><br>    x = torch.randn(batch_size, seq_len, hidden_size)<br>    <br>    <span class="hljs-comment"># 创建 Encoder 模块</span><br>    encoder = Encoder(hidden_size, num_heads, ff_size, num_layers)<br>    <br>    <span class="hljs-comment"># 计算 Encoder 输出</span><br>    output = encoder(x)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, x.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br></code></pre></td></tr></table></figure><h2 id="堆叠-decoder">堆叠 Decoder</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Decoder</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_size, num_heads, ff_size, num_layers, dropout_prob=<span class="hljs-number">0.1</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.layers = nn.ModuleList([<br>            DecoderLayer(hidden_size, num_heads, ff_size, dropout_prob)<br>            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)<br>        ])  <span class="hljs-comment"># 堆叠多个 DecoderLayer</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x, encoder_output, self_attention_mask=<span class="hljs-literal">None</span>, encoder_attention_mask=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:<br>            x = layer(x, encoder_output, self_attention_mask, encoder_attention_mask)  <span class="hljs-comment"># 逐层传递输入</span><br>        <br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 测试 Decoder 的 main 函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    batch_size = <span class="hljs-number">2</span><br>    seq_len = <span class="hljs-number">4</span><br>    hidden_size = <span class="hljs-number">512</span><br>    num_heads = <span class="hljs-number">8</span><br>    ff_size = <span class="hljs-number">2048</span><br>    num_layers = <span class="hljs-number">6</span><br>    <br>    <span class="hljs-comment"># 随机生成输入数据 (batch_size, seq_len, hidden_size)</span><br>    x = torch.randn(batch_size, seq_len, hidden_size)<br>    encoder_output = torch.randn(batch_size, seq_len, hidden_size)<br>    <br>    <span class="hljs-comment"># 创建 Decoder 模块</span><br>    decoder = Decoder(hidden_size, num_heads, ff_size, num_layers)<br>    <br>    <span class="hljs-comment"># 计算 Decoder 输出</span><br>    output = decoder(x, encoder_output)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, x.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Encoder output shape:&quot;</span>, encoder_output.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br></code></pre></td></tr></table></figure><h2 id="transformer">Transformer</h2><p>Transformer 由三个主要部分组成：输入嵌入（Token Embedding 和Positional Embedding）、Encoder 堆叠、Decoder堆叠。下面将这些部分组合在一起，实现一个完整的 Transformer 类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Transformer</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, vocab_size, hidden_size, num_heads, ff_size, num_layers, max_seq_len, dropout_prob=<span class="hljs-number">0.1</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.token_embedding = TokenEmbedding(vocab_size, hidden_size)  <span class="hljs-comment"># Token Embedding 层</span><br>        self.positional_embedding = PositionalEmbedding(hidden_size, max_seq_len)  <span class="hljs-comment"># Positional Embedding 层</span><br><br>        self.encoder = Encoder(hidden_size, num_heads, ff_size, num_layers, dropout_prob)  <span class="hljs-comment"># Encoder 堆叠</span><br>        self.decoder = Decoder(hidden_size, num_heads, ff_size, num_layers, dropout_prob)  <span class="hljs-comment"># Decoder 堆叠</span><br>        <br>        self.output_linear = nn.Linear(hidden_size, vocab_size)  <span class="hljs-comment"># 输出线性层</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, src, tgt, src_mask=<span class="hljs-literal">None</span>, tgt_mask=<span class="hljs-literal">None</span>, src_tgt_mask=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-comment"># 获取输入序列的嵌入表示</span><br>        src_emb = self.token_embedding(src) + self.positional_embedding(src)  <span class="hljs-comment"># (batch_size, src_seq_len, hidden_size)</span><br>        tgt_emb = self.token_embedding(tgt) + self.positional_embedding(tgt)  <span class="hljs-comment"># (batch_size, tgt_seq_len, hidden_size)</span><br><br>        <span class="hljs-comment"># 通过 Encoder 获取编码器输出</span><br>        encoder_output = self.encoder(src_emb, src_mask)  <span class="hljs-comment"># (batch_size, src_seq_len, hidden_size)</span><br><br>        <span class="hljs-comment"># 通过 Decoder 获取解码器输出</span><br>        decoder_output = self.decoder(tgt_emb, encoder_output, tgt_mask, src_tgt_mask)  <span class="hljs-comment"># (batch_size, tgt_seq_len, hidden_size)</span><br><br>        <span class="hljs-comment"># 线性层映射到词汇表大小</span><br>        output = self.output_linear(decoder_output)  <span class="hljs-comment"># (batch_size, tgt_seq_len, vocab_size)</span><br><br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-comment"># 测试 Transformer 的 main 函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    vocab_size = <span class="hljs-number">10000</span><br>    hidden_size = <span class="hljs-number">512</span><br>    num_heads = <span class="hljs-number">8</span><br>    ff_size = <span class="hljs-number">2048</span><br>    num_layers = <span class="hljs-number">6</span><br>    max_seq_len = <span class="hljs-number">100</span><br>    dropout_prob = <span class="hljs-number">0.1</span><br><br>    batch_size = <span class="hljs-number">2</span><br>    src_seq_len = <span class="hljs-number">10</span><br>    tgt_seq_len = <span class="hljs-number">10</span><br><br>    <span class="hljs-comment"># 随机生成源序列和目标序列 (batch_size, seq_len)</span><br>    src = torch.randint(<span class="hljs-number">0</span>, vocab_size, (batch_size, src_seq_len))<br>    tgt = torch.randint(<span class="hljs-number">0</span>, vocab_size, (batch_size, tgt_seq_len))<br><br>    <span class="hljs-comment"># 创建 Transformer 模块</span><br>    transformer = Transformer(vocab_size, hidden_size, num_heads, ff_size, num_layers, max_seq_len, dropout_prob)<br>    <br>    <span class="hljs-comment"># 计算 Transformer 输出</span><br>    output = transformer(src, tgt)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Source shape:&quot;</span>, src.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Target shape:&quot;</span>, tgt.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>手撕经典算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>手撕经典算法 #2 神经网络篇</title>
    <link href="/Manual-Coding-2.html"/>
    <url>/Manual-Coding-2.html</url>
    
    <content type="html"><![CDATA[<p>本文对常见的几种神经网络组件进行了简单的实现和注释，便于理解。包括：</p><ul><li>层归一化（Layer Normalization，LN）</li><li>RMSNorm（Root Mean Square Layer Normalization）</li><li>批次归一化（Batch Normalization，BN）</li><li>Dropout</li></ul><h2 id="layernorm">LayerNorm</h2><p>Layer Normalization (LN)是一种归一化技术，旨在改善神经网络的训练稳定性和性能。LN的基本思想是<strong>对每个样本在特征维度上</strong>进行归一化，而不是在批次维度上。对于<strong>每个输入</strong>$ $ ，LN 的公式如下：</p><p><span class="math display">\[\mu = \frac{1}{H} \sum_{i=1}^{H} x_i\]</span></p><p><span class="math display">\[\sigma^2 = \frac{1}{H} \sum_{i=1}^{H} (x_i - \mu)^2\]</span></p><p><span class="math display">\[\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}\]</span></p><p>其中，$ H $ 是特征维度的大小， $ $ 和 $ ^2 $分别是特征维度上的均值和方差。归一化后，会应用可学习的缩放参数 $ $和偏移参数 $ $ ：</p><p><span class="math display">\[y_i = \gamma \hat{x}_i + \beta\]</span></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LayerNorm</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_size, eps=<span class="hljs-number">1e-6</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.hidden_size = hidden_size  <span class="hljs-comment"># 隐藏状态的大小</span><br>        self.eps = eps  <span class="hljs-comment"># 用于数值稳定性的一个小值</span><br>        <br>        <span class="hljs-comment"># 初始化可学习的缩放和平移参数</span><br>        self.gamma = nn.Parameter(torch.ones(hidden_size))  <span class="hljs-comment"># 缩放参数，初始值为全1</span><br>        self.beta = nn.Parameter(torch.zeros(hidden_size))  <span class="hljs-comment"># 平移参数，初始值为全0</span><br>        <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-comment"># x 形状: (batch_size, seq_len, hidden_size)</span><br>        <br>        <span class="hljs-comment"># 计算每个样本的均值和方差</span><br>        mean = x.mean(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># 计算最后一个维度的均值，形状: (batch_size, seq_len, 1)</span><br>        variance = x.var(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>, unbiased=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># 计算最后一个维度的方差，形状: (batch_size, seq_len, 1)</span><br>        <br>        <span class="hljs-comment"># 进行归一化</span><br>        x_normalized = (x - mean) / torch.sqrt(variance + self.eps)  <span class="hljs-comment"># 归一化，形状: (batch_size, seq_len, hidden_size)</span><br>        <br>        <span class="hljs-comment"># 应用缩放和平移参数</span><br>        output = self.gamma * x_normalized + self.beta  <span class="hljs-comment"># 形状: (batch_size, seq_len, hidden_size)</span><br>        <br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_layer_norm</span>():</span><br>    batch_size = <span class="hljs-number">2</span><br>    seq_len = <span class="hljs-number">4</span><br>    hidden_size = <span class="hljs-number">8</span><br>    <br>    <span class="hljs-comment"># 随机生成输入数据</span><br>    x = torch.randn(batch_size, seq_len, hidden_size)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>    <br>    <span class="hljs-comment"># 创建 LayerNorm 模块</span><br>    layer_norm = LayerNorm(hidden_size)<br>    <br>    <span class="hljs-comment"># 计算 LayerNorm 输出</span><br>    output = layer_norm(x)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, x.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br>    <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>test_layer_norm()<br></code></pre></td></tr></table></figure><h2 id="rmsnorm">RMSNorm</h2><p>LayerNorm 需同时计算输入数据的均值和方差。研究发现 LayerNorm的优势在于缩放不变性（即方差的计算，让模型对于输入和权重上的偏移噪声不敏感），而不是重新居中（即均值的计算，让模型在输入和权重都被随机缩放时保持输出表示不变）。</p><p>所以 RMSNorm省略了归一化过程中的均值计算，仅计算特征的<strong>均方根</strong>（RootMean Square），使得算法更加简洁，而效果不减，且运算效率显著提升。RMSNorm的公式如下： <span class="math display">\[RMS=\sqrt{\frac{1}{H} \sum_{i=1}^{H} x_i^2 + \epsilon}\]</span></p><p><span class="math display">\[\hat{x}_i = \frac{x_i}{RMS}\]</span></p><p>此外，通常仅保留<strong>缩放参数</strong> $ $，省略偏移参数 $$。实验表明，中心化（减均值）对性能影响有限，移除后仍能保持模型表达能力。</p><p><span class="math display">\[y_i = \gamma \hat{x}_i\]</span> 代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RMSNorm</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_size, eps=<span class="hljs-number">1e-6</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.hidden_size = hidden_size<br>        self.eps = eps  <span class="hljs-comment"># 用于数值稳定性的一个小值</span><br>        <br>        <span class="hljs-comment"># 仅保留缩放参数 gamma，移除 beta</span><br>        self.gamma = nn.Parameter(torch.ones(hidden_size))  <span class="hljs-comment"># 初始化为全 1</span><br>        <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-comment"># 计算均方根 (RMS)，不进行均值中心化</span><br>        rms = torch.sqrt(torch.mean(x.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>), dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>) + self.eps)  <span class="hljs-comment"># 平方后求均值，再开根</span><br>        <br>        <span class="hljs-comment"># 归一化：x / RMS</span><br>        x_normalized = x / rms  <span class="hljs-comment"># 不减去均值</span><br>        <br>        <span class="hljs-comment"># 仅应用 gamma 参数，无 beta</span><br>        output = self.gamma * x_normalized<br>        <br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_rms_norm</span>():</span>  <span class="hljs-comment"># 修改测试函数名</span><br>    batch_size = <span class="hljs-number">2</span><br>    seq_len = <span class="hljs-number">4</span><br>    hidden_size = <span class="hljs-number">8</span><br>    <br>    x = torch.randn(batch_size, seq_len, hidden_size)<br>    rms_norm = RMSNorm(hidden_size)  <span class="hljs-comment"># 使用 RMSNorm 类</span><br>    output = rms_norm(x)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, x.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Params:&quot;</span>, <span class="hljs-built_in">list</span>(rms_norm.parameters()))  <span class="hljs-comment"># 仅有一个参数 gamma</span><br>    <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    test_rms_norm()  <span class="hljs-comment"># 调用测试函数</span><br></code></pre></td></tr></table></figure><h2 id="batchnorm">BatchNorm</h2><p>Batch Normalization (BN)是另一种归一化技术，主要用于加速神经网络的训练。BN <strong>对每个mini-batch的数据在批次维度上</strong>进行归一化。对于<strong>所有输入</strong> $ $，BN 的公式如下：</p><p><span class="math display">\[\mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i\]</span></p><p><span class="math display">\[\sigma_B^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2\]</span></p><p><span class="math display">\[\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}\]</span></p><p>其中， $ m $ 是 mini-batch 的大小， $ _B $ 和 $ _B^2 $分别是批次维度上的均值和方差。归一化后，也会应用可学习的缩放参数 $ $和偏移参数 $ $ ：</p><p><span class="math display">\[y_i = \gamma \hat{x}_i + \beta\]</span></p><blockquote><p><strong>LN 和 BN 的区别</strong>：</p><ol type="1"><li><strong>归一化的维度</strong>：LN 在特征维度上进行归一化，而 BN在批次维度上进行归一化。</li><li><strong>应用场景</strong>：LN 更适用于 Recurrent Neural Networks(RNNs) 和 Transformer 等序列模型，而 BN 通常用于 Convolutional NeuralNetworks (CNNs)。</li><li><strong>批量大小依赖性</strong>：LN不依赖于批量大小，因此在小批量甚至单样本的情况下也能很好地工作。BN依赖于较大的批量大小以稳定均值和方差的估计。</li></ol><p><strong>为什么 Transformer 使用 LN 而不是 BN</strong>：</p><ul><li>Transformer模型通常处理变长序列数据，其批次大小可能会变化或者在推理阶段可能只有一个样本。</li><li>BN依赖于批次维度的均值和方差估计，因此在这种情况下表现可能不稳定。LN则对每个样本独立进行归一化，不依赖于批次大小，因此更适合于 Transformer这种模型。</li><li>此外，LN对序列模型的时间步长无关的归一化方式有助于保持输入数据的顺序特性，从而提高模型的性能和稳定性。</li></ul></blockquote><p>在 BatchNorm的实现中，通常会区分训练（training）和推理（inference）阶段，这是因为在这两个阶段中，BN的行为有所不同：</p><ol type="1"><li><p><strong>训练阶段（training）</strong>：</p><ul><li>在训练阶段，BN 会计算当前 mini-batch的均值和方差，并使用这些统计量对数据进行归一化。具体公式同前文所述。</li><li>训练过程中，BN<strong>还会使用移动平均的方法更新运行时均值和方差</strong>，这样在推理阶段就可以<strong>使用这些全局统计量来代替mini-batch 的统计量</strong>。更新规则如下： <spanclass="math display">\[\text{running\_mean} = (1 - \text{momentum}) \times \text{running\_mean}+ \text{momentum} \times \mu_B\]</span> <span class="math display">\[\text{running\_var} = (1 - \text{momentum}) \times \text{running\_var} +\text{momentum} \times \sigma_B^2\]</span></li></ul></li><li><p><strong>推理阶段（inference）</strong>：</p><ul><li>在推理阶段，BN 不再使用 mini-batch的均值和方差，而是<strong>使用训练阶段累积的运行时均值和方差</strong>。这是因为在推理阶段，<strong>通常批量大小很小甚至为1</strong>，使用 mini-batch的统计量会导致不稳定的输出。推理阶段的归一化公式如下： <spanclass="math display">\[\hat{x}_i = \frac{x_i - \text{running\_mean}}{\sqrt{\text{running\_var}+ \epsilon}}\]</span></li></ul></li></ol><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BatchNorm</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_size, eps=<span class="hljs-number">1e-5</span>, momentum=<span class="hljs-number">0.1</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.hidden_size = hidden_size  <span class="hljs-comment"># 隐藏状态的大小</span><br>        self.eps = eps  <span class="hljs-comment"># 用于数值稳定性的一个小值</span><br>        self.momentum = momentum  <span class="hljs-comment"># 用于计算运行时均值和方差的动量</span><br>        <br>        <span class="hljs-comment"># 初始化可学习的缩放和平移参数</span><br>        self.gamma = nn.Parameter(torch.ones(hidden_size))  <span class="hljs-comment"># 缩放参数，初始值为全1</span><br>        self.beta = nn.Parameter(torch.zeros(hidden_size))  <span class="hljs-comment"># 平移参数，初始值为全0</span><br>        <br>        <span class="hljs-comment"># 初始化运行时均值和方差</span><br>        self.running_mean = torch.zeros(hidden_size)  <span class="hljs-comment"># 运行时均值，初始值为全0</span><br>        self.running_var = torch.ones(hidden_size)  <span class="hljs-comment"># 运行时方差，初始值为全1</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-comment"># x 形状: (batch_size, seq_len, hidden_size)</span><br>        <span class="hljs-keyword">if</span> self.training:<br>            <span class="hljs-comment"># 计算当前批次的均值和方差</span><br>            batch_mean = x.mean(dim=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), keepdim=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># 计算前两个维度的均值，形状: (hidden_size)</span><br>            batch_var = x.var(dim=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), keepdim=<span class="hljs-literal">False</span>, unbiased=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># 计算前两个维度的方差，形状: (hidden_size)</span><br><br>            <span class="hljs-comment"># 更新运行时均值和方差</span><br>            self.running_mean = (<span class="hljs-number">1</span> - self.momentum) * self.running_mean + self.momentum * batch_mean<br>            self.running_var = (<span class="hljs-number">1</span> - self.momentum) * self.running_var + self.momentum * batch_var<br>            <br>            mean = batch_mean<br>            variance = batch_var<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 使用运行时均值和方差</span><br>            mean = self.running_mean<br>            variance = self.running_var<br>        <br>        <span class="hljs-comment"># 进行归一化</span><br>        x_normalized = (x - mean) / torch.sqrt(variance + self.eps)  <span class="hljs-comment"># 归一化，形状: (batch_size, seq_len, hidden_size)</span><br>        <br>        <span class="hljs-comment"># 应用缩放和平移参数</span><br>        output = self.gamma * x_normalized + self.beta  <span class="hljs-comment"># 形状: (batch_size, seq_len, hidden_size)</span><br>        <br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_batch_norm</span>():</span><br>    batch_size = <span class="hljs-number">2</span><br>    seq_len = <span class="hljs-number">4</span><br>    hidden_size = <span class="hljs-number">8</span><br>    <br>    <span class="hljs-comment"># 随机生成输入数据</span><br>    x = torch.randn(batch_size, seq_len, hidden_size)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>    <br>    <span class="hljs-comment"># 创建 BatchNorm 模块</span><br>    batch_norm = BatchNorm(hidden_size)<br>    <br>    <span class="hljs-comment"># 计算 BatchNorm 输出</span><br>    output = batch_norm(x)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, x.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br>    <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>test_batch_norm()<br></code></pre></td></tr></table></figure><blockquote><p>注意：这里的实现其实不太合理，因为 BN通常不用于变长序列模型的输入，因此这里的 <code>seq_len</code>维度只是摆设。</p></blockquote><h2 id="dropout">Dropout</h2><p>Dropout是一种正则化技术，用于防止神经网络的过拟合。通过在<strong>训练过程中随机「丢弃」一部分神经元</strong>，Dropout使得模型不会过度依赖某些特定的神经元，从而增强模型的泛化能力。</p><p><strong>在推理阶段，所有神经元都被激活</strong>，并根据 Dropout概率进行<strong>缩放</strong>，主要目的是为了在训练和推理阶段<strong>保持一致的输出期望值</strong>。</p><p>具体而言，假设在训练阶段，输入神经元的激活值为 $ x $，Dropout的概率为 $ p $。每个神经元以 $ 1 - p $的概率被保留（即不被丢弃）。因此，每个神经元的激活值期望为：</p><p><span class="math display">\[E[\text{激活值}] = x \cdot (1 - p)\]</span></p><p>为了<strong>使得训练和推理阶段的输出期望一致</strong>，我们需要在训练阶段对保留的神经元进行缩放，即乘以<span class="math inline">\(\frac{1}{1 -p}\)</span>，这样可以抵消丢弃神经元带来的期望值减少。</p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Dropout</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, dropout_prob=<span class="hljs-number">0.1</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dropout_prob = dropout_prob  <span class="hljs-comment"># Dropout 的概率</span><br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-keyword">if</span> self.training:<br>            <span class="hljs-comment"># 生成与输入形状相同的掩码，元素为 0 或 1，按照 dropout_prob 的概率为 0</span><br>            mask = (torch.rand(x.shape) &gt; self.dropout_prob).<span class="hljs-built_in">float</span>()  <span class="hljs-comment"># 掩码，形状与 x 相同</span><br>            <span class="hljs-comment"># 归一化掩码，使得训练阶段和推理阶段的一致性，除法保持训练和推理时的期望值一致</span><br>            output = mask * x / (<span class="hljs-number">1.0</span> - self.dropout_prob)  <span class="hljs-comment"># 形状与 x 相同</span><br>        <span class="hljs-keyword">else</span>:<br>            output = x  <span class="hljs-comment"># 推理阶段，不进行 Dropout</span><br>        <br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_dropout</span>():</span><br>    batch_size = <span class="hljs-number">2</span><br>    seq_len = <span class="hljs-number">4</span><br>    hidden_size = <span class="hljs-number">8</span><br>    <br>    <span class="hljs-comment"># 随机生成输入数据</span><br>    x = torch.randn(batch_size, seq_len, hidden_size)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>    <br>    <span class="hljs-comment"># 创建 Dropout 模块</span><br>    dropout = Dropout(dropout_prob=<span class="hljs-number">0.1</span>)<br>    <br>    <span class="hljs-comment"># 设置为训练模式</span><br>    dropout.train()<br>    output_train = dropout(x)<br>    <br>    <span class="hljs-comment"># 设置为推理模式</span><br>    dropout.<span class="hljs-built_in">eval</span>()<br>    output_eval = dropout(x)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, x.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape during training:&quot;</span>, output_train.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape during evaluation:&quot;</span>, output_eval.shape)<br>    <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>test_dropout()<br></code></pre></td></tr></table></figure><h2 id="backpropagation">Backpropagation</h2><p>简单的示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br><span class="hljs-comment"># 加载必要组件，假设已经定义好</span><br>model = MyModel()<br>optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>)<br>criterion = nn.CrossEntropyLoss()<br>dataloader = ...<br><br><span class="hljs-comment"># 训练循环</span><br>model.train()<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-keyword">for</span> inputs, labels <span class="hljs-keyword">in</span> data_loader:       <br>        <span class="hljs-comment"># 前向传播</span><br>        preds = model(inputs)<br>        loss = criterion(preds, labels)<br>        <br>        <span class="hljs-comment"># 反向传播与优化</span><br>        loss.backward()<br>        optimizer.step()<br>        optimizer.zero_grad()<br></code></pre></td></tr></table></figure><h2 id="gradient-accumulation">Gradient Accumulation</h2><p>Gradient Accumulation核心思想是将多个小批量的<strong>梯度累积求和</strong>后，再统一更新模型参数，而非每个小批量单独更新。优点是在显存受限情况下，能够模拟更大batch size 的训练。</p><p><strong>梯度缩放</strong>：直接累积梯度作为一次参数更新的梯度，会使梯度值比实际大<code>gradient_accumulation_steps</code> 倍。而Pytorch的参数更新是写在<code>optimizer.step()</code>方法内部，无法手动控制。所以为了得到多个小批次的梯度平均后再更新模型，通常将loss 除以<code>gradient_accumulation_steps</code>。这一步等同于<strong>对 loss进行缩放，来达到缩放梯度、求多个批次梯度的均值的目的</strong>。</p><p><strong>梯度累加机制</strong>：在 PyTorch 中，每次反向传播<code>loss.backward()</code>，计算得到的梯度会自动累积在<code>Tensor.grad</code> 中，除非调用 <code>optimizer.zero_grad()</code>手动清零。因此，只需控制好<strong>梯度清零与参数更新的间隔步数</strong>，无需重复累加操作。</p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">gradient_accumulation_steps = <span class="hljs-number">4</span>  <br><br><span class="hljs-keyword">for</span> batch_idx, (inputs, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):<br>    <span class="hljs-comment"># 前向传播</span><br>    preds = model(inputs)<br>    loss  = criterion(preds, labels)<br><br>    <span class="hljs-comment"># 关键，损失按累积步数缩放（等价于梯度求平均）</span><br>    loss = loss / gradient_accumulation_steps <br><br>    <span class="hljs-comment"># 反向传播，自动累加梯度</span><br>    loss.backward()<br><br>    <span class="hljs-comment"># 达到累积步数，或者处理末尾不完整批次时更新参数</span><br>    <span class="hljs-keyword">if</span> ((batch_idx + <span class="hljs-number">1</span>) % gradient_accumulation_steps == <span class="hljs-number">0</span>) <span class="hljs-keyword">or</span> (batch_idx + <span class="hljs-number">1</span> == <span class="hljs-built_in">len</span>(data_loader)):<br>        optimizer.step()<br>        optimizer.zero_grad()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>手撕经典算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>手撕经典算法 #1 Attention篇</title>
    <link href="/Manual-Coding-1.html"/>
    <url>/Manual-Coding-1.html</url>
    
    <content type="html"><![CDATA[<p>本文对常见的几种注意力机制进行了简单的实现和注释，便于理解。包括：</p><ul><li><strong>缩放点积注意力（Scaled Dot-Product Attention）</strong><ul><li>2014 年<a href="https://arxiv.org/abs/1409.0473">《Neural MachineTranslation by Jointly Learning to Align andTranslate》</a>提出的单头注意力，输入的 Query、Key 和 Value矩阵都是完整的张量。</li></ul></li><li><strong>多头注意力（Multi Head Attention，MHA）</strong><ul><li>2017 年开山之作<ahref="https://arxiv.org/abs/1706.03762">《Attention is all youneed》</a>所提出的一种 Attention 形式，可以说它是当前主流 LLM的基础工作。每个头有自己单独的 Query、Key 和 Value 矩阵。</li><li>在自回归 LLM 中通过 Mask 可以实现 Causal Attention，而在 Next TokenPrediction 时，新预测的第 <span class="math inline">\(t+1\)</span> 个token 不会影响到已经算好的 <span class="math inline">\(k_{\le t},v_{\let}\)</span>，因此这部分结果我们可以<strong>缓存下来供后续生成调用，避免不必要的重复计算，这就是所谓的KV Cache</strong>。</li></ul></li><li><strong>多查询注意力（Multi Query Attention，MQA）</strong><ul><li>围绕「<strong>如何减少 KV Cache同时尽可能地保证效果</strong>」这个主题发展而来的产物。只有一组key-value 对，由<a href="https://arxiv.org/abs/1911.02150">《FastTransformer Decoding: One Write-Head is All You Need》</a>在 2019年提出。</li><li>与 MHA 不同的是，MQA 让所有的头之间共享同一份 Key 和 Value矩阵，每个头只单独保留了一份 Query 参数，从而<strong>大大减少 Key 和Value 矩阵的参数量</strong>。使用 MQA 的模型包括 PaLM、StarCoder、Gemini等。</li></ul></li><li><strong>分组查询注意力（Grouped Query Attention，GQA）</strong><ul><li>有人担心 MQA 对 KV Cache 的压缩太严重，于是提出了一个折中版本，出自2023 年论文<a href="https://arxiv.org/abs/2305.13245">《GQA: TrainingGeneralized Multi-Query Transformer Models from Multi-HeadCheckpoints》</a>。</li><li>其思想是将将所有 Head 分为 <span class="math inline">\(g\)</span>个组（<span class="math inline">\(g\)</span> 可以整除 <spanclass="math inline">\(h\)</span>），每组共享同一对 Key 和 Value 矩阵。当<span class="math inline">\(g=h\)</span> 时就是 MHA， <spanclass="math inline">\(g=1\)</span> 时就是 MQA，当 <spanclass="math inline">\(1&lt;g&lt;h\)</span> 时，它只将 KV Cache 压缩到<span class="math inline">\(g / h\)</span> ，<strong>压缩率不如MQA，但同时也提供了更大的自由度</strong>，效果上更有保证。</li><li>GQA 最知名的使用者，大概是 Meta 开源的 Llama-2-70B，以及 Llama-3全系列。在 Llama-2/3-70B 中，<spanclass="math inline">\(g=8\)</span>，可以部署到一台机器的 8张卡上，每张卡负责计算一组 K、V 对应的 AttentionHead，减少卡间通信。</li></ul></li><li><strong>多头隐注意力（Multi-head Latent Attention，MLA）</strong><ul><li>2024 年在 <a href="https://arxiv.org/pdf/2405.04434">DeepSeek-V2技术报告</a> 中提到的新技术，用更一般的线性变换来替代了之前的操作，使得<span class="math inline">\(k,v\)</span>都不需要被完整存储，进一步压缩了 KV Cache。</li></ul></li></ul><h2 id="缩放点积注意力sdpa">缩放点积注意力（SDPA）</h2><p>缩放点积注意力早于 Transformer被提出，受到的关注并不多，其内部只实现了 <spanclass="math inline">\(q,k,v\)</span> 的注意力计算。</p><ul><li>输入是 query 和 key-value，注意力机制首先计算 query 与每个 key的关联性</li><li>每个关联性作为每个 value 的权重 (weight)，各个权重与 value的乘积相加得到输出。</li><li><strong>SDPA 可以被认为是 MHA 的中间步骤</strong>！</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ScaledDotProductAttention</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, query, key, value, causal_mask=<span class="hljs-literal">None</span>, padding_mask=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-comment"># query, key, value 形状: (batch_size, seq_len, hidden_size)</span><br>        <br>        <span class="hljs-comment"># 计算注意力分数</span><br>        <span class="hljs-comment"># key.transpose(-1, -2) 将最后两个维度进行转置，以进行点积</span><br>        <span class="hljs-comment"># attention_scores 形状: (batch_size, seq_len, seq_len)</span><br>        d_k = query.size(-<span class="hljs-number">1</span>)  <span class="hljs-comment"># 获取 hidden_size</span><br>        attention_scores = torch.matmul(query, key.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))<br>        <br>        <span class="hljs-comment"># 添加注意力掩码（seq_len, seq_len），掩码位置（1）的值为负无穷</span><br>        <span class="hljs-keyword">if</span> causal_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            attention_scores += causal_mask * -<span class="hljs-number">1e9</span>  <br><br>        <span class="hljs-comment"># 添加填充位置的掩码，每个句子不一样（batch_size, seq_len)</span><br>        <span class="hljs-keyword">if</span> padding_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            padding_mask = padding_mask.unsqueeze(<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">1</span>)<br>            attention_scores += padding_mask * -<span class="hljs-number">1e9</span><br>                <br>        <span class="hljs-comment"># 对注意力分数进行归一化，得到注意力概率</span><br>        attention_probs = torch.softmax(attention_scores, dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch_size, num_heads, seq_len, seq_len)</span><br>        <br>        <span class="hljs-comment"># 计算注意力输出，通过注意力概率加权值</span><br>        attention_output = torch.matmul(attention_probs, value)  <span class="hljs-comment"># (batch_size, num_heads, seq_len, hidden_size)</span><br>        <br>        <span class="hljs-keyword">return</span> attention_output<br>    <br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_attn</span>():</span><br>    batch_size = <span class="hljs-number">128</span><br>    seq_len = <span class="hljs-number">512</span><br>    hidden_size = <span class="hljs-number">1024</span><br>    <br>    query = torch.randn(batch_size, seq_len, hidden_size)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>    key = torch.randn(batch_size, seq_len, hidden_size)    <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>    value = torch.randn(batch_size, seq_len, hidden_size)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br><br>    sdpa = ScaledDotProductAttention()<br>    output = sdpa(query, key, value)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Query shape:&quot;</span>, query.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Key shape:&quot;</span>, key.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Value shape:&quot;</span>, value.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br>    <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>test_attn()<br></code></pre></td></tr></table></figure><h2 id="多头注意力mha">多头注意力（MHA）</h2><p>多头注意力机制是 Transformer模型中的核心组件。在其设计中，「多头」意味着该机制并不只计算一种注意力权重，而是并行计算多种权重，每种权重都从不同的「视角」捕获输入的不同信息。具体步骤如下：</p><ol type="1"><li><p>为输入序列中计算 <span class="math inline">\(Q, K, V\)</span>，这是通过将输入词向量与三个权重矩阵相乘实现的: <spanclass="math display">\[\begin{aligned}&amp; Q = X W_q \\&amp; K = X W_k \\&amp; V = X W_v\end{aligned}\]</span></p></li><li><p>计算 <span class="math inline">\(Q, K\)</span> 注意力得分，其中，<span class="math inline">\(d_k\)</span> 是 <spanclass="math inline">\(k\)</span> 的维度： <span class="math display">\[\operatorname{score}(Q, K) = \frac{Q \cdot K^T}{\sqrt{d_k}}\]</span></p></li><li><p>使用 Softmax 得到注意力权重： <span class="math display">\[\operatorname{Attention}(Q, K) =\operatorname{softmax}(\operatorname{score}(Q,K))=\operatorname{softmax}\left(\frac{Q \cdot K^T}{\sqrt{d_k}}\right)\]</span></p></li><li><p>使用注意力权重和 <spanclass="math inline">\(V\)</span>，计算输出： <spanclass="math display">\[\text{Output} = \operatorname{Attention}(Q, K) \cdot V =\operatorname{softmax}\left(\frac{Q \cdot K^T}{\sqrt{d_k}}\right) \cdotV\]</span></p></li><li><p>拼接多头输出，乘以 <spanclass="math inline">\(W_O\)</span>，得到最终输出： <spanclass="math display">\[\text{MultiHeadOutput} = \text{Concat} (\text{Output}^1,\text{Output}^2, \ldots, \text{Output}^H) W_O\]</span></p></li></ol><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MultiHeadAttention</span>(<span class="hljs-params">torch.nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_size, num_heads</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.num_heads = num_heads<br>        self.head_dim = hidden_size // num_heads  <span class="hljs-comment"># 每个头的维度，二者必须整除</span><br>        <br>        <span class="hljs-comment"># 初始化 Q、K、V 的投影矩阵，将输入词向量线性变换为 Q、K、V，维度保持一致</span><br>        self.q_linear = nn.Linear(hidden_size, hidden_size) <br>        self.k_linear = nn.Linear(hidden_size, hidden_size)<br>        self.v_linear = nn.Linear(hidden_size, hidden_size)<br>        <br>        <span class="hljs-comment"># 输出线性层，将拼接后的多头注意力输出变换为所需的输出维度，这里维度保持一致</span><br>        self.o_linear = nn.Linear(hidden_size, hidden_size)<br>        <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, hidden_state, causal_mask=<span class="hljs-literal">None</span>, padding_mask=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-comment"># hidden_state 形状: (batch_size, seq_len, hidden_size)</span><br>        batch_size = hidden_state.size(<span class="hljs-number">0</span>)  <span class="hljs-comment"># 获取批量大小</span><br><br>        <span class="hljs-comment"># 计算 Q、K、V，线性变换，得到形状：(batch_size, seq_len, hidden_size)</span><br>        query = self.q_linear(hidden_state)<br>        key = self.k_linear(hidden_state)<br>        value = self.v_linear(hidden_state)<br>        <br>        <span class="hljs-comment"># 将每个头的维度拆分出来，得到形状：(batch_size, num_heads, seq_len, head_dim)</span><br>        query = query.view(batch_size, -<span class="hljs-number">1</span>, self.num_heads, self.head_dim).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        key = key.view(batch_size, -<span class="hljs-number">1</span>, self.num_heads, self.head_dim).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        value = value.view(batch_size, -<span class="hljs-number">1</span>, self.num_heads, self.head_dim).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><br>        <span class="hljs-comment"># 计算注意力分数，attention_scores 形状: (batch_size, num_heads, seq_len, seq_len)</span><br>        attention_scores = torch.matmul(query, key.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>)) \<br>        / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))<br>        <br>        <span class="hljs-comment"># 添加因果注意力掩码（seq_len, seq_len），掩码位置（1）的值为负无穷，自动广播</span><br>        <span class="hljs-keyword">if</span> causal_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            attention_scores += causal_mask * -<span class="hljs-number">1e9</span><br>        <br>        <span class="hljs-comment"># 添加填充位置的掩码，每个句子不一样（batch_size, seq_len)</span><br>        <span class="hljs-keyword">if</span> padding_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            padding_mask = padding_mask.unsqueeze(<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch_size, 1, 1, seq_len)</span><br>            attention_scores += padding_mask * -<span class="hljs-number">1e9</span><br>            <br>        <span class="hljs-comment"># 对注意力分数进行归一化，得到注意力概率</span><br>        attention_probs = torch.softmax(attention_scores, dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch_size, num_heads, seq_len, seq_len)</span><br>        <span class="hljs-comment"># 如果有 dropout 操作就加在这，self.dropout(attention_probs)，也可以在函数外面加</span><br><br>        <span class="hljs-comment"># 计算注意力输出，通过注意力概率加权值</span><br>        output = torch.matmul(attention_probs, value)  <span class="hljs-comment"># (batch_size, num_heads, seq_len, head_dim)</span><br>        <br>        <span class="hljs-comment"># 对多头注意力输出进行拼接，将形状调整为 (batch_size, seq_len, hidden_size)</span><br>        <span class="hljs-comment"># 先 output.transpose(1, 2) 将 num_heads 和 seq_len 维度转置</span><br>        output = output.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).view(batch_size, -<span class="hljs-number">1</span>, self.head_dim * self.num_heads)<br>        <br>        <span class="hljs-comment"># 通过线性层将拼接后的输出变换为所需的输出维度</span><br>        output = self.o_linear(output)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_MHA</span>():</span><br>    batch_size = <span class="hljs-number">128</span><br>    seq_len = <span class="hljs-number">512</span><br>    hidden_size = <span class="hljs-number">1024</span><br>    num_heads = <span class="hljs-number">8</span><br>    <br>    <span class="hljs-comment"># 随机生成输入数据</span><br>    hidden_state = torch.randn(batch_size, seq_len, hidden_size)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>    <br>    <span class="hljs-comment"># 生成因果掩码（下三角矩阵），这里就不刻意生成 padding_mask</span><br>    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=<span class="hljs-number">1</span>).<span class="hljs-built_in">bool</span>()<br>    <br>    <span class="hljs-comment"># 创建多头注意力模块</span><br>    mha = MultiHeadAttention(hidden_size, num_heads)<br>    <br>    <span class="hljs-comment"># 计算多头注意力输出</span><br>    output = mha(hidden_state, causal_mask=causal_mask)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, hidden_state.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br>    <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>test_MHA()<br></code></pre></td></tr></table></figure><h2 id="mha-with-kv-cache">MHA with KV Cache</h2><p>键值缓存（KV Cache）主要用于 Decoder 的 Next Token Prediction时减少重复计算，通过缓存并逐步更新键（Key）和值（Value），来用空间换时间。</p><p>但要注意即使是 Decoder-only的模型，在预处理输入（Prefill）的时候也不需要利用 KV Cache（P/D分离），本代码只作为示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MultiHeadAttention</span>(<span class="hljs-params">torch.nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_size, num_heads</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.num_heads = num_heads<br>        self.head_dim = hidden_size // num_heads<br>        <br>        self.q_linear = nn.Linear(hidden_size, hidden_size) <br>        self.k_linear = nn.Linear(hidden_size, hidden_size)<br>        self.v_linear = nn.Linear(hidden_size, hidden_size)<br>        self.o_linear = nn.Linear(hidden_size, hidden_size)<br>        <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, hidden_state, causal_mask=<span class="hljs-literal">None</span>, past_key_value=<span class="hljs-literal">None</span>, use_cache=<span class="hljs-literal">False</span></span>):</span><br>        batch_size = hidden_state.size(<span class="hljs-number">0</span>)<br>        <br>        <span class="hljs-comment"># 计算 Q、K、V，注意此时只有一个 Token</span><br>        query = self.q_linear(hidden_state)  <span class="hljs-comment"># (batch_size, 1, hidden_size)</span><br>        key = self.k_linear(hidden_state)<br>        value = self.v_linear(hidden_state)<br>        <br>        <span class="hljs-comment"># 分割多头，得到形状：(batch_size, num_heads, 1, head_dim)</span><br>        query = query.view(batch_size, -<span class="hljs-number">1</span>, self.num_heads, self.head_dim).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        key = key.view(batch_size, -<span class="hljs-number">1</span>, self.num_heads, self.head_dim).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        value = value.view(batch_size, -<span class="hljs-number">1</span>, self.num_heads, self.head_dim).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        <br>        <span class="hljs-comment"># 若存在缓存，拼接当前 K、V</span><br>        <span class="hljs-keyword">if</span> past_key_value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            past_key, past_value = past_key_value<br>            key = torch.cat([past_key, key], dim=<span class="hljs-number">2</span>)  <span class="hljs-comment"># (batch_size, num_heads, seq_len, head_dim)</span><br>            value = torch.cat([past_value, value], dim=<span class="hljs-number">2</span>)<br>        <br>        <span class="hljs-comment"># 保存新的缓存</span><br>        new_past_key_value = (key, value) <span class="hljs-keyword">if</span> use_cache <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        <br>        <span class="hljs-comment"># 计算注意力分数，attention_scores 形状: (batch_size, num_heads, 1, seq_len)</span><br>        attention_scores = torch.matmul(query, key.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>)) \<br>        / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))<br>        <br>        <span class="hljs-comment"># 应用因果掩码（若需要）</span><br>        <span class="hljs-keyword">if</span> causal_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            attention_scores += causal_mask * -<span class="hljs-number">1e9</span><br>            <br>        <span class="hljs-comment"># 计算注意力输出</span><br>        attention_probs = torch.softmax(attention_scores, dim=-<span class="hljs-number">1</span>)<br>        output = torch.matmul(attention_probs, value)<br>        <br>        <span class="hljs-comment"># 合并多头并线性变换</span><br>        output = output.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).view(batch_size, -<span class="hljs-number">1</span>, self.num_heads * self.head_dim)<br>        output = self.o_linear(output)<br>        <br>        <span class="hljs-keyword">return</span> (output, new_past_key_value) <span class="hljs-keyword">if</span> use_cache <span class="hljs-keyword">else</span> output<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_MHA_with_cache</span>():</span><br>    batch_size = <span class="hljs-number">2</span><br>    seq_len = <span class="hljs-number">5</span><br>    hidden_size = <span class="hljs-number">64</span><br>    num_heads = <span class="hljs-number">4</span><br>    <br>    <span class="hljs-comment"># 构造输入，模拟整个序列</span><br>    hidden_state = torch.randn(batch_size, seq_len, hidden_size)<br>    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=<span class="hljs-number">1</span>).<span class="hljs-built_in">bool</span>()<br>    <br>    <span class="hljs-comment"># 刻意分步处理，使用 KV 缓存</span><br>    past_key_value = <span class="hljs-literal">None</span><br>    outputs = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(seq_len):<br>        <span class="hljs-comment"># 当前步骤的输入（单个 token）</span><br>        current_input = hidden_state[:, i:i+<span class="hljs-number">1</span>, :]<br>        <span class="hljs-comment"># 生成当前步骤的因果掩码（仅允许关注到当前位置及之前的）</span><br>        current_causal_mask = causal_mask[i:i+<span class="hljs-number">1</span>, :i+<span class="hljs-number">1</span>]  <span class="hljs-comment"># (1, i+1)</span><br>        <span class="hljs-comment"># 前向传播</span><br>        output_step, past_key_value = mha(<br>            current_input,<br>            causal_mask=current_causal_mask,<br>            past_key_value=past_key_value,<br>            use_cache=<span class="hljs-literal">True</span><br>        )<br>        outputs.append(output_step)<br>    <br>    <span class="hljs-comment"># 合并分布输出</span><br>    output = torch.cat(outputs, dim=<span class="hljs-number">1</span>)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, hidden_state.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    test_MHA_with_cache()<br></code></pre></td></tr></table></figure><h2 id="多查询注意力mqa">多查询注意力（MQA)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MultiQueryAttention</span>(<span class="hljs-params">torch.nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_size, num_heads</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.num_heads = num_heads<br>        self.head_dim = hidden_size // num_heads<br>        <br>        <span class="hljs-comment"># 初始化 Q、K、V 投影矩阵，注意这里的 K V 比原来更小</span><br>        self.q_linear = nn.Linear(hidden_size, hidden_size)<br>        self.k_linear = nn.Linear(hidden_size, self.head_dim)<br>        self.v_linear = nn.Linear(hidden_size, self.head_dim)<br>        <br>        self.o_linear = nn.Linear(hidden_size, hidden_size)<br>        <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, hidden_state, causal_mask=<span class="hljs-literal">None</span>, padding_mask=<span class="hljs-literal">None</span></span>):</span><br>        batch_size = hidden_state.size(<span class="hljs-number">0</span>)<br>        <br>        query = self.q_linear(hidden_state)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>        key = self.k_linear(hidden_state)    <span class="hljs-comment"># (batch_size, seq_len, head_dim)</span><br>        value = self.v_linear(hidden_state)  <span class="hljs-comment"># (batch_size, seq_len, head_dim)</span><br>        <br>        <span class="hljs-comment"># 分割头部，K V 矩阵也要加上一个维度</span><br>        query = self.split_head(query)  <span class="hljs-comment"># (batch_size, num_heads, seq_len, head_dim)</span><br>        key = self.split_head(key, <span class="hljs-number">1</span>)   <span class="hljs-comment"># (batch_size, 1, seq_len, head_dim)</span><br>        value = self.split_head(value, <span class="hljs-number">1</span>) <span class="hljs-comment"># (batch_size, 1, seq_len, head_dim)</span><br>        <br>        <span class="hljs-comment"># 计算注意力分数，自动广播，(batch_size, num_heads, seq_len, seq_len)</span><br>        attention_scores = torch.matmul(query, key.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))<br>        <br>        <span class="hljs-keyword">if</span> causal_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            attention_scores += causal_mask * -<span class="hljs-number">1e9</span>  <br><br>        <span class="hljs-keyword">if</span> padding_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            padding_mask = padding_mask.unsqueeze(<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">1</span>)<br>            attention_scores += padding_mask * -<span class="hljs-number">1e9</span><br>            <br>        attention_probs = torch.softmax(attention_scores, dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch_size, num_heads, seq_len, seq_len)</span><br><br>        output = torch.matmul(attention_probs, value)  <span class="hljs-comment"># (batch_size, num_heads, seq_len, head_dim)</span><br>        <br>        <span class="hljs-comment"># 对注意力输出进行拼接，(batch_size, seq_len, hidden_size)</span><br>        output = output.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).view(batch_size, -<span class="hljs-number">1</span>, self.head_dim * self.num_heads)<br>        <br>        output = self.o_linear(output)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>        <br>        <span class="hljs-keyword">return</span> output<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">split_head</span>(<span class="hljs-params">self, x, head_num=<span class="hljs-literal">None</span></span>):</span><br>        batch_size = x.size(<span class="hljs-number">0</span>)  <span class="hljs-comment"># 获取批量大小</span><br>        <span class="hljs-keyword">if</span> head_num <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            head_num = self.num_heads  <span class="hljs-comment"># 默认使用类中的 num_heads</span><br>        <br>        <span class="hljs-comment"># 返回形状: (batch_size, head_num, seq_len, head_dim)</span><br>        <span class="hljs-keyword">return</span> x.reshape(batch_size, -<span class="hljs-number">1</span>, head_num, self.head_dim).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h2 id="分组查询注意力gqa">分组查询注意力（GQA）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GroupQueryAttention</span>(<span class="hljs-params">torch.nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_size, num_heads, group_num</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.num_heads = num_heads<br>        self.head_dim = hidden_size // num_heads<br>        self.group_num = group_num  <span class="hljs-comment"># 组的数量</span><br>        <br>        <span class="hljs-comment"># 初始化 Q、K、V 投影矩阵，注意这里的 K V 做了折衷</span><br>        self.q_linear = nn.Linear(hidden_size, hidden_size)  <span class="hljs-comment"># (hidden_size, hidden_size)</span><br>        self.k_linear = nn.Linear(hidden_size, self.group_num * self.head_dim)  <span class="hljs-comment"># (hidden_size, group_num * head_dim)</span><br>        self.v_linear = nn.Linear(hidden_size, self.group_num * self.head_dim)  <span class="hljs-comment"># (hidden_size, group_num * head_dim)</span><br>        <br>        self.o_linear = nn.Linear(hidden_size, hidden_size)  <span class="hljs-comment"># (hidden_size, hidden_size)</span><br>        <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, hidden_state, causal_mask=<span class="hljs-literal">None</span>, padding_mask=<span class="hljs-literal">None</span></span>):</span><br>        batch_size = hidden_state.size(<span class="hljs-number">0</span>)<br>        <br>        query = self.q_linear(hidden_state)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>        key = self.k_linear(hidden_state)    <span class="hljs-comment"># (batch_size, seq_len, group_num * head_dim)</span><br>        value = self.v_linear(hidden_state)  <span class="hljs-comment"># (batch_size, seq_len, group_num * head_dim)</span><br>        <br>        <span class="hljs-comment"># 分割头部，将每个头的维度拆分出来</span><br>        query = self.split_head(query)  <span class="hljs-comment"># (batch_size, num_heads, seq_len, head_dim)</span><br>        key = self.split_head(key, self.group_num)  <span class="hljs-comment"># (batch_size, num_heads, seq_len, head_dim)</span><br>        value = self.split_head(value, self.group_num)  <span class="hljs-comment"># (batch_size, num_heads, seq_len, head_dim)</span><br>        <br>        <span class="hljs-comment"># 计算注意力分数，自动广播，(batch_size, num_heads, seq_len, seq_len)</span><br>        attention_scores = torch.matmul(query, key.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))<br>        <br>        <span class="hljs-keyword">if</span> causal_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            attention_scores += causal_mask * -<span class="hljs-number">1e9</span>  <br><br>        <span class="hljs-keyword">if</span> padding_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            padding_mask = padding_mask.unsqueeze(<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">1</span>)<br>            attention_scores += padding_mask * -<span class="hljs-number">1e9</span><br>        <br>        attention_probs = torch.softmax(attention_scores, dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch_size, num_heads, seq_len, seq_len)</span><br>        <br>        output = torch.matmul(attention_probs, value)  <span class="hljs-comment"># (batch_size, num_heads, seq_len, head_dim)</span><br>        <br>        <span class="hljs-comment"># 对注意力输出进行拼接，形状: (batch_size, seq_len, hidden_size)</span><br>        output = output.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).view(batch_size, -<span class="hljs-number">1</span>, self.head_dim * self.num_heads)<br>        <br>        <span class="hljs-comment"># 通过线性层将拼接后的输出变换为所需的输出维度</span><br>        output = self.o_linear(output)  <span class="hljs-comment"># (batch_size, seq_len, hidden_size)</span><br>        <br>        <span class="hljs-keyword">return</span> output<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">split_head</span>(<span class="hljs-params">self, x, group_num=<span class="hljs-literal">None</span></span>):</span><br>        batch_size, seq_len = x.size()[:<span class="hljs-number">2</span>]  <span class="hljs-comment"># 获取批量大小和序列长度</span><br>        <br>        <span class="hljs-keyword">if</span> group_num <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span> x.view(batch_size, -<span class="hljs-number">1</span>, self.num_heads, self.head_dim).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 将 hidden_size 分割为 group_num 和 head_dim</span><br>            x = x.view(batch_size, -<span class="hljs-number">1</span>, group_num, self.head_dim).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>            <span class="hljs-comment"># 再将其手动 expand 到相同大小</span><br>            x = x[:, :, <span class="hljs-literal">None</span>, :, :].expand(batch_size, group_num, self.num_heads // group_num, seq_len, self.head_dim).view(batch_size, self.num_heads, seq_len, self.head_dim)<br>            <span class="hljs-keyword">return</span> x <span class="hljs-comment"># 形状: (batch_size, num_heads, seq_len, head_dim)</span><br></code></pre></td></tr></table></figure><h2 id="多头隐注意力mla">多头隐注意力（MLA）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">TODO<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>手撕经典算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker学习笔记 #2 常用命令</title>
    <link href="/Docker-Note-2.html"/>
    <url>/Docker-Note-2.html</url>
    
    <content type="html"><![CDATA[<p>软件开发最大的麻烦事之一就是<strong>环境配置、操作系统设置、各种库和组件的安装</strong>。目前最流行的Linux 容器解决方案之一就是 Docker，它极大地缓解了上述问题。</p><p>本篇将介绍一些 Docker 的核心命令，用于高效管理 Docker环境。此外，还补充了 Buildah 工具的简单用法。</p><h2 id="docker-服务认证">Docker 服务认证</h2><p>在个人电脑上，要想拉取私有镜像<strong>必须在镜像仓库上进行身份验证</strong>。使用 docker 命令工具来登录到 Docker Hub。为了上传镜像，你需要登录 DockerHub帐户并且<strong>拥有适当的权限</strong>。上传的镜像会自动标记为<strong>私有</strong>，除非你手动将其设为公开。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 登录命令，之后会被要求输入 Docker Hub 的用户名和密码。</span><br>docker login<br></code></pre></td></tr></table></figure><p>如果是在特定的私人或公司仓库，则需要指定仓库地址登录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 指定地址登录，之后会被要求输入用户名和密码</span><br>docker login registry.example.com<br><br><span class="hljs-comment"># 直接指定用户名和密码登录</span><br>docker login --username &lt;USERNAME&gt; --password &lt;PASSWORD&gt; registry.example.com<br></code></pre></td></tr></table></figure><blockquote><p>某些公司仓库可能需要使用证书进行认证。这时需要将证书文件放在 Docker的配置目录中。</p></blockquote><h2 id="管理镜像与容器">管理镜像与容器</h2><h3 id="docker-镜像相关命令">Docker 镜像相关命令</h3><p>在 Docker中，一个镜像的标识符包含<strong>名称和标签</strong>两部分，例如在<code>docker pull ubuntu:latest</code> 命令中，<code>ubuntu</code>是镜像的名称，<code>latest</code>表示标签（tag），通常用于指定镜像的版本。默认情况下，<code>latest</code>表示最新版本的镜像。</p><p><strong>拉取已有镜像</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 搜索镜像，默认在 Docker Hub 上查找所需的镜像，会显示包含关键字的公共镜像列表</span><br>docker search ubuntu<br><br><span class="hljs-comment"># 拉取镜像，获取指定版本的镜像文件</span><br>docker pull ubuntu:latest<br><br><span class="hljs-comment"># 列出本地镜像，显示所有已经下载到本地的镜像</span><br>docker images<br><br><span class="hljs-comment"># 删除镜像，移除不再需要的镜像</span><br>docker rmi ubuntu:latest<br></code></pre></td></tr></table></figure><p><strong>构建与上传镜像</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 构建镜像，根据指定的 Dockerfile 文件创建一个新镜像，其中 -t 用于指定标识符</span><br><span class="hljs-comment"># . 表示 Dockerfile 和所有相关配置文件所在目录</span><br>docker build -t my-nginx-image:1.0 .<br><br><span class="hljs-comment"># 推送镜像到 Docker Hub (默认)</span><br>docker push username/my-nginx-image:1.0<br><br><span class="hljs-comment"># 推送镜像到私有仓库 registry.example.com</span><br><span class="hljs-comment"># 为了将镜像指定到目标仓库，需要重新打上标签。注意这里不会删除原来的标签，多个标签可以同时存在！</span><br><span class="hljs-comment"># 镜像名称的格式通常为：[仓库地址]/[用户名或组织名]/[镜像名]:[标签]</span><br>docker tag my-nginx-image:1.0 registry.example.com/hewei2001/my-nginx-image:1.0<br>docker push registry.example.com/hewei2001/my-nginx-image:1.0<br></code></pre></td></tr></table></figure><h3 id="docker-容器相关命令">Docker 容器相关命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建和运行容器，可以指定镜像名称、环境变量、端口映射等</span><br><span class="hljs-comment"># -d 表示以分离模式运行容器，即容器在后台运行。</span><br><span class="hljs-comment"># -p 80:80 表示将容器的 80 端口映射到主机的 80 端口，主机可以访问 80 端口来访问容器中的服务</span><br><span class="hljs-comment"># --name 表示为容器指定一个名称</span><br><span class="hljs-comment"># nginx 指定要运行的镜像名称，该镜像须在本地存储</span><br>docker run -d -p 80:80 --name myweb my-nginx-image:1.0<br><br><span class="hljs-comment"># 查看正在运行的容器，加上 -a 选项可以查看所有容器，包括已停止的容器</span><br>docker ps<br>docker ps -a<br><br><span class="hljs-comment"># 停止正在运行的容器</span><br>docker stop myweb<br><br><span class="hljs-comment"># 重新启动已停止的容器</span><br>docker start myweb<br><br><span class="hljs-comment"># 通过 bash 进入一个正在运行的容器，进行操作和调试，这里的 /bin/bash 被称为 Entrypoint</span><br>docker <span class="hljs-built_in">exec</span> -it myweb /bin/bash<br><br><span class="hljs-comment"># 删除容器</span><br>docker rm myweb<br></code></pre></td></tr></table></figure><h2 id="管理数据与网络">管理数据与网络</h2><h3 id="数据的转移与同步">数据的转移与同步</h3><p>Docker 提供多种方式管理容器中的数据，确保数据持久化和跨容器共享。</p><p><strong>一次性文件/目录传输</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 从容器复制到本地</span><br>docker cp myweb:/path/to/file/<span class="hljs-keyword">in</span>/container /path/to/<span class="hljs-built_in">local</span>/destination<br>  <br><span class="hljs-comment"># 从本地复制到容器</span><br>docker cp /path/to/<span class="hljs-built_in">local</span>/file myweb:/path/to/destination/<span class="hljs-keyword">in</span>/container<br></code></pre></td></tr></table></figure><p><strong>持续同步目录</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 使用卷 (Volumes) 创建一个数据卷进行同步</span><br><span class="hljs-comment"># `myvolume` 存在于主机文件系统中，独立于容器生命周期。数据卷可以在容器之间共享和重用</span><br>docker volume create myvolume<br><br><span class="hljs-comment"># 运行一个容器并挂载数据卷 `myvolume` 到容器内的 `/path/in/container` 路径</span><br>docker run -d -v myvolume:/path/<span class="hljs-keyword">in</span>/container --name mycontainer myimage<br><br><span class="hljs-comment"># 此外，也可以直接指定本地主机路径。/path/on/host 是本地路径，/path/in/container 是容器内的路径</span><br><span class="hljs-comment"># 本地目录和容器目录会实时同步所有变化</span><br>docker run -d -v /path/on/host:/path/<span class="hljs-keyword">in</span>/container --name mycontainer myimage<br><br><span class="hljs-comment"># 绑定挂载类似于卷，但它允许你直接挂载本地文件系统中的具体路径到容器中</span><br>docker run --mount <span class="hljs-built_in">type</span>=<span class="hljs-built_in">bind</span>,<span class="hljs-built_in">source</span>=/path/on/host,target=/path/<span class="hljs-keyword">in</span>/container --name mycontainer myimage<br></code></pre></td></tr></table></figure><p><strong>自动同步</strong>：</p><p>如果你经常需要处理复杂的容器配置，<code>docker-compose</code>是一个很好的工具。你可以在 <code>docker-compose.yml</code>文件中配置卷和绑定挂载，这样就不需要每次运行容器时都手动设置它们。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">version:</span> <span class="hljs-string">&#x27;3&#x27;</span><br><span class="hljs-attr">services:</span><br>  <span class="hljs-attr">myservice:</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">image_name</span><br>      <span class="hljs-attr">volumes:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">/path/on/host:/path/in/container</span><br></code></pre></td></tr></table></figure><p>这样配置后，每次使用 <code>docker-compose up</code>启动服务时，都会自动应用这些挂载设置。</p><h3 id="网络连接">网络连接</h3><p>Docker 容器的网络管理可以通过 Docker提供的网络功能实现，包括桥接网络、主机网络和自定义网络等。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 列出所有 Docker 网络</span><br>docker network ls<br><br><span class="hljs-comment"># 创建自定义网络</span><br>docker network create mynetwork<br><br><span class="hljs-comment"># 连接容器到自定义网络</span><br>docker network connect mynetwork mycontainer<br><br><span class="hljs-comment"># 从网络断开容器</span><br>docker network disconnect mynetwork mycontainer<br></code></pre></td></tr></table></figure><h2 id="buildah-工具">Buildah 工具</h2><p>Buildah 是一个开源工具，用于构建和管理 OCI（Open ContainerInitiative）容器镜像。它是由 Red Hat 开发的，并且可以独立于 Docker使用。Buildah提供了一种更加灵活和轻量级的方式来构建容器镜像，尤其<strong>在没有Docker Daemon 的环境中</strong>。相比 Docker，其优点如下：</p><ol type="1"><li>无守护进程模式：不像 Docker 需要一个长期运行的守护进程（DockerDaemon），Buildah可以在没有守护进程的情况下操作。因此也可以<strong>低成本地地实现Docker-in-Docker (DinD)操作</strong>，在公司场景可能会用到，因为公司申请的跳板机资源本身可能就是Docker。</li><li>安全性：由于 Buildah 可以在没有 root权限的用户空间中运行，因此可以提供更高的安全性。</li><li>兼容性：支持与 Docker 和 Podman 兼容的镜像和容器格式。</li></ol><p><strong>常用命令</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 列出全部</span><br>buildah images<br><br><span class="hljs-comment"># 离线安装</span><br>buildah pull mirrors.xxx.com/hewei2001/vllm-openai:latest<br><br><span class="hljs-comment"># 从镜像创建容器，得到 vllm-openai-working-container</span><br>buildah from mirrors.xxx.com/hewei2001/vllm-openai:latest<br><br><span class="hljs-comment"># 查看容器和镜像信息</span><br>buildah inspect vllm-openai-working-container<br><br><span class="hljs-comment"># 打开容器内的 bash</span><br>buildah run vllm-openai-working-container -- bash<br><br><span class="hljs-comment"># 将本地文件复制到容器中，例如，要将本地的 example.txt 文件复制到容器的 /tmp 目录</span><br>buildah copy vllm-openai-working-container example.txt /vllm-workspace/tmp/example.txt<br><br><span class="hljs-comment"># 修改容器的配置，比如设置环境变量、工作目录等</span><br>buildah config --env MY_VAR=value vllm-openai-working-container<br><br><span class="hljs-comment"># 创建一个新的镜像并命名为 my-new-image</span><br>buildah commit vllm-openai-working-container my-new-image<br><br><span class="hljs-comment"># 给你的镜像添加一个标签，这样可以更容易地识别和引用它</span><br>buildah tag my-new-image mirrors.xxx.com/hewei2001/vllm_dev:latest<br><br><span class="hljs-comment"># 将标记的镜像推送到远程仓库。确保你已经登录到相应的容器仓库</span><br>buildah push mirrors.xxx.com/hewei2001/vllm_dev:latest<br></code></pre></td></tr></table></figure><p><strong>数据的转移和同步</strong>：和 Docker 类似，但 Buildah<strong>不支持直接从容器拷贝内容到主机</strong>！可以先通过<code>buildah mount</code>命令挂载容器的文件系统，然后使用普通的文件操作命令（如<code>cp</code>）来复制文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 将本地文件复制到容器中</span><br>buildah copy vllm-openai-working-container example.txt /tmp/example.txt<br><br><span class="hljs-comment"># 将本地目录整个复制到容器中</span><br>buildah copy vllm-openai-working-container /path/to/<span class="hljs-built_in">local</span>/dir /path/<span class="hljs-keyword">in</span>/container<br><br><span class="hljs-comment"># 从容器拷贝内容到主机的步骤</span><br><span class="hljs-comment"># step1: 挂载容器的文件系统</span><br>mount_path=$(buildah mount vllm-openai-working-container)<br><br><span class="hljs-comment"># step2: 使用 `cp` 命令从挂载的文件系统复制文件到本地（该方法也支持本地到容器）</span><br>cp <span class="hljs-variable">$&#123;mount_path&#125;</span>/path/<span class="hljs-keyword">in</span>/container/file.txt /<span class="hljs-built_in">local</span>/path<br><br><span class="hljs-comment"># step3: 完成操作后，可以取消挂载</span><br>buildah umount vllm-openai-working-container<br></code></pre></td></tr></table></figure><blockquote><p>如果需要频繁地在本地和容器之间同步文件或目录，可以考虑使用脚本自动化<code>buildah copy</code> 和 <code>cp</code>操作。此外，对于更复杂的同步需求，可能需要考虑使用版本控制系统（如Git），或者配置持续集成/持续部署（CI/CD）流程来自动化代码和资源的部署。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker学习笔记 #1 基本概念与原理</title>
    <link href="/Docker-Note-1.html"/>
    <url>/Docker-Note-1.html</url>
    
    <content type="html"><![CDATA[<p>软件开发最大的麻烦事之一就是<strong>环境配置、操作系统设置、各种库和组件的安装</strong>。目前最流行的Linux 容器解决方案之一就是 Docker，它极大地缓解了上述问题。</p><p>Docker 是一个基于 Go 语言实现的开源项目，由 dotCloud 公司在 2013年初发起。其目标是「Build, Ship and Run Any App,Anywhere」——<strong>构建、传输并运行任意应用程序，随处可运行</strong>。</p><p>Docker主要的概念包括<strong>镜像、容器和仓库</strong>。其核心技术基于 Linux容器技术，能够有效地将操作系统资源划分为多个独立的组，以更好地管理资源使用。</p><h2 id="什么是-docker">什么是 Docker？</h2><h3 id="虚拟机与-linux-容器">虚拟机与 Linux 容器</h3><p>虚拟机（VirtualMachine，VM）是一种运行完整操作系统的虚拟化解决方案。它允许在一个操作系统中运行另一个操作系统，例如在Windows 上运行Linux。对于应用程序来说，虚拟机与真实系统无异，而对于底层系统来说，虚拟机只是一个文件，便于创建和删除。尽管虚拟机能够还原软件的原始环境，但也存在以下缺点：</p><ol type="1"><li><strong>资源占用多</strong>：虚拟机会独占一部分内存和硬盘空间，甚至在虚拟机内的应用程序仅占用少量资源时，虚拟机本身仍然需要大量的内存和存储来运行。</li><li><strong>冗余步骤多</strong>：作为一个完整的操作系统，虚拟机需要执行一系列系统级操作，例如用户登录，这些步骤往往无法省略。</li><li><strong>启动慢</strong>：启动虚拟机的时间与启动操作系统相当，通常需要数分钟才能开始运行应用程序。</li></ol><p>为了克服虚拟机的这些不足，Linux发展出了一种更轻量级的虚拟化技术——<strong>Linux 容器</strong>（LinuxContainers，LXC）。Linux容器并不模拟一个完整的操作系统，而是通过隔离进程来实现。对于容器内部的进程来说，它们接触到的资源都是虚拟化的，从而实现与底层系统的隔离。</p><p>相较于虚拟机，Linux 容器有以下优势：</p><ol type="1"><li><strong>启动快</strong>：容器内的应用程序直接作为底层系统的一个进程运行，启动速度快，类似于启动一个本地进程。</li><li><strong>资源占用少</strong>：容器仅使用所需的资源，不会占用未使用的资源。而虚拟机作为完整的操作系统，需要占用所有资源。此外，多个容器可以共享资源，而虚拟机则是资源独享的。</li><li><strong>体积小</strong>：容器只包含所需的组件，因此文件大小远小于包含整个操作系统的虚拟机文件。</li></ol><h3 id="通俗解释-docker">通俗解释 Docker</h3><p>Docker的理念类似于集装箱。在运输业中，集装箱标准化了货物的存储和运输方式，使得各种不同的货物可以安全、整齐地放置在同一艘船上，而不会相互影响。</p><p>Docker也是类似的思想：它将应用程序及其运行环境打包成标准化的单元，确保在不同的系统和环境中能够一致地运行。它具有以下优势：</p><ol type="1"><li><strong>隔离不同的应用环境</strong>：不同的应用程序可能依赖不同的环境配置，如.NET 和 PHP可能需要不同的库和依赖。在传统环境下，这可能需要在同一服务器上安装不同的依赖，导致配置冲突。使用Docker，可以在同一物理机上运行「<strong>多个隔离的容器</strong>」，每个容器都有自己的环境配置，类似于虚拟机，但资源开销更小。</li><li><strong>一致的开发和生产环境</strong>：开发人员可能使用Ubuntu，而运维团队使用CentOS。传统情况下，从开发环境到生产环境的迁移可能会遇到兼容性问题。使用Docker，开发人员可以将开发环境封装在容器中，运维团队只需部署该容器即可，无需担心「<strong>底层系统的差异</strong>」。</li><li><strong>高效的资源利用</strong>：虚拟机会占用大量的系统资源，即使虚拟机内部的应用程序负载较轻。Docker通过更高效的资源共享机制，使得多容器在同一物理机上能够更好地利用系统资源。</li></ol><h2 id="docker-中的三个概念">Docker 中的三个概念</h2><h3 id="镜像-image">镜像 | Image</h3><p>镜像（Image）是 Docker的基础概念之一，可以类比为虚拟机中的镜像。它包含应用程序运行所需的所有内容，包括操作系统、应用程序代码、库和依赖项。<strong>镜像是不可变的、只读的文件，类似于容器的模板</strong>。通过镜像，可以创建一个或多个Docker 容器。例如，一个 Ubuntu 镜像包含了 Ubuntu 操作系统的环境，而一个Apache 镜像则是在 Ubuntu 镜像的基础上加入了 Apache 服务器。</p><p>镜像是以二进制文件的形式存在的，<strong>可以从已有的镜像基础上定制新的镜像</strong>。一般情况下，推荐使用已有的镜像文件，而不是从零开始创建。通常，我们会从Docker 仓库拉取已经配置好的镜像。</p><p>此外，镜像可以通过 <strong>Dockerfile文件</strong>进行构建，这是一种<strong>脚本</strong>文件，用于定义如何从基础镜像生成新的镜像。以下是一个示例，用于创建一个基于Nginx 的 Web 服务器：</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs dockerfile"><span class="hljs-comment"># 使用官方的 Nginx 镜像作为基础镜像</span><br><span class="hljs-keyword">FROM</span> nginx:alpine<br><br><span class="hljs-comment"># 指定镜像的维护者信息</span><br><span class="hljs-keyword">MAINTAINER</span> Your Name &lt;your.email@example.com&gt;<br><br><span class="hljs-comment"># 复制本地的 HTML 文件到镜像中的默认 Nginx 网站目录</span><br><span class="hljs-keyword">COPY</span><span class="bash"> ./index.html /usr/share/nginx/html/index.html</span><br><br><span class="hljs-comment"># 用于在镜像构建过程中执行命令，如安装软件包、设置环境等。</span><br><span class="hljs-keyword">RUN</span><span class="bash"> apt-get update</span><br><br><span class="hljs-comment"># 设置环境变量</span><br><span class="hljs-keyword">ENV</span> PORT <span class="hljs-number">8080</span><br><br><span class="hljs-comment"># 设置工作目录，后续的命令将以该目录为当前目录</span><br><span class="hljs-keyword">WORKDIR</span><span class="bash"> /app</span><br><br><span class="hljs-comment"># 暴露 80 端口</span><br><span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">80</span><br><br><span class="hljs-comment"># 指定容器启动时要执行的命令，这里是运行 Nginx</span><br><span class="hljs-keyword">CMD</span><span class="bash"> [<span class="hljs-string">&quot;nginx&quot;</span>, <span class="hljs-string">&quot;-g&quot;</span>, <span class="hljs-string">&quot;daemon off;&quot;</span>]</span><br></code></pre></td></tr></table></figure><p>之后就可以通过 <code>docker build -t my-nginx-image:1.0 .</code>来创建镜像了。</p><h3 id="容器-container">容器 | Container</h3><p>容器（Container）是镜像的运行实例。它是一个轻量级的、独立的执行环境，可以看作是一个极简的Linux 系统，包括独立的文件系统、网络和进程空间。容器利用 Docker引擎来运行和隔离应用程序，各个容器之间是相互独立的。</p><p>容器的特点是启动快、资源消耗少。它们<strong>由镜像创建而来</strong>，当启动一个容器时，Docker会在镜像的基础上添加一个<strong>可写层</strong>。这意味着镜像本身不变，而容器的任何修改都记录在这个可写层中。</p><p>容器可以创建、启动、停止和删除。即使容器停止运行，其对应的容器文件依然存在，直到显式删除为止。删除容器时，可以使用<code>docker container rm</code> 命令，同时可以使用<code>docker container ls --all</code>查看所有容器，包括已经停止的容器。</p><h3 id="仓库-repository">仓库 | Repository</h3><p>仓库（Repository）是存放镜像的地方，可以类比为代码仓库。它是 Docker<strong>镜像文件的集中存放地</strong>。一个仓库可以包含多个版本的镜像，通过<strong>标签</strong>（tag）来区分。Docker的官方公共仓库是 Docker Hub，它提供了大量的基础镜像，如 CentOS、Ubuntu等，以及由社区或个人维护的镜像。</p><p>仓库可以分为<strong>公共仓库和私有仓库</strong>。公共仓库是开放的，任何人都可以访问和下载镜像。而私有仓库则是用户自己搭建的镜像存储环境，常用于企业内部。Docker提供了一个名为 <code>registry</code> 的官方镜像，用于搭建私有仓库。</p><p>在实际使用中，镜像可以从仓库中下载，也可以上传到仓库进行共享。对于企业和开发者来说，合理地使用和管理仓库，可以大大提高应用的开发和部署效率。</p><h2 id="docker-原理">Docker 原理</h2><p>Docker的核心原理在于通过<strong>操作系统级别的虚拟化技术</strong>提供独立的运行环境。Docker通过利用 Linux内核中的多项功能，如命名空间（Namespaces）和控制组（Cgroups），来实现容器的隔离、资源管理和安全性。接下来介绍Docker 工作原理的主要组成部分。</p><h3 id="命名空间-namespaces">命名空间 | Namespaces</h3><p>命名空间是 Linux内核的一项功能，它提供了一种隔离机制，使得不同的进程看起来像是在独立的操作系统中运行。Docker使用多种类型的命名空间来隔离容器之间的环境，包括：</p><ul><li><strong>PID 命名空间</strong>：隔离进程 ID，使得容器内的进程 ID不与宿主机或其他容器的进程 ID 冲突。</li><li><strong>网络命名空间</strong>：隔离网络接口和网络栈，确保容器有自己的虚拟网络接口和IP 地址，互相隔离。</li><li><strong>挂载命名空间</strong>：提供独立的文件系统视图，使得容器只能看到其自身的文件系统，不会影响宿主机的文件系统。</li><li><strong>UTS命名空间</strong>：隔离主机名和域名，允许容器拥有独立的主机名。</li><li><strong>IPC 命名空间</strong>：隔离系统 V共享内存、信号量和消息队列，使得这些资源在容器之间不共享。</li><li><strong>用户命名空间</strong>：隔离用户和组ID，允许在容器中运行的进程以不同于宿主机的用户身份运行，提高安全性。</li></ul><h3 id="控制组-cgroups">控制组 | Cgroups</h3><p>控制组是一种 Linux 内核功能，用于限制、计量和隔离进程组的资源使用，如CPU、内存、磁盘 I/O 和网络带宽。Docker利用控制组来管理和限制容器的资源使用，确保容器之间不会相互干扰，并且能够有效利用系统资源。</p><h3 id="联合文件系统-unionfs">联合文件系统 | UnionFS</h3><p>Docker使用联合文件系统（UnionFS）来构建镜像和容器。联合文件系统<strong>允许将多个文件系统层叠加在一起</strong>，并提供一个统一的视图。Docker镜像<strong>由多层只读文件系统组成，最上层是可写的</strong>。这种分层设计使得镜像可以复用已有层，减少存储空间的占用并加速构建速度。</p><p>当创建一个容器时，Docker会在镜像的最上层添加一个可写层。容器运行时的所有变化都只发生在这个可写层上，而基础镜像层保持不变。这种设计不仅提高了效率，还简化了容器的版本管理和部署。</p><h3 id="容器引擎-docker-engine">容器引擎 | Docker Engine</h3><p>Docker Engine 是 Docker的核心组件，是一个基于客户端-服务器（Client-Server）架构的应用程序，负责构建、运行和管理Docker 容器。</p><p>Docker Engine 由以下几个主要部分组成：</p><ul><li>Docker 守护进程（Docker Daemon）：运行在宿主机上，负责处理 Docker客户端发送的请求。它是客户端-服务器模型中的<strong>服务器部分</strong>。DockerDaemon 管理所有的 Docker对象，包括容器、镜像、网络和数据卷。它还负责构建和分发容器镜像。使用<strong>REST API</strong> 提供服务，客户端通过这个 API 与 Daemon进行通信。</li><li>Docker REST API：Docker Daemon 暴露的编程接口，允许外部工具与 Docker进行交互。它提供了一组<strong>标准化的 HTTP 请求</strong>，可以执行所有Docker 操作，例如创建容器、管理镜像和设置网络等。通过 RESTAPI，可以使用编程语言创建自定义的 Docker 工具和自动化脚本。任何支持 HTTP请求的工具都可以通过 API 与 Docker 交互，例如发送 <code>curl</code>请求。</li><li>Docker CLI：Docker 提供的命令行工具，是与 Docker Daemon进行<strong>交互的主要方式</strong>。通过 Docker CLI，用户可以执行各种Docker操作，如启动容器（<code>docker run</code>）、查看正在运行的容器（<code>docker ps</code>）、构建镜像（<code>docker build</code>）等。DockerCLI 可以运行在任何操作系统上，并能够通过网络与远程的 Docker Daemon通信。</li><li>Docker Machine：用于在各种主机上安装和管理 DockerEngine。它简化了在不同平台上（如本地开发机、云服务器等）配置 Docker环境的过程。通过 Docker Machine，可以自动创建主机、安装 DockerEngine，并配置 Docker 客户端连接到这些主机。</li></ul><h3 id="docker-registry">Docker Registry</h3><p>Docker Registry 是一个集中存储和分发 Docker 镜像的系统。Docker Hub是最流行的公共 DockerRegistry，提供了大量的公共镜像供用户使用。用户也可以搭建私有 DockerRegistry，用于企业内部的镜像管理。Docker 客户端通过<code>docker push</code> 和 <code>docker pull</code> 命令与 Registry进行交互，上传和下载镜像。</p>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vim学习笔记 #1 基础操作与配置</title>
    <link href="/Vim-Note-1.html"/>
    <url>/Vim-Note-1.html</url>
    
    <content type="html"><![CDATA[<p>Vim常常因其极高的学习门槛劝退很多新手，但在在一些特殊的场景中，你可能不得不抛弃图形文本编辑器，例如：实验室的服务器、公司的安全域主机（物理隔绝网络）等。</p><p>本文将介绍 Vim自带的基础操作，以及如何配置文件来实现更高级的操作。Vim 在大多数 Unix系统下都可以使用，本文的版本为 <code>vim8</code>，使用<code>vim --version</code> 即可查看。</p><h2 id="速查表">速查表</h2><p>Normal 模式下：</p><ul><li><code>w</code>：移动到下一个单词的开头</li><li><code>b</code>：移动到前一个单词的开头</li><li><code>u</code>：撤销上一步操作</li><li><code>Ctrl + r</code>：重做撤销的操作</li><li><code>y</code>：复制（yank）<ul><li><code>yy</code>：复制当前行</li><li><code>yw</code>：复制当前单词</li></ul></li><li><code>p</code>：粘贴（put）<ul><li><code>p</code>：在光标后粘贴</li><li><code>P</code>：在光标前粘贴</li></ul></li><li><code>d</code>：删除（delete）<ul><li><code>dd</code>：删除当前行</li><li><code>dw</code>：删除当前单词</li><li><code>ggdG</code>：删除全部内容</li></ul></li><li><code>x</code>：删除（剪切）光标所在字符</li><li><code>/pattern</code>：向下搜索 <code>pattern</code></li><li><code>?pattern</code>：向上搜索 <code>pattern</code></li><li><code>n</code>：跳到下一个匹配</li><li><code>N</code>：跳到上一个匹配</li></ul><h2 id="基础操作">基础操作</h2><h3 id="打开与退出">打开与退出</h3><p>输入 <code>vim</code> 即可进入 Vim 编辑器并打开空文件；输入<code>vim &lt;文件名&gt;</code>，即可用 Vim去编辑对应文件，如果文件不存在则会新建文件。</p><p>进入 Vim 之后是 Normal 模式，此时无法输入任何东西，使用<code>:q</code> 可以退出 Vim 编辑器，使用 <code>:w</code>可以保存文件，使用 <code>:wq</code> 可以保存文件并退出。</p><h3 id="insert-模式">Insert 模式</h3><p>要编辑文件，必须进入<strong>编辑模式</strong>，在 Normal 模式下输入<code>i</code>、<code>a</code>、<code>o</code> 即可进入：</p><ul><li><code>i</code>：在当前<strong>光标</strong>的位置插入（insert）</li><li><code>a</code>：在当前光标<strong>后</strong>的位置插入（append）</li><li><code>o</code>：在当前光标的<strong>下一行</strong>插入（open a linebelow）</li></ul><p>Vim 的开发人员为了提高编辑效率，大写字母<code>I</code>、<code>A</code>、<code>O</code> 也可以进入编辑模式：</p><ul><li><code>I</code>：从该行<strong>最前面</strong>开始编辑</li><li><code>A</code>：从从该行<strong>最后面</strong>开始编辑</li><li><code>O</code>：从光标所在行的<strong>上面另起一行</strong>开始编辑</li></ul><p>在编辑模式下，按 Esc 键即可退回到 Normal 模式，通常建议将 Esc键映射到键盘的 CapsLock 键，下文将介绍如何映射。</p><h3 id="光标移动">光标移动</h3><p>键盘自带的上、下、左、右方向键可以在任何模式使用。此外，在 Normal模式下，还可以使用 <code>hjkl</code>来代替方向键，分别代表<strong>左、下、上、右</strong>，这样就可以在手不离开键盘的情况下移动光标。</p><p>在 Normal 模式下，可以按 <code>gi</code>键以跳动到从编辑模式<strong>退出时</strong>光标所在的位置，并进入编辑模式。</p><p>此外 Normal 模式还支持更多光标移动的快捷键：</p><ol type="1"><li><p><strong>以单词为单位</strong>移动，其中小写字母的表示以<strong>非字母</strong>为分割单位（标点符号、空白符都是分割单位），大写字母仅以<strong>空白符</strong>为分割的单位：</p><ul><li><p><code>w/W</code>：移动到下一个单词的开头</p></li><li><p><code>e/E</code>：移动到下一个单词的结尾</p></li><li><p><code>b/B</code>：移动到上一个单词的开头</p></li></ul></li><li><p><strong>行间单个字符搜索</strong>移动：</p><ul><li><code>f&lt;char&gt;</code>：快速移动到 <char> 上面</li><li><code>F&lt;char&gt;</code>：快速移动到 <char>上面，从行尾开始搜索</li><li><code>t&lt;char&gt;</code>：快速移动到 <char> 的前一个字符</li><li><code>T&lt;char&gt;</code>：快速移动到 <char>的后一个字符，从行尾开始搜索</li><li>如果一行中有多个 <char>：可以用 <code>;</code> 继续搜索该行下一个<char>，或用 <code>,</code> 继续搜索该行上一个 <char></li></ul></li><li><p>快速移动到一行的<strong>行首或行尾</strong>:</p><ul><li><code>0</code>：快速移动到行首</li><li><code>$</code>：快速移动到行尾</li><li><code>^</code>：快速移动到非空白字符行首</li><li><code>g_</code>：快速移动到非空白字符行尾</li></ul></li><li><p>快速<strong>垂直</strong>移动：</p><ul><li><code>()</code>：在句子间移动（粒度更细）</li><li><code>&#123;&#125;</code>：在段落之间移动</li></ul></li><li><p>快速<strong>页面</strong>移动：</p><ul><li><code>gg</code>：移动到文件<strong>第一行</strong></li><li><code>G</code>：移动到文件<strong>最后一行</strong></li><li><code>H</code>：移动到屏幕的开头</li><li><code>M</code>：移动到屏幕的中间</li><li><code>L</code>：移动到屏幕的结尾</li><li><code>zz</code>：把当前行设置为屏幕中间</li><li><code>ctrl + u</code>：上翻页</li><li><code>ctrl + f</code>：下翻页</li></ul></li></ol><h3 id="删除修改">删除修改</h3><p>在 Normal 模式下，按 <code>x</code>可以快速剪切一个字符，<strong>剪切后可以粘贴</strong>，如果不粘贴即当删除使用。</p><p>同时，使用 <code>d</code>配合<strong>文本对象</strong>可以快速剪切一个单词，比如：</p><ul><li><code>dw</code>：剪切光标<strong>后面</strong>的单词（deleteword）</li><li><code>daw</code>：剪切光标<strong>所在</strong>的单词及旁边的空格（deletearound word）</li><li><code>diw</code>：剪切光标<strong>所在</strong>的单词，但<strong>不包括</strong>单词旁边的空格</li></ul><p>配合快速移动光标，可以：</p><ul><li><code>d0</code>：剪切当前光标到行首的内容</li><li><code>d$</code>：剪切当前光标到行尾的内容</li><li><code>dd</code>：剪切当前行</li><li><code>dt&lt;char&gt;</code>：从光标位置剪切到 <char> 之前的字符</li></ul><p>搭配数字可以<strong>多次执行</strong>：</p><ul><li><code>5dd</code>：剪切当前开始的 5 行</li><li><code>5x</code>：剪切 5 个字符</li></ul><p>在 Normal 模式下，支持多种快速修改方式：</p><ul><li><code>r</code>：替换一个字符，直接输入需要替换的字符即可</li><li><code>R</code>：持续替换，后续的输入都会替换光标所在字符</li><li><code>s</code>：删除当前字符并且进入编辑模式</li><li><code>S</code>：删除当前行并且进入编辑模式</li><li><code>c</code>：快速删除一个单词并且进入编辑模式，也支持三种文本对象：<code>cw</code>、<code>caw</code>、<code>ciw</code></li><li><code>C</code>：删除当前光标到行尾的内容并且进入编辑模式</li></ul><h3 id="撤销还原">撤销还原</h3><p>在 Normal 模式下：</p><ul><li><code>u</code>：撤销上次操作</li><li><code>ctrl + r</code>：还原上次撤销（撤销上一个<code>u</code>）</li></ul><h2 id="高级操作">高级操作</h2><h3 id="view-模式">View 模式</h3><p>Vim有三种视图模式，在此模式下，用命令移动光标，<strong>光标起始位置和当前位置之间的文本</strong>将会以高亮显示。这时就可以对这些高亮显示的文本进行整体的操作。</p><p>在此模式下也支持 <code>hjkl</code>移动光标，推荐配合数字键快速移动。移动的单位分为字符、行、列：</p><ul><li><code>v</code>：激活面向<strong>字符</strong>的视图模式</li><li><code>V</code>：激活面向<strong>行</strong>的视图模式</li><li><code>ctrl + v</code>：激活面向<strong>列块</strong>的视图模式</li><li><code>gv</code>：重选上次的高亮选区</li><li><code>o</code>：切换到高亮选区的起始段，从而选择另一侧</li><li><code>O</code>：在块模式下，切换到当前行的另一侧</li></ul><p>针对高亮显示的文本块，我们可以用命令 <code>~</code>进行大小写转换。用命令 <code>&gt;</code> 增加缩进，或用命令<code>&lt;</code> 减少缩进。用命令 <code>d</code> 删除全部，或用命令<code>y</code> 复制全部。</p><p>当我们想要退出可视化模式时，可以按 <code>ESC</code> 键或<code>Ctrl + c</code> 键。</p><h3 id="多文件操作">多文件操作</h3><p>Vim 中有以下概念：</p><ul><li><strong>buffer</strong>：指打开的<strong>一个文件的内存缓冲区</strong>，每个打开的文件对应一个buffer，之后的修改都是针对内存中的缓冲区，<strong>并不会直接保存到文件中</strong>。你可以在不同buffer 之间切换，从而在多个文件之间操作。当我们在命令行输入<code>:w</code> 的时候，才会将缓冲区中的内容写到文件中。</li><li><strong>window</strong>：是 buffer 的可视化的分割区域。一个 window可以显示一个 buffer的内容<strong>，通过分割窗口可以同时查看和编辑多个文件</strong>。</li><li><strong>tab</strong>：多个 window 组成的一个工作区。<strong>一个 tab页面可以包含多个分割窗口</strong>，每个窗口可以显示不同的buffer，从而提供一个更加灵活的多文件编辑环境。</li></ul><p>管理 Buffer：</p><ul><li><code>:e filename</code>：打开文件<code>filename</code>，并将其加载到新的 buffer 中。</li><li><code>:ls</code> 或 <code>:buffers</code>：列出所有已加载的buffer。</li><li><code>:bnext</code> 或 <code>:bn</code>：切换到下一个 buffer。</li><li><code>:bprev</code> 或 <code>:bp</code>：切换到上一个 buffer。</li><li><code>:b #</code>：切换到上一个使用的 buffer。</li><li><code>:bd</code>：关闭当前 buffer。</li><li><code>:bd [n]</code>：关闭编号为 <code>[n]</code> 的 buffer。</li></ul><blockquote><p><strong>注意</strong>：直接关闭一个 buffer (<code>:bd</code> 或<code>:bd [n]</code>) 时，其内容不会自动保存。如果 buffer有未保存的更改，Vim 会提示保存更改。你可以选择保存(<code>:w</code>)，放弃更改 (<code>:bd!</code>)，或者取消关闭操作。</p></blockquote><p>管理 Window：</p><ul><li><code>:split</code> 或 <code>:sp</code>：水平分割窗口，并打开当前buffer，<strong>默认显示同一个文件</strong>。</li><li><code>:vsplit</code> 或 <code>:vsp</code>：垂直分割窗口，并打开当前buffer，<strong>默认显示同一个文件</strong>。</li><li><code>Ctrl + w, w</code>：切换窗口焦点，循环。</li><li><code>Ctrl + w, q</code>：关闭当前窗口。</li><li><code>Ctrl + w, s</code>：水平分割窗口。</li><li><code>Ctrl + w, v</code>：垂直分割窗口。</li><li><code>Ctrl + w, h</code>：移动到左边的窗口。</li><li><code>Ctrl + w, j</code>：移动到下边的窗口。</li><li><code>Ctrl + w, k</code>：移动到上边的窗口。</li><li><code>Ctrl + w, l</code>：移动到右边的窗口。</li></ul><blockquote><p><strong>分割窗口</strong>：使用 <code>:sp</code> 或 <code>:vsp</code>命令分割窗口后，两个界面<strong>默认显示相同的 buffer的内容</strong>。可以可以使用 <code>:e</code> 或 <code>:bn</code>等操作切换窗口内的 buffer。窗口的大小可以用鼠标进行拖拽。</p></blockquote><p>管理 Tab：</p><ul><li><code>:tabnew</code> 或 <code>:tabnew filename</code>：打开新的 tab页，并在其中打开文件 <code>filename</code>。</li><li><code>:tabc</code> 或 <code>:tabclose</code>：关闭当前 tab 页。</li><li><code>:tabo</code> 或 <code>:tabonly</code>：关闭所有其他 tab页，只保留当前 tab 页。</li><li><code>:tabn</code> 或 <code>:tabnext</code>：切换到下一个 tab页。</li><li><code>:tabp</code> 或 <code>:tabprevious</code>：切换到上一个 tab页。</li><li><code>:tabs</code>：列出所有 tab 页及其编号。</li></ul><h3 id="查找替换">查找替换</h3><p>查询（Normal 模式下）：</p><ul><li><code>/&lt;内容&gt;</code> 向下查找内容</li><li><code>?&lt;内容&gt;</code> 向上查找内容</li><li><code>n</code> 查找之后，按 n 跳转到下一个匹配</li><li><code>N</code> 查找之后，按 N 跳转到上一个匹配</li><li><code>*</code> 查找之后，再对当前光标所在单词进行向下查找</li><li><code>#</code> 查找之后，再对当前光标所在单词进行向上查找</li></ul><p>替换（Normal模式下）则相对比较复杂，整体格式为：<code>[range]s/&#123;string1&#125;/&#123;string2&#125;/[flags]</code>，其中：</p><ul><li>range：要替换的范围，比如 <code>10,20</code>： 10-20行，<code>%</code>：全部</li><li>string1：要替换的字符串，string2：替换成为的字符串</li><li>flags：标志，常用的有 <code>g</code>（表示全局范围内执行），没有<code>g</code>的话，每一行只会替换<strong>该行中第一个匹配成功的</strong>。<code>c</code>：确认，在进行替换时可以<strong>选择是否</strong>进行替换。<code>n</code>：匹配的次数，但<strong>不进行替换</strong>。</li></ul><p>例如：</p><ul><li><code>s/s1/s2</code>：替换当前行第一个 s1 为 s2</li><li><code>s/s1/s2/g</code>：替换当前行中所有 s1 为 s2</li><li><code>%s/s1/s2/g</code>：替换文件中所有 s1 为 s2</li><li><code>%s/\&lt;num\&gt;/digit/g</code>：支持<strong>正则</strong>，只匹配num 并替换为 digit</li></ul><h3 id="复制粘贴">复制粘贴</h3><p>在 Normal 模式下，复制（称为 yank）和剪贴（称为delete）的基本命令分别是 <code>y</code> 和 <code>d</code>，而粘贴命令是<code>p</code>。<strong>这些命令都可以配合文本对象使用</strong>，以实现更灵活的操作。</p><ol type="1"><li><p>复制操作（yank）</p><ul><li><p><code>yy</code>：复制当前行。</p></li><li><p><code>yw</code>：复制当前单词。</p></li><li><p><code>y$</code>：复制从光标位置到行尾的内容。</p></li><li><p><code>y0</code>：复制从光标位置到行首的内容。</p></li></ul></li><li><p>剪切操作（delete）</p><ul><li><p><code>dd</code>：剪切当前行。</p></li><li><p><code>dw</code>：剪切当前单词。</p></li><li><p><code>d$</code>：剪切从光标位置到行尾的内容。</p></li><li><p><code>d0</code>：剪切从光标位置到行首的内容。</p></li></ul></li><li><p>粘贴操作（put）</p><ul><li><p><code>p</code>：在光标后粘贴。</p></li><li><p><code>P</code>：在光标前粘贴。</p></li></ul></li></ol><p><strong>进入 Visual模式后，可以选择要复制的部分</strong>，然后使用相应命令进行复制或剪切：</p><ul><li><code>v</code>：进入 Visual 模式，选择字符。</li><li><code>V</code>：进入 Visual Line 模式，选择整行。</li><li><code>Ctrl + v</code>：进入 Visual Block 模式，选择矩形块。</li></ul><p>在选择文本后，可以使用以下命令：</p><ul><li><code>y</code>：复制选中的文本。</li><li><code>d</code>：剪切选中的文本。</li><li><code>p</code>：粘贴文本。</li></ul><blockquote><p>Vim的复制粘贴功能非常复杂且强大，但随着时间的推移，一些功能更强大、更加方便的插件已经取代了内置的复制粘贴功能。这些插件提供了更直观和高级的文本操作方式，使得用户可以更加高效地处理文本。</p><p>由于插件的多样性和功能强大，本文不再深入探讨 Vim内置的复杂复制粘贴功能。推荐使用插件来增强 Vim 的复制粘贴体验，例如 <ahref="https://github.com/easymotion/vim-easymotion">vim-easymotion</a>和 <a href="https://github.com/tpope/vim-surround">vim-surround</a>等。</p></blockquote><h3 id="代码补全">代码补全</h3><blockquote><p>现在 Vim 的补全功能已经由插件实现了，Vim自带的补全功能基本不再使用了，这个将在插件篇进行详解。</p></blockquote><h2 id="配置文件">配置文件</h2><h3 id="基础配置">基础配置</h3><p>在 Vim 中有非常多的设置，例如 <code>set number</code>用于显示行号。但是，每次打开 Vim都要手动进行这些操作非常麻烦。我们可以将这些设置写在 Vim的配置文件中，每次启动 Vim时，它会自动读取配置文件中的内容。这个配置文件位于<code>~/.vimrc</code>。</p><p>以下是一些常用的基础配置：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-comment">&quot; 显示行号</span><br><span class="hljs-keyword">set</span> <span class="hljs-keyword">nu</span><br><br><span class="hljs-comment">&quot; 设置主题</span><br><span class="hljs-keyword">colorscheme</span> hybrid<br><br><span class="hljs-comment">&quot; 开启语法高亮</span><br><span class="hljs-keyword">syntax</span> <span class="hljs-keyword">on</span><br><br><span class="hljs-comment">&quot; 保持上一行的缩进</span><br><span class="hljs-keyword">set</span> autoindent<br><br><span class="hljs-comment">&quot; 设置缩进单位为 4 个空格</span><br><span class="hljs-keyword">set</span> <span class="hljs-built_in">shiftwidth</span>=<span class="hljs-number">4</span><br><br><span class="hljs-comment">&quot; 设置 Tab 键的宽度为 4 个空格</span><br><span class="hljs-keyword">set</span> tabstop=<span class="hljs-number">4</span><br><br><span class="hljs-comment">&quot; 将 Tab 键自动转换为空格</span><br><span class="hljs-keyword">set</span> expandtab<br></code></pre></td></tr></table></figure><h3 id="按键映射">按键映射</h3><p>Vim的按键映射是将一个操作映射到另一个操作上，常用于设置一些方便的快捷键。按键映射可以分为几种模式：Normal模式、Visual 模式和 Insert 模式。在不同模式下定义映射的方法：</p><ul><li><code>nmap</code>：只在 Normal 模式下有效。</li><li><code>vmap</code>：只在 Visual 模式下有效。</li><li><code>imap</code>：只在 Insert 模式下有效。</li></ul><p>以下是一些用法举例：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-comment">&quot; 在 Normal 模式下，按下空格选择整个单词</span><br><span class="hljs-keyword">nmap</span> <span class="hljs-symbol">&lt;space&gt;</span> viw<br><br><span class="hljs-comment">&quot; 在 Normal 模式下，使用 Ctrl + d 执行 `dd` 删除一行</span><br><span class="hljs-keyword">nmap</span> <span class="hljs-symbol">&lt;c-d&gt;</span> dd<br><br><span class="hljs-comment">&quot; 在 Normal 模式下，按 `&lt;leader&gt;o`（通常 `&lt;leader&gt;` 键默认为 `\`）在当前行下插入一行，并返回 Normal 模式。</span><br><span class="hljs-keyword">nmap</span> <span class="hljs-symbol">&lt;leader&gt;</span><span class="hljs-keyword">o</span> <span class="hljs-keyword">o</span><span class="hljs-symbol">&lt;Esc&gt;</span><span class="hljs-keyword">k</span><br><br><span class="hljs-comment">&quot; 在 Visual 模式下，按 `&lt;leader&gt;y` 将选中的文本复制到系统剪贴板。</span><br><span class="hljs-keyword">vmap</span> <span class="hljs-symbol">&lt;leader&gt;</span><span class="hljs-keyword">y</span> <span class="hljs-comment">&quot;+y</span><br><br><span class="hljs-comment">&quot; 在 Insert 模式下，按 `jk` 返回 Normal 模式。</span><br><span class="hljs-keyword">imap</span> jk <span class="hljs-symbol">&lt;Esc&gt;</span><br></code></pre></td></tr></table></figure><h2 id="小技巧">小技巧</h2><ol type="1"><li>不小心按了 Ctrl + S 后编辑器假死，其实只是 Vim停止向终端输出了，只要按下 Ctrl + Q 就可以恢复。</li></ol>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Vim</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Vim</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zotero使用技巧：坚果云同步、常用插件</title>
    <link href="/Zotero-Note-1.html"/>
    <url>/Zotero-Note-1.html</url>
    
    <content type="html"><![CDATA[<p>在新电脑上配置 Zotero云同步过程较为繁琐，现将整个过程记录下来，方便下次使用。</p><p>Zotero 自带的云存储空间只有 300M，目前常用的方法是使用坚果云 WebDAV功能辅助，每个月有 1GB 上传、3GB 下载，足够日常使用。</p><h2 id="zotero端">Zotero端</h2><ol type="1"><li><p>官网下载最新版本 App：https://www.zotero.org/</p></li><li><p>打开Zotero：编辑——首选项——同步——设置，输入用户名和密码进行登陆。</p></li><li><p>登录后下方出现「文件同步」，将附件同步方式改成 WebDAV后，填入坚果云<strong>服务器地址、账户、用户密码</strong>。</p><ul><li>需要先在坚果云网页端生成 WebDAV应用密码，<strong>详见下文</strong>。</li><li>服务器地址末尾记得加上<code>/works</code>，与坚果云同步文件夹匹配。</li></ul></li><li><p>安装最新版本插件 ZotFile：https://github.com/jlegewie/zotfile</p><ul><li>选择 .xpi 文件下载，打开Zotero：工具——附加组件——从文件安装插件——重启。</li><li>此时在文库列表应该可以看到自己的论文了，但是点击之后会显示<strong>找不到附件</strong>。</li></ul></li><li><p>Zotero 附件访问路径配置：编辑——首选项——高级——文件和文件夹</p><ul><li>在旧电脑、新电脑的 D 盘创建 <code>\MyZotero\Collection</code>目录，注意<strong>盘符一定要保持一致</strong>。</li><li>在旧电脑、新电脑的「文件和文件夹」中，将根目录设置为<code>D:\MyZotero\Collection</code>。</li><li>数据存储位置也可以自定义到<code>D:\MyZotero\Database</code>，这一步不影响同步，单纯节省 C盘空间。</li></ul></li><li><p>插件设置：工具——ZotFile Preferences。</p><ul><li>将 General Settings——Location of Files 中的目录设置为第二个 CustomLocation，选择刚刚创建的 <code>D:\MyZotero\Collection</code>。</li><li>将下面的子目录名称设置为<code>\%c</code>，这一步用于<strong>附件同步</strong>。</li><li>将 Renaming Rules——Format 两个都修改为<code>&#123;%y-&#125;&#123;%F-&#125;&#123;%t&#125;</code>，下方的设置可以自行设定，但<strong>必须和旧电脑保持一致</strong>，防止文件命名错乱。</li></ul></li><li><p>等待同步：操作坚果云。</p><ul><li>先在旧电脑上刷新，直到所有文件成功迁移到 D盘新目录，并且上传坚果云完成。</li><li>再到旧电脑上，等待坚果云下载文件到 D 盘新目录，下载完成后刷新 Zotero即可。</li></ul></li></ol><h2 id="坚果云端">坚果云端</h2><ol type="1"><li>登录网页端坚果云账号，点击右上角的账号名称——账户信息——安全选项——第三方应用管理，可以看到WebDAV 功能。</li><li>添加应用，应用名称写 Zotero 即可，之后会生成一个应用密码。</li><li>在坚果云同步文件夹下，新建文件夹<code>/works/zotero/</code>，该文件夹不存放任何东西，也不需要设置同步，单纯为WebDAV 服务。</li><li>官网下载最新版本 App：https://www.jianguoyun.com/<ul><li>本地登录坚果云账号（<strong>旧电脑、新电脑</strong>都要）。</li><li>在坚果云同步文件夹下，新建文件夹<code>/Collecntion/</code>，用于同步文献 PDF附件，笔记附件以坚果云自带的云空间存储即可。</li></ul></li><li>在新、旧电脑上都保持坚果云同步开启，完成 Zotero端配置后，进行如下操作：<ul><li>将旧电脑 D 盘下面的 <code>\MyZotero\Collection</code>目录设置同步，等待上传。</li><li>确保<strong>旧电脑完成上传</strong>之后，将新电脑 D 盘下面的<code>\MyZotero\Collection</code>目录也设置同步，<strong>注意路径一定要保持一致</strong>。</li></ul></li></ol><h2 id="常用插件">常用插件</h2><p>更多插件可以在 <ahref="https://www.zotero.org/support/plugins">Zotero 官方网站</a>查看。</p><h3 id="zotfile">ZotFile</h3><blockquote><p>https://github.com/jlegewie/zotfile</p></blockquote><p>ZotFile 主要用于管理附件：自动将 PDF附件<strong>重命名、移动和附加</strong>到 Zotero 项目，并从 PDF文件中提取元信息等。插件的设置路径为：工具——ZotFilePreferences，包括附件存储路径、文件重命名格式等。上文已经介绍过用法。</p><p>其中，文件重命名格式有几个常用选项：</p><table><thead><tr class="header"><th style="text-align: center;">选项</th><th style="text-align: center;">含义</th><th style="text-align: center;">备注</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">％a</td><td style="text-align: center;">作者的名</td><td style="text-align: center;">在「其他设置」下更改最大作者数量</td></tr><tr class="even"><td style="text-align: center;">％F</td><td style="text-align: center;">作者的名 + 姓的缩写</td><td style="text-align: center;">例如 EinsteinA，常用</td></tr><tr class="odd"><td style="text-align: center;">%A</td><td style="text-align: center;">作者的首字母缩写</td><td style="text-align: center;">例如 EA，按作者分文件夹时会用</td></tr><tr class="even"><td style="text-align: center;">％y</td><td style="text-align: center;">年份</td><td style="text-align: center;">从「日期」信息中提取</td></tr><tr class="odd"><td style="text-align: center;">％t</td><td style="text-align: center;">标题</td><td style="text-align: center;">标题其余部分的最大长度可以更改</td></tr><tr class="even"><td style="text-align: center;">％j</td><td style="text-align: center;">期刊名称</td><td style="text-align: center;">该选项不支持会议文章、预印本</td></tr><tr class="odd"><td style="text-align: center;">％s</td><td style="text-align: center;">期刊缩写</td><td style="text-align: center;">同上</td></tr></tbody></table><h3 id="zotero-better-notes">Zotero Better Notes</h3><blockquote><p>https://github.com/windingwind/zotero-better-notes</p></blockquote><p>必装插件，主要用于优化笔记功能，并且支持双向链表管理笔记（类似Obsidian）。主要将笔记分为了三部分：<strong>大纲区域、主笔记区域、预览区域</strong>。</p><p><strong>大纲区域</strong>展示了主笔记区域的内容架构，有三种呈现模式：树视图、思维导图、气泡视图。在气泡视图下，可以像其它笔记软件一样，拖动一个结点，找到与之关联的结点。</p><p><strong>主笔记区域</strong>展示了 Zotero文献库中的文献笔记结构，需要自己构建。但是目前仅支持一个主笔记，对于领域、论文较多的情况会显得冗杂。</p><p><strong>预览区域</strong>则跟原生的笔记功能类似，但是可以<strong>设置笔记模板</strong>：编辑——笔记模板编辑器中，可以从笔记新建模板，模板名称需要以<code>[Text]</code>开头，在每次创建笔记条目时<strong>右键点击空白区域即可添加</strong>。</p><blockquote><p>笔记小技巧：通过拖拽可以直接从正文、翻译弹窗中复制内容到预览区域。</p></blockquote><h3 id="zotero-pdf-translate">Zotero PDF Translate</h3><blockquote><p>https://github.com/windingwind/zotero-pdf-translate</p></blockquote><p>支持划词翻译、拼接翻译，在界面右侧继承翻译工具，但是一般只能挂 VPN使用谷歌翻译。其他的 DeepL 等工具可能需要密钥。</p><p>打开编辑——首选项——翻译，可以进行配置，推荐关闭「自动添加到笔记」的功能，并且调整字号为16。</p><h3 id="zotero-reference">Zotero Reference</h3><blockquote><p>https://github.com/muisedestiny/zotero-reference</p></blockquote><p>支持在侧边栏「参考文献」点击解析/获取当前 PDF的所有参考文献，也支持浮窗显示参考文献的详细信息、摘要、引用数量等。</p><p>支持在主阅读界面中点击超链接分栏快速跳转，在查看图表、引用的时候非常方便。</p><h3 id="jasminum">Jasminum</h3><blockquote><p>https://github.com/l0o0/jasminum</p></blockquote><p>提供中文文献支持，包括<strong>知网</strong>元数据抓取、作者中文姓名的切分、中文引用样式Word 支持等。</p><h3 id="zotero-citation-update">Zotero Citation Update</h3><blockquote><p>https://mp.weixin.qq.com/s/ShegV8fyQm0ut3jANJTVCg</p></blockquote><p>可以从 Google Scholar 更新文章的引用数目。</p><h3 id="doi-manager">DOI Manager</h3><blockquote><p>https://github.com/bwiernik/zotero-shortdoi</p></blockquote><p>主要功能：检查期刊的 DOI 是否有效，将长 DOI 替换为短 DOI，将短 DOI替换为原始 DOI。</p><h2 id="常用设置">常用设置</h2><h3 id="取消自动添加引注">取消自动添加引注</h3><blockquote><p>https://zhuanlan.zhihu.com/p/589095825</p></blockquote><p>在复制 PDF 正文中的内容时，Zotero默认会自动添加一个引注，用于快速链接到原文。如果想要取消可以：首选项——高级——设置编辑器——搜索：annotations.noteTemplates。</p><p>原始格式：<code>&lt;p&gt;&#123;&#123;highlight&#125;&#125; &#123;&#123;citation&#125;&#125; &#123;&#123;comment&#125;&#125;&lt;/p&gt;</code>，表示当进行简单复制的时候，会自动添加引注信息。</p><p>取消引号和引注：<code>&lt;p&gt;&#123;&#123;highlight quotes='false'&#125;&#125; &#123;&#123;comment&#125;&#125;&lt;/p&gt;</code></p><h2 id="论文推荐">论文推荐</h2><p><ahref="https://github.com/TideDra/zotero-arxiv-daily">Zotero-arXiv-Daily</a>根据您 Zotero 库的上下文找到可能吸引您的 arXiv论文，然后将结果发送到您的邮箱📮。它可以作为 Github Action Workflow部署，零成本、无需安装，并且只需很少的 Github Action环境变量配置，即可实现每日自动交付。</p><p>功能：</p><ul><li>完全免费！所有计算都可以在本地使用Github Actionrunner在其配额内完成（对于公共仓库）。</li><li>AI 生成的简短摘要，让您快速找到目标论文。</li><li>论文的隶属关系已解析并呈现。</li><li>电子邮件中提供了 PDF 和代码实现（如果有）的链接。</li><li>按您最近的研究兴趣排序的论文列表。</li><li>通过分叉此仓库 Github Action 页面设置环境变量来实现快速部署。</li><li>支持使用 LLM API 生成论文的简短摘要。</li><li>使用 gitignore 风格的模式忽略不需要的 Zotero 论文。</li></ul>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Zotero</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Zotero</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch笔记 #2 神经网络</title>
    <link href="/PyTorch-Note-2.html"/>
    <url>/PyTorch-Note-2.html</url>
    
    <content type="html"><![CDATA[<p>本文介绍使用 PyTorch搭建神经网络模型的方法，由于涉及的内容众多，本文将以<strong>完成一个神经网络实验的顺序</strong>展开介绍。分别是：<strong>数据加载、模型搭建、训练阶段、评估阶段、模型保存与加载</strong>。</p><p>本文内容基于 <ahref="https://pytorch.org/docs/stable/index.html">官方 API 文档</a> 及<a href="https://pytorch-cn.readthedocs.io/zh/latest/">PyTorch中文文档</a>、网上的博客等，实战部分推荐以下教程：</p><ul><li><ahref="https://github.com/yunjey/pytorch-tutorial">pytorch-tutorial</a>：逐步搭建网络，包括FFNN、CNN、RNN、ResNet、BiLSTM 等经典模型，代码简练。</li><li><ahref="https://github.com/hunkim/PyTorchZeroToAll">PyTorchZeroToAll</a>：HKUST的课程，有视频讲解，也是经典模型的搭建过程。</li><li><a href="https://zhuanlan.zhihu.com/p/396666255">深度学习 PyTorch训练代码模板</a>：完整的深度学习模板，在处理轻量级任务时可以参考。在<strong>快速开发</strong>时建议不要重复造轮子，优先使用模板（比赛的公开baseline、要对标的开源模型等）。</li></ul><h2 id="数据加载">数据加载</h2><p>PyTorch 中的 <code>torch.utils.data</code>模块封装了所有关于数据集的操作，可以帮助我们使用一些预加载数据集或者自定义的数据，其中<code>Dataset</code> 可以储存样本以及对应标签，而<code>Dataloader</code> 则用于将 <code>Dataset</code>封装成<strong>可迭代对象</strong>，使得我们能够更加容易地获取样本数据。</p><h3 id="预加载-or-自定义">预加载 or 自定义</h3><p>PyTorch 中提供了大量<strong>预加载</strong>的数据集（如MNIST、FashionMNIST），存放在不同的领域库中，分别是：<ahref="https://pytorch.org/vision/stable/datasets.html">torchvision</a>、<ahref="https://pytorch.org/text/stable/datasets.html">torchtext</a>、<ahref="">torchaudio</a> 库。</p><p>下面以 FashionMNIST 为例介绍<strong>加载流程</strong>，其包含 60000个训练样本以及 10000个测试样本，每个样本由一张灰度图、一个类别标签组成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor<br><br>training_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<span class="hljs-comment"># root 是训练/测试数据所存储的路径，相对于当前位置</span><br>    train=<span class="hljs-literal">True</span>,<span class="hljs-comment"># train 指定训练或测试数据集</span><br>    download=<span class="hljs-literal">True</span>,<span class="hljs-comment"># 如果数据集在 root 中不存在则从互联网上下载</span><br>    transform=ToTensor()<span class="hljs-comment"># 特征与标签的变换，这里转成张量</span><br>)<br>test_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">False</span>,<span class="hljs-comment"># False 表示指定测试集</span><br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br></code></pre></td></tr></table></figure><p>自定义数据集需要通过<strong>面向对象</strong>操作来完成，案例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> torchvision.io <span class="hljs-keyword">import</span> read_image<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><br><span class="hljs-comment"># 定义 MyDataset() 类, 继承自 Dataset 父类，重写 init、getitem、len 方法</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyDataset</span>(<span class="hljs-params">Dataset</span>):</span><br>    <br>    <span class="hljs-comment"># 实例化数据集对象，从文件目录中获取数据集，数据变换也在这里定义</span><br>    <span class="hljs-comment"># 如果数据特征过大，无法一次读入，这里就要记录文件目录，按需读取即可</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, annotations_file, img_dir, transform=<span class="hljs-literal">None</span>, target_transform=<span class="hljs-literal">None</span></span>):</span><br>        self.img_labels = pd.read_csv(annotations_file) <span class="hljs-comment"># (file_name, label) 列表</span><br>        self.img_dir = img_dir<br>        self.transform = transform<br>        self.target_transform = target_transform<br>    <br>    <span class="hljs-comment"># 返回数据集中样本的数量</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span><br><span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.img_labels)<br>    <br>    <span class="hljs-comment"># 加载并返回数据集中给定索引 idx 的样本特征与标签</span><br>    <span class="hljs-comment"># 如果数据集不在内存中，要从指定目录中读取，并根据实际情况组织数据</span><br>    <span class="hljs-comment"># 通常在这里对数据进行变换和增强，并转为 Tensor 返回</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span>(<span class="hljs-params">self, idx</span>):</span><br>img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="hljs-number">0</span>]) <span class="hljs-comment"># 获取文件路径</span><br>        image = read_image(img_path)<br>        label = self.img_labels.iloc[idx, <span class="hljs-number">1</span>] <span class="hljs-comment"># 获取对应图像的标签</span><br>        <span class="hljs-keyword">if</span> self.transform:<br>            image = self.transform(image)<br>        <span class="hljs-keyword">if</span> self.target_transform:<br>            label = self.target_transform(label)<br>        <span class="hljs-keyword">return</span> image, label<br></code></pre></td></tr></table></figure><h3 id="数据集迭代访问">数据集迭代访问</h3><p>获取的数据会以列表的形式存储，特征和标签构成一对<strong>元组</strong>。如果这样通过遍历列表的形式来访问数据集，则效率太低，我们希望以<strong>小批次</strong>（Mini-batch）为单位读取样本，并且在每一个epoch 中<strong>打乱数据的顺序</strong>以防止模型过拟合，同时利用 Python的<strong>并行处理</strong>（Multi-Processing）功能加速数据处理。</p><p>使用 <code>DataLoader</code> 模块就可以实现上述功能：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br><span class="hljs-comment"># 声明 DataLoader 模块，得到可迭代对象</span><br>train_dataloader = DataLoader(training_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">4</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">4</span>)<br><br><span class="hljs-comment"># 用 iter() 转迭代器，再用 next() 访问，每次获取一个批次，分包含 64 个特征与标签</span><br>train_features, train_labels = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_dataloader))<br><br><span class="hljs-comment"># 可迭代对象可以用 for 循环遍历，训练时每个 epoch 中的顺序会变化</span><br>num_epoches = <span class="hljs-number">100</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epoches):<br>    <span class="hljs-keyword">for</span> img, label <span class="hljs-keyword">in</span> train_dataloader:<br>        <span class="hljs-comment"># 训练代码，这里每一组 img, label 都是 64 个样本</span><br></code></pre></td></tr></table></figure><p>其中 <code>num_workers</code>参数代表在声明时一次性创建的工作进程数量，此时<code>batch_sampler</code> 会将指定 batch 分配给每个 worker，worker负责将 batch <strong>载入显存</strong>。当 <code>DataLoader</code>在某轮迭代需要用到该 batch时，可以<strong>直接使用</strong>。分配的进程数量越多，获取下一 batch的速度越快，因为它可能早就放在显存中了，但这也加重了 CPU负担。一般将该值设置为 <strong>CPU核心数量</strong>，根据性能变化逐步调整。</p><p>除了以上四个常用参数，有时还会用到一个 <code>collate_fn</code>参数，顾名思义，该函数用于对样本进行<strong>核对、校勘</strong>。在<code>Dataset</code> 类中调用 <code>__getitem__</code>后，每次返回的都是一个样本，最后构成一个 Batch的样本。如果再这个过程中出现出现什么<strong>样本错误</strong>（如存在空值、错误值），或者需要<strong>对批次进行特殊处理</strong>（如对齐整个Batch 样本的长度），则需要在该函数中完成。</p><p><code>collate_fn</code> 默认是等于 <code>default_collate</code>，这是PyTorch 自带的处理函数，也可以自己定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入的 batch 是 list of tuple (img, label)，本例子为过滤 None 数据</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">collate_fn</span>(<span class="hljs-params">batch</span>):</span><br>    batch = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>, batch))<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(batch) == <span class="hljs-number">0</span>: <br>        <span class="hljs-keyword">return</span> torch.Tensor()<br>    <span class="hljs-keyword">return</span> default_collate(batch)<br><br><span class="hljs-comment"># 假设 label 是文本，本例子为对齐文本的长度</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">collate_fn</span>(<span class="hljs-params">batch</span>):</span><br>    batch.sort(key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x[<span class="hljs-number">1</span>]), reverse=<span class="hljs-literal">True</span>)<br>    img, label = <span class="hljs-built_in">zip</span>(*batch)<br>    pad_label = []<br>    lens = []<br>    max_len = <span class="hljs-built_in">len</span>(label[<span class="hljs-number">0</span>])<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(label)):<br>        temp_label = [<span class="hljs-number">0</span>] * max_len<br>        temp_label[:<span class="hljs-built_in">len</span>(label[i])] = label[i]<br>        pad_label.append(temp_label)<br>        lens.append(<span class="hljs-built_in">len</span>(label[i]))<br>    <span class="hljs-keyword">return</span> img, pad_label, lens<br></code></pre></td></tr></table></figure><h3 id="数据预处理">数据预处理</h3><p>前文中已经出现过 <code>torchvision.transforms</code>模块，该模块的功能是对图像进行预处理，内置各种<strong>数据增强</strong>操作，包括随机裁剪、翻转和旋转、变形和填充、修改属性等。每一个单独的变换都是一个<strong>函数</strong>，输入数据即可得到结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">preprocess = transforms.Resize([<span class="hljs-number">256</span>, <span class="hljs-number">256</span>])<br>img_transformed = preprocess(img)<br></code></pre></td></tr></table></figure><p>如果要连续进行多种操作，则可以用 <code>transforms.Compose()</code>组合，参数为操作列表。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><br>myTransforms = transforms.Compose([<br>    transforms.RandomResizedCrop(<span class="hljs-number">224</span>),<span class="hljs-comment"># 将给定图像随机裁剪并缩放到 224x224</span><br>    transforms.RandomHorizontalFlip(),<span class="hljs-comment"># 给定的概率随机水平旋转，默认 0.5</span><br>    transforms.ToTensor(),<span class="hljs-comment"># 将给定图像转为 Tensor</span><br>    transforms.Normalize([<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], [<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]) <span class="hljs-comment"># 归一化处理</span><br>])<br><br>data = MyDataset(<br>annotations_file=<span class="hljs-string">&quot;annotations.csv&quot;</span>,<br>    img_dir=<span class="hljs-string">&quot;data/img&quot;</span>,<br>    transform=myTransforms<span class="hljs-comment"># 调用自定义的预处理组合</span><br>)<br></code></pre></td></tr></table></figure><blockquote><p>该案例中归一化处理传入的两个参数分别代表均值和标准差，这里取的是ImageNet 数据集的值，这是 CV领域一种常见的做法，通常只要数据集是<strong>自然场景的普通照片</strong>就能使用。如果是特殊场景、特殊风格的数据，则需要重新计算。</p></blockquote><h2 id="模型搭建">模型搭建</h2><p>网络的定义也是通过<strong>面向对象</strong>操作来完成，<code>torch.nn</code>模块封装了神经网络的大部分操作，案例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-comment"># 定义 MyNet() 类, 继承自 nn.Module 父类</span><br><span class="hljs-comment"># 父类中封装了 parameters() train() eval() zero_grad() 等常用方法</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <br>    <span class="hljs-comment"># 重写构造函数 __init__(), 在实例化 MyNet() 时自动调用</span><br>    <span class="hljs-comment"># 输入参数除了必须的 self, 还包含其他实例化对象时必备的参数</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">10</span></span>):</span><br>        <span class="hljs-comment"># 由于子类定义了构造函数，父类的构造函数不再自动执行，需要显式调用</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 定义对象变量, 通过实例化 nn.Conv2d() 等类获得</span><br>        self.conv = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>)<br>        self.fc = nn.Linear(<span class="hljs-number">6</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, num_classes)<br><br>    <span class="hljs-comment"># 重写前向传播方法 forward(), 对输入 x 进行操作</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-comment"># 可以调用 self 的对象变量，也可以直接调用 F 函数</span><br>        x = F.max_pool2d(F.relu(self.conv(x)), <span class="hljs-number">2</span>)<br>        x = torch.flatten(x, <span class="hljs-number">1</span>) <span class="hljs-comment"># 将除了 batch 的维度展开到 1 维</span><br>        x = F.relu(self.fc(x))<br>        <span class="hljs-keyword">return</span> x<br><br>net = MyNet()<br><span class="hljs-built_in">input</span> = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>) <span class="hljs-comment"># (batch_size, channel, hight, width)</span><br>out = net(<span class="hljs-built_in">input</span>)<br></code></pre></td></tr></table></figure><p>如果这时候<code>print(net)</code>，则会打印出类的所有属性，即每个构造函数中的网络部件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">MyNet(<br>  (conv): Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (fc): Linear(in_features=<span class="hljs-number">150</span>, out_features=<span class="hljs-number">10</span>, bias=<span class="hljs-literal">True</span>)<br>)<br></code></pre></td></tr></table></figure><p>由于继承了父类的方法，调用 <code>net.paprameters()</code>可以得到<strong>参数列表</strong>，各个层的参数依次排列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>params = <span class="hljs-built_in">list</span>(net.parameters())<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(params)) <span class="hljs-comment"># 参数的长度，这里卷积层和全连接层各有 weight 和 bias</span><br><span class="hljs-comment"># 4,</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(params[<span class="hljs-number">0</span>].size()) <span class="hljs-comment"># 按索引访问指定参数</span><br><span class="hljs-comment"># torch.Size([6, 3, 3, 3])</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>([(name, param.shape) <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> net.named_parameters()]) <span class="hljs-comment"># 一次性访问全部</span><br><span class="hljs-comment"># [(&#x27;conv.weight&#x27;, torch.Size([6, 3, 3, 3])),</span><br><span class="hljs-comment">#  (&#x27;conv.bias&#x27;, torch.Size([6])),</span><br><span class="hljs-comment">#  (&#x27;fc.weight&#x27;, torch.Size([10, 150])),</span><br><span class="hljs-comment">#  (&#x27;fc.bias&#x27;, torch.Size([10]))]</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">sum</span>(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> net.parameters() <span class="hljs-keyword">if</span> p.requires_grad)) <span class="hljs-comment"># 计算总参数量</span><br></code></pre></td></tr></table></figure><h3 id="预加载的网络">预加载的网络</h3><p>PyTorch 中自带了搭建好的经典模型，可以直接调用，包括 <ahref="https://pytorch.org/vision/stable/models/alexnet.html">AlexNet</a>、<ahref="https://pytorch.org/vision/stable/models/vgg.html">VGG</a>等，详见 <ahref="https://pytorch.org/vision/stable/models.html">官方文档</a>。可以选择是否使用<strong>预训练好的权重、原有的转换</strong>。下面的例子展示了使用预加载网络推理一个样本的过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision.io <span class="hljs-keyword">import</span> read_image<br><span class="hljs-keyword">from</span> torchvision.models <span class="hljs-keyword">import</span> resnet50, ResNet50_Weights<br><br>img = read_iamge(<span class="hljs-string">&quot;test.jpg&quot;</span>)<br><br><span class="hljs-comment"># 使用预训练好的权重参数调用模型，也可以不使用预训练，即随机初始化权重</span><br>weights = ResNet50_Weights.DEFAULT<br>model = resnet50(weights=weights) <span class="hljs-comment"># model = resnet50(weights=None)</span><br>model.<span class="hljs-built_in">eval</span>() <span class="hljs-comment"># 进入验证模式</span><br><br><span class="hljs-comment"># 使用模型默认的预处理方法（与权重对应）</span><br>preprocess = weights.transforms()<span class="hljs-comment"># 使用 preprocess(img) 调用变换</span><br><br><span class="hljs-comment"># 此处只有一张照片，因此我们在 dim=0 处补 1 表示 batch_size</span><br>batch = preprocess(img).unsqueeze(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 使用预加载的网络预测出类别，并打印</span><br>prediction = model(batch).squeeze(<span class="hljs-number">0</span>).softmax(<span class="hljs-number">0</span>)<br>class_id = prediction.argmax().item()<br>score = prediction[class_id].item()<br>category_name = weights.meta[<span class="hljs-string">&quot;categories&quot;</span>][class_id]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;category_name&#125;</span>: <span class="hljs-subst">&#123;<span class="hljs-number">100</span> * score:<span class="hljs-number">.1</span>f&#125;</span>%&quot;</span>)<br></code></pre></td></tr></table></figure><p>此外，我们还可以对已经定义好的模型进行<strong>添加、修改</strong>网络层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 添加网络层，第一个参数是网络层的命名</span><br>vgg16.classifier.add_module(<span class="hljs-string">&quot;add_linear&quot;</span>, nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">10</span>)) <span class="hljs-comment"># 在 classifier 里加一层</span><br><span class="hljs-comment"># 修改网络层，直接指定层数修改</span><br>vgg16.classifier[<span class="hljs-number">6</span>] = nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>) <span class="hljs-comment"># 将 classifier 最后一层的输出由 1000 改为 10</span><br><span class="hljs-comment"># print(vgg16)</span><br></code></pre></td></tr></table></figure><h3 id="nn-网络模块">nn 网络模块</h3><p>这里介绍一下在构建网络时常见的一些网络模块，即在<code>__init__</code> 中定义的部分，当然也可以在<code>nn.Sequential()</code> 中组合，稍后会介绍。</p><ul><li><strong>全连接层</strong>：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">in_features: 输入特征张量的大小，可以理解为该层神经元个数，即 (batch_size, in_features)</span><br><span class="hljs-string">out_features: 输出特征张量的大小，可以理解为下一层神经元个数</span><br><span class="hljs-string">bias: 是否启用偏置，除了 weight 数组，还会初始化一个 bias 数组</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>nn.Flatten()<span class="hljs-comment"># 展平层，通常放在全连接层之前，如 (64, 3, 5, 5) -&gt; (64, 75)</span><br>nn.Linear(in_features, out_features, bias=<span class="hljs-literal">True</span>) <span class="hljs-comment"># 如 (64, 75) -&gt; (64, 10)</span><br></code></pre></td></tr></table></figure><ul><li><strong>常用的卷积层</strong>，最终输出的张量维度为 $( , ) $：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">in_channels/out_channels: 输入/输出通道数，四维张量 [N, C, H, W] 中的 C</span><br><span class="hljs-string">kernel_size: 卷积核大小，可输入一个值或元组</span><br><span class="hljs-string">stride: 步幅，默认为 1，可输入一个值或元组</span><br><span class="hljs-string">padding: 填充，默认不填充，可输入一个值或元组</span><br><span class="hljs-string">padding_mode: 可选 zeros(零) reflect(镜像) replicate(复制) circular(循环)</span><br><span class="hljs-string">dilation: 是否采用空洞卷积，默认为 1 不采用</span><br><span class="hljs-string">卷积核参数量: [out_channels, in_channels, kernel_height, kernel_width]</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">True</span>, padding_mode=<span class="hljs-string">&#x27;zeros&#x27;</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>常用的池化层</strong>（无参数），最终输出的张量维度同上：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">kernel_size: 池化窗口大小，可输入一个值或元组</span><br><span class="hljs-string">stride: 默认为窗口的大小，即每个窗口不重叠，与卷积不同</span><br><span class="hljs-string">return_indices: 是否返回最大值位置索引</span><br><span class="hljs-string">ceil_mode: 输出的形状取整方向，默认为 False 向下取整</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>nn.MaxPool2d(kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, return_indices=<span class="hljs-literal">False</span>, ceil_mode=<span class="hljs-literal">False</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">count_include_pad: 计算平均值时是否包括零填充，默认包括</span><br><span class="hljs-string">divisor_override: 除数大小，默认就是池化窗口大小，但可以自己指定</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>nn.AvgPool2d(kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>, ceil_mode=<span class="hljs-literal">False</span>, count_include_pad=<span class="hljs-literal">True</span>, divisor_override=<span class="hljs-literal">None</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">output_size: 固定输出尺寸，即可自适应窗口大小，可输入一个值或元组</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>nn.MaxPool2d(output_size)<br>nn.AdaptiveAvgPool2d(output_size)<br></code></pre></td></tr></table></figure><ul><li><strong>常用激活函数</strong>：也作为层来定义，但都没有参数，默认<code>inplace=Flase</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Sigmoid() <span class="hljs-comment"># S 型激活函数，[0, 1]，存在梯度消失/梯度爆炸/不好收敛/效率低的问题</span><br>nn.Tanh() <span class="hljs-comment"># 双曲正切函数，[-1, 1]，相对更好收敛，但还是容易梯度消失/梯度爆炸/效率低</span><br>nn.ReLU() <span class="hljs-comment"># 线性修正单元，计算快，单侧饱和，不存在梯度消失问题，但会有 dead 问题</span><br>nn.LeakyReLU(negative_slope=<span class="hljs-number">0.01</span>) <span class="hljs-comment"># 负区间也采用线性，梯度不再为 0，解决 dead 问题，但不再单侧饱和</span><br>nn.PReLU(num_parameters=<span class="hljs-number">1</span>) <span class="hljs-comment"># 可学习 alpha 的 LeakyReLU，需要有较多数据时才能使用</span><br>nn.ELU(alpha=<span class="hljs-number">1.0</span>) <span class="hljs-comment"># 负区间采用指数函数，既单侧饱和，又解决了 dead 问题</span><br>nn.Softplus(beta=<span class="hljs-number">1</span>, threshold=<span class="hljs-number">20</span>) <span class="hljs-comment"># ReLU 的近似，保证输出为正数</span><br>nn.Softmax(dim=<span class="hljs-literal">None</span>) <span class="hljs-comment"># 最后对输出值的处理，[0, 1] 且和为 1</span><br></code></pre></td></tr></table></figure><ul><li><strong>特殊功能层</strong>：注意 <spanclass="math inline">\(\textrm{Dropout}\)</span>只在<strong>训练阶段</strong>启用，并且不带参数。而 <spanclass="math inline">\(\textrm{BN}\)</span>层实际是<strong>有参数存储</strong>的，包括仿射参数和<strong>测试阶段</strong>的移动平均。同样<span class="math inline">\(\textrm{LN}\)</span>层也是<strong>有参数存储</strong>的，即仿射参数，但不会跟踪全局的移动平均，<strong>所有阶段</strong>都一致。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">p: 张量元素被置为 0 的概率，置为 0 的神经元整个不激活</span><br><span class="hljs-string">inplace: 是否进行原地操作，默认为否</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-comment"># 主要用于一维数据，一般在线性层、激活函数之后</span><br>nn.Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 主要用于二维数据，一般在卷积层、激活函数之后</span><br>nn.Dropout2d(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">num_features: 通道数，1d 中代表 [N, L] 中的 L, 2d 中代表 [N, C, H, W] 中的 C</span><br><span class="hljs-string">eps: 归一化时加到分母（方差）上的小量，防止除零溢出</span><br><span class="hljs-string">momentum: 预测模型下进行移动平均的动量</span><br><span class="hljs-string">affine: 是否具有可学习的仿射参数 gamma 和 beta，用于拉伸和偏移分布</span><br><span class="hljs-string">track_running_stats: 是否跟踪均值和方差，进行移动平均</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-comment"># 一维 BN 是取 axis=0 进行归一化的，一般在线性层和激活函数之间</span><br>nn.BatchNorm1d(num_features, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 二维 BN 是取 axis=(0,2,3) 进行归一化的，一般在卷积层和激活函数之间</span><br>nn.BatchNorm2d(num_features, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">normalized_shape: 传入一个整数表示最后一维的长度，传入列表表示最后两维的长度</span><br><span class="hljs-string">eps: 归一化时加到分母（方差）上的小量，防止除零溢出</span><br><span class="hljs-string">elementwise_affine: 是否具有可学习的仿射参数 gamma 和 beta，用于拉伸和偏移分布</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-comment"># 取 axis=-1 或 axis=(-2, -1) 进行归一化</span><br>nn.LayerNorm(normalized_shape, eps=<span class="hljs-number">1e-05</span>, elementwise_affine=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>词嵌入层</strong>：在 NLP 任务中经常使用，用来代替 One-Hot实现降维，并学习词之间的联系。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">num_embeddings: 词典的大小，如果有 5000 词，id 就是 0-4999</span><br><span class="hljs-string">embedding_dim: 嵌入向量的维度，即用多少维的空间来表示一个词向量</span><br><span class="hljs-string">padding_idx: 输入句子长度不一致时，末尾的 PAD 的索引值</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>nn.Embedding(num_embeddings, embedding_dim, padding_idx=<span class="hljs-literal">None</span>)<br><span class="hljs-comment"># 使用 Embedding 时要在输入数据前就预处理好格式，具体步骤如下：</span><br><span class="hljs-comment"># 原始句子 [&#x27;I am a boy.&#x27;,&#x27;How are you?&#x27;,&#x27;I am very lucky.&#x27;]</span><br><span class="hljs-comment"># 分词映射到索引   [[3,6,5,6,7],[6,4,7,9,5]，[4,5,8,7]]</span><br><span class="hljs-comment"># 填充[EOS][PAD]  [[3,6,5,6,7,1],[6,4,7,9,5,1],[4,5,8,7,1,2]]</span><br><span class="hljs-comment"># 切片用于序列输入   [[3,6,4],[6,4,5],[5,7,8],[6,9,7],[7,5,1],[1,1,2]]</span><br><span class="hljs-comment"># 转为张量batch = torch.LongTensor(batch)</span><br>embed = torch.nn.Embedding(num_embeddings, embedding_dim)<br>embed_batch = embed(batch) <span class="hljs-comment"># [seq_len, batch_size] -&gt; [seq_len, batch_size, embedding_dim]</span><br></code></pre></td></tr></table></figure><p>如果想观察网络中每层的通道数、数据规模是否符合预期，可以<strong>构造一个单独的样本</strong>来前向传播，并实时获取其形状：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X=layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__, <span class="hljs-string">&#x27;\t\ output shape:\t&#x27;</span>, X.shape)<br></code></pre></td></tr></table></figure><p>需要注意的是，如果要设计全新的网络模块，除了实现一个<strong>运算函数</strong>，还需要定义<strong>可训练的参数</strong>。<code>nn.Parameter()</code>将一个<strong>不可训练</strong>的 Tensor转换成<strong>可训练</strong>的类型 Parameter，并将这个 Parameter<strong>绑定</strong>到模型的参数列表里面。比起参数<code>requires_grad=True</code> 还多了绑定的过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Conv2D</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, kernel_size</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.weight = nn.Parameter(torch.rand(kernel_size))<br>        self.bias = nn.Parameter(torch.zeros(<span class="hljs-number">1</span>))<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-keyword">return</span> corr2d(x, self.weight) + self.bias<br></code></pre></td></tr></table></figure><h3 id="f-函数式编程">F 函数式编程</h3><p>在 PyTorch 的 <code>nn</code>模块中，我们不需要手动定义网络层的权重和偏置，这就是 PyTorch 比Tensorflow 简洁的地方。当然，PyTorch 也提供了 <code>nn.Functional</code>函数式编程的方法，其中的 <code>F.conv2d()</code> 就和 Tensorflow一样，要<strong>预先定义好</strong>卷积核的权重和偏置<code>nn.Parameter()</code>，作为形参之一输入。</p><p>当然，为了不多此一举，网络中具有<strong>可学习参数</strong>的层（如全连接层、卷积层）通常会放在<code>__init__</code>中<strong>显式定义</strong>，而<strong>不具有参数</strong>的层（如ReLU、Dropout、BN 层）一般放在 <code>forward</code> 中使用<code>F</code> 函数调用。</p><ul><li><strong>无参数的池化操作</strong>：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">F.max_pool2d(<span class="hljs-built_in">input</span>, kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>, return_indices=<span class="hljs-literal">False</span>)<br>F.avg_pool2d(<span class="hljs-built_in">input</span>, kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>, ceil_mode=<span class="hljs-literal">False</span>, count_include_pad=<span class="hljs-literal">True</span>, divisor_override=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>无参数的激活函数</strong>：以下函数都支持<code>F.relu_</code> 的原地版本。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">F.sigmoid(<span class="hljs-built_in">input</span>)<br>F.tanh(<span class="hljs-built_in">input</span>)<br>F.relu(<span class="hljs-built_in">input</span>)<br>F.leaky_relu(<span class="hljs-built_in">input</span>, negative_slope=<span class="hljs-number">0.01</span>)<br>F.prelu(<span class="hljs-built_in">input</span>, weight) <span class="hljs-comment"># 特例：这里 weight 是个可学习的参数，一般不会使用函数式编程</span><br>F.elu(<span class="hljs-built_in">input</span>, alpha=<span class="hljs-number">1.0</span>)<br>F.softplus(<span class="hljs-built_in">input</span>, beta=<span class="hljs-number">1</span>, threshold=<span class="hljs-number">20</span>)<br>F.softmax(<span class="hljs-built_in">input</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>无参数的特殊功能函数</strong>：包括 Dropout和一些计算向量距离的函数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">F.dropout(<span class="hljs-built_in">input</span>, p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>F.dropout2d(<span class="hljs-built_in">input</span>, p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>F.cosine_similarity(x1, x2)  <span class="hljs-comment"># 计算两个向量的余弦相似度</span><br>F.pairwise_distance(x1, x2, p=<span class="hljs-number">2.0</span>) <span class="hljs-comment"># 计算两个向量的 p 范数距离</span><br>F.one_hot(x, num_classes)<span class="hljs-comment"># 生成长度为 num_classes 的 one_hot 张量组</span><br></code></pre></td></tr></table></figure><blockquote><p>注意在 F 函数式编程中，函数名<strong>全都为小写</strong>，与 nn网络模块中不同！</p></blockquote><h3 id="使用块搭建深层网络">使用块搭建深层网络</h3><p>块（Block）比单个层（Layer）大，又比整个模型（Model）小，是实现一个深层网络的必备操作，通常用于封装以重复模式排列的若干层，最早在<span class="math inline">\(\textrm{VGG}\)</span> 中得到应用。</p><p>从编程角度来看，每个块也需要用类定义，并实现<strong>初始化、前向传播</strong>功能。实际中PyTorch 提供了一个简单的方法<code>nn.Sequential</code>，可以将不同神经网络层进行<strong>顺序拼接</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">self.net = nn.Sequential(<br>    nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">256</span>),<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>)<br>)<br>output = net(torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">20</span>)) <span class="hljs-comment"># output.shape = (2, 10)</span><br></code></pre></td></tr></table></figure><p>如果希望<strong>多个层共享参数</strong>，只需先实例化一个层，并将其作为网络的多个部分，此时所有层的权重张量<strong>共享一个内存空间</strong>。这就相当于<code>__init__</code> 中的同一层在 <code>forward</code>中被调用了两次，因此<strong>每次调用的 grad也会相加</strong>。如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">sharedLayer = nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>)<br>net = nn.Sequential(nn.Linear(<span class="hljs-number">4</span>, <span class="hljs-number">8</span>), nn.ReLU(),<br>                    sharedLayer, nn.ReLU(),<br>                    sharedLayer, nn.ReLU(),<br>                    nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><p>如果要封装更复杂的块，例如 VGG 中的<code>VGG_Block</code>，则可以将其单独用类定义，并设置好参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">VGG_Block</span>(<span class="hljs-params">num_convs, in_channels, out_channels</span>):</span><br>    layers = [] <span class="hljs-comment"># 以列表的形式拼接网络模块</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_convs):<br>        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>))<br>        layers.append(nn.ReLU())<br>        in_channels = out_channels<br>    layers.append(nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">return</span> nn.Sequential(*layers) <span class="hljs-comment"># 这里 * 表示解包，将列表拆分</span><br></code></pre></td></tr></table></figure><h3 id="网络初始化">网络初始化</h3><p>默认情况下，PyTorch会根据一个范围<strong>均匀地初始化</strong>权重和偏置矩阵，这个范围是根据输入和输出维度计算出的。其中<code>nn.init</code>模块提供了多种预置初始化方法。包括：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.init.zeros_(weight)<span class="hljs-comment"># 初始化为全零</span><br>nn.init.ones_(weight)<span class="hljs-comment"># 初始化为全一</span><br>nn.init.eye_(weight)<span class="hljs-comment"># 对二维矩阵，初始化为单位矩阵</span><br>nn.init.orthogonal_(weight) <span class="hljs-comment"># 对二维矩阵，初始化为正交矩阵</span><br>nn.init.constant_(weight, val) <span class="hljs-comment"># 初始化为指定常数</span><br>nn.init.uniform_(weight, a=<span class="hljs-number">0</span>, b=<span class="hljs-number">1</span>) <span class="hljs-comment"># 初始化为均匀分布 U(a,b)</span><br>nn.init.normal_(weight, mean=<span class="hljs-number">0</span>, std=<span class="hljs-number">1</span>) <span class="hljs-comment"># 从给定均值和标准差，初始化为正态分布；</span><br>nn.init.xavier_uniform_(weight) <span class="hljs-comment"># 使用 Xavier 初始化生成均匀分布，范围由扇入、扇出确定</span><br>nn.init.xavier_normal_(weight)<span class="hljs-comment"># 使用 Xavier 初始化生成正态分布，方差由扇入、扇出确定</span><br>nn.init.kaiming_uniform_(weight, a=<span class="hljs-number">0</span>, mode=<span class="hljs-string">&#x27;fan_in&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;leaky_relu&#x27;</span>)<br>nn.init.kaiming_normal_(weight, a=<span class="hljs-number">0</span>, mode=<span class="hljs-string">&#x27;fan_in&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;leaky_relu&#x27;</span>) <br><span class="hljs-comment"># 使用 He 初始化，范围由 mode 决定，可选扇入或扇出，a 为这层后 Leaky ReLU 的斜率系数，rulu 则不需要</span><br></code></pre></td></tr></table></figure><p>调用上述内置的初始化器，我们可以实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_weights</span>(<span class="hljs-params">m</span>):</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear: <span class="hljs-comment"># 仅当网络层的类型匹配时采用</span><br>        nn.init.normal_(m.weight, mean=<span class="hljs-number">0</span>, std=<span class="hljs-number">0.01</span>)<br>        nn.init.zeros_(m.bias)<br>net.apply(init_weights) <span class="hljs-comment"># 对所有层应用初始化方法</span><br></code></pre></td></tr></table></figure><p>注意到上面的方法要求<strong>网络类型完全匹配</strong>，而 PyTorch中自带有很多相似的层，如<code>Conv2d</code>、<code>ConvTranspose2d</code>等，此时可以获取<strong>类名</strong>进行字符串匹配：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_weights</span>(<span class="hljs-params">m</span>):</span><br>    classname = m.__class__.__name__    <span class="hljs-comment"># 返回 m 的类名</span><br>    <span class="hljs-keyword">if</span> classname.find(<span class="hljs-string">&#x27;Conv&#x27;</span>) != -<span class="hljs-number">1</span>:<span class="hljs-comment"># 如果类名包含 Conv</span><br>    nn.init.kaiming_uniform_(m.weight)<br>        nn.init.uniform_(weight, a=-<span class="hljs-number">1</span>, b=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">elif</span> classname.find(<span class="hljs-string">&#x27;BatchNorm&#x27;</span>) != -<span class="hljs-number">1</span>:           <br>        nn.init.normal_(m.weight, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.02</span>)<br>        nn.init.zeros_(m.bias)<br>net.apply(init_weights)<br></code></pre></td></tr></table></figure><h2 id="训练阶段">训练阶段</h2><p>假设我们已经定义好 <code>MyDataset</code> 类和 <code>MyModel</code>类，现在正式进入训练阶段。除了实例化数据集和模型，还需要额外定义<strong>损失函数和优化器</strong>。下面给出一个完整的训练过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实例化数据集和模型</span><br>dataset = MyDataset(file)<br>train_set = DataLoader(dataset, <span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>)<br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>model = MyModel().to(device) <span class="hljs-comment"># 将模型放到指定的设备上</span><br><span class="hljs-comment"># 定义损失函数和优化器</span><br>criterion = nn.MSELoss()<br>optimizer = torch.optim.SGD(model.parameters(), <span class="hljs-number">0.1</span>)<br><span class="hljs-comment"># 训练过程</span><br>model.train()<span class="hljs-comment"># 将模型切换到「训练模式」，也可以放到循环内</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br><span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_set:<br>optimizer.zero_grad()<span class="hljs-comment"># 每个 batch 前都要清空梯度</span><br>        X, y = X.to(device), y.to(device)<span class="hljs-comment"># 将数据放到指定的设备上</span><br>        pred = model(X)<br>        loss = criterion(pred, y)<span class="hljs-comment"># 计算损失</span><br>        loss.backward()<span class="hljs-comment"># 反向传播</span><br>        optimizer.step()<span class="hljs-comment"># 每个 batch 更新一次参数</span><br></code></pre></td></tr></table></figure><p>所谓的「<strong>训练模式</strong>」，是相对另一个「<strong>评估模式</strong>」下的模型传播路径。实现上，模型通过<code>model.train()</code> 将变量 <code>self.training</code> 置为True，通过 <code>model.eval()</code> 将 <code>self.training</code> 置为False，所以即便训练与测试共用一个模型，也能通过该变量来<strong>区分现在属于训练还是测试</strong>。</p><p>当模型中使用了 Dropout、BatchNorm等模块，在训练阶段和测试阶段的<strong>操作不同</strong>，此时区分模式就很有必要。此外，如果网络中有一部分层只在训练阶段启用，则可以用<code>if self.training:</code> 来执行。</p><h3 id="损失函数">损失函数</h3><p>PyTorch 在 <code>torch.nn</code> 模块中已经定义了大部分常用的Loss，具体的计算过程详见 <ahref="https://pytorch.org/docs/stable/nn.html#loss-functions">手册</a>：</p><table><thead><tr class="header"><th style="text-align: center;">损失函数</th><th style="text-align: center;">名称</th><th style="text-align: center;">适用场景</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>nn.L1Loss()</code></td><td style="text-align: center;">绝对值误差损失</td><td style="text-align: center;">回归</td></tr><tr class="even"><td style="text-align: center;"><code>nn.MSELoss()</code></td><td style="text-align: center;">均方误差损失</td><td style="text-align: center;">回归</td></tr><tr class="odd"><td style="text-align: center;"><code>nn.SmoothL1Loss()</code></td><td style="text-align: center;">Huber 损失（结合 L1 和 L2）</td><td style="text-align: center;">回归</td></tr><tr class="even"><td style="text-align: center;"><code>nn.BCELoss()</code></td><td style="text-align: center;">二分类交叉熵损失</td><td style="text-align: center;">二分类</td></tr><tr class="odd"><td style="text-align: center;"><code>nn.BCEWithLogitsLoss()</code></td><td style="text-align: center;">自带 Sigmoid 的 BCELoss</td><td style="text-align: center;">二分类</td></tr><tr class="even"><td style="text-align: center;"><code>nn.CrossEntropyLoss()</code></td><td style="text-align: center;">交叉熵损失</td><td style="text-align: center;">多分类</td></tr><tr class="odd"><td style="text-align: center;"><code>nn.NLLLoss()</code></td><td style="text-align: center;">负对数似然函数损失</td><td style="text-align: center;">多分类</td></tr><tr class="even"><td style="text-align: center;"><code>nn.NLLLoss2d()</code></td><td style="text-align: center;">图像负对数似然函数损失</td><td style="text-align: center;">图像分割</td></tr><tr class="odd"><td style="text-align: center;"><code>nn.KLDivLoss()</code></td><td style="text-align: center;">KL 散度损失</td><td style="text-align: center;">分布学习</td></tr><tr class="even"><td style="text-align: center;"><code>nn.MarginRankingLoss()</code></td><td style="text-align: center;">排序相似度的损失</td><td style="text-align: center;">推荐排序</td></tr><tr class="odd"><tdstyle="text-align: center;"><code>nn.MultiLabelMarginLoss()</code></td><td style="text-align: center;">多标签分类的损失</td><td style="text-align: center;">多标签分类</td></tr><tr class="even"><td style="text-align: center;"><code>nn.SoftMarginLoss()</code></td><td style="text-align: center;">多标签二分类问题的损失</td><td style="text-align: center;">多标签二分类</td></tr></tbody></table><p>大部分损失函数的用法都是先实例化，再输入<strong>预测值和标签值</strong>，预测值形状为<code>(batch_size, num_class)</code>，对应<strong>每个类的概率</strong>，标签值形状为<code>(batch_size, num_class)</code> 或<code>(batch_size, 1)</code>，前者代表<strong>标签平滑或多标签</strong>，后者仅仅只有<strong>目标类的索引</strong>。具体用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">criterion = nn.MSELoss()<br>loss = criterion(pred, target) <span class="hljs-comment"># 输出的 loss 是零维张量</span><br></code></pre></td></tr></table></figure><blockquote><p>其中 <code>nn.CrossEntropyLoss()</code> 的 <code>target</code>输入必须是 LongTensor 类型，可以使用 <code>.long()</code>进行类型强转。而其他 Loss 则是采用 Float 作为目标输入。</p></blockquote><p>需要注意所有损失函数都会<strong>自动对 Loss取均值</strong>，原因是默认参数<code>reduction='mean'</code>，可以修改成 <code>'sum'</code>进行求和，或 <code>'none'</code> 保留 <code>(batch_size, 1)</code>形状。这会影响到梯度的大小，用 <code>mean</code>时，其<strong>学习率大小</strong>应为用 <code>sum</code> 时的<code>batch_size</code> 倍。有时候自己写的损失函数，也会在调用时加上<code>.mean()</code> 来对齐结果。</p><p>此外，有的损失函数会有 <code>weight</code> 参数，BCEWithLogitsLoss()有 <code>pos_weight</code>参数，这些都是用来帮助模型解决样本类别不均衡问题的。需要传入张量参数<code>orch.from_numpy(np.array([1,15])).float()</code>。</p><h3 id="优化器">优化器</h3><p>PyTorch 在 <code>torch.optim</code>中定义了十多种优化器，这里介绍常用的几种：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">params: 待优化的模型参数、或者是模型中参数组的字典（定制不同策略）</span><br><span class="hljs-string">lr: 学习率起始值，给定常数即可，如 1e-3, 5e-6</span><br><span class="hljs-string">momentum: 动量因子，默认为 0，通常设置为 0.9 0.8</span><br><span class="hljs-string">weight_decay: 权重衰减（L2惩罚的系数），默认为 0，通常设置为 1e-4 1e-3 再微调</span><br><span class="hljs-string">nesterov: 是否采用牛顿动量（NAG），较少设置</span><br><span class="hljs-string">### 变化量（速度） v = mo * v + dx, 参数更新 x -= lr * v</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>optim.SGD(params, lr, momentum=<span class="hljs-number">0</span>, weight_decay=<span class="hljs-number">0</span>, nesterov=<span class="hljs-literal">False</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">lr: 全局学习率，设定后就不再改变，默认为 0.1 一般不修改（会自适应）</span><br><span class="hljs-string">### 根据梯度的大小自适应学习率，梯度大则学习率小，更新 x -= lr * dx / sqrt(dx^2)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>optim.Adagrad(params, lr=<span class="hljs-number">0.01</span>, weight_decay=<span class="hljs-number">0</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">alpha: 避免学习率下降太快，采用滑动平均的保留比例，默认为 0.99</span><br><span class="hljs-string">eps: 为了增加数值计算的稳定性而加到分母里的项，不用理会</span><br><span class="hljs-string">### 缓存 cache = α * cache + (1-α) * dx^2, 更新 x -= lr * dx / sqrt(cache)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>optim.RMSprop(params, lr=<span class="hljs-number">0.01</span>, alpha=<span class="hljs-number">0.99</span>, eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0</span>, momentum=<span class="hljs-number">0</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">betas: 在缓存的基础上再对梯度加上记忆，推荐参数 (0.9, 0.999)</span><br><span class="hljs-string">### 记忆 memory = β1 * memory + (1 ‐ β1) * dx</span><br><span class="hljs-string">### 缓存 cache = β2 * cache + (1 - β2) * dx^2</span><br><span class="hljs-string">### 更新 x -= lr * memory / sqrt(cache)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>optim.Adam(params, lr=<span class="hljs-number">0.001</span>, betas=(<span class="hljs-number">0.9</span>, <span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0</span>)<br>optim.Adamax(params, lr=<span class="hljs-number">0.002</span>, betas=(<span class="hljs-number">0.9</span>, <span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0</span>) <span class="hljs-comment"># 变体1</span><br>optim.SparseAdam(params, lr=<span class="hljs-number">0.001</span>, betas=(<span class="hljs-number">0.9</span>,<span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>) <span class="hljs-comment"># 变体2，适用于稀疏张量</span><br>optim.AdamW(params, lr=<span class="hljs-number">0.001</span>, betas=(<span class="hljs-number">0.9</span>,<span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0.01</span>) <span class="hljs-comment">#变体3</span><br></code></pre></td></tr></table></figure><p>具体用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br>optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.9</span>) <span class="hljs-comment"># 用于模型训练</span><br>optimizer = optim.Adam([var1, var2], lr=<span class="hljs-number">0.0001</span>) <span class="hljs-comment"># 也可以用于任何变量</span><br><br>optimizer.zero_grad()<span class="hljs-comment"># 把上一轮 loss 关于 weight 的梯度变成 0，防止自动累计</span><br>optimizer.step()<span class="hljs-comment"># 进行一次优化迭代，即 x += Δ 的过程</span><br></code></pre></td></tr></table></figure><p>如果想对网络中不同部分采取不同的优化策略，则需要用到<strong>参数组</strong>（param_group），其保存了各个参数及其对应的学习率、动量等，可以通过<strong>列表嵌套字典</strong>的形式来设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 默认情况，一个参数组</span><br>optim.SGD(model.parameters(), lr=<span class="hljs-number">1e-2</span>, momentum=<span class="hljs-number">0.9</span>)<br><span class="hljs-comment"># 两个参数组，此时 len(optim.param_gruops) == 2</span><br>optim.SGD([&#123;<span class="hljs-string">&#x27;params&#x27;</span>: model.base.parameters(), <span class="hljs-string">&#x27;weight_decay&#x27;</span>: wd&#125;,<br>           &#123;<span class="hljs-string">&#x27;params&#x27;</span>: model.classifier.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">1e-3</span>&#125;], <span class="hljs-comment"># 访问模型局部参数即可</span><br>          lr=<span class="hljs-number">1e-2</span>, momentum=<span class="hljs-number">0.9</span>)<br></code></pre></td></tr></table></figure><h3 id="调度器">调度器</h3><p>注意到上述优化器参数都有一个固定的<code>lr</code>，即使通过不同策略使学习率改变，也会有一个全局<code>lr</code>。为了更好地收敛模型，PyTorch 在<code>torch.optim.lr_scheduler</code>里封装了主动进行学习率衰减的方法。通过一个调度器（Scheduler）来控制优化器，具体用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=<span class="hljs-number">1e-4</span>)<br>scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=<span class="hljs-number">5</span>, verbose=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br><span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_set:<br>optimizer.zero_grad()<br>        pred = model(X)<br>        loss = criterion(pred, y)<br>        loss.backward()<br>        optimizer.step()<br>    scheduler.step()<span class="hljs-comment"># 调度器工作，每个 epoch 调度一次</span><br></code></pre></td></tr></table></figure><p>当然还有许多不同的调度器，可以<strong>有规律地间隔调整</strong>，或者<strong>根据指标的变化情况</strong>调整。参数<code>verbose=True</code>表示每次更新都会输出一条信息，适用于所有调度器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 等间隔调整，每隔 step_size 个 epoch，调整为 lr * gamma</span><br>optim.lr_scheduler.StepLR(optimizer, step_size, gamma=<span class="hljs-number">0.1</span>)<br><span class="hljs-comment"># 多间隔调整，milestones 传入参数列表，表示在第几个 epoch 更新</span><br>optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=<span class="hljs-number">0.1</span>)<br><span class="hljs-comment"># 指数衰减调整，每个 epoch 更新为 lr * gamma ^ epoch，gamma 一般设为 0.9</span><br>optim.lr_scheduler.ExponentialLR(optimizer, gamma)<br><span class="hljs-comment"># 余弦退火函数调整，走势类似 cos(x)，当 epoch = T_max 时，学习率取最低点 eta_min</span><br>optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 根据指标调整，当给定指标在最近 patience 个 epoch 中都没有变化，学习率下降</span><br>optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="hljs-string">&#x27;min&#x27;</span>, factor=<span class="hljs-number">0.1</span>, patience=<span class="hljs-number">10</span>)<br>scheduler.step(val_loss) <span class="hljs-comment"># loss 对应 mode=&#x27;min&#x27;，表示当指标不再降低时更新</span><br>scheduler.step(val_acc) <span class="hljs-comment"># acc 对应 mode=&#x27;max&#x27;，表示当指标不再升高时更新</span><br><br><span class="hljs-comment"># 自定义倍率调整，通过匿名函数为不同参数组设置不同规则，更新为 lr * lambda(epoch)</span><br>optimizer = optim.SGD([&#123;<span class="hljs-string">&#x27;params&#x27;</span>: base_params&#125;,<br>           &#123;<span class="hljs-string">&#x27;params&#x27;</span>: net.fc.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">1e-2</span>&#125;],<br>                      lr=<span class="hljs-number">1e-3</span>, momentum=<span class="hljs-number">0.9</span>, weight_decay=<span class="hljs-number">1e-4</span>)<br>lambda1 = <span class="hljs-keyword">lambda</span> epoch: epoch // <span class="hljs-number">3</span><br>lambda2 = <span class="hljs-keyword">lambda</span> epoch: <span class="hljs-number">0.95</span> ** epoch<br>scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])<br></code></pre></td></tr></table></figure><h2 id="评估阶段">评估阶段</h2><p>前文提到，「<strong>评估模式</strong>」是相对于「<strong>训练模式</strong>」的模型传播路径，通常在<strong>验证集、测试集</strong>上推理时会开启。开启命令为<code>model.eval()</code>，也可以用<code>model.train(False)</code>。在该模式下，Dropout、BatchNorm等模块会有所区别。并且模型不需要进行反向传播，也就无需进行梯度计算，此时使用<code>torch.is_grad_enabled()</code> 会返回 False。</p><p>需要注意的是，为了<strong>防止</strong> PyTorch自动对张量进行梯度计算，要用 <code>with torch.no_grad():</code>包裹评估阶段的代码。关闭梯度计算后模型的<strong>推理速度</strong>也会大大增加。下面介绍具体用法。</p><ul><li><strong>验证过程</strong>：通常与训练阶段一起运行，即每个 epoch训练完都在验证集上验证，并计算 Loss：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epoch):<br>model.train()<br>    <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> tr_set:<br>        <span class="hljs-comment"># 训练过程略去</span><br><br>    model.<span class="hljs-built_in">eval</span>()<span class="hljs-comment"># 打开评估模式</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<span class="hljs-comment"># 关闭梯度计算</span><br>        total_loss = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> val_set:<br>            X, y = X.to(device), y.to(device)<br>            pred = model(X)<br>            loss = criterion(pred, y)<br>            total_loss += loss.cpu().item() * <span class="hljs-built_in">len</span>(X) <span class="hljs-comment"># loss 会默认取 mean</span><br></code></pre></td></tr></table></figure><ul><li><strong>测试阶段</strong>：无需优化、无需计算损失，只需前向推理预测答案即可：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">eval</span>()<br>preds = []<br><span class="hljs-keyword">with</span> torch.no_grad():<br>    <span class="hljs-keyword">for</span> X <span class="hljs-keyword">in</span> test_set:<br>        X = X.to(device)<br>        pred = model(X)<br>        preds.append(pred.cpu())<br></code></pre></td></tr></table></figure><h3 id="tensorboard-可视化">TensorBoard 可视化</h3><p>TensorBoard用于可视化网络的相关信息，包括训练过程的指标、输出输出、网络内部参数等。使用前需要安装<code>pip install tensorboard</code>，并在代码中导入 Writer。Writer会将我们关心的数据保存在一个文件夹中，在<strong>显示到浏览器</strong>上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br>writer = SummaryWriter(<span class="hljs-string">&#x27;./path/to/log&#x27;</span>) <span class="hljs-comment"># 实例化 writer</span><br>writer.close()<span class="hljs-comment"># 关闭 writer，代码结束时用</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tag/main_tag: 可视化时图像的标签名，是图的唯一标识</span><br><span class="hljs-string">scalar_value: 代码中的变量名，必须是一个标量</span><br><span class="hljs-string">tag_scalar_dict: 传入变量字典，key 相当于可视化的 tag，value 是代码中的变量名</span><br><span class="hljs-string">global_step: 可视化时的横坐标，传入 epoch 就表示随 epoch 的指标变化</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>writer.add_scalar(tag, scalar_value, global_step=<span class="hljs-literal">None</span>, walltime=<span class="hljs-literal">None</span>)<br>writer.add_scalars(main_tag, tag_scalar_dict, global_step=<span class="hljs-literal">None</span>, walltime=<span class="hljs-literal">None</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">img_tensor: 代码中的变量名，可以传入张量，如输入数据、各层的输入输出</span><br><span class="hljs-string">dataformats: 数据格式，如 CHW，HWC，HW</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>writer.add_image(tag, img_tensor, global_step=<span class="hljs-literal">None</span>, walltime=<span class="hljs-literal">None</span>, dataformats=<span class="hljs-string">&#x27;CHW&#x27;</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">model: 传入模型，必须是 nn.Module 类</span><br><span class="hljs-string">input_to_model: 输入给模型的数据</span><br><span class="hljs-string">verbose: 是否打印计算图结构信息</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>writer.add_graph(model, input_to_model=<span class="hljs-literal">None</span>, verbose=<span class="hljs-literal">False</span>, use_strict_trace=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>运行上述代码后，数据将被保存到文件夹中，接下来在终端中输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tensorboard --logdir=./path/to/<span class="hljs-built_in">log</span> --port 6006<br></code></pre></td></tr></table></figure><p>就可以在 <code>localhost:6006</code>看到可视化结果。如果是在远程服务器中，则需要将服务器的 6006端口转发到本机的一个端口，在本机浏览器中访问端口。</p><h2 id="模型保存与加载">模型保存与加载</h2><p>PyTorch 文件后缀为 <code>.pt</code>、<code>.pth</code> 和<code>.pkl</code>，三者在使用上没有区别，都可以保存<strong>张量</strong>、<strong>模型</strong>（结构、权重参数），张量的保存和加载方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(t, <span class="hljs-string">&#x27;save_tensor.pt&#x27;</span>)<span class="hljs-comment"># 保存张量包括其 device 属性</span><br>t = torch.load(<span class="hljs-string">&#x27;save_tensor.pt&#x27;</span>)<span class="hljs-comment"># 是否加载到 GPU 由 save 之前的参数决定</span><br>t = torch.load(<span class="hljs-string">&#x27;save_tensor.pt&#x27;</span>, map_location=torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>))<span class="hljs-comment"># 加载到 CPU</span><br>t = torch.load(<span class="hljs-string">&#x27;save_tensor.pt&#x27;</span>, map_location=<span class="hljs-string">&quot;cuda:0&quot;</span>)<span class="hljs-comment"># 加载到指定 GPU</span><br></code></pre></td></tr></table></figure><p>如果有多个张量，可以通过字典的形式进行存储，将字符串映射到张量，这也是模型的保存形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor_dict = &#123;<span class="hljs-string">&#x27;x&#x27;</span>: x, <span class="hljs-string">&#x27;y&#x27;</span>: y&#125;<br>torch.save(tensor_dict, <span class="hljs-string">&#x27;save_dict.pt&#x27;</span>)<br>tensor_dict2 = torch.load(<span class="hljs-string">&#x27;save_dict.pt&#x27;</span>)<br></code></pre></td></tr></table></figure><p>模型（Model）分为<strong>网络的结构和权重参数</strong>两部分，后者位于<code>model.stack_dict()</code>中，以<strong>字典</strong>的形式存储，将每一层映射成它的参数张量，通过<code>.keys()</code> 可以查看所有键。模型的保存和加载方法有两种：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 只保存模型的权重参数，不保存模型结构，速度更快</span><br>torch.save(model.state_dict(), <span class="hljs-string">&#x27;save_param.pt&#x27;</span>)<br>model = Model(*args, **kwargs)<span class="hljs-comment"># 需要重新 init 模型，模型是否加载到 GPU 由参数决定</span><br>model.load_state_dict(torch.load(<span class="hljs-string">&#x27;save_param.pt&#x27;</span>, map_location=<span class="hljs-string">&quot;cuda:0&quot;</span>))<span class="hljs-comment"># 参数加载</span><br>model = model.to(device)<br>model.<span class="hljs-built_in">eval</span>()<br><br><span class="hljs-comment"># 保存整个模型，包括网络结构和权重参数</span><br>torch.save(model, <span class="hljs-string">&#x27;save_model.pt&#x27;</span>)<br>model = torch.load(<span class="hljs-string">&#x27;save_model.pt&#x27;</span>)<span class="hljs-comment"># 模型是否加载到 GPU 由 save 之前的参数决定</span><br>model = model.to(device)<br>model.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></table></figure><p>注意第二种方法中，<strong>网络模型定义的代码</strong>要与 load代码写在一起，或者从其他文件中<code>import</code>，这样才能成功加载。</p><h3 id="checkpoint">Checkpoint</h3><p>在适当的时候保存<strong>模型的断点</strong>，可以在训练意外终止后<strong>不必从头开始训练</strong>，也方便必要的时候<strong>回滚</strong>到历史状态。除了模型的参数，还包括Loss、Epoch等训练信息。最容易忽略的是<strong>优化器的参数</strong>，也是使用<code>state_dict()</code>访问，包括：学习率、动量值、衰减系数等参数。一般使用 tar文件格式来保存这些检查点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % checkpoint_interval == <span class="hljs-number">0</span>:<br>    checkpoint = &#123;<span class="hljs-string">&#x27;epoch&#x27;</span>: epoch+<span class="hljs-number">1</span>,<br>                  <span class="hljs-string">&#x27;loss&#x27;</span>: loss,<br>                  <span class="hljs-string">&#x27;best_loss&#x27;</span>: best_loss,<br>                  <span class="hljs-string">&#x27;model_state_dict&#x27;</span>: net.state_dict(),<br>                  <span class="hljs-string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),<br>                 &#125;<br>    path_checkpoint = <span class="hljs-string">&quot;./history/ckp_&#123;&#125;_epoch.pth.tar&quot;</span>.<span class="hljs-built_in">format</span>(epoch)<br>    torch.save(checkpoint, path_checkpoint)<br><br>model = TheModelClass(*args, **kwargs)<br>optimizer = TheOptimizerClass(*args, **kwargs)<br><br>checkpoint = torch.load(path_checkpoint)<br>model.load_state_dict(checkpoint[<span class="hljs-string">&#x27;model_state_dict&#x27;</span>])<br>optimizer.load_state_dict(checkpoint[<span class="hljs-string">&#x27;optimizer_state_dict&#x27;</span>])<br>epoch = checkpoint[<span class="hljs-string">&#x27;epoch&#x27;</span>]<br>loss = checkpoint[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>best_loss = checkpoint[<span class="hljs-string">&#x27;best_loss&#x27;</span>]<br><br>model.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></table></figure><h3 id="切换设备">切换设备</h3><p>load 提供<strong>设备重载</strong>的功能，可以在 CPU、GPU之间切换，也可以在不同 GPU 间切换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 强制所有 GPU 张量加载到 CPU 中</span><br>torch.load(<span class="hljs-string">&#x27;tensors.pt&#x27;</span>, map_location=<span class="hljs-keyword">lambda</span> storage, loc: storage)<br>torch.load(<span class="hljs-string">&#x27;tensors.pt&#x27;</span>, map_location=torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>)) <br><span class="hljs-comment"># 把所有的张量加载到 GPU 1 中</span><br>torch.load(<span class="hljs-string">&#x27;tensors.pt&#x27;</span>, map_location=<span class="hljs-keyword">lambda</span> storage, loc: storage.cuda(<span class="hljs-number">1</span>))<br>torch.load(<span class="hljs-string">&#x27;tensors.pt&#x27;</span>, map_location=<span class="hljs-string">&#x27;cuda:1&#x27;</span>)<br><span class="hljs-comment"># 把张量从GPU 1 移动到 GPU 0</span><br>torch.load(<span class="hljs-string">&#x27;tensors.pt&#x27;</span>, map_location=&#123;<span class="hljs-string">&#x27;cuda:1&#x27;</span>:<span class="hljs-string">&#x27;cuda:0&#x27;</span>&#125;)<br></code></pre></td></tr></table></figure><p>上述代码只有模型在一个 GPU 上训练时才生效，如果在<strong>多个GPU</strong> 中，则需要使用 <code>nn.DataParallel</code>模块，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = nn.DataParalle(model) <span class="hljs-comment"># 自动将任务发送到所有 GPU 上执行</span><br>torch.save(model.module.state_dict(), <span class="hljs-string">&#x27;model.pt&#x27;</span>)<br></code></pre></td></tr></table></figure><p>注意到此时模型的参数名都带上了 <code>module</code>前缀，如果再想把模型加载到<strong>单个</strong> GPU 或 CPU上，则需要手动将 key 中的<strong>前缀去除</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 原始通过 DataParallel 保存的文件</span><br>state_dict = torch.load(<span class="hljs-string">&#x27;model.pt&#x27;</span>)<br><br><span class="hljs-comment"># 创建一个不包含 module. 的新 OrderedDict</span><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br>new_state_dict = OrderedDict()<br><span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> state_dict.items():<br>    name = key[<span class="hljs-number">7</span>:] <span class="hljs-comment"># 去掉 module.</span><br>    new_state_dict[name] = value<br><br><span class="hljs-comment"># 加载参数</span><br>model.load_state_dict(new_state_dict)<br></code></pre></td></tr></table></figure><h3 id="加载部分模型">加载部分模型</h3><p>在迁移学习或者训练新的复杂模型时，<strong>加载部分模型</strong>是很常见的。利用经过训练的参数，即使只有少数参数可用，也将有助于<strong>预热</strong>训练过程，并且使模型更快收敛。</p><p>在加载部分模型参数进行预训练的时候，很可能会碰到<strong>键不匹配</strong>的情况（模型权重都是按键值对的形式保存并加载回来的）。因此，无论是缺少键还是多出键的情况，都可以通过在<code>load_state_dict()</code> 函数中设定 <code>strict</code> 参数为<code>False</code> 来<strong>忽略不匹配的键</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(modelA.state_dict(), PATH)<br>modelB = TheModelBClass(*args, **kwargs)<br>modelB.load_state_dict(torch.load(PATH), strict=<span class="hljs-literal">False</span>) <span class="hljs-comment"># 忽略不匹配的键</span><br></code></pre></td></tr></table></figure><p>如果想将某一层的参数加载到其他层，但是有些键不匹配，可以通过<strong>强制修改</strong><code>state_dict</code> 中参数的 <code>key</code> 解决。</p>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>PyTorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>PyTorch</tag>
      
      <tag>DL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch笔记 #1 基础操作</title>
    <link href="/PyTorch-Note-1.html"/>
    <url>/PyTorch-Note-1.html</url>
    
    <content type="html"><![CDATA[<p>本文大部分内容基于 <a href="https://pytorch.org">PyTorch 官方网站</a>的简易上手教程：<ahref="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">DeepLearning with PyTorch: A 60 Minute Blitz</a>，此外还参考了 <ahref="https://pytorch.org/docs/stable/index.html">官方 API 文档</a> 及<a href="https://pytorch-cn.readthedocs.io/zh/latest/">PyTorch中文文档</a>。本文将持续更新。</p><p>使用 <code>help(函数名)</code> 命令可以快捷查看帮助文档，在 IPython中使用 <code>函数名?</code> 可以显示帮助文档，使用 <code>函数名??</code>可以显示函数的源码。</p><h2 id="pytorch-简介">PyTorch 简介</h2><p>PyTorch 是 Torch 在 Python 上的衍生，是一个针对深度学习，并且使用 GPU和 CPU 来优化的 Tensor Library（张量库）。使用 PyTorch深度学习框架，不仅能够实现强大的 GPU加速，同时还支持<strong>动态</strong>神经网络。从某种程度上说，PyTorch是在神经网络领域的 NumPy 的代替品。</p><h3 id="pytorch-vs.-tensorflow">PyTorch vs. Tensorflow</h3><p>作为两种最热门的深度学习框架，难免被人比较，这里就我目前的了解列出几点：</p><ul><li>PyTorch 在<strong>学术界</strong>更为热门，Tensorflow在<strong>工业界</strong>更为热门。</li><li>PyTorch 更简单，与 NumPy 类似，与 Python生态融合紧密，能快速实现从而验证idea。其支持<strong>动态图</strong>计算，能更有效地处理一些科研问题。</li><li>Tensorflow高度工业化，更有利于快速<strong>部署</strong>。其底层代码繁杂，但是性能更高（对研究者可能没有意义，但对工业界可能区别很大）。</li><li>PyTorch 的 API 设计更规范，很容易 clone 得到module，更受研究者青睐。Tensorflow 的 API混乱繁多、版本各异，可读性不强，难以复现研究。</li></ul><h3 id="pytorch-环境">PyTorch 环境</h3><p>PyTorch 的安装过程也比 Tensorflow 友好很多，下面给出本文的环境：</p><ol type="1"><li>安装 Anaconda，如果已经装过，在命令行查看版本<code>conda -V</code>，显示 4.10.3。</li><li>安装 CUDA，最好有 GPU 环境。在官网选择历史版本 11.1下载。安装时选择自定义安装，取消一些不需要的勾选，装在 C盘即可。在命令行查看版本 <code>nvcc --version</code>，显示11.1.105。</li><li>安装 PyTorch 1.9.1，官网选择 Windows 下的 pip安装，选择对应版本后复制到命令行执行。</li></ol><h2 id="pytorch-基础">PyTorch 基础</h2><p>PyTorch 最为常用的两个库是 <code>torch</code> 和<code>torchvision</code>，此外还有 <code>torchaudio</code> 和<code>torchtext</code>，分别用于处理不同领域的问题。下面要介绍的基础内容需要<code>import torch</code>。</p><h3 id="cuda-模块">CUDA 模块</h3><p>CUDA（Compute Unified DeviceArchitecture）是一种<strong>并行计算平台</strong>，它允许开发者利用Nvidia GPU 的并行计算能力来加速应用程序的运行。与传统的 CPU 相比，GPU在处理大规模并行计算任务时具有显著的优势。</p><p><code>torch.cuda</code> 模块是 PyTorch 用于与 CUDA相关功能交互的核心模块，支持使用 GPU加速计算任务。因此，该模型的首要功能就是 GPU<strong>设备管理</strong>：</p><ul><li><code>torch.cuda.is_available()</code>：返回true/false，表示当前系统上是否有可用的 GPU。</li><li><code>torch.cuda.current_device()</code>：返回当前所选 GPU设备的索引。</li><li><code>torch.cuda.device_count()</code>：返回系统中可用的 GPU数量，会受到程序可见 GPU 设置的影响。</li><li><code>torch.cuda.get_device_name(index)</code>：返回指定设备的名称，<code>index</code>为 GPU 号。</li></ul><p>此外，还可以进行<strong>内存管理</strong>，在代码的关键部分插入以下语句，即可监控显存占用：</p><ul><li><code>torch.cuda.memory_allocated()</code>：返回当前分配给张量的 GPU显存<strong>字节数</strong>，张量需要移动到 CUDA device上才可以查看。<code>torch.cuda.memory_allocated()/1024/1024/1024</code>即可得到显存 GB 数。</li><li><code>torch.cuda.max_memory_allocated()</code>：同上，但是返回<strong>到目前为止程序运行过程中分配的最大显存量</strong>。</li></ul><blockquote><p>需要注意的是，如果在某个点上显存被释放了，使用该函数看到的将是释放后的显存使用情况。</p></blockquote><p>在使用多卡训练、多卡推理的时候，需要设置可见设备，以下是常见的几种方法：</p><ol type="1"><li><strong>使用 PyTorch 进行管理</strong>：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">num_gpus = torch.cuda.device_count()<br><br><span class="hljs-comment"># 指定第一张卡作为默认显卡</span><br><span class="hljs-keyword">if</span> num_gpus &gt; <span class="hljs-number">0</span>:<br>    torch.cuda.set_device(<span class="hljs-number">0</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Current CUDA device set to GPU at index 0.&quot;</span>)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;No CUDA devices available.&quot;</span>)<br>    <br><span class="hljs-comment"># 也可以在初始化张量、模型的时候进行设置，详见下方</span><br></code></pre></td></tr></table></figure><ol start="2" type="1"><li><strong>在脚本入口使用 os 管理</strong>：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="hljs-string">&quot;0,1&quot;</span><br>    main()<br></code></pre></td></tr></table></figure><ol start="3" type="1"><li><strong>通过 argparse 传参设置</strong>：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> argparse<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse_config</span>():</span><br>    parser = argparse.ArgumentParser(description=<span class="hljs-string">&#x27;Set visible CUDA devices&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--gpus&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&quot;0,1&quot;</span>)<br>    args = parser.parse_args()<br>    <span class="hljs-keyword">return</span> args<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    args = parse_config()<br>os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = args.gpus<br>main()<br>    <br><span class="hljs-comment"># 运行时采用 python main.py --gpus 0,1</span><br></code></pre></td></tr></table></figure><ol start="4" type="1"><li><strong>在命令行中直接设置</strong>：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">CUDA_VISIBLE_DEVICES=0,1 python main.py<br></code></pre></td></tr></table></figure><h3 id="tensor-对象">Tensor 对象</h3><p>Tensor（张量）是 PyTorch 的核心数据结构，与 NumPy 中的 ndarrary十分相似，表示<strong>多维矩阵</strong>，区别是 Tensor 支持 GPU上的分布式运算，且支持「<strong>自动微分</strong>」。</p><p>Tensor具有三个主要属性，<code>dtype</code>、<code>shape</code>、<code>device</code>，分别表示元素的数据类型、形状参数和<strong>所属设备</strong>。前两者与ndarray 类似，通过 <code>.</code> 运算符可以查看，下面是显示的区别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br><span class="hljs-comment"># a = array([1, 2])</span><br><span class="hljs-comment"># a.dtype = dtype(&#x27;int32&#x27;)</span><br><span class="hljs-comment"># a.shape = (2,)</span><br><span class="hljs-comment"># a.size = 2 （元素个数）</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>t = torch.tensor(a)<br><span class="hljs-comment"># t = tensor([1, 2], dtype=torch.int32)</span><br><span class="hljs-comment"># t.dtype = torch.int32</span><br><span class="hljs-comment"># t.shape = torch.Size([2])</span><br><span class="hljs-comment"># t.size() = torch.Size([2])</span><br><span class="hljs-comment"># t.device = device(type=&#x27;cpu&#x27;)</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.is_tensor(a), torch.is_tensor(t)<br><span class="hljs-comment"># False True</span><br></code></pre></td></tr></table></figure><p>在进行 Tensor的数学运算时，与其他语言不同，<strong>高精度数据类型不能赋值给低精度类型</strong>，例如将float 赋值给 int，将 non-boolen 赋值给 bool。</p><p>对于 Tensor 独有的 <code>device</code>属性，需要通过<strong>字符串</strong>变量定义，可选的类型有：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>)<span class="hljs-comment"># CPU 上运行，无法使用 GPU 加速</span><br><span class="hljs-comment"># device(type=&#x27;cpu&#x27;)</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.device(<span class="hljs-string">&#x27;cuda:0&#x27;</span>)<span class="hljs-comment"># 指定编号，在多 GPU 时指定单卡</span><br><span class="hljs-comment"># device(type=&#x27;cuda&#x27;, index=0)</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>)<span class="hljs-comment"># 默认调取首个 GPU 设备</span><br><span class="hljs-comment"># device(type=&#x27;cuda&#x27;)</span><br></code></pre></td></tr></table></figure><p>可以在创建张量时指定其所属设备，也可以在创建后使用 <code>to</code>切换，<code>to</code>会返回一份<strong>新设备上的拷贝</strong>，不会覆盖旧设备，因此通常用<code>=</code> 主动覆盖：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 初始化时设置参数</span><br>t1 = torch.tensor([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>], dtype=torch.float64, device=torch.device(<span class="hljs-string">&#x27;cuda:0&#x27;</span>))<br><br><span class="hljs-comment"># 也可以预先定义好设备，在初始化时更方便</span><br>my_cuda = torch.device(<span class="hljs-string">&#x27;cuda:1&#x27;</span>)<br>t2 = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], dtype=torch.int32, device=my_cuda)<br><br><span class="hljs-comment"># 先定义，后根据实际条件切换参数</span><br>t3 = torch.tensor([<span class="hljs-number">1</span>])<br><span class="hljs-keyword">if</span> torch.cuda.is_available(): t3 = t3.to(<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>    <br><span class="hljs-comment"># 训练时设置参数</span><br>self.device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br>self.Model = self.Model.to(self.device)<br><br><span class="hljs-comment"># 需要注意的是，如果张量是以列表形式存放的，则无法对列表使用 to（常见于文本任务）</span><br><span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(TList, <span class="hljs-built_in">list</span>):<br>    TList = [t.to(device) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> TList]<br></code></pre></td></tr></table></figure><p>除了三个主要属性，张量还支持一些形如 <code>t.func()</code>的「<strong>函数型属性</strong>」：</p><ul><li><code>t.item()</code>：对于一个<strong>只含有一个元素</strong>的张量（无论多少维度），返回一个<strong>标量值</strong>，常用于打印Loss、sum。</li><li><code>t.cpu()</code>：将张量、模型<strong>移动</strong>到 CPU上，用于进行普通的 Python 或 NumPy 操作，常用<code>t.cpu().numpy()</code>。</li><li><code>t.size()</code>：返回张量的形状，等价于<code>t.shape</code>。带参数时 <code>t.size(0)</code> 相当于<code>t.shape[0]</code>。</li><li><code>t.numel()</code>：返回张量中<strong>元素的个数</strong>，某些运算不需要形状相同，只需要元素个数相同，就得先判断。</li><li><code>t.detach()</code>：分离出原张量的一个<strong>拷贝</strong>，但是<code>requires_grad=False</code>，二者<strong>共享内存空间</strong>。反向传播到该张量时会停止，常用于<strong>冻结</strong>网络参数、<strong>限制</strong>梯度传播距离。</li><li><code>t.is_continuous()</code>：判断张量是否<strong>连续</strong>，连续的张量在各种运算上的速度会更快。由于张量的索引、拼接等操作都是浅拷贝，本质是用一个结构体存储了「切片规则」，因此在访问时会不再连续。</li><li><code>t.continuous()</code>：开辟一个新的存储区给张量，并改变存放顺序，使其变得<strong>连续化</strong>。</li></ul><h3 id="tensor-构造">Tensor 构造</h3><p>根据不同的需求，有各种构造 Tensor 的方法。</p><ul><li><strong>已有容器转化张量</strong></li></ul><p>生成 Tensor 的基本函数是 <code>torch.tensor()</code>，其参数可以是list 、tuple、<strong>另一个 Tensor 甚至ndarray</strong>，其函数接口如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    data: 传入原数组，深拷贝数据，且创建的 Tensor 没有微分历史</span><br><span class="hljs-string">    dtype: 指定数据类型，默认根据原数组推断</span><br><span class="hljs-string">    device: 创建 Tensor 的设备，如果 data 也是 Tensor 则跟随，否则会默认在 CPU 上</span><br><span class="hljs-string">    requires_grad: 是否支持自动求导，默认不支持</span><br><span class="hljs-string">    pin_memory: 是否存于锁页内存，加载数据时可以更快地从 CPU 拷贝到 GPU，但可能导致内存爆炸</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>torch.tensor(data, *, dtype=<span class="hljs-literal">None</span>, device=<span class="hljs-literal">None</span>, requires_grad=<span class="hljs-literal">False</span>, pin_memory=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>样例测试如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.tensor(<span class="hljs-number">1</span>)<span class="hljs-comment"># 创建一个零维张量（不等同于标量）</span><br>torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<span class="hljs-comment"># 创建一个一维张量</span><br>torch.tensor([[<span class="hljs-number">0.1</span>, <span class="hljs-number">1.2</span>], [<span class="hljs-number">2.2</span>, <span class="hljs-number">3.1</span>]])<span class="hljs-comment"># 创建一个二维张量</span><br>torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]], dtype=torch.float64, device=torch.device(<span class="hljs-string">&#x27;cuda:0&#x27;</span>))<span class="hljs-comment"># 设置参数</span><br>torch.tensor([<span class="hljs-number">0.5</span>, <span class="hljs-number">1.5</span>], device=torch.device(<span class="hljs-string">&#x27;cuda:0&#x27;</span>), requires_grad=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 设置参数</span><br>torch.tensor(np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]))<span class="hljs-comment"># 从 ndarray 直接创建</span><br></code></pre></td></tr></table></figure><p>如果不想通过 <code>dtype</code> 和 <code>device</code>参数设置，也可以直接用以下函数创建张量，区别在于这些方法的参数可以是<strong>各个维度的大小</strong>，相当于<code>torch.zeros(shape, dtype=...)</code>：</p><table><thead><tr class="header"><th style="text-align: center;">Data type</th><th style="text-align: center;">初始化 CPU tensor</th><th style="text-align: center;">初始化 GPU tensor</th><th style="text-align: center;">类型强转</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">32-bit floating point</td><td style="text-align: center;"><code>torch.FloatTensor()</code> 或<code>torch.Tensor()</code></td><td style="text-align: center;"><code>torch.cuda.FloatTensor()</code> 或<code>torch.cuda.Tensor()</code></td><td style="text-align: center;"><code>t.float()</code></td></tr><tr class="even"><td style="text-align: center;">64-bit floating point</td><td style="text-align: center;"><code>torch.DoubleTensor()</code></td><tdstyle="text-align: center;"><code>torch.cuda.DoubleTensor()</code></td><td style="text-align: center;"><code>t.double()</code></td></tr><tr class="odd"><td style="text-align: center;">16-bit floating point</td><td style="text-align: center;">N/A</td><tdstyle="text-align: center;"><code>torch.cuda.HalfTensor()</code></td><td style="text-align: center;"><code>t.half()</code></td></tr><tr class="even"><td style="text-align: center;">8-bit integer (unsigned)</td><td style="text-align: center;"><code>torch.ByteTensor()</code></td><tdstyle="text-align: center;"><code>torch.cuda.ByteTensor()</code></td><td style="text-align: center;"><code>t.byte()</code></td></tr><tr class="odd"><td style="text-align: center;">8-bit integer (signed)</td><td style="text-align: center;"><code>torch.CharTensor()</code></td><tdstyle="text-align: center;"><code>torch.cuda.CharTensor()</code></td><td style="text-align: center;"><code>t.char()</code></td></tr><tr class="even"><td style="text-align: center;">16-bit integer (signed)</td><td style="text-align: center;"><code>torch.ShortTensor()</code></td><tdstyle="text-align: center;"><code>torch.cuda.ShortTensor()</code></td><td style="text-align: center;"><code>t.short()</code></td></tr><tr class="odd"><td style="text-align: center;">32-bit integer (signed)</td><td style="text-align: center;"><code>torch.IntTensor()</code></td><td style="text-align: center;"><code>torch.cuda.IntTensor()</code></td><td style="text-align: center;"><code>t.int()</code></td></tr><tr class="even"><td style="text-align: center;">64-bit integer (signed)</td><td style="text-align: center;"><code>torch.LongTensor()</code></td><tdstyle="text-align: center;"><code>torch.cuda.LongTensor()</code></td><td style="text-align: center;"><code>t.long()</code></td></tr></tbody></table><ul><li><strong>与 NumPy 的 ndarray 共享内存</strong></li></ul><p>使用 <code>torch.from_numpy()</code>可以转换，并且和原始的 ndarray<strong>共享内存空间</strong>，且不能转移到 GPU 设备！此时如果修改Tensor，则原始的 ndarray 也会修改，且 Tensor<strong>不能使用变形操作</strong>。以下为样例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = numpy.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>t = torch.from_numpy(a)<br><span class="hljs-meta">&gt;&gt;&gt; </span>t<br><span class="hljs-comment"># tensor([ 1,  2,  3])</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>t[<span class="hljs-number">0</span>] = -<span class="hljs-number">1</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>a<br><span class="hljs-comment"># array([-1,  2,  3])</span><br></code></pre></td></tr></table></figure><p>同理，Tensor 也可以转化为 ndarray，只需用 <code>t.numpy()</code>就能返回一个 ndarray，并且<strong>共享内存</strong>，前提是原 Tensor不能位于 GPU 设备！</p><ul><li><strong>生成已初始化的张量、随机张量</strong></li></ul><p>和 Numpy 类似的初始化函数，增加了 <code>like</code>类型的函数，其他参数和 <code>torch.tensor()</code>一致，以下为样例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入的参数是表示 shape 的元组，也可以直接传入维度（和 Numpy 不同）</span><br>t1 = torch.zeros((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<span class="hljs-comment"># 全为 0 的张量</span><br>t2 = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<span class="hljs-comment"># 全为 1 的张量</span><br>t3 = torch.empty((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<span class="hljs-comment"># 未初始化张量</span><br>t4 = torch.full((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), x)<span class="hljs-comment"># 全为 x 的张量</span><br>t5 = torch.rand((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<span class="hljs-comment"># 0 到 1 之间的浮点数</span><br>t6 = torch.randn((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<span class="hljs-comment"># 均值为 0，方差为 1 的正态分布浮点数</span><br>t7 = torch.eye(n=<span class="hljs-number">2</span>, m=<span class="hljs-literal">None</span>)<span class="hljs-comment"># n * m 的对角线值为 1 的二维张量，省略 m 时自动取 n</span><br><br><span class="hljs-comment"># 输入的 input 是另一个 Tensor，返回形状相同的一个初始化张量</span><br>t1_like = torch.zeros_like(<span class="hljs-built_in">input</span>)<br>t2_like = torch.ones_like(<span class="hljs-built_in">input</span>)<br>t3_like = torch.empty_like(<span class="hljs-built_in">input</span>)<br>t4_like = torch.full_like(<span class="hljs-built_in">input</span>, x)<br>t5_like = torch.rand_like(<span class="hljs-built_in">input</span>)<br>t6_like = torch.randn_like(<span class="hljs-built_in">input</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>基于范围生成数组</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 范围在 [start, end) 之间，缺省其一则视为 [0, end)，步长为 step</span><br>torch.arange(start=<span class="hljs-number">0</span>, end, step=<span class="hljs-number">1</span>, dtype=int64)<br><span class="hljs-comment"># 范围在 [start, end] 之间，不可缺省，且结果包含 end，步长为 step</span><br>torch.<span class="hljs-built_in">range</span>(start, end, step=<span class="hljs-number">1</span>, dtype=float32)<br><span class="hljs-comment"># 生成等差数列，[start, stop] 之间，限定总数为 steps，endpoint 表示是否包含 stop 端点</span><br>torch.linspace(start, end, steps=<span class="hljs-number">100</span>, endpoint=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 生成等比数列，注意范围是 [base^start, base^stop] 之间,限定总数为 num</span><br>torch.logspace(start, end, steps=<span class="hljs-number">100</span>, endpoint=<span class="hljs-literal">True</span>, base=<span class="hljs-number">10.0</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>从外部文件导入</strong></li></ul><p>PyTorch 文件后缀为 <code>.pt</code>、<code>.pth</code> 和<code>.pkl</code>，三者在使用上没有区别，都可以保存<strong>张量</strong>、<strong>模型</strong>（结构、权重参数），张量的保存和加载方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(t, <span class="hljs-string">&#x27;save_tensor.pt&#x27;</span>)<span class="hljs-comment"># 保存张量包括其 device 属性</span><br>t = torch.load(<span class="hljs-string">&#x27;save_tensor.pt&#x27;</span>)<span class="hljs-comment"># 是否加载到 GPU 由 save 之前的参数决定</span><br>t = torch.load(<span class="hljs-string">&#x27;save_tensor.pt&#x27;</span>, map_location=torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>))<span class="hljs-comment"># 加载到 CPU</span><br>t = torch.load(<span class="hljs-string">&#x27;save_tensor.pt&#x27;</span>, map_location=<span class="hljs-string">&quot;cuda:0&quot;</span>)<span class="hljs-comment"># 加载到指定 GPU</span><br></code></pre></td></tr></table></figure><p>模型（Module）分为<strong>网络的结构和权重参数</strong>两部分，后者位于<code>module.stack_dict()</code>中，以字典的形式存储，模型的保存和加载方法有两种：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 只保存模型的权重参数，不保存模型结构</span><br>torch.save(model.state_dict(), <span class="hljs-string">&#x27;save_param.pt&#x27;</span>)<br>model = Model(*args, **kwargs)<span class="hljs-comment"># 需要重新 init 模型，模型是否加载到 GPU 由参数决定</span><br>model.load_state_dict(torch.load(<span class="hljs-string">&#x27;save_param.pt&#x27;</span>, map_location=<span class="hljs-string">&quot;cuda:0&quot;</span>))<span class="hljs-comment"># 参数加载</span><br>model.<span class="hljs-built_in">eval</span>()<br><br><span class="hljs-comment"># 保存整个模型，包括网络结构和权重参数</span><br>torch.save(model, <span class="hljs-string">&#x27;save_model.pt&#x27;</span>)<br>model = torch.load(<span class="hljs-string">&#x27;save_model.pt&#x27;</span>)<span class="hljs-comment"># 模型是否加载到 GPU 由 save 之前的参数决定</span><br>model.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></table></figure><ul><li><strong>从已有张量构造</strong></li></ul><p><code>torch.where(condition, x, y)</code>：条件选择函数，<code>condition</code>为布尔张量，三个张量形状相同。当<code>condition</code> 为 True 时，选择 <code>x</code> 的值；当<code>condition</code> 为 False 时，选择 <code>y</code> 的值。</p><h3 id="tensor-变形">Tensor 变形</h3><p>PyTorch 中实现了对 Tensor的上百种操作，包括常见的索引、切片、变形。其中<strong>索引和切片</strong>的使用与Numpy 类似，这里不展开介绍。主要介绍的是 Tensor通过连接、换位等各种<strong>变形</strong>操作。下列所有对 <code>t</code>的函数都可以改写成 <code>t.func(..)</code> 形式。</p><p><strong>拼接与分块</strong>：</p><ul><li><code>torch.cat([a, b, c], dim=0)</code>：在<strong>指定维度</strong>上对输入的<strong>张量序列</strong>进行连接操作，要求除了指定维度<code>dim</code>外，<strong>其他维度形状必须相同</strong>。最后返回一个张量，其<code>dim</code>维度是输入的张量序列该维度<strong>长度之和</strong>。</li><li><code>torch.chunk(t, chunks, dim=0)</code>：在<strong>指定维度</strong>上对输入张量进行分块操作，<code>chunks</code>为分块的个数，如果不能整除则最后一块不完整，最后返回一个<strong>张量元组</strong>。</li><li><code>torch.split(t, size, dim=0)</code>：在<strong>指定维度</strong>上对输入张量进行分块操作，<code>size</code>为分块的大小，如果不能整除则最后一块不完整，最后返回一个<strong>张量元组</strong>。</li><li><code>torch.stack([a, b, c], dim=0)</code>：沿着<strong>一个新维度</strong>对输入<strong>张量序列</strong>进行连接，要求<strong>所有张量都为相同形状</strong>。最后返回一个张量，在原形状上多出一个<code>dim</code> 维度，该维度就是输入张量的<strong>个数</strong>。</li><li><code>torch.unbind(t, dim=0)</code>：移除<strong>指定维度</strong>后，返回一个<strong>张量元组</strong>，包含沿着指定维切片后的各个切片，元组长度为被移除维度的取值。</li></ul><p><strong>压缩与扩充</strong>：</p><ul><li><code>torch.squeeze(t, dim)</code>：去除张量中<strong>大小为1</strong>的所有维度，返回一个张量。返回张量与输入张量<strong>共享内存</strong>。也可以<strong>指定维度</strong>删除，如果指定维度大小不为1，则原样返回。</li><li><code>torch.unsqueeze(t, dim)</code>：在指定位置插入<strong>大小为1</strong>的维度，返回一个张量。返回张量与输入张量<strong>共享内存</strong>。如果<code>dim</code>取<strong>负值</strong>，则相当于<strong>反向索引</strong>插入。</li></ul><p><strong>按值切片</strong>：</p><ul><li><code>torch.index_select(t, dim, index)</code>：沿着<strong>指定维度</strong>对输入进行切片，<code>index</code>需为<strong>长整型一维张量</strong>，将 <code>dim</code> 维度上取出<code>index</code>索引的项，返回一个张量，与原始张量<strong>不共享内存</strong>。</li><li><code>torch.masked_select(t, mask)</code>：对输入进行掩码，<code>mask</code>需为<strong>布尔型一维张量</strong>，形状需与输入张量<strong>显式相等或隐式相等</strong>（广播机制），返回一个<strong>一维张量</strong>，由<span class="math inline">\(mask=1\)</span> 的元素构成。</li><li><code>torch.nonzero(t)</code>：返回一个包含输入中<strong>非零元素索引</strong>的<strong>二维张量</strong>，第一维是非零元素个数，第二维是输入张量的<strong>维度数</strong>（返回一个矩阵，每行都是一个非零元素的<strong>坐标</strong>）。</li></ul><p><strong>维度交换与重塑</strong>：</p><ul><li><code>torch.transpose(t, dim0, dim1)</code>：返回输入张量的<strong>转置</strong>（交换维度<code>dim0</code> 和<code>dim1</code>），返回张量与输入张量<strong>共享内存</strong>，所以改变其中一个的内容会改变另一个。</li><li><code>torch.permute(t, order)</code>：返回一个维度<strong>重组</strong>的张量，输入的<code>order</code> 是一个 <span class="math inline">\(0\)</span> 到<span class="math inline">\(n-1\)</span>的元组，表示原始维度在新张量中的顺序。</li><li><code>torch.flatten(t, dim)</code>：将张量的维度展开，以<code>dim</code>维为<strong>起点</strong>的所有维度长度相乘，<strong>前面的保持不变</strong>。</li><li><code>torch.reshape(t, shape)</code>：调整形状，默认将张量<strong>按行展开</strong>后填到新形状，要求<strong>规模匹配</strong>。当<code>shape</code> 中某一维度取 <span class="math inline">\(-1\)</span>时，其长度由其他维度计算。<strong>不需要满足连续性条件</strong>，功能更强。</li><li><code>torch.view(t, shape)</code>：与 <code>reshape</code>的功能类似，但是只适用于<strong>满足连续性条件</strong>的张量（不能索引、转置、拼接），否则需要用<code>t.contiguous().view()</code>，相当于<code>t.reshape()</code>。</li><li><code>torch.flip(t, dims)</code>：指定维度反转，即反序复制一份新的数据。</li></ul><p><strong>重复</strong>：</p><ul><li><code>t.repeat(*sizes)</code>：将张量沿着指定维度重复若干次，传入参数为一系列整数，<strong>倒着执行</strong>重复。</li><li><code>torch.repeat_interleave(t, repeats)</code>：将张量中的每个元素重复<code>repeats</code> 次，并按顺序<strong>展开为一维张量</strong>。</li></ul><h2 id="常用数学函数">常用数学函数</h2><p>除了上述对 Tensor的变形操作，还有一系列涉及计算的操作，下面列举常用的部分。以下所有对<code>t</code> 的函数都可以改写成 <code>t.func(..)</code>形式。更多内容可查阅 <ahref="https://pytorch.org/docs/stable/index.html">文档</a>。</p><h3 id="原地操作">原地操作</h3><p>PyTorch 在大部分数学函数都封装了<strong>原地操作</strong>（In-PlaceOperation）版本。只需将 <code>t.func()</code>形式函数名后面加上<strong>单下划线</strong>即可，例如<code>t.add_()</code>；或在参数中设置 <code>inplace=True</code>。</p><p>所谓非原地操作，可以理解为「<strong>计算&amp;赋值</strong>」的过程。例如：<code>x = torch.add(x, y)</code>，我们先对<strong>旧内存</strong><code>x</code> 和 <code>y</code>进行了求和，产生的结果存放于<strong>新内存</strong>，然后再用<code>=</code> 更新 <code>x</code> 的引用。</p><p>而原地操作可以理解为省略了「<strong>赋值</strong>」的过程，直接在旧内存上更改数值。这种原地操作更加<strong>节省内存</strong>，但是如果该内存可能被其他变量引用，可能导致<strong>计算一致性</strong>、<strong>自动微分出错</strong>的问题，所以在使用时应当尽量避免。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>self.conv1 = nn.Conv2d(...)<br>self.conv2 = nn.Conv2d(...)<br>self.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>x = self.conv1(x)<br>h = self.relu(x)<span class="hljs-comment"># 这里的 relu 是原地操作，则 x 的值被修改</span><br>h0 = self.conv2(x) <span class="hljs-comment"># 这里的 conv2 已经不是对输入 x 的操作了</span><br></code></pre></td></tr></table></figure><h3 id="随机采样">随机采样</h3><ul><li><code>torch.manual_seed(seed)</code>：设定生成随机数的种子，使得每次随机采样的结果一致，模型具有<strong>可复现性</strong>。</li><li><code>torch.bernoulli(t)</code>：采样伯努利分布，返回一个和输入<strong>相同大小</strong>的张量，输入张量的每一个元素<span class="math inline">\(p\in[0,1]\)</span> 作为采样 <spanclass="math inline">\(1\)</span> 的概率。</li><li><code>torch.multinomial(t, num)</code>：采样多项式分布，<code>t</code>的<strong>每一行</strong>作为一组多项式系数，<code>num</code>为在每一行采样的元素个数。</li><li><code>torch.normal(means, std)</code>：采样正态分布，返回一个和输入<strong>相同大小</strong>的张量，输入的两个张量代表输出均值和标准差。输入的两个张量大小不必相同，符合广播机制。</li><li><code>torch.poisson(t)</code>：采样泊松分布，返回一个和输入<strong>相同大小</strong>的张量，输入张量的每一个元素非负。</li></ul><p>以下函数只有<strong>原地操作</strong>版本，采样的形状与<code>t</code> 相同：</p><ul><li><code>t.uniform_(from, to)</code>：采样均匀分布，从 <spanclass="math inline">\([from, to]\)</span> 中以等概率采样。</li><li><code>t.exponential_(lambd=1)</code>：采样指数分布，从 <spanclass="math inline">\(f(x)=\lambda e^{-\lambda x}\)</span> 中采样。</li><li><code>t.geometric_(p)</code>：采样几何分布，从 $f( X=k ) =p^{k-1}(1-p ) $ 中采样。</li></ul><h3 id="普通运算">普通运算</h3><ul><li><code>torch.abs(t)</code>、<code>torch.ceil(t)</code>、<code>torch.floor(t)</code>：作用于每个元素取绝对值、向上取整、向下取整。</li><li><code>torch.round(t)</code>：作用于每个元素，四舍五入到最近的整数。</li><li><code>torch.clamp(t, min, max)</code>：作用于每个元素<strong>夹紧</strong>到<span class="math inline">\([min, max]\)</span>区间，可以只输入一侧边界。</li><li><code>torch.frac(t)</code>、<code>torch.sign(t)</code>：返回每个元素的小数部分、符号部分。</li><li><code>torch.neg(t)</code>、<code>torch.reciprocal(t)</code>：作用于每个元素取相反数、倒数。</li><li><code>torch.add(t, value, other)</code>：作用于每个元素加上标量值<span class="math inline">\(value\)</span>，如果输入另一个张量 <spanclass="math inline">\(other\)</span>，该张量形状需与 <code>t</code>一致，返回 <span class="math inline">\(t+value\timesother\)</span>。</li><li><code>torch.div(t, value)</code>、<code>torch.mul(t, value)</code>：作用于每个元素除以、乘以标量值<span class="math inline">\(value\)</span>。</li><li><code>torch.log(t)</code>、<code>troch.log1p(t)</code>：作用于每个元素计算自然对数、加上<span class="math inline">\(1\)</span> 后的自然对数。</li><li><code>torch.sin(t)</code>、<code>torch.cos(t)</code>、<code>torch.tan(t)</code>：作用于每个元素求三角函数。</li><li><code>torch.asin(t)</code>、<code>torch.acos(t)</code>、<code>torch.atan(t)</code>：作用于每个元素求反三角函数。</li><li><code>torch.sinh(t)</code>、<code>torch.cosh(t)</code>、<code>torch.tanh(t)</code>：作用于每个元素求双曲三角函数。</li><li><code>torch.pow(base, exp)</code>：求幂次，两者中<strong>一个为标量，另一个就为张量</strong>。返回一个张量，大小和输入张量一致。</li><li><code>torch.exp(t)</code>：作用于每个元素，求其 <spanclass="math inline">\(e^x\)</span> 值。</li><li><code>torch.sqrt(t)</code>、<code>torch.rsqrt(t)</code>：作用于每个元素，求其平方根、平方根倒数。</li><li><code>torch.sigmoid(t)</code>：作用于每个元素，求其 <spanclass="math inline">\(\text{sigmoid}\)</span> 值。</li><li><code>torch.nan_to_num(x, nan=2.0, posinf=1.0, neginf=-1.0)</code>：替换NaN 值和无穷值。</li></ul><h3 id="塌缩运算">塌缩运算</h3><p>塌缩运算中，指定的维度 <code>dim</code>会<strong>塌缩</strong>消失，如果想要输入输出<strong>维度个数与输入相同</strong>（即保持该维度为<span class="math inline">\(1\)</span>），则需要加上参数<code>keepdims=True</code>。<code>dim</code>也可以取多个维度，以列表形式传参即可。</p><ul><li><code>torch.mean(t, dim)</code>、<code>torch.std(t, dim)</code>、<code>torch.std(t, dim)</code>：返回给定维度上的所有元素的均值、标准差、方差，指定维度会塌缩。</li><li><code>torch.sum(t, dim)</code>、<code>torch.prod(t, dim)</code>：返回给定维度上的所有元素的和、积，指定维度会塌缩。</li><li><code>torch.cumsum(t, dim)</code>、<code>torch.cumprod(t, dim)</code>：返回给定维度上的<strong>前缀</strong>和、积，形状保持不变。</li><li><code>torch.norm(t, p=2, dim)</code>：返回给定维度的 <spanclass="math inline">\(p\)</span> 范数，指定维度会塌缩，不加<code>dim</code> 则结果<strong>塌缩到零维</strong>张量。</li><li><code>torch.dist(t1, t2, p=2)</code>：返回 <spanclass="math inline">\(t_1-t_2\)</span> 的 <spanclass="math inline">\(p\)</span>范数，两个输入张量形状相同，结果<strong>塌缩到零维</strong>张量。</li><li><code>torch.max(t, dim)</code>、<code>torch.min(t, dim)</code>：返回给定维度上的<strong>最大值、最小值</strong>，指定维度会塌缩。同时也会返回最值所在的<strong>索引</strong>，相当于<code>argmax</code>。</li><li><code>torch.max(t1, t2)</code>、<code>torch.min(t1, t2)</code>：相应位置的元素对比，返回最小值到输出张量。</li><li><code>torch.argmax(t, dim)</code>、<code>torch.argmin(t, dim)</code>：求给定维度上最值的索引，指定维度会塌缩，如果有多个最值则会返回第一个出现的位置。</li><li><code>torch.all(t)</code>、<code>torch.any(t)</code>：输入一个布尔型张量，判断是否全True、含有 True，返回一个 True 或 False 的张量。通常会将 <code>t</code>写作一个<strong>二元运算表达式</strong>，如 <code>x == y</code>。</li></ul><h3 id="矩阵运算">矩阵运算</h3><ul><li><code>mat1 @ mat2</code>：<strong>矩阵点乘</strong>，要求第一个矩阵的列数等于第二个矩阵的行数。<ul><li><code>torch.mm(mat1, mat2)</code>、<code>mat1.mm(mat2)</code>、<code>mat1.matmul(mat)</code>：<strong>矩阵点乘</strong>，要求同上。</li><li><code>torch.mv(mat, vec)</code>、<code>mat.mv(vec)</code>：<strong>矩阵点乘向量</strong>，矩阵<span class="math inline">\(n\times m\)</span>，向量长度是 <spanclass="math inline">\(m\)</span>，<strong>不区分行列向量</strong>。</li><li><code>torch.dot(vec1, vec2)</code>、<code>vec1.dot(vec2)</code>：<strong>向量点乘</strong>，输入不能是多维张量，返回一个零维张量。</li><li><code>torch.bmm(mat1, mat2)</code>：<strong>批量矩阵点乘</strong>，要求矩阵大小为<span class="math inline">\((n,a,b)\)</span> 和 <spanclass="math inline">\((n,b,c)\)</span>，返回 <spanclass="math inline">\((n,a,c)\)</span>。</li></ul></li><li><code>mat1 * mat2</code>：<strong>对应位置相乘</strong>，即 Hadmard积，要求两个矩阵<strong>各个维度长度相等</strong>。<ul><li><code>torch.mul(t1, t2)</code>：<strong>对应位置相乘</strong>，不限制张量维度数，各个维度长度相等即可。</li></ul></li><li><code>torch.t(mat)</code>、<code>mat.t()</code>、<code>mat.T</code>：<strong>矩阵转置</strong>。等价于<code>torch.ranspose(t, 0, 1)</code>。</li><li><code>torch.inverse(mat)</code>、<code>mat.inverse()</code>：返回输入<strong>方阵</strong>的逆矩阵。</li><li><code>torch.diag(t)</code>：对角化，有两种模式：<ul><li>如果输入是一个向量（一维张量），则返回一个以 <code>t</code>为对角线元素的方阵；</li><li>如果输入是一个矩阵（二维张量），则返回一个包含 <code>t</code>对角线元素的张量。</li></ul></li><li><code>torch.trace(mat)</code>：返回输入二维矩阵对角线元素的和（迹）。</li><li><code>torch.tril(mat, k=0)</code>：返回输入二维矩阵的下三角部分，其他部分为<span class="math inline">\(0\)</span>，<code>k</code>控制是否<strong>包含主对角线</strong>：<ul><li><code>k=0</code> 含主对角线；<code>k&gt;0</code> 含主对角线之上<span class="math inline">\(k\)</span> 条；<code>k&lt;0</code>不含主对角线之下。</li></ul></li><li><code>torch.triu(mat, k=0)</code>：返回输入二维矩阵的上三角部分，其他部分为<span class="math inline">\(0\)</span>，<code>k</code>控制是否<strong>包含主对角线</strong>，同上。</li><li><code>torch.svd(mat)</code>：返回输入<strong>矩阵</strong>的 SVD分解，分别返回 <span class="math inline">\(U,S,V\)</span>三个张量。</li></ul><h2 id="autograd-自动微分">Autograd 自动微分</h2><p>Tensor支持自动微分，这在神经网络的反向传播中十分便利，只需要在创建张量时设置<code>requires_grad=True</code>，就会为该张量进行Autograd。如果张量已经创建，则可以用<strong>原地操作</strong>的<code>t.requires_grad_(True)</code>进行设置。需要注意的是，梯度只能用于计算<strong>浮点张量</strong>。</p><p>Autograd的原理是创建一个<strong>反向图</strong>，跟踪应用于他们的每个操作，使用所谓的<strong>动态计算图</strong>（DCG）计算梯度。这个图的叶节点是输入张量，根节点是输出张量。梯度是通过跟踪<strong>从根到叶</strong>的图形，并使用<strong>链式法则</strong>将每个梯度相乘来计算的。</p><h3 id="梯度反向传播">梯度反向传播</h3><p>神经网络可以理解为一个「<strong>精心调整以输出所需结果的复合数学函数</strong>」，而调整就是通过「反向传播」完成的。反向传播是用来计算<strong>损失函数的梯度</strong>到<strong>输入参数的梯度</strong>的整个过程，以便以后更新权值，最终减少损失。</p><p>现在这个复合数学函数用一张<strong>反向图</strong>表示，这个数学函数（前向传播）最终的输出值（Loss），可以理解为反向图的<strong>根节点</strong>，而前面一层层的网络（<code>weight</code>和<code>bias</code>）就是<strong>叶节点</strong>。在我们搭建网络进行前向传播时，反向图就已经创建，并且记录了每个节点之间用来什么操作来连接。</p><p>在我们对<strong>根节点</strong> <code>z</code> 使用<code>z.backward()</code>时，<strong>沿着反向图</strong>，计算每个<strong>叶节点</strong>关于<code>z</code>的微分。这里之所以沿着反向图，就是利用了梯度链式法则。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>], [-<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]], requires_grad=<span class="hljs-literal">True</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>z = x.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>()<br><span class="hljs-meta">&gt;&gt;&gt; </span>z.backward()<br><span class="hljs-meta">&gt;&gt;&gt; </span>x.grad<br><span class="hljs-comment"># tensor([[ 2.,  0.],</span><br><span class="hljs-comment">#         [-2.,  2.]])</span><br></code></pre></td></tr></table></figure><p>在上面例子中，我们构建的反向图： <span class="math display">\[x=\left[ \begin{matrix}    1&amp;      0\\    -1&amp;     1\\\end{matrix} \right] \quad\quad z=\sum_i{\sum_j{x_{i,j}^{2}}}\]</span> 反向传播的过程： <span class="math display">\[\frac{\partial z}{\partial x_{i,j}}=2x_{i,j}\quad\quad \frac{\partialz}{\partial x}=\left[ \begin{matrix}    2&amp;      0\\    -2&amp;     2\\\end{matrix} \right]\]</span></p><h3 id="动态计算图">动态计算图</h3><p>首先了解这个反向图中每个叶节点 <code>x</code> 的构成：</p><ul><li><code>data</code>：该叶节点持有的数据，就是张量本身。</li><li><code>requires_grad</code>：如果为 True，则从 <code>x</code><strong>开始跟踪</strong>所有的操作，形成一个用于 <code>x</code>梯度计算的向后图。</li><li><code>grad</code>：保存梯度值，当调用 <code>out.backward()</code>时，这里的值就是 <span class="math inline">\(\partial out/\partialx\)</span>。</li><li><code>grad_fn</code>：用来计算梯度的<strong>后向函数</strong>，PyTorch自动推导，打印张量可以看到。</li><li><code>is_leaf</code>：当显式创建一个张量时，该值为True；当张量被显式赋值时，该值也为 True。</li></ul><p>在调用 <code>backward()</code> 时，只计算 <code>requires_grad</code>和 <code>is_leaf</code> <strong>同时为真</strong>的节点的梯度。</p><blockquote><p>PyTorch 是如何实现这个计算过程的呢？其实在调用<code>z.backward()</code>函数时，会有一个<strong>外部梯度</strong>隐式参与：<code>z.backward(torch.tensor(1.0))</code>，这个值作为<code>z.grad</code> 沿着反向图的每一个节点，计算<code>x.grad = grad_fn(z.grad)</code>，以此类推，直到遍历完所有叶节点。</p><p>当输出不是一个标量的时候，这个外部梯度必须<strong>显式指定</strong>，且形状和<code>z</code>相同，例如初始化为全一张量：<code>z.backward(torch.ones_like(z)</code>。</p></blockquote><p>这些数值会一直保留在张量中，如果反向图有变化，需要重新反向传播，此时需要先用<strong>原地操作</strong>的<code>x.grad.zero_()</code>将梯度清零，否则会<strong>自动累计</strong>上一次的值。在神经网络中也会用类似的<code>optimizer.zero_grad()</code>。</p>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>PyTorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>PyTorch</tag>
      
      <tag>DL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>复旦 NLP-LI 课题组近期论文索引</title>
    <link href="/FudanNLP-LI-Bib.html"/>
    <url>/FudanNLP-LI-Bib.html</url>
    
    <content type="html"><![CDATA[<p>本文分类索引 <a href="https://nlp.fudan.edu.cn/">FudanNLP</a>复旦大学自然语言处理课题组 <a href="http://qizhang.info/">张奇</a>老师和 <a href="https://guitaowufeng.github.io/">桂韬</a>老师的近期论文，通过梳理来尝试找出自己的研究兴趣。由于基础知识储备不足，恐挂一漏万，本文仅供个人娱乐。</p><h2 id="高效性-efficient-nlp">高效性 | Efficient NLP</h2><h3 id="插件微调-plug-and-play">插件微调 | Plug-and-play</h3><ul><li>Making <strong>Parameter-efficient Tuning</strong> More Efficient: AUnified Framework for Classification Tasks<ul><li>Coling 2022 长文Oral，周鑫，马若恬，邹易澄，陈炫婷，谢睿，武威，桂韬，张奇，黄萱菁</li><li>动机：PLM的高效调参（只更新少量任务特有参数）的做法忽略了由<strong>任务特有的输出层</strong>（不同任务不同分类器）引起的参数低效问题。</li><li>模型：插件微调（plugin-tuning），将不同分类任务的标签空间映射到同一个词表空间（<strong>同义替换</strong>），从而使用一个统一分类器，节省了参数量。</li></ul></li><li>PlugAT: A <strong>Plug and Play</strong> Module to Defend againstTextual Adversarial Attack<ul><li>Coling 2022长文，郑锐，包容，刘勤，桂韬，张奇，黄萱菁，谢睿，武威</li><li>动机：对抗训练有效提升模型的鲁棒性，但是需要<strong>更新所有参数</strong>，且<strong>从零开始</strong>训练。</li><li>PlugAT模型：保持原始模型参数冻结，向模型输入和注意力层中<strong>注入可学习的参数</strong>+基于<strong>遗忘约束机制</strong>的对抗训练方法（防止对原任务造成副作用，过滤掉损伤原始性能的不良对抗样本）。</li></ul></li><li>Plug-Tagger: A <strong>Pluggable</strong> Sequence LabelingFramework Using Language Models<ul><li>AAAI 2021 长文，周鑫，马若恬，桂韬，谈一丁，张奇，黄萱菁</li><li>动机：插件用于将预训练模型快速部署到不同任务上，目前的<strong>前缀插件</strong>在不同的<strong>文本生成</strong>任务上表现很好，但是在序列标注这个<strong>分类任务</strong>上却失效，原因是不同的数据集<strong>需要不同的标签，需要训练不同的分类器</strong>。</li><li>模型：将序列标注<strong>当成生成任务</strong>来做 +用一个<strong>高频标签代替</strong>每个类别 +<strong>任务专有前缀</strong>插件</li></ul></li></ul><h3 id="提示学习-prompt-tuning">提示学习 | Prompt Tuning</h3><ul><li><strong>Template-free</strong> Prompt Tuning for <strong>Few-shotNER</strong><ul><li>NAACL 2022长文，马若恬，周鑫，桂韬，谈一丁，李林阳，张奇，黄萱菁</li><li>动机：Prompt 在<strong>句子级</strong> few-shot任务上表现良好，精巧的模版设计和 label word 选择功不可没；但在 NER等<strong>词级别</strong>任务上，构造模版的方式遍历所有位置是非常费时的。</li><li>EntLM 模型：去除模板构造步骤，但保留 PLMs的<strong>词预测范式</strong>（在实体位置预测类别相关的 label word +在非实体位置预测原词）+ <strong>自动化选取</strong>合适 label word +利用 prompt 语言模型<strong>减少预训练和下游任务的隔阂</strong></li></ul></li></ul><h3 id="彩票网络-lottery-ticket">彩票网络 | Lottery Ticket</h3><ul><li><strong>Efficient</strong> Adversarial Training with RobustEarly-Bird Tickets<ul><li>EMNLP 2022 主会，奚志恒*，郑锐*，桂韬，张奇，黄萱菁</li><li>动机：对抗训练可以提升鲁棒性，但是需要通过<strong>投影梯度下降生成对抗样本</strong>，比传统的微调更昂贵。本文发现在对抗训练的<strong>早期阶段</strong>，（通常是0.15~0.3个epochs，<strong>远在参数收敛之前</strong>），鲁棒的网络连接模式就浮现了。</li><li>鲁棒早鸟彩票网络：在对抗训练<strong>早期阶段寻找</strong>具有<strong>结构化稀疏度</strong>的鲁棒彩票（彩票收敛指标，帮助尽早搜索到彩票）+<strong>剩余时间内</strong>对鲁棒彩票进行微调</li></ul></li><li><strong>Robust Lottery Tickets</strong> for Pre-trained LanguageModels<ul><li>ACL 2022长文，郑锐，包容，周钰皓，梁迪，王思睿，武威，桂韬，张奇，黄萱菁</li><li>动机：彩票网络（PLMs中<strong>媲美原始网络性能的子网络</strong>）在遭受<strong>对抗攻击</strong>时，表现出了比原始网络更差的鲁棒性。</li><li>模型：可学习权重掩码（识别<strong>鲁棒</strong>彩票网络），使用concrete distribution 对掩码建模（解决二值掩码带来的离散优化问题）+ L0范数平滑近似（促进掩码稀疏度）+ 对抗损失目标（确保准确性和鲁棒性）</li></ul></li></ul><h2 id="可靠性-reliable-nlp">可靠性 | Reliable NLP</h2><h3 id="数据集偏差-bias">数据集偏差 | Bias</h3><ul><li>Kernel-Whitening: Overcome Dataset Bias with <strong>IsotropicSentence Embedding</strong><ul><li>EMNLP 2022 主会，高颂杨，窦士涵，张奇，黄萱菁</li><li>动机：目前主流解决数据集偏差的方案是设计一个<strong>表层模型来预先识别有偏差</strong>的数据，但是<strong>两阶段</strong>方法增加了训练复杂度，在减轻偏见的同时导致了<strong>有效信息的衰减</strong>。</li><li>核白化：表示标准化（消除编码句子的特征之间的相关性，通过提供各向同性的数据分布来消除偏见问题）</li></ul></li><li>Decorrelate Irrelevant, Purify Relevant: Overcome <strong>TextualSpurious Correlations</strong> from a Feature Perspective<ul><li>Coling 2022 长文Oral，窦士涵，郑锐，伍婷，高颂杨，单俊杰，张奇，吴月明，黄萱菁</li><li>动机：NLU模型往往依赖伪相关性（数据集偏差），导致分布外数据集性能差。现有的去偏方法大多用<strong>有偏的特征</strong>来识别并尽可能忽略这些<strong>有偏样本</strong>，然而却阻碍了模型从这些样本的<strong>非偏部分</strong>学习。</li><li>模型：随机傅里叶特征和加权重采样（去除特征之间的依赖关系，减轻伪相关性）+互信息提纯特征（学习与任务更相关的特征）。</li></ul></li><li>Less is Better: Recovering Intended-<strong>FeatureSubspace</strong> to <strong>Robustify NLU</strong> Models<ul><li>Coling 2022 长文，伍婷，桂韬</li><li>动机：现有的数据集去偏方法过度依赖于已知的偏差类型及属性（<strong>有监督</strong>），而不能捕获潜在的未知偏差。</li><li>RISK 模型：不直接去除偏差导致的shortcuts（将其视为<strong>冗余特征</strong>输入）+将高维特征空间转换至<strong>低维流形</strong>上（使用一个 AutoEncoder得到一个<strong>只包含有用特征</strong>的子空间）</li></ul></li><li>TextFlint: <strong>Unified</strong> Multilingual <strong>RobustnessEvaluation Toolkit</strong> for Natural Language Processing<ul><li>ACL-IJCNLP 2021 System Demo，王枭，刘勤，桂韬，张奇</li><li>文本试金石（多语言鲁棒性检测工具）：用于<strong>数据集增强</strong>，包含多种文本转换、对抗样本生成、人群样本生成等，同时生成分析报告。在<strong>该工具生成的数据集上训练模型</strong>可以增强鲁棒性。</li></ul></li></ul><h3 id="对抗鲁棒性-robustness">对抗鲁棒性 | Robustness</h3><ul><li>Flooding-X: Improving BERT's <strong>Resistance to AdversarialAttacks</strong> via Loss-Restricted Fine-Tuning<ul><li>ACL 2022长文，刘勤，郑锐，包容，刘婧漪，刘志华，程战战，乔梁，桂韬，张奇，黄萱菁</li><li>动机：对抗学习增强鲁棒性的方法是<strong>为每个输入样本</strong>生成对抗扰动，复杂性随着对抗样本所需要的<strong>梯度计算次数</strong>成倍上升</li><li>Flooding模型：一种低成本的防御方法，但是依赖于<strong>参数的选择</strong></li><li>Flooding-X模型：<strong>不需要生成额外的对抗扰动</strong>来训练模型，其时间消耗近似于模型微调</li></ul></li><li>Searching for an Effective Defender: Benchmarking<strong>Defense</strong> against <strong>Adversarial WordSubstitution</strong><ul><li>EMNLP 2021，Zongyi Li，徐健涵，JiehangZeng，李林阳，郑骁庆，张奇</li><li>动机：NLP任务中今进行<strong>词替换</strong>是常见的<strong>对抗攻击</strong>方法，但是现有方法缺少一个在相同条件的<strong>横向对比</strong>。</li><li>FreeLB++ 模型：在已有的 FreeLB 模型上改动，用 L2范数扩大搜索范围，找到<strong>最易受攻击影响</strong>的点</li></ul></li></ul><h3 id="隐私保护-privacy">隐私保护 | Privacy</h3><ul><li>TextFusion: <strong>Privacy-Preserving</strong> Pre-trained ModelInference via <strong>Token Fusion</strong><ul><li>EMNLP 2022 主会，周鑫，陆劲竹，桂韬，马若恬，费子楚，王宇然，丁勇，张轶博，张奇，黄萱菁</li><li>动机：预训练云服务允许缺乏计算资源的用户将数据上传到云端完成推理，但纯文本可能包含私人信息，因此用户更愿意<strong>在本地进行部分计算得到中间表示</strong>后上传，但研究表明，<strong>中间表示也容易被还原为纯文本</strong>。</li><li>TextFusion：融合预测器（将多个可能含有隐私的词表示<strong>动态地融合为一个难以识别的词表示</strong>）+误导性的训练方案（用词向量空间中相近的词替换，使这些表示进一步被干扰）</li></ul></li></ul><h2 id="信息抽取-information-extraction">信息抽取 | InformationExtraction</h2><h3 id="事件抽取-event-argument-extraction">事件抽取 | Event ArgumentExtraction</h3><ul><li>A Multi-Format <strong>Transfer Learning</strong> Model for EventArgument Extraction via Variational Information Bottleneck<ul><li>Coling 2022 长文 Oral，周杰，张奇，陈琴，贺梁，黄萱菁</li><li>动机：事件抽取往往需要<strong>对特定数据集设计特定事件框架</strong>，但这些框架难以迁移到新场景中。而新场景的重新标注往往太过复杂。</li><li>EAE模型：共享-特定的提示框架（从包含不同形式的数据集学习<strong>形式特定和形式共享</strong>的知识）+变分信息瓶颈（保留共享的知识，<strong>遗忘无关的知识</strong>）</li></ul></li></ul><h3 id="论辩学习-argument">论辩学习 | Argument</h3><ul><li>A <strong>Structure-Aware</strong> Argument Encoder for<strong>Literature Discourse</strong> Analysis<ul><li>Coling 2022短文，李寅子，陈伟，魏忠钰，黄煜俊，王楚珺，王思远，张奇，黄萱菁，吴力波</li><li>动机：论点表示学习倾向于<strong>平等对待</strong>句子中的词元（token），而忽略了形成<strong>论辩语境的隐含结构</strong>；特别是在科学文献中含有大量术语没有得到关注。</li><li>模型：将 token 分为框架词和主题词 +<strong>论点注意力机制</strong>建模（对 token间的交互建模<strong>结构信息</strong>）+考虑段落级的位置信息来学习论点的高级结构。</li></ul></li><li><strong>Discrete</strong> Argument Representation Learning forInteractive <strong>Argument Pair Identification</strong><ul><li>NAACL-HLT 2021，Lu Ji，魏忠钰，Jing Li，张奇，黄萱菁</li><li>动机：在双方辩论的过程中，通常会对一个主题进行<strong>不同角度的阐述</strong>，本文旨在找出同一角度的<strong>论辩对</strong>。</li><li>模型：获取论辩语言的<strong>离散</strong>表征 +层级结构建模（融合<strong>上下文</strong>知识）</li></ul></li></ul><h3 id="命名实体识别-ner">命名实体识别 | NER</h3><ul><li>MINER: Improving <strong>Out-of-Vocabulary</strong> Named EntityRecognition from an Information Theoretic Perspective<ul><li>ACL 2022长文，王枭，窦士涵，熊立茂，邹易澄，张奇，桂韬，乔梁，程战战，黄萱菁</li><li>动机：过去的 NER 方法过度依赖实体词本身的信息，以至于对<strong>OOV</strong>的识别很差，现实中的实体词往往呈<strong>长尾分布</strong>，意味着效果不好。</li><li>模型：基于互信息的训练目标（泛化信息最大化 +多余信息最小化），强化上下文，防止过度关注实体本身。</li></ul></li><li>Searching for Optimal <strong>Subword</strong> Tokenization in<strong>Cross-domain NER</strong><ul><li>IJCAI 2022 长文，马若恬，谈一丁，周鑫，陈炫婷，桂韬，张奇</li><li>动机：无监督领域适应（UDA）任务常通过<strong>域无关表示学习</strong>完成，但对于NER这种<strong>词级别</strong>的任务，更需要对<strong>实体词进行迁移和对齐</strong>，应该把<strong>域无关表征放到subword 上</strong>。</li><li>X-Piece 模型：将实体词重新 tokenize 成subword，使<strong>源域和目标域尽量靠近</strong>，当成最优化问题解决。</li></ul></li></ul><h3 id="序列标注-sequence-labeling">序列标注 | Sequence Labeling</h3><ul><li><strong>Uncertainty-Aware</strong> Sequence Labeling<ul><li>TASLP 2022，叶佳成，周翔，郑骁庆，桂韬，张奇</li><li>动机：CRF用于序列标注无法同时建模<strong>局部和全局依赖</strong>。</li><li>两阶段框架：先生成 draft标签，再使用<strong>双流自注意力模型</strong>修改（基于长距离标签依赖）+ <strong>贝叶斯网络</strong>查找可能出错的 draft（减少错误 draft的边际效应）</li></ul></li><li><strong>Larger-Context</strong> Tagging: When and Why Does It Work?<ul><li>NAACL 2021，Jinlan Fu，Liangjing Feng，张奇，黄萱菁</li><li>动机：<strong>句子级序列标注</strong>任务十分热门，但很少人关注更多的上下文信息是否有用，例如<strong>文档级序列标注</strong>。本文在多个任务和数据集上进行了验证，并提出了一种<strong>属性辅助评估方法</strong>。</li></ul></li></ul><h3 id="社交媒体-social-media">社交媒体 | Social Media</h3><ul><li>A Progressive Framework for Role-Aware <strong>Rumor</strong>Resolution<ul><li>Coling 2022长文，陈蕾，李冠颖，魏忠钰，杨洋，周葆华，张奇，黄萱菁</li><li>新任务：引爆点识别，识别出在对谣言传播具有推动作用或对谣言判别有指示作用的消息。</li><li>模型：非对称的图循环神经网络（建模信息传播树）+渐进式预测（引爆点识别、角色感知、谣言判别）</li></ul></li></ul><h3 id="关键短语生成-keyphrase-generation">关键短语生成 | KeyphraseGeneration</h3><ul><li>Searching Effective Transformer for <strong>Seq2Seq KeyphraseGeneration</strong><ul><li>NLPCC 2021 长文，Yige Xu，Yichao Luo，邹易澄，ZhengyanLi，张奇，邱锡鹏，黄萱菁</li><li>动机：Transformer 在 KG 任务上表现不如传统的 RNN模型，本文探究其原因并提出「<strong>信息稀疏假说</strong>」，设计实验并验证其正确性。主要原因是KG 任务中<strong>关键信息过于稀疏</strong>，而原始的 Transformer由于全连接注意力，容易<strong>过度关注</strong> local context。</li><li>模型：限制自注意力机制 + 引入导向信息。</li></ul></li><li>Keyphrase Generation with <strong>Fine-GrainedEvaluation</strong>-Guided <strong>Reinforcement Learning</strong><ul><li>EMNLP 2021 长文 Finding，Yichao Luo，YigeXu，叶佳成，邱锡鹏，张奇</li><li>动机：过去的 KG 评价指标通常采用 F1得分进行<strong>短语级的精确匹配</strong>，但忽略了<strong>语义上可能正确</strong>的答案。</li><li>模型：<strong>细粒度评价指标</strong>（词级 F1、编辑距离等）+原始的短语级 F1 <strong>共同作为</strong> RL 的 reward</li></ul></li><li>One2Set: Generating <strong>Diverse Keyphrases as a Set</strong><ul><li>ACL-IJCNLP 2021 长文，叶佳成，桂韬，Yichao Luo，Yige Xu，张奇</li><li>动机：现有的 Seq2seq模型会将关键短语<strong>拼接成序列</strong>作为目标序列训练，但却忽略了关键短语本身是<strong>无序的</strong>。</li><li>One2Set模型：一组可学习控制编码（生成<strong>并行、无序的</strong>关键短语）+ K步目标指派（双向匹配预测值与目标值，减少预测结果中的<strong>重复值</strong>，增加多样性）</li></ul></li><li><strong>Heterogeneous Graph Neural Networks</strong> for KeyphraseGeneration<ul><li>EMNLP 2021 长文，叶佳成，Ruijian Cai，桂韬，张奇</li><li>动机：KG任务需要预测文档中<strong>出现的或未出现的</strong>关键短语，而对于<strong>未出现词</strong>往往生成的不可控、不准确。</li><li>模型：先检索和源文档<strong>相似的文档-短语对</strong>，放入<strong>多级图模型</strong>抽取<strong>不同粒度</strong>的关系，解码器使用<strong>多级注意力</strong>+ <strong>拷贝机制</strong>（直接拷贝相似文档中合适的答案）</li></ul></li></ul><h3 id="实体关系抽取-relation-extraction">实体关系抽取 | RelationExtraction</h3><ul><li>A Relation-Oriented Clustering Method for <strong>Open RelationExtraction</strong><ul><li>EMNLP 2021 长文，赵君，桂韬，张奇，周雅倩</li><li>动机：基于聚类的无监督方法成为开放域关系抽取的重要方法，但是向量空间中的距离并不能完全代表关系语义相似性（对齐），会导致关系分类错误。</li><li>RoCORE：将实例的向量表示聚集到关系质心，最小化具有相同关系的实例之间的距离。</li></ul></li><li>SENT: <strong>Sentence-level Distant</strong> Relation Extractionvia <strong>Negative Training</strong><ul><li>ACL-IJCNLP 2021长文，马若恬，桂韬，李林阳，张奇，黄萱菁，周雅倩</li><li>动机：实体关系抽取通常采用 bag labels 形式，即对<strong>匹配的sentence bag</strong>在<strong>标签池</strong>中选择标签，但是会引入很多噪声（<strong>多标签噪声、没有正确标签噪声</strong>）。</li><li>NegativeTraining：用互补标签表示「<strong>该对象不属于此类标签</strong>」来降低噪声。</li><li>SENT框架：对噪声数据进行互补标签构建，并将其转化为可利用的数据。</li></ul></li></ul><h2 id="文本生成-text-generation">文本生成 | Text Generation</h2><h3 id="多跳问答-multi-hop-qa">多跳问答 | Multi-hop QA</h3><ul><li>Locate Then Ask: Interpretable <strong>Stepwise</strong> Reasoningfor <strong>Multi-hop Question Answering</strong><ul><li>Coling 2022 长文，王思远，魏忠钰，范智昊，张奇，黄萱菁</li><li>动机：多跳推理需要聚合多个文档来回答复杂问题，过去通常将其分别为多个单跳问题来解答，但是每个单跳推理步骤缺乏支持事实，可能导致<strong>不可解释的错误分解</strong>。</li><li>模型：每个中间步骤 = 单跳支持句识别 +单跳问题生成，采用<strong>统一阅读器模型</strong>进行中间跳推理和最终跳推理。</li></ul></li><li>CQG: A Simple and Effective Controlled Generation Framework for<strong>Multi-hop Question Generation</strong><ul><li>ACL 2022 长文，费子楚，张奇，桂韬，梁迪，王思睿，武威，黄萱菁</li><li>动机：现有的生成模型已经能够生成与答案相对应的正确问题，但无法保证<strong>生成问题的复杂性</strong>（浅层问题）。</li><li>CQG 模型：多跳推理链中关键实体的多跳问题 + 基于 Transformer的可控解码器</li></ul></li><li><strong>Iterative GNN-based Decoder</strong> for Question Generation<ul><li>EMNLP 2021，费子楚，张奇，周雅倩</li><li>动机：QG任务旨在从一个文档中生成问答对，过去的解码器忽略了<strong>先前生成文本</strong>中的<strong>结构信息</strong>，且忽略了文本中<strong>重复出现的词</strong>的影响。因此，要将前面生成的词在后续生成中<strong>充当辅助信息</strong>才行。</li><li>IGND：每一个解码步骤都使用图神经网络（更能<strong>捕获段落中的依赖关系</strong>）对先前的文本建模</li></ul></li></ul><h3 id="数学问题-math-word-problem">数学问题 | Math Word Problem</h3><ul><li>Automatic Math Word Problem <strong>Generation</strong> WithTopic-Expression <strong>Co-Attention</strong> Mechanism and<strong>Reinforcement Learning</strong><ul><li>TASLP 2022，Qinzhuo Wu，张奇，黄萱菁</li><li>动机：给定主题和表达式生成对应的可解数学问题，常规的生成方法容易导致<strong>主题无关或者问题不可解</strong>。</li><li>MWPGen模型：<strong>主题-表达式</strong>注意力机制（抽取二者的相关信息）+<strong>强化学习</strong>（以问题的标答为 reward）</li></ul></li><li>An Edge-Enhanced <strong>Hierarchical</strong><strong>Graph-to-Tree</strong> Network for Math Word Problem Solving<ul><li>EMNLP 2021，Qinzhuo Wu，张奇，魏忠钰</li><li>动机：利用图神经网络解决数学问题是，可能没有考虑到<strong>图中的边缘标签信息</strong>和跨句子的<strong>远程词</strong>关系，只关注和当前词最相关的区域。</li><li>EEH-G2T：句子级聚合（边缘增强，聚合边缘标签信息）+问题级聚合（学习远程词）+ 树结构（注意到问题的不同部分）</li></ul></li><li>Math Word Problem <strong>Solving</strong> with Explicit<strong>Numerical Values</strong><ul><li>ACL-IJCNLP 2021，Qinzhuo Wu，张奇，魏忠钰，黄萱菁</li><li>动机：现有的数学问题解决模型，都只将<strong>数值看作数字符号</strong>，而有时候数值类型也会<strong>决定表达式</strong>。</li><li>NumS2T 模型：用<strong>序列到树模型</strong>表示数值 +数值属性预测机制（预测<strong>数值类型</strong>及其<strong>在表达式中的地位</strong>）</li></ul></li><li>A Knowledge-Aware Sequence-to-Tree Network for Math Word ProblemSolving<ul><li>EMNLP 2020，Qinzhuo Wu，张奇，Jinlan Fu，黄萱菁</li></ul></li></ul><h3 id="对话系统-dialogue-systems">对话系统 | Dialogue Systems</h3><ul><li>Thinking Clearly, Talking Fast: <strong>Concept-Guided</strong><strong>Non-Autoregressive Generation</strong> for Open-Domain DialogueSystems<ul><li>EMNLP 2021 长文，邹易澄，刘志华，胡星武，张奇</li><li>动机：现有的 seq2seq对话系统难以掌握<strong>同类概念迁移</strong>，且无法在回答中引入<strong>多个关联的概念</strong>。</li><li>CG-nAR模型：多概念决策模块（从<strong>概念图</strong>模型中找出相关概念）+<strong>插入式Transformer</strong>（插入一个概念后非自回归地<strong>补全</strong>回答）</li></ul></li><li><strong>Topic-Oriented</strong> Spoken Dialogue Summarization forCustomer Service with <strong>Saliency-Aware</strong> Topic Modeling<ul><li>AAAI 2021 长文，邹易澄，Minlong Peng，张奇，黄萱菁</li><li>动机：对话摘要任务中，大量<strong>噪声和共同语义</strong>导致潜在信息难以发掘，普通的<strong>主题模型</strong>难以应用。在客服对话中，还需要根据顾客特定信息进行回复。</li><li>模型：<strong>两阶段</strong>对话摘要（TDS）+<strong>显著性感知</strong>神经主题模型（SAMT，减少无关噪声的影响）</li></ul></li><li><strong>Unsupervised Summarization</strong> for <strong>ChatLogs</strong> with Topic-Oriented Ranking and<strong>Context-Aware</strong> Auto-Encoders<ul><li>AAAI 2021 长文，邹易澄，张奇，黄萱菁</li><li>动机：不同于传统文本摘要，聊天记录中含有<strong>碎片化、递进的主题</strong>，和大量<strong>晦涩难懂的句子</strong>。</li><li>RankAE模型：面向主题的排序策略（筛选具有<strong>中心性和多样性</strong>的主题言论）+去噪自编码器（根据选中的言论生成摘要）</li></ul></li><li><strong>Low-Resource Dialogue Summarization</strong> with<strong>Domain-Agnostic Multi-Source</strong> Pretraining<ul><li>EMNLP 2021 长文，邹易澄，朱柏霖，胡星武，桂韬，张奇</li><li>动机：对话摘要的标注数据过少，现有的<strong>低资源</strong>方法都在<strong>其他领域预训练模型后微调</strong>，但他们都忽略了对话和传统问答的区别。</li><li>模型：多资源对抗学习（<strong>域不可知</strong>的摘要能力）+用大规模领域内数据<strong>分别训练</strong>对话<strong>编码器</strong>和摘要<strong>解码器</strong></li></ul></li></ul><h3 id="知识图谱答案生成-kg">知识图谱答案生成 | KG</h3><ul><li>LFKQG: A Controlled Generation Framework with Local Fine-tuning for<strong>Question Generation</strong> over <strong>KnowledgeBases</strong><ul><li>Coling 2022 长文，费子楚，周鑫，张奇，桂韬，黄萱菁</li><li>动机：KBQG基于知识图谱子图中的<strong>指定答案实体</strong>生成问题，但只关注与答案实体最相关的部分，忽略了其他部分；且KBQG 无法处理 OOV 谓词。</li><li>LFKQG 模型：受控生成方法（确保问题和整个子图相关）+ 局部微调（利用PLM 适应 OOV 的能力）</li></ul></li></ul><h3 id="证明生成-proof-generation">证明生成 | Proof Generation</h3><ul><li>ProofInfer: Generating Proof via <strong>Iterative HierarchicalInference</strong><ul><li>EMNLP 2022 主会，费子楚，张奇，周鑫，桂韬，黄萱菁</li><li>任务描述：给定一系列自然语言表达的 Facts 和 Rules，构建证明树证明Hypothesis。</li><li>动机：过去的模型采用逐步将<strong>单个节点链接到结论</strong>，生成几个证明路径再合并，而不是整个树。</li><li>ProofInfer：迭代层次推理生成整个树（每一步生成一层 +采用<strong>文本到文本范式来预测一层</strong>的多个节点） +分治（将证明树<strong>递归编码</strong>为纯文本，而不会丢失结构信息）</li></ul></li></ul><h2 id="文本匹配-text-matching">文本匹配 | Text Matching</h2><h3 id="文本语义匹配-text-semantic-matching">文本语义匹配 | TextSemantic Matching</h3><ul><li>Divide and Conquer: Text <strong>Semantic Matching</strong> withDisentangled <strong>Keywords and Intents</strong><ul><li>ACL 2022长文，邹易澄，刘宏伟，桂韬，王浚哲，张奇，唐萌，李海翔，Daniel Wang</li><li>动机：基于 PLMs的文本匹配方法通过处理<strong>句子中的单词</strong>来进行文本内容匹配，但是待匹配的句子通常包含<strong>不同匹配粒度</strong>的内容：关键词（需要<strong>严格匹配</strong>的事实）+意图（可以通过<strong>多种表述</strong>传达）</li><li>DC-Match 策略：将关键词和意图分离（Mask互补）分而治之，当关键词和意图均匹配才匹配句子。</li></ul></li></ul><h3 id="跨语言语义匹配-cross-linguistic">跨语言语义匹配 |Cross-Linguistic</h3><ul><li><strong>Cross-Linguistic Syntactic Difference</strong> inMultilingual BERT: How Good is It and How Does It Affect Transfer?<ul><li>EMNLP 2022主会，徐凝雨，桂韬，马若恬，张奇，叶婧婷，张梦翰，黄萱菁</li><li>动机：多语言 BERT拥有较强跨语言句法能力，在<strong>某些语言之间</strong>能够有效地进行句法知识的<strong>零样本跨语言</strong>迁移。</li><li>实验：对 24 种类型迥异的语言，研究了 mBERT得到的<strong>依存句法关系表示的分布</strong>，证明了不同语言分布之间的距离与<strong>语言间的形式句法差异</strong>高度一致。因此，可以在零样本时根据<strong>不同语言的形态句法属性</strong>来选择最佳的迁移<strong>源语言</strong>。</li></ul></li></ul><h2 id="情感分析-sentiment-analysis">情感分析 | Sentiment Analysis</h2><h3 id="隐式情感分析-implicit">隐式情感分析 | Implicit</h3><ul><li><strong>Causal intervention</strong> improves<strong>implicit</strong> sentiment analysis<ul><li>Coling 2022长文，王思尹，周杰，孙长志，叶俊杰，桂韬，张奇，黄萱菁</li><li>动机：现有的模型依赖<strong>伪相关性</strong>（只关注明确的情感词），导致隐式情感分析存在困难。</li><li>ISAIV模型：引入<strong>工具变量</strong>来消除混杂因果效应（随机扰动句子）+两阶段训练</li></ul></li><li>Learning Implicit Sentiment in <strong>Aspect-based SentimentAnalysis</strong> with <strong>Supervised ContrastivePre-Training</strong><ul><li>EMNLP 2021，Zhengyan Li，邹易澄，Chong Zhang，张奇，魏忠钰</li><li>动机：ABSA 中也具有隐式情感问题（意见词的表意不明显）。</li><li>SCAPT：将隐式情感词和具有相同情感的显式情感词对齐，进行预训练，使得模型可以捕捉隐式情感</li></ul></li></ul><h2 id="多模态-multi-model">多模态 | Multi-Model</h2><h3 id="文档实体识别-doc-ner">文档实体识别 | Doc NER</h3><ul><li>Read Extensively, Focus Smartly: A <strong>Cross-document</strong>Semantic Enhancement Method for Visual <strong>Documents NER</strong><ul><li>Coling 2022 长文Oral，赵君，赵鑫，詹文煜，桂韬，张奇，乔梁，程战战，蒲世良</li><li>动机：过去的<strong>富视觉文档实体识别</strong>往往将注意力分配在文档内部的不相关区域，忽略了相关文档蕴含的信息。</li><li>模型：注意力掩码机制（减少当前文档不相关区域的关注）+跨文档感知（从相关文档收集信息辅助预测）。</li></ul></li></ul><h3 id="博客标签推荐-hashtag-rec">博客标签推荐 | Hashtag Rec</h3><ul><li><strong>Co-Attention</strong> Memory Network for<strong>Multimodal</strong> Microblog's Hashtag Recommendation<ul><li>TKDE 2021 长文，Renfeng Ma，邱锡鹏，张奇，Xiangkun Hu, Yu-GangJiang，黄萱菁</li><li>动机：标签推荐经常被视为<strong>多标签分类</strong>任务，但事实上用户总时在<strong>创造新标签</strong>，固定的标签集效果不好。</li><li>模型：将其视为<strong>匹配任务</strong>，互注意力机制（学习<strong>多模态</strong>博客内容，并从记录中抽取词汇作为标签）</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>自然语言处理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #13 树</title>
    <link href="/LeetCode-Tree.html"/>
    <url>/LeetCode-Tree.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「树」类型题中的：前中后序遍历、层序遍历、树高和树深、树上路径问题、二叉搜索树、N叉树等。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><p>树的定义天然具有递归的性质，因此许多树的问题都可以用递归方法来解决。</p><p>这里提到的「树」大多以指针的形式存储，如果要用数组存储也可以。</p><h2 id="前中后序遍历">前中后序遍历</h2><h3 id="二叉树的前序遍历-e">144. 二叉树的前序遍历 (E)</h3><p><ahref="https://leetcode.cn/problems/binary-tree-preorder-traversal/">题目描述</a>：给定二叉树的根节点<code>root</code>，返回它节点值的<strong>前序</strong>遍历。</p><p><strong>方法1</strong>：递归，输出中间节点，再递归遍历<strong>左子树、右子树</strong>。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度平均 <spanclass="math inline">\(O(\log n)\)</span>，最坏情况下（链状）为 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：迭代，模拟栈，根节点入栈，当栈不为空时弹出栈顶元素，再压入<strong>右节点、左节点</strong>。复杂度同上。</p><p><strong>方法3</strong>：Morris遍历，<strong>利用树的大量空指针实现空间开销的极限缩减</strong>。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度平均 <spanclass="math inline">\(O(1)\)</span>。</p><blockquote><p>构建 Morris树：以某个根节点开始，找到它<strong>左子树的最右侧节点</strong>之后与这该根节点进行连接（用末尾的空指针指向根节点）。</p><p>遍历 Morris 树：用一个 <code>cur</code> 指针，从根节点开始按照「左... 左 + 右 ... 右」的顺序遍历，即可回到根节点，并访问完整个左子树。然后<code>cur</code> 向右走一步，重复以上操作，直到右指针为空。</p></blockquote><p><strong>坑点</strong>：如果需要将完整遍历结果放到一个向量中返回，因此函数传参时要<strong>传引用调用</strong>。</p><h3 id="二叉树的中序遍历-e">94. 二叉树的中序遍历 (E)</h3><p><ahref="https://leetcode.cn/problems/binary-tree-inorder-traversal/">题目描述</a>：给定二叉树的根节点<code>root</code>，返回它节点值的<strong>中序</strong>遍历。</p><p><strong>方法1</strong>：递归，时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度平均 <spanclass="math inline">\(O(\log n)\)</span>，最坏情况下（链状）为 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：迭代，模拟栈，但不能像前序、后序那样<strong>简单地将两个子节点都入栈</strong>，而是需要<strong>一路向左</strong>！</p><blockquote><p>一路向左：先入栈，<code>root</code>向左移，满足「<strong>左</strong>-中-右」顺序。</p><p>左移到空：说明到了最左边的尽头，此时可以弹出栈顶元素作为<code>root</code>，并打印，满足「左-<strong>中</strong>-右」顺序。</p><p>向右一步：<code>root</code>向右移，然后再入栈，满足「左-中-<strong>右</strong>」顺序。重复以上三步。</p><p>右移到空：说明这个<strong>左子树</strong>已经遍历结束，此时可以弹出栈顶元素<code>root</code>，并打印，满足「左-<strong>中</strong>-右」顺序。</p></blockquote><h3 id="从前序与中序遍历序列构造二叉树-m">105.从前序与中序遍历序列构造二叉树 (M)</h3><p><ahref="https://leetcode.cn/problems/construct-binary-tree-from-preorder-and-inorder-traversal/">题目描述</a>：给定两个整数数组<code>preorder</code> 和<code>inorder</code>，均<strong>无重复元素</strong>，构造二叉树并返回其根节点。</p><p><strong>方法1</strong>：递归，前序遍历的<strong>第一个元素为根节点</strong>，可以将中序遍历<strong>一分为二</strong>，分别得到左右子树的中序遍历长度，对应到前序遍历，得到<code>l1, r1, l2, r2</code> 四个下标继续递归。时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：递归 +哈希表，找根节点的过程由于<strong>无重复元素</strong>，可以先用哈希表记录每个元素在中序序列的下标，时间复杂度优化到<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：迭代，模拟递归的工作栈，过于复杂暂且不表。</p><h3 id="二叉树展开为链表-m">114. 二叉树展开为链表 (M)</h3><p><ahref="https://leetcode.cn/problems/flatten-binary-tree-to-linked-list/">题目描述</a>：给定二叉树的根节点，将其以<strong>先序遍历</strong>顺序展开为单链表，链表用<code>TreeNode</code> 实现，<strong>只使用右指针</strong>。</p><p><strong>方法1</strong>：递归，维护 <code>pre</code>表示「<strong>前驱节点</strong>」，每次递归时，先将根节点放入链表，而<code>pre</code> 更新为根节点。递归展开左子树，此时 <code>pre</code>会走到<strong>左子树的最右节点</strong>，也是右子树的前驱节点。接着递归展开右子树。总复杂度<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：迭代，维护 <code>cur</code>表示「<strong>当前节点</strong>」，对于一个没有左子树的点，可以直接跳过；否则，注意到左子树的最右节点就是右子树的前驱节点，<strong>直接将右子树拼接到此处</strong>，并将<strong>左子树移到</strong><code>cur</code> 的右子树，将 <code>cur</code>的左指针置空。继续迭代下一个节点。时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="二叉树的序列化与反序列化-h">297. 二叉树的序列化与反序列化(H)</h3><p><ahref="">题目描述</a>：实现两个方法，将一个二叉树<strong>序列化</strong>为<strong>连续的字符串</strong>结构，并采用相反方式<strong>反序列化</strong>得到二叉树。</p><p><strong>方法</strong>：DFS 先序遍历，序列化时将节点值输出为<code>val,</code> 将<strong>空指针</strong>输出为 <code>None,</code>逗号作为分隔符。采用<strong>先序递归</strong>即可。反序列化时先将字符串split为<strong>数组</strong>，再<strong>递归先序建树</strong>，可以直接顺序扫描。</p><h3 id="todo表达式计算">Todo表达式计算</h3><p>用中序遍历容易得到原始表达式，但计算机并不方便计算。如果用后序遍历，使用栈来处理表达式会变得更加容易。每遇到一个操作符，就可以从栈中弹出栈顶的两个元素，计算并将结果返回到栈中。</p><h2 id="遍历并修改">遍历并修改</h2><p>通过递归调用自身，可以简洁地实现整个过程，需要注意的是：<strong>修改的步骤可能要放在子树操作的前、中、后</strong>。</p><h3 id="对称二叉树-e">101. 对称二叉树 (E)</h3><p><ahref="https://leetcode.cn/problems/symmetric-tree/">题目描述</a>：给定一个二叉树的根节点<code>root</code> ， 检查它是否轴对称。</p><p><strong>方法</strong>：镜像遍历，DFS 传入 <code>root-&gt;left</code>和 <code>root-&gt;right</code>，子问题是判断 <code>left-&gt;left</code>和 <code>right-&gt;right</code> 是否镜像、判断<code>left-&gt;right</code> 和 <code>right-&gt;left</code>是否镜像。时间复杂度 <span class="math inline">\(O(n)\)</span>。</p><p><strong>坑点</strong>：进入 DFS后要判断空，只有<strong>两个指针都非空</strong>才向深处 DFS。</p><h3 id="翻转二叉树-e">226. 翻转二叉树 (E)</h3><p><ahref="https://leetcode.cn/problems/invert-binary-tree/">题目描述</a>：给定一棵二叉树的根节点<code>root</code>，<strong>镜像翻转</strong>这棵二叉树，并返回其根节点。</p><p><strong>方法</strong>：DFS先序遍历，先对<strong>左右子树</strong>进行翻转，再将<strong>当前节点的左右子节点</strong>交换即可，复杂度<span class="math inline">\(O(n)\)</span>。</p><p><strong>坑点</strong>：本题不能镜像遍历，因为<strong>不能保证对称位置也有节点</strong>，直接先序遍历即可。注意交换时要用<strong>临时变量</strong>保存中间值。</p><h3 id="合并二叉树-e">617. 合并二叉树 (E)</h3><p><ahref="https://leetcode.cn/problems/merge-two-binary-trees/">题目描述</a>：给定两棵二叉树的根节点，将两棵树合并：如果<strong>两个节点重合</strong>，则节点值相加；否则<strong>不为空的节点直接作为新节点</strong>。</p><p><strong>方法1</strong>：DFS，先判断当前比对的两个节点<strong>是否有空节点</strong>，如果有就返回另一个节点；否则就递归合并左、右子树，并将合并后的答案都指向<code>root1</code> 作为返回值。复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h2 id="层序遍历">层序遍历</h2><p>基础模板，使用队列存储：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c++">vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">levelOrder</span>(TreeNode* root) &#123;<br>    vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; res;<br>    queue&lt;TreeNode*&gt; q;<br>    <span class="hljs-keyword">if</span>(root) q.<span class="hljs-built_in">push</span>(root);<br>    <span class="hljs-keyword">while</span>(!q.<span class="hljs-built_in">empty</span>())&#123;<br>        <span class="hljs-keyword">int</span> n = q.<span class="hljs-built_in">size</span>();<br>        res.<span class="hljs-built_in">push_back</span>(vector&lt;<span class="hljs-keyword">int</span>&gt; &#123;&#125;);<br>        <span class="hljs-keyword">while</span>(n--)&#123;<br>            TreeNode* temp = q.<span class="hljs-built_in">front</span>();<br>            q.<span class="hljs-built_in">pop</span>();<br>            res.<span class="hljs-built_in">back</span>().<span class="hljs-built_in">push_back</span>(temp-&gt;val);<br>            <span class="hljs-keyword">if</span>(temp-&gt;left) q.<span class="hljs-built_in">push</span>(temp-&gt;left);<br>            <span class="hljs-keyword">if</span>(temp-&gt;right) q.<span class="hljs-built_in">push</span>(temp-&gt;right);<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="二叉树的层序遍历-m">102. 二叉树的层序遍历 (M)</h3><p><ahref="https://leetcode.cn/problems/binary-tree-level-order-traversal/">题目描述</a>：给定二叉树的根节点<code>root</code>，逐层地<strong>从左到右</strong>访问所有节点。</p><p><strong>方法1</strong>：BFS，维护一个队列，当队列不为空时，出队并将子节点入队。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：DFS，传参时传入 depth 和 res 数组，将节点放到<code>res[depth]</code> 中即可。空间复杂度为 <spanclass="math inline">\(O(\log n)\)</span>。</p><p><strong>坑点</strong>：输出结果时需要<strong>分层</strong>，因此 BFS可以在<strong>遍历每层的时候先记录下当前栈里的元素个数</strong>（该层的元素个数），再<code>while(n--)</code> 遍历当前层。DFS 在传参时传入 depth 即可。</p><h3 id="二叉树的锯齿形层序遍历-m">103. 二叉树的锯齿形层序遍历 (M)</h3><p><ahref="https://leetcode.cn/problems/binary-tree-zigzag-level-order-traversal/">题目描述</a>：给定二叉树的根节点<code>root</code>，<strong>先从左往右，再从右往左</strong>逐层遍历，层与层之间交替方向。</p><p><strong>方法</strong>：BFS +双端队列，奇数层右进左出，偶数层左进右出，时空复杂度均为 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="二叉树的右视图-m">199. 二叉树的右视图 (M)</h3><p><ahref="https://leetcode.cn/problems/binary-tree-right-side-view/">题目描述</a>：给定一个二叉树的<strong>根节点</strong><code>root</code>，按照从顶部到底部的顺序，返回<strong>站在右侧所能看到</strong>的节点值。</p><p><strong>方法</strong>：BFS，维护一个队列，用两个 <code>while</code>分层，每层的最后一个节点保存。时空复杂度均为 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="在每个树行中找最大值-m">515. 在每个树行中找最大值 (M)</h3><p><ahref="https://leetcode.cn/problems/find-largest-value-in-each-tree-row/">题目描述</a>：给定一个二叉树的<strong>根节点</strong><code>root</code>，返回该二叉树中每一层的最大值。</p><p><strong>方法</strong>：BFS，维护一个队列，用两个 <code>while</code>分层，每层的维护一个最大值。时空复杂度均为 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="反转二叉树的奇数层-m">2415. 反转二叉树的奇数层 (M)</h3><p><ahref="https://leetcode.cn/problems/reverse-odd-levels-of-binary-tree/">题目描述</a>：给定一颗<strong>满二叉树</strong>，反转这棵树中每个<strong>奇数</strong>层的节点值（左右镜像）。</p><p><strong>方法1</strong>：DFS，遍历两次，第一次按层数存下所有节点，第二次按层数赋值节点。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：BFS +层序遍历，用<strong>两个双端队列</strong>存下左右两侧的节点，两侧的出队方向不同，只需遍历一次。</p><p><strong>方法3</strong>：DFS + <strong>镜像遍历</strong>，DFS传参不再是传 <code>root</code>，而是传入 <code>root-&gt;left</code> 和<code>root-&gt;right</code>，每次 DFS 的子问题分别是交换<code>left-&gt;left</code> 和 <code>right-&gt;right</code>、交换<code>left-&gt;right</code> 和 <code>right-&gt;left</code>。</p><h2 id="树的属性">树的属性</h2><p>一次 DFS 可以求出树的属性包括：深度、节点数、每个节点的父节点。</p><p>两次 DFS 可以求出树的属性包括：直径。</p><h3 id="二叉树的最大深度-e">104. 二叉树的最大深度 (E)</h3><p><ahref="https://leetcode.cn/problems/maximum-depth-of-binary-tree/">题目描述</a>：给定一个二叉树，找出其最大深度，即根节点到最远叶子节点路径上的节点数。</p><p><strong>方法1</strong>：递归，如果当前为空指针，返回0。否则递归调用自身求出<strong>左右子树的最大深度</strong>，并<strong>加1 返回</strong>。复杂度 <span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：BFS + 层序遍历，每一层深度加一。复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="移除子树后的二叉树高度-h">2831. 移除子树后的二叉树高度 (H)</h3><p><ahref="https://leetcode.cn/problems/height-of-binary-tree-after-subtree-removal-queries/">题目描述</a>：二叉树的中有<span class="math inline">\(n&lt;1e5\)</span> 个节点，每个节点被分配<span class="math inline">\(1\)</span> 到 <spanclass="math inline">\(n\)</span> 的不同值，给定 <spanclass="math inline">\(m&lt;1e4\)</span>个查询，每次查询<strong>删除掉某个节点的所有子树</strong>后，整个树的<strong>高度</strong>（一个节点高度为0）。</p><p><strong>方法1</strong>：DFS 暴力，第一次 DFS 维护每个节点的 <spanclass="math inline">\(height\)</span>，父节点及兄弟节点编号。每次查询时再自底向上回溯，先看有无兄弟，如果有兄弟再看兄弟的高度分类讨论。时间复杂度<span class="math inline">\(O(nm)\)</span>，超时。</p><p><strong>方法2</strong>：两次 DFS + 树形 DP，第一次 DFS<strong>自底向上回溯</strong>算出 <spanclass="math inline">\(height\)</span>，第二次 DFS<strong>自顶向下</strong>算出 <spanclass="math inline">\(depth\)</span>，同时在第二次 DFS中，还可以<strong>自顶向下处理出答案</strong>，最后查询时直接取答案即可。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>设删掉节点 <code>root</code> 的所有子树后的高度为 <spanclass="math inline">\(restH\)</span>，则删掉其左子树的所有子树后整个树的高度有两种可能：</p><ul><li>不包含 <code>root</code> 节点的路径最长：则这条节点贡献的高度就是<span class="math inline">\(restH\)</span>；</li><li>包含 <code>root</code> 节点的路径最长：则这条路径肯定来自<code>root</code> 的右子树，因此就是此时 <code>root</code> 的 <spanclass="math inline">\(depth\)</span> 加上右子树的 <spanclass="math inline">\(height\)</span>。</li></ul><p>其中 <span class="math inline">\(restH\)</span>只与父节点有关，因此可以自顶向下计算，这时一种 DP 的思想。</p></blockquote><p><strong>方法3</strong>：转 DFS 序 + 前缀处理，先 DFS遍历并<strong>按顺序存下遍历过的节点编号</strong>（DFS序），并预处理出每个节点的深度，并存储每个节点<strong>管辖的子树区间</strong>。每次删除点就相当于删除一段连续区间，并将原数组分为两段，分别代表子树的左侧和右侧。两端区间中每个节点深度的最大值即为答案，因此先预处理<strong>前缀和后缀MAX</strong>，总时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><blockquote><p>从 DFS 序来看，<strong>同一棵子树节点所对应的 DFS序是连续的一段区间</strong>，每个节点管辖的子树区间会紧跟着该节点。</p><p>如何求管辖区间？用一个全局变量 <spanclass="math inline">\(idx\)</span> 记录当前的点的 DFS 序，在每次 DFS过程中，子树区间始于 <span class="math inline">\(idx+1\)</span>，并在DFS 左子树和右子树后，新的 <span class="math inline">\(idx\)</span>就是右端点。</p></blockquote><h2 id="树上路径问题">树上路径问题</h2><h3 id="二叉树的直径-e">543. 二叉树的直径 (E)</h3><p><ahref="https://leetcode.cn/problems/diameter-of-binary-tree/">题目描述</a>：一棵二叉树的直径长度是<strong>任意两个结点路径长度中的最大值</strong>，等价于<strong>树上最长路径</strong>经过节点数减一。</p><p><strong>方法</strong>：DFS，任意一条路径可以看作「<strong>左路径 +右路径</strong>」拼接而成，在这里左路径就是<strong>左子树的深度</strong>，右路径同理。因此只需递归计算深度，同时在回溯到每个节点时<strong>维护最大值</strong>即可。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(Depth)\)</span>。</p><h3 id="todo.-1245.-树的直径-m">Todo. 1245. 树的直径 (M)</h3><p><ahref="https://blog.csdn.net/qq_34176797/article/details/120356354">题目描述</a>：给定一棵<strong>无向树</strong>（N叉、任意节点可以作为根），求树的直径。</p><p><strong>方法1</strong>：两次 DFS，</p><p><strong>方法2</strong>：树形 DP，</p><h3 id="二叉树中的最大路径和-h">124. 二叉树中的最大路径和 (H)</h3><p><ahref="https://leetcode.cn/problems/binary-tree-maximum-path-sum/">题目描述</a>：树的路径定义为<strong>不分叉的节点连线</strong>，在节点值可正可负的情况下，求最大路径和。</p><p><strong>方法1</strong>：递归，当前节点可以作为<strong>路径的拐角</strong>，也可以组成<strong>路径的直线</strong>。前者<strong>不能再回溯</strong>，后者可以<strong>回溯给父节点</strong>，两者中的最值是当前节点的最大路径和，用于<strong>维护全局最大值</strong>。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>设左子树的递归结果为 <spanclass="math inline">\(L\)</span>，右子树的递归结果为 <spanclass="math inline">\(R\)</span>，当前节点值为 <spanclass="math inline">\(V\)</span>，则：</p><ul><li>当前节点作为拐角的结果：<span class="math inline">\(\max(L,R,L+R+V)\)</span></li><li>当前节点组成路径的直线：<span class="math inline">\(\max(L+V, R+V,V)\)</span></li></ul><p>两者<strong>共同维护全局最大值</strong>，同时只有<strong>后者作为返回值</strong>回溯。</p></blockquote><h3 id="二叉树的最近公共祖先-m">236. 二叉树的最近公共祖先 (M)</h3><p><ahref="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree/">题目描述</a>：给定一个二叉树，找到该树中两个<strong>指定节点</strong>的<strong>最近</strong>公共祖先。<strong>一个节点也可以是它自己的祖先</strong>。</p><p><strong>方法1</strong>：递归，递归函数的返回值传递一个信息「<strong>该二叉树中是否含有P 或 Q</strong>」，具体的返回值需要分类讨论。递归一次，时间复杂度 <spanclass="math inline">\(O(n)\)</span>，最坏情况下二叉树退化成链表，空间复杂度<span class="math inline">\(O(n)\)</span>。</p><blockquote><ol type="1"><li><p>如果当前节点为空，则返回空，这里直接用<strong>当前节点</strong>。</p></li><li><p>如果当前节点就是 P 或Q，则分两种情况，但是都是<strong>返回当前节点</strong>：</p><ol type="1"><li>另一个节点在当前节点的子树中，则<strong>当前节点就是答案</strong>。</li><li>另一个节点不在当前子树，说明当前节点只是其祖先节点在调用递归，需要<strong>返回一个非空值表示找到</strong>。</li></ol></li><li><p>其他情况下，P 和 Q可能在<strong>当前节点的左、右子树</strong>中，也可能<strong>不在当前子树</strong>：</p><ol type="1"><li>如果左、右子树的递归返回值都不为空，则 P、Q分布在左、右子树中，<strong>当前节点就是答案</strong>。</li><li>如果只有一个子树的返回值不为空，则要么 P、Q都在该子树，答案一定在该子树中；要么只有一个在该子树，需要返回一个非空值表示找到。都要<strong>返回递归该子树的结果</strong>。</li><li>如果都为空，则说明 P Q不在当前子树，则返回空，这里直接用<strong>子树的递归结果</strong>。</li></ol></li></ol></blockquote><p><strong>方法2</strong>：后序遍历，由方法 1可知，当前节点会接收子节点的状态（是否含有）<strong>并把这个状态往上传递</strong>，直到满足祖先节点的条件。所以用后序遍历的模板也可完成。复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：存储父节点，遍历一次二叉树并用<strong>哈希表存储每个节点的父节点</strong>，则从P、Q开始往上搜索，就变成了<strong>求两个链表相交点问题</strong>，再用一个哈希表或双指针即可。复杂度均为<span class="math inline">\(O(n)\)</span>。</p><h2 id="二叉搜索树">二叉搜索树</h2><p>二叉搜索树特性：</p><ul><li>左子树小于等于父节点，右子树大于等于父节点。</li><li>中序遍历可以转化为有序的序列。</li><li>不满足平衡性，可能会<strong>退化成链表</strong>。</li></ul><p>二叉搜索树可以 DFS查找，也可以<strong>非递归查找</strong>，非递归速度更快，参考如下代码：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">find</span><span class="hljs-params">(TreeNode* root, <span class="hljs-keyword">int</span> target)</span> </span>&#123;<br>    <span class="hljs-keyword">while</span> (root) &#123;<br>        <span class="hljs-keyword">if</span> (root-&gt;val == target) &#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>        &#125;<span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (root-&gt;val &gt; target) &#123;<br>            root = root-&gt;left;<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            root = root-&gt;right;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="验证二叉搜索树-m">98. 验证二叉搜索树 (M)</h3><p><ahref="https://leetcode.cn/problems/validate-binary-search-tree/">题目描述</a>：给定一个二叉树的根节点，判断其是否是一个有效的二叉搜索树。</p><p><strong>方法1</strong>：递归，根节点<strong>给左子树规定了上界，给右子树规定了下界</strong>。以此类推，每次递归都会<strong>缩小边界</strong>，如果每个节点都满足祖先节点规定的边界，则其为二叉搜索树。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：中序遍历，二叉搜索树构成的序列一定是<strong>升序</strong>的，因此在中序遍历时检查当前节点<strong>是否大于上一个节点</strong>即可。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><h3 id="修剪二叉搜索树-m">669. 修剪二叉搜索树 (M)</h3><p><ahref="https://leetcode.cn/problems/trim-a-binary-search-tree/">题目描述</a>：给定二叉搜索树的根节点<code>root</code>，同时给定最小边界<code>low</code> 和最大边界<code>high</code>。通过修剪二叉搜索树，使得所有节点的值在<code>[low, high]</code> 中，且不改变树节点的相对结构。</p><p><strong>方法1</strong>：递归，根据二叉搜索树的定义，<code>root&lt;low</code>时删去整个左子树，<code>root&gt;high</code>时删去整个右子树，其他时候保留两个子树。再递归处理剩下的子树，时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：迭代，从 <code>root</code>出发找到<strong>第一个满足范围的节点</strong>，就是要返回的根节点。当根节点符合范围时，修剪左子树时只需考虑删除<code>&lt;low</code> 的节点，修剪右子树时只需考虑删除<code>&gt;high</code> 的节点。时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="二叉搜索树的最近公共祖先-m">235. 二叉搜索树的最近公共祖先(M)</h3><p><ahref="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-search-tree/">题目描述</a>：给定一个二叉搜索树,找到该树中两个指定节点的最近公共祖先。</p><p><strong>方法1</strong>：递归，由于二叉搜索树的性质，可以<strong>根据值的大小提前判断</strong>节点位置。因此<strong>递归无需传递信息</strong>，因为不可能的一侧必定不会展开递归，无需回溯。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><blockquote><ol type="1"><li>当前节点是 P 或Q，直接返回当前节点。注意<strong>不可能出现当前节点为空</strong>的情况，因为空的一侧不可能递归到。</li><li>根据节点值判断：<ol type="1"><li>P 和 Q 都在左子树：返回递归左子树的结果。</li><li>P 和 Q 都在右子树：返回递归右子树的结果。</li><li>P 和 Q 分别位于左右子树：返回当前节点。</li></ol></li></ol></blockquote><p><strong>方法2</strong>：存储父节点，复杂度均为 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="把二叉搜索树转换为累加树-m">538. 把二叉搜索树转换为累加树(M)</h3><p><ahref="https://leetcode.cn/problems/convert-bst-to-greater-tree/">题目描述</a>：给定一棵二叉搜索树，将其转换为<strong>累加树</strong>，使每个节点的新值等于原树中<strong>大于或等于当前节点</strong>的值之和。</p><p><strong>方法1</strong>：DFS暴力，每个节点需要加上「<strong>当前节点的右子树之和</strong>」和「<strong>祖先节点传递下来的值</strong>」。其中前者可以一次DFS<strong>预处理</strong>子树之和，后者需要<strong>分类讨论</strong>：对于左子节点，传递<strong>当前节点的累加值</strong>；对于右子节点，传递<strong>父节点的传递值</strong>，时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：<strong>反序中序遍历</strong>，二叉搜索树中序遍历（左-中-右）的结果为递增序列，反序中序遍历（<strong>右-中-左</strong>）的结果为递减序列。因此反序遍历时维护一个<strong>全局变量</strong><span class="math inline">\(sum\)</span>，每次都将当前节点更新为 <spanclass="math inline">\(sum\)</span> 即可。复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h2 id="n-叉树">N 叉树</h2><p>N叉树就是广义的树，其存储和遍历规则都可以当作<strong>图</strong>来处理，唯一不同的是其还保留树的一些特性：</p><ul><li>删掉任意一条边可以将整个树分割为两个连通块，即<strong>每条边都是割边</strong>。</li><li>从根节点开始一次 DFS即可访问所有节点，且<strong>不会重复访问</strong>陷入死循环。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 一维数组 nums 存点, 二维数组 edges 存边</span><br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">g</span>(nums.<span class="hljs-built_in">size</span>());<span class="hljs-comment">// 大小也可以取 edges.size() - 1</span><br><span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> &amp;e : edges) &#123;<br>    <span class="hljs-keyword">int</span> x = e[<span class="hljs-number">0</span>], y = e[<span class="hljs-number">1</span>];<br>    g[x].<span class="hljs-built_in">push_back</span>(y);<br>    g[y].<span class="hljs-built_in">push_back</span>(x); <span class="hljs-comment">// 无向图，父子关系根据 root 的选择而定</span><br>&#125;<br><span class="hljs-comment">// DFS, x 是结点编号, fa 是父节点，在无向图中防止重复遍历</span><br>function&lt;<span class="hljs-built_in"><span class="hljs-keyword">int</span></span>(<span class="hljs-keyword">int</span>, <span class="hljs-keyword">int</span>)&gt; dfs = [&amp;](<span class="hljs-keyword">int</span> x, <span class="hljs-keyword">int</span> fa) &#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> y : g[x])<br>        <span class="hljs-keyword">if</span> (y != fa) &#123;<br>            <span class="hljs-keyword">int</span> res = <span class="hljs-built_in">dfs</span>(y, x);<br>        &#125;<br>    <span class="hljs-keyword">return</span> res + <span class="hljs-number">1</span>;<br>&#125;;<br><span class="hljs-comment">// 从根节点开始 DFS，不会重复访问，父节点设为 -1</span><br><span class="hljs-keyword">return</span> <span class="hljs-built_in">dfs</span>(<span class="hljs-number">0</span>, <span class="hljs-number">-1</span>); <br></code></pre></td></tr></table></figure><h3 id="todo-310.-最小高度树-m">Todo 310. 最小高度树 (M)</h3><p><ahref="https://leetcode.cn/problems/minimum-height-trees/description/">题目描述</a>：给定一颗N叉树，选择任意一个节点为根，使得树的<strong>高度最小化</strong>，返回所有最小高度树。</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：拓扑排序</p><p><strong>坑点</strong>：</p><h3 id="到达首都的最少油耗-m">2477. 到达首都的最少油耗 (M)</h3><p><ahref="https://leetcode.cn/problems/minimum-fuel-cost-to-report-to-the-capital/">题目描述</a>：给定<span class="math inline">\(n\)</span> 个节点的 N叉树，每个点代表一座城市，编号 <code>0</code>的城市是首都，每个城市需要派出<strong>一名代表</strong>去往首都，期间需要乘坐汽车，<strong>可换乘</strong>，每辆汽车有<span class="math inline">\(seats\)</span>个座位，求总共的最少行程。</p><p><strong>方法</strong>：N 叉树 DFS +贪心回溯，总共的行程等于<strong>每条边上的车数总和</strong>，因此 DFS回溯每个节点的子节点个数（代表人数），总人数除以座位数<strong>向上取整</strong>就是边上的车数，时间复杂度<span class="math inline">\(O(n)\)</span>。</p><h3 id="最大价值和与最小价值和的差值-h">6294.最大价值和与最小价值和的差值 (H)</h3><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：N 叉树 DFS + <strong>记忆化</strong>，</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p><h3 id="todo-2440.-创建价值相同的连通块-h">Todo 2440.创建价值相同的连通块 (H)</h3><p><ahref="https://leetcode.cn/problems/create-components-with-same-value/">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #12 字符串</title>
    <link href="/LeetCode-String.html"/>
    <url>/LeetCode-String.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「字符串」类型题中的：打印输出、KMP 算法、Trie树、字符串哈希等。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><h2 id="打印输出">打印输出</h2><p>面试常考的一类题，利用简单的技巧将繁琐的输出简化，考验思维。</p><h3 id="回文数-e">9. 回文数 (E)</h3><p><ahref="https://leetcode.cn/problems/palindrome-number/">题目描述</a>：给你一个整数<code>x</code>，判断 <code>x</code> 是否为一个回文整数。</p><p><strong>方法1</strong>：逐位分解，先特判数字 <spanclass="math inline">\(0\)</span> 和 <spanclass="math inline">\(10\)</span>的倍数，然后从末位开始取出数字，同时放到一个临时变量中判断。</p><p><strong>方法2</strong>：整数转字符串，将原数字用<code>to_string</code> 和 <code>reverse</code>翻转，再和原数字比较。</p><p><strong>坑点</strong>：方法 2和原数字比较时，应该把两个数都转为字符串，如果将翻转后的数<code>stoi</code> 则<strong>可能会溢出</strong>！</p><h3 id="整数反转-m">7. 整数反转 (M)</h3><p><a href="">题目描述</a>：给你一个 32位的<strong>有符号整数</strong>，返回将数字反转后的结果，如果反转后超过INT 的取值则返回 0。</p><p><strong>方法</strong>：利用一个 64位整数暂存，<strong>取出负号最后处理</strong>，最后结果判断先是否在范围内再返回。</p><p><strong>坑点</strong>：负号也可以无需处理，<strong>负数对正数</strong>的除法和取模运算会保留负号，但是<strong>不同语言、机器的特性</strong>不同，可能会出错，因此最好取出负号单独处理！</p><h3 id="z-字形变换-m">6. Z 字形变换 (M)</h3><p><ahref="https://leetcode.cn/problems/zigzag-conversion/description/">题目描述</a>：给定字符串<code>s</code> 和行数 <code>r</code>，以从上往下、从左到右进行 Z字形排列，再从左往右逐行读取，产生出一个新的字符串。</p><p><strong>方法1</strong>：二维矩阵模拟，遍历字符串并按 Z字形填写矩阵，再逐行输出，时空复杂度均为 <span class="math inline">\(O(r\times n)\)</span>。</p><p><strong>方法2</strong>：按行模拟，创建 <spanclass="math inline">\(r\)</span> 个空字符串，遍历 <code>s</code>并折返写入每行（用一个<strong>方向标记</strong>即可），时空复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：直接构造，每个周期有 <spanclass="math inline">\(t=2r-2\)</span> 个字符，通过模 <spanclass="math inline">\(t\)</span>取余可以枚举<strong>每一行的下一个字符</strong>。第一行和最后一行的余数为<span class="math inline">\(0\)</span> 和 <spanclass="math inline">\(r-1\)</span>，中间第 <spanclass="math inline">\(i\)</span>行<strong>每周期内有两个字符</strong>，余数分别是 <spanclass="math inline">\(i\)</span> 和 <spanclass="math inline">\(t-i\)</span>。空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="旋转图像-m">48. 旋转图像 (M)</h3><p><ahref="https://leetcode.cn/problems/rotate-image/">题目描述</a>：给定一个<span class="math inline">\(n\times n\)</span>的二维矩阵表示图像，将其顺时针旋转 90度，要求<strong>原地操作</strong>。</p><p><strong>方法1</strong>：转置 +翻转，先沿着<strong>对角线</strong>转置，再进行<strong>水平</strong>翻转，空间复杂度<span class="math inline">\(O(1)\)</span>。</p><p><strong>方法2</strong>：直接交换，矩阵中对应的<strong>四个点为一组</strong>，用一个<span class="math inline">\(temp\)</span> 变量就能完成四个点的旋转。当<span class="math inline">\(n\)</span>为偶数的分组显而易见；为奇数时正中央的点无需旋转，四个角各有一个矩形。空间复杂度<span class="math inline">\(O(1)\)</span>。</p><h2 id="kmp">KMP</h2><p>BF 暴力算法：</p><ul><li>从主串 S 的第一个字符起，与模式 T的第一个字符比较，若相等，则继续逐个比较后续字符；否则从 S的下一个字符起，重新和 T 的字符比较。此时的 T<strong>每次失败都要完全回溯</strong>。</li><li><strong>最坏时间复杂度</strong>为 <spanclass="math inline">\(O(nm)\)</span>，其中 <spanclass="math inline">\(n\)</span> 和 <spanclass="math inline">\(m\)</span>​分别为主串和模式串的长度。例如，当模式串为 00001，而主串为000000000000000000000000001时，每趟匹配都是比较到模式的最后一个字符时才发现不等，产生大量不必要的回溯。</li></ul><p>KMP 算法：</p><ul><li>整个匹配过程中，子串不回溯，通过 <code>next</code>数组快速前移，<span class="math inline">\(O(n+m)\)</span>的时间完成串的模式匹配操作。</li><li><code>next[j]</code>的含义是：代表当前字符<strong>之前</strong>的字符串中，<strong>有多大长度的相同前缀后缀</strong>（意味着<strong>失配的时候，成功匹配过的后缀是可以利用的</strong>）。在子串的第<span class="math inline">\(j\)</span>个字符与主串发生失配时，则将子串的第 <code>next[j]</code>位置前移到当前主串失配的位置，重新比较。</li><li>如何求 <code>next</code>：<strong>动态规划</strong>递推求，若<code>next[j-1]=2</code>，则 <code>next[j]</code>只要看新的一位是否等于前缀的后一位。注意整个 <code>next</code>是<strong>往后偏移一格</strong>的，<code>next[0]</code> 默认置为<code>-1</code>，因为第一个就失配时子串右移一格。</li><li><code>nextVal[j]</code> 的含义：在 <code>next</code>数组的基础上，如果 <code>next[j]</code> 为<code>k</code>，就比较模式串的 <code>j</code> 和 <code>k</code>位，如果相同则置为 -1。</li></ul><p>KMP 算法模板：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">strStr</span><span class="hljs-params">(string s, string p)</span> </span>&#123;<br>    <span class="hljs-keyword">int</span> n = s.<span class="hljs-built_in">size</span>(), m = p.<span class="hljs-built_in">size</span>();<br>    <span class="hljs-keyword">if</span>(m == <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>    <span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">next</span><span class="hljs-params">(m)</span></span>;<br>    <span class="hljs-comment">// DP 预处理 next 数组</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>, j = <span class="hljs-number">0</span>; i &lt; m; i++)&#123;<br>        <span class="hljs-keyword">while</span>(j &gt; <span class="hljs-number">0</span> &amp;&amp; p[i] != p[j]) j = next[j - <span class="hljs-number">1</span>];<br>        <span class="hljs-keyword">if</span>(p[i] == p[j]) j++;<br>        next[i] = j;<br>    &#125;<br>    <span class="hljs-comment">// 匹配过程</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>, j = <span class="hljs-number">0</span>; i &lt; n; i++)&#123;<br>        <span class="hljs-keyword">while</span>(j &gt; <span class="hljs-number">0</span> &amp;&amp; s[i] != p[j]) j = next[j - <span class="hljs-number">1</span>];<br>        <span class="hljs-keyword">if</span>(s[i] == p[j]) j++;<br>        <span class="hljs-keyword">if</span>(j == m) <span class="hljs-keyword">return</span> i - m + <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="金典-01.09.-字符串轮转-e">金典 01.09. 字符串轮转 (E)</h3><p><ahref="https://leetcode.cn/problems/string-rotation-lcci/">题目描述</a>：给定两个字符串<code>s1</code>和<code>s2</code>，检查<code>s2</code>是否为<code>s1</code>轮转而成。</p><p><strong>方法1</strong>：暴力，把 <code>s2</code>中的每个字符当作起点匹配 <code>s1</code>，要进行 <code>% n</code>操作，复杂度 <span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：拼接法，对于所有<strong>环形序列</strong>问题，都可以把<strong>原序列复制拼接</strong>在后面，当成线性序列进行字符串匹配。C++的 find 复杂度 <span class="math inline">\(O(n^2)\)</span>，手写 KMP算法复杂度 <span class="math inline">\(O(n)\)</span>。</p><p><strong>坑点</strong>：如果两个字符串<strong>长度不一样，则必不可能进行轮转互换</strong>，但是用拼接法可能会误判。</p><h3 id="重复的子字符串-e">459. 重复的子字符串 (E)</h3><p><ahref="https://leetcode.cn/problems/repeated-substring-pattern/">题目描述</a>：给定一个非空的字符串<code>s</code>，检查是否可以通过由<strong>它的一个子串</strong>重复多次构成。</p><p><strong>方法1</strong>：枚举，显然子串的长度是总长度的<strong>因数</strong>，由于子串至少重复一次，只需在<span class="math inline">\([1, n/2]\)</span> 范围内枚举长度即可，再取<code>s</code> 的前缀进行匹配，时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：拼接法，如果一个字符串有重复性质，则<strong>原串复制拼接</strong>后的<code>s+s</code> 也具有重复性质，移除 <code>s+s</code>的<strong>第一个和最后一个</strong>字符，如果 <code>s</code>是新字符串<strong>的子串</strong>，则满足题目要求。时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。</p><p><strong>方法3</strong>：拼接法 + KMP，手写 KMP 查找子串，时间复杂度<span class="math inline">\(O(n)\)</span>。</p><h2 id="trie-树">Trie 树</h2><p>Trie 树是一种 N 叉树（26 叉树）：</p><ul><li><strong>每个节点包含一个字符</strong>（根节点除外），从<strong>根节点到某一节点的路径</strong>上的字符代表该节点对应的字符串。</li><li>两个有公共前缀的单词，在 Trie 树中前缀部分的路径相同，所以 Trie树又叫做<strong>前缀树</strong>（Prefix Tree）。</li><li>每个节点的子节点包含的字符<strong>各不相同</strong>，根节点通往不同字符串的路径唯一。</li><li>通常在实现时，会在节点中设置一个<strong>标志位</strong>，用来标记该节点处是否构成一个单词。</li></ul><p>Trie 树可以实现 <span class="math inline">\(O(m)\)</span>插入、查询字符串，<span class="math inline">\(m\)</span>是字符串的长度。虽然哈希表的复杂度 <spanclass="math inline">\(O(1)\)</span>看起来更快，但是求哈希值也需要遍历字符串，且在字符串过多时<strong>可能会大量冲突</strong>，时间效率不高。此外Trie 树还可以对关键字按<strong>字典序</strong>排序（先序遍历）。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Trie</span> &#123;</span><br><span class="hljs-keyword">private</span>:<br>    Trie* son[<span class="hljs-number">26</span>] = &#123;&#125;;<br>    <span class="hljs-keyword">bool</span> isWord = <span class="hljs-literal">false</span>;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-built_in">Trie</span>() &#123;<br>        isWord = <span class="hljs-literal">false</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">26</span>; i++) son[i] = <span class="hljs-literal">nullptr</span>;<br>    &#125;<br>    <br>    ~<span class="hljs-built_in">Trie</span>()&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">26</span> ; i++)&#123;<br>            <span class="hljs-keyword">if</span>(son[i] != <span class="hljs-literal">nullptr</span>) <span class="hljs-keyword">delete</span> son[i];<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">insert</span><span class="hljs-params">(<span class="hljs-keyword">const</span> string&amp; word)</span> </span>&#123;<br>        Trie* root = <span class="hljs-keyword">this</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span> &amp;x: word)&#123;<br>            <span class="hljs-keyword">int</span> cur = x - <span class="hljs-string">&#x27;a&#x27;</span>;<br>            <span class="hljs-keyword">if</span>(root-&gt;son[cur] == <span class="hljs-literal">nullptr</span>) root-&gt;son[cur] = <span class="hljs-keyword">new</span> <span class="hljs-built_in">Trie</span>();<br>            root = root-&gt;son[cur];<br>        &#125;<br>        root-&gt;isWord = <span class="hljs-literal">true</span>;<br>    &#125;<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">search</span><span class="hljs-params">(<span class="hljs-keyword">const</span> string&amp; word)</span> </span>&#123;<br>        Trie* root = <span class="hljs-keyword">this</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span> &amp;x : word)&#123;<br>            <span class="hljs-keyword">int</span> cur = x - <span class="hljs-string">&#x27;a&#x27;</span>;<br>            <span class="hljs-keyword">if</span>(root-&gt;son[cur] == <span class="hljs-literal">nullptr</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>            root = root-&gt;son[cur];<br>        &#125;<br>        <span class="hljs-keyword">return</span> root-&gt;isWord;<br>    &#125;<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">startsWith</span><span class="hljs-params">(<span class="hljs-keyword">const</span> string&amp; prefix)</span> </span>&#123;<br>        Trie* root = <span class="hljs-keyword">this</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span> &amp;x : prefix)&#123;<br>            <span class="hljs-keyword">int</span> cur = x - <span class="hljs-string">&#x27;a&#x27;</span>;<br>            <span class="hljs-keyword">if</span>(root-&gt;son[cur] == <span class="hljs-literal">nullptr</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>            root = root-&gt;son[cur];<br>        &#125;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h3 id="实现-trie-前缀树-m">208. 实现 Trie (前缀树) (M)</h3><p><ahref="https://leetcode.cn/problems/implement-trie-prefix-tree/">题目描述</a>：实现Trie 类，包含初始化、插入、检索字符串、检索前缀。</p><p><strong>方法1</strong>：二维数组，用一个 <spanclass="math inline">\(trie[N][26]\)</span>来存储<strong>树节点的子节点下标</strong>，<spanclass="math inline">\(end[N]\)</span> 数组记录树节点是否被当作结尾，一个<span class="math inline">\(idx\)</span>变量标记最后一个有值的树节点。此方法开销更小，但需要<strong>提前估算大小</strong>，适合ACM 竞赛。</p><blockquote><p>Insert：遍历单词的每个字符，用 <code>p</code> 表示下标，从数组起点<code>trie[0][x-'a']</code> 向下寻找子树，当该值为 0时，到达尽头，需要插入新的节点到 <code>++idx</code>下标所在位置，继续扩展树。扩展结束后，置最后一个位置<code>end[p]=1</code>。</p><p>Search：遍历单词的每个字符，当 <code>trie[p][x-'a']</code> 为 0时，代表不存在此单词，返回 false；如果遍历结束，直接返回<code>end[p]</code> 即可知道是否存在。</p><p>StartWith：同上，但是遍历结束时，可以直接返回 true。</p></blockquote><p><strong>方法2</strong>：动态申请 Trie 节点，Trie 类私有指针数组指向26 个节点，以及一个 <span class="math inline">\(isWord\)</span>标志位。在构造函数中将标志位置 false，指针置空。此方法适合面向对象。</p><blockquote><p>Insert：遍历单词的每个字符，用 <code>Trie*</code>指针遍历节点，遍历到尽头时扩展，需要 <code>new</code> 一个 Trie节点。扩展结束后，置最后一个节点 <code>isWord</code> 为 true。</p><p>Search、StartWith 和方法 1 类似，不再赘述。</p></blockquote><h3 id="字符串的前缀分数和-h">2416. 字符串的前缀分数和 (H)</h3><p><ahref="https://leetcode.cn/problems/sum-of-prefix-scores-of-strings/">题目描述</a>：给定长度为<code>n</code> 的数组<code>words</code>，数组中<strong>每个字符串的每个前缀</strong>，每出现一次就得一分，返回一个长度为<code>n</code> 的数组 <code>answer</code>，其中 <code>answer[i]</code>是 <code>words[i]</code> 的每个非空前缀的分数<strong>总和</strong>。</p><p><strong>方法1</strong>：哈希表，第一次遍历每个单词，将每个前缀在哈希表中的分数增一，第二次遍历时累加每个前缀的分数。复杂度<span class="math inline">\(O(n^3)\)</span>，因为 <strong>C++对字符串进行哈希运算</strong>同样需要 <spanclass="math inline">\(O(Len)\)</span>，换成 Python 或 JS等语言可过。</p><p><strong>方法2</strong>：Trie，但是不需要 <spanclass="math inline">\(isWord\)</span>标志位，只需在每个节点中<strong>存储当前前缀的分数</strong>，在插入时路径上的分数增一，检索时累加路径上的分数。复杂度<span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法3</strong>：字符串单哈希，仍用 C++ 无序映射，但是将键换成<code>long long</code>，用 <code>x = (x * p + ch) % mod</code>设置键，其中 <code>p</code>设置为<strong>较大质数防止冲突</strong>。复杂度 <spanclass="math inline">\(O(n^2)\)</span>。</p><h2 id="字符串哈希">字符串哈希</h2><p>本质是「将<strong>匹配两个字符串相等</strong>的复杂度由 <spanclass="math inline">\(O(n)\)</span> 优化到 <spanclass="math inline">\(O(1)\)</span>」，常用的字符串哈希方法： <spanclass="math display">\[hash[i]=(hash[i-1]\times Base+idx(s[i]))\;\%\;MOD\]</span></p><ul><li>自然溢出：不定义 <span class="math inline">\(MOD\)</span>，开unsigned long long 自然溢出，默认 <spanclass="math inline">\(MOD=2^{64}-1\)</span>。</li><li><strong>单哈希</strong>：需要开 long long取<strong>大质数</strong>，常用 <span class="math inline">\(Base=31,MOD=1e9+7\)</span>。</li><li>双哈希：公式同上，用两个 <span class="math inline">\(Base\)</span>和 <span class="math inline">\(MOD\)</span>得到<strong>二元组</strong>，同时比对两个值更安全。</li></ul><p>存储<strong>单哈希</strong>结果可以用一个<strong>二维矩阵</strong>，<spanclass="math inline">\(hash[i][j]\)</span> 表示 <spanclass="math inline">\(s[i\cdotsj]\)</span>，也可以用<strong>一维矩阵</strong>存储，再 <spanclass="math inline">\(O(1)\)</span> 查询，注意防止负数，同时 <spanclass="math inline">\(Base\)</span> 的幂次也要顺便存储： <spanclass="math display">\[res=\left( \left( hash[j]-hash[i-1]\times \mathrm{Base}^{j-i+1} \right)\;\%\;MOD+MOD \right) \;\%\;MOD\]</span></p><blockquote><p>尽管可能性很低，但还是可能存在哈希冲突。字符串哈希可以<strong>帮助迅速排除绝大多数不相等/元素不存在的情况</strong>，对于剩下的情况，如果必<strong>须确认两个字符串是否相等，或者元素是否存在于容器内</strong>，我们还是要再一次进行完整的查验。</p></blockquote><h3 id="字母异位词分组-m">49. 字母异位词分组 (M)</h3><p><ahref="https://leetcode.cn/problems/group-anagrams/">题目描述</a>：给定一些字符串，将其划分为若干组，使得<strong>每组中的单词都是字母异位词</strong>，即<strong>字母相同但顺序不同</strong>。</p><p><strong>方法1</strong>：排序 +哈希表，将<strong>每个字符串内部排序</strong>，则所有字母异位词都会变成同一个单词，将其作为哈希的键即可存下所有字母异位词，再将其分组输出。时间复杂度<span class="math inline">\(O(nk \log k)\)</span>。</p><p><strong>方法2</strong>：计数 + 哈希表，<strong>计数的桶用 char来表示</strong>，整个计数表就是一个字符串，作为哈希的键，时间复杂度<span class="math inline">\(O(nk)\)</span>。</p><p><strong>方法3</strong>：<strong>顺序无关</strong>字符串哈希，用<strong>不同的质数</strong>表示26个字母，把每个字母的值相乘取模，就能确保字母异位词的乘积必定是相等的，且<strong>非字母异位词的乘积不会相等</strong>。时间复杂度<span class="math inline">\(O(nk)\)</span>。</p><h3 id="对字母串可执行的最大删除数-h">2430. 对字母串可执行的最大删除数(H)</h3><p><ahref="https://leetcode.cn/problems/maximum-deletions-on-a-string/">题目描述</a>：给定一个字符串<code>s</code>，在一步操作中，你可以：删除整个序列；任取<code>i</code>，如果有 <code>s[:i]==s[i:2*i]</code>，删除<code>s[:i]</code>。返回删除 <code>s</code>可能的<strong>最大操作数</strong>。</p><p><strong>方法1</strong>：DP +字符串单哈希，将字符串<strong>逆序</strong>，<spanclass="math inline">\(dp[i]\)</span> 表示删除前 <spanclass="math inline">\(i\)</span> 个字母可能的最大操作数，显然 <spanclass="math inline">\(dp[0]=0\)</span>，答案为 <spanclass="math inline">\(dp[n-1]\)</span>，每次<strong>枚举可删除的字母长度</strong><span class="math inline">\(j\)</span>，如果 <spanclass="math inline">\(s[i-j+1:i]=s[i-2j+1:i-j]\)</span>则转移，预处理一个<strong>二维哈希值矩阵</strong>，则可以 <spanclass="math inline">\(O(1)\)</span> 匹配。复杂度 <spanclass="math inline">\(O(n^2)\)</span>。 <span class="math display">\[dp[i]=\max \{dp[i],\; dp[i-j]+1 \}\]</span> <strong>方法2</strong>：DP + 最长公共前缀(LCP)，不需要将字符串<strong>逆序</strong>，从后往前DP。不用预计算哈希值，而是用先 DP 得到 LCP 结果，<spanclass="math inline">\(lcp[i][j]\)</span> 表示 <code>s[i:]</code> 和<code>s[j:]</code> 的最长公共前缀，如果 <spanclass="math inline">\(lcp[i][i+j]&gt;=j\)</span> 则转移。复杂度 <spanclass="math inline">\(O(n^2)\)</span>。 <span class="math display">\[dp[i]=\max \{dp[i],\; dp[i+j]+1 \}\]</span></p><blockquote><p>LCP：<span class="math inline">\(lcp[i][n]\)</span> 和 <spanclass="math inline">\(lcp[n][j]\)</span> 都是 0，从后往前DP，每个状态可以转移给<strong>同一对角线的下一个状态</strong>，状态转移方程如下：<span class="math display">\[lcp[i][j]=lcp[i+1][j+1]+1\;\;\;\text{if}(s[i]==s[j])\]</span></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #11 栈&amp;队列</title>
    <link href="/LeetCode-Stack-Queue.html"/>
    <url>/LeetCode-Stack-Queue.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「栈&amp;队列」类型题中的：栈和队列模拟、括号配对、单调栈等。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><h2 id="栈和队列模拟">栈和队列模拟</h2><h3 id="验证栈序列-m">946. 验证栈序列 (M)</h3><p><ahref="https://leetcode.cn/problems/validate-stack-sequences/">题目描述</a>：给定<code>pushed</code> 和 <code>popped</code>序列，当其可能是一个真实空栈上发生的序列时，返回 true。</p><p><strong>方法</strong>：栈模拟，将 <code>pushed</code>入栈，同时判断栈顶元素是否为 <code>popped</code>序列当前指向的元素，如果是则<strong>连续出栈</strong>，如果不是则继续入栈。当<code>pushed</code> 遍历完时，模拟的栈为空则代表真实可行。</p><p><strong>坑点</strong>：取栈顶元素时如果栈为空则会发生<strong>数组越界</strong>错误！</p><h3 id="最大频率栈-h">895. 最大频率栈 (H)</h3><p><a href="">题目描述</a>：设计一个 <code>FreqStack</code>类，将元素推入堆栈，并从堆栈中弹出<strong>出现频率</strong>最高的元素。</p><p><strong>方法</strong>：哈希表 + 多栈，用哈希表 <spanclass="math inline">\(freq\)</span>存储每个值出现的次数，记当前最大频率为 <spanclass="math inline">\(maxFreq\)</span>，<strong>为每一种频率单独设置一个栈</strong>，所有栈用哈希表<span class="math inline">\(ss\)</span> 维护。所有操作的时间复杂度均为<span class="math inline">\(O(1)\)</span>。</p><blockquote><ul><li>每次出栈都发生在 <span class="math inline">\(maxFreq\)</span>对应的栈，当该栈为空时，自动转到下一个。</li><li>每次入栈都会将 <span class="math inline">\(val\)</span>加入其当前频率对应的栈，且不会删除之前栈中已有的 <spanclass="math inline">\(val\)</span>。</li><li>如果有最大频率对应着多个数字，也能保证弹出的一定是最后入栈的元素。</li></ul></blockquote><h3 id="todo-用两个栈模拟队列-m">Todo 用两个栈模拟队列 (M)</h3><h2 id="括号配对">括号配对</h2><h3 id="有效的括号-e">20. 有效的括号 (E)</h3><p><ahref="https://leetcode.cn/problems/valid-parentheses/">题目描述</a>：给定一个只包含<code>()&#123;&#125;[]</code>的字符串，验证括号是否有效，不区分三种括号的优先级。</p><p><strong>方法1</strong>：栈模拟，当遇到一种左括号时，将<strong>对应的右括号</strong>入栈；遇到一种右括号时，<strong>比对栈顶元素是否相等</strong>，相等则出栈。最后栈为空则有效，时间复杂度<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：栈模拟 +哈希表，上述写法通常需要若干个条件判断语句，如果用<strong>哈希表存储</strong><code>左括号-&gt;右括号</code>的映射就能减少代码的冗余，但在本题上不会显得更高效。</p><p><strong>坑点</strong>：当字符串长度为奇数时，可以直接返回 false。</p><h3 id="使括号有效的最少添加-m">921. 使括号有效的最少添加 (M)</h3><p><ahref="https://leetcode.cn/problems/minimum-add-to-make-parentheses-valid/">题目描述</a>：给定一串括号序列，只有当括号<strong>正确配对</strong>时序列才是有效的，正确配对的括号包括<code>(())</code> 和 <code>()()</code>等。计算要<strong>使原括号序列有效而必须添加的最少括号数</strong>。</p><p><strong>方法1</strong>：贪心 +栈模拟，括号问题最容易想到的就是栈模拟法，当遇到左括号时入栈，遇到右括号时出栈匹配。如果遇到右括号时栈为空，则<strong>只能添加一个左括号进行匹配</strong>。最终答案是<strong>添加的</strong>左括号数和<strong>栈中剩余的</strong>左括号数，复杂度<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：贪心 +计数，上一个方法中，栈中只会存放<strong>左括号</strong>，因此可以<strong>简化为一个变量</strong>来计数栈中的元素个数，空间复杂度降为<span class="math inline">\(O(1)\)</span>。</p><h3 id="括号的分数-m">856. 括号的分数 (M)</h3><p><ahref="https://leetcode.cn/problems/score-of-parentheses/">题目描述</a>：给定一串<strong>平衡括号序列</strong>（正确匹配），按下列规则计算字符串的分数：<code>()</code>得 1 分，<code>AB</code> 得 A + B 分，<code>(A)</code> 得 2A 分，其中<code>A</code> 和 <code>B</code> 都是平衡括号序列。</p><p><strong>方法1</strong>：前缀和 + 分治，对原串进行分解，如果是<code>AB</code> 则递归处理两边，如果是 <code>(A)</code>则去掉最外层括号处理内层。时间复杂度为 <spanclass="math inline">\(O(n^2)\)</span>，空间复杂度为递归深度 <spanclass="math inline">\(O(n)\)</span>。</p><blockquote><p>如何判断该分为哪种？将左括号记为 <spanclass="math inline">\(1\)</span>，右括号记为 <spanclass="math inline">\(-1\)</span>，累积<strong>前缀和</strong>，当前缀和为<span class="math inline">\(0\)</span>时，说明前缀就是一个平衡括号序列，可以分治。这种方法的前提是<strong>括号要正确匹配</strong>！</p></blockquote><p><strong>方法2</strong>：栈模拟，栈中无需记录括号类型（只会存放<strong>左括号</strong>），只需记录当前分数。遇到左括号时入栈分数0，遇到右括号时，弹出栈顶元素，<strong>计算新分数并累加</strong>到下一个栈顶元素（如果<strong>没有下一个栈顶元素</strong>则可以累加到答案）。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：计数，最终的分数只与 <code>()</code>有关，其他情况<strong>只改变其所在深度</strong>而间接改变结果，当深度为<code>dep</code> 时分数为<code>1&lt;&lt;dep</code>。每次<strong>遇到左括号时当前深度加一，遇到右括号时深度减一</strong>。每次<strong>遇到右括号就更新一次</strong>答案，时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="字符串解码-m">394. 字符串解码 (M)</h3><p><ahref="https://leetcode.cn/problems/decode-string/">题目描述</a>：给定形如<code>3[a2[b]]</code>的加密字符串，方括号内部的字符会重复前面的数字的次数，返回解码后的字符串。</p><p><strong>方法1</strong>：栈模拟，当数字时处理完进栈，遇到字母或<code>[</code> 时直接进栈，遇到 <code>]</code>时将栈顶的字母或字符串<strong>全部弹出到 vec 并翻转vec</strong>，并且此时 <code>[</code>的下一个元素一定是数字，将栈顶的字符串<strong>重复后再入栈</strong>，当遍历结束后栈中就是结果。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：双栈，数字存放在<strong>数字栈</strong>，字母存放在<strong>字母栈</strong>，遇到右括号时候弹出一个数字栈，字母栈弹到左括号为止。处理起来相对简单，其他步骤细节不变。</p><blockquote><p>注意最后栈中元素的正确顺序应当从栈底到栈顶，所以可以用 vector来模拟栈。此外，栈顶弹出时需要放到 vector，再翻转整个vector，则是考虑到有些字符串已经翻转过了，而单字母还没翻转过。</p></blockquote><p><strong>方法3</strong>：递归，定义一个全局指针 <spanclass="math inline">\(now=0\)</span>，递归结束条件是 <spanclass="math inline">\(now=n\)</span> 或者遇到<code>]</code>；递归返回时由于可能会有<strong>多个平衡串</strong>，返回结果是<spanclass="math inline">\(res+getSring()\)</span>，继续向后递归。时空复杂度均为<span class="math inline">\(O(n)\)</span>，<spanclass="math inline">\(n\)</span> 为解码后的串长。</p><blockquote><p>如果遇到<strong>纯字母</strong>，则可以将<strong>连续的一串字母</strong>取出，继续递归下一个元素；</p><p>如果遇到<strong>数字</strong>，则处理出数字串后，下一个是<code>[</code>，<strong>跳过</strong>并递归访问子串，子串结果保存在<strong>临时变量</strong>中，出递归后还要跳过<code>]</code>。最后将临时变量重复若干次，继续递归。</p></blockquote><h3 id="最长有效括号-h">32. 最长有效括号 (H)</h3><p><ahref="https://leetcode.cn/problems/longest-valid-parentheses/">题目描述</a>：给定只包含<code>(</code> 和 <code>)</code>的字符串，找出其中<strong>最长有效</strong>（正确匹配）<strong>连续子串</strong>的长度。</p><p><strong>方法1</strong>：一维 DP，定义 <spanclass="math inline">\(dp[i]\)</span> 表示以下标 <spanclass="math inline">\(i\)</span><strong>为结尾</strong>的最长有效括号长度，有效的子串一定以<code>)</code> 结尾，由于子串要求连续，<spanclass="math inline">\(dp[i]\)</span> 与 <spanclass="math inline">\(s[i-1]\)</span> 的取值有关。时空复杂度均为 <spanclass="math inline">\(O(n)\)</span> 。</p><blockquote><p>如果 <span class="math inline">\(s[i-1]\)</span> 取<code>(</code>，则恰好与 <span class="math inline">\(s[i]\)</span>配对，状态转移方程为： <span class="math display">\[dp\left[ i \right] =dp\left[ i-2 \right] +2\]</span> 如果 <span class="math inline">\(s[i-1]\)</span> 取<code>)</code>，则 <span class="math inline">\(dp[i-1]\)</span>可能是一个非零值，代表 <span class="math inline">\(s[i-dp[i-1]]\)</span>到 <span class="math inline">\(s[i-1]\)</span> 是一个有效的子串，则如果<span class="math inline">\(s[i-dp[i-1]-1]\)</span> 取<code>(</code>，就能和当前的 <span class="math inline">\(s[i]\)</span>配对。考虑到配对后的子串还会<strong>向前延伸</strong>，状态转移方程如下：<span class="math display">\[dp\left[ i \right] =dp\left[ i-1 \right] +dp\left[ i-dp\left[ i-1\right] -2 \right] +2\]</span></p></blockquote><p><strong>方法2</strong>：栈模拟，遇到 <code>(</code>时<strong>将下标入栈</strong>，遇到 <code>)</code>时弹出栈顶元素，此时「<strong>当前下标 -下一个栈顶元素</strong>」就是以当前下标<strong>为结尾</strong>的最长有效括号长度。如果栈为空，则说明当前的<code>)</code><strong>不可能匹配成功</strong>，将其入栈作为<strong>分界点</strong>。由此可以推断一开始时应该将<code>-1</code> 入栈，表示<strong>最左分界点</strong>。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>该方法巧妙之处在于，同一个栈顶元素可能被多次访问，而<strong>中间已经配对的子串会自动被考虑</strong>，类似DP 向前延申。</p></blockquote><p><strong>方法3</strong>：贪心 +计数，分别记录<strong>左右括号</strong>的个数，当 <spanclass="math inline">\(right\)</span> 计数器比 <spanclass="math inline">\(left\)</span>计数器大时，<strong>必不可能匹配</strong>，二者同时清零；当二者相等时，答案就是<strong>二者之和</strong>，且不需要清零。只正向遍历会漏掉<code>(()</code> 的情况，因此需要再<strong>反向遍历</strong>，时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h2 id="表达式配对">表达式配对</h2><h2 id="单调栈">单调栈</h2><p>针对 <strong>下/上</strong> 一个更 <strong>大/小</strong>值问题，<strong>正/反</strong> 向遍历数组，用一个单调递<strong>增/减</strong>的栈存储可能值，将不可能的值提前出栈，就可以把复杂度优化到 <spanclass="math inline">\(O(n)\)</span>。基本模板如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 正向遍历，单调递增栈，left[i] 存放左边第一个小于 nums[i] 的数</span><br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)&#123;<br>    <span class="hljs-keyword">while</span>(!s.<span class="hljs-built_in">empty</span>() &amp;&amp; s.<span class="hljs-built_in">top</span>() &gt;= nums[i]) s.<span class="hljs-built_in">pop</span>(); <span class="hljs-comment">// 这里的符号与要求的相反</span><br>    <span class="hljs-keyword">if</span>(s.<span class="hljs-built_in">empty</span>()) left[i] = <span class="hljs-number">-1</span>; <span class="hljs-comment">// 可以通过初始化省掉这句</span><br>    <span class="hljs-keyword">else</span> left[i] = s.<span class="hljs-built_in">top</span>();<br>    s.<span class="hljs-built_in">push</span>(nums[i]);<br>&#125;<br></code></pre></td></tr></table></figure><p>此外，单调栈还可以「反用」，不仅可以为「新入栈的元素」找到<strong>目标值</strong>，还可以为「出栈的元素」找到另一个方向遍历的<strong>反目标值</strong>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 正向遍历，left[i] 存放左边第一个小于 nums[i] 的数的下标</span><br><span class="hljs-comment">// right[s.top()] 存放右边第一个小于等于 nums[s.top()] 的数的下标，两个数组都初始化为 -1</span><br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)&#123;<br>    <span class="hljs-keyword">while</span>(!s.<span class="hljs-built_in">empty</span>() &amp;&amp; nums[s.<span class="hljs-built_in">top</span>()] &gt;= nums[i])&#123;<br>        right[s.<span class="hljs-built_in">top</span>()] = i; <br>        s.<span class="hljs-built_in">pop</span>();<br>    &#125;<br>    <span class="hljs-keyword">if</span>(!s.<span class="hljs-built_in">empty</span>()) left[i] = s.<span class="hljs-built_in">top</span>();<br>    s.<span class="hljs-built_in">push</span>(i); <span class="hljs-comment">// 此时 left right 还有 s 中都存放的是下标</span><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="每日温度-m">739. 每日温度 (M)</h3><p><ahref="https://leetcode.cn/problems/final-prices-with-a-special-discount-in-a-shop/">题目描述</a>：给一个温度数组，求对于每一天，<strong>下一个更高温度</strong>出现在几天后，如果没有更高温度则返回零。</p><p><strong>方法1</strong>：暴力，两层循环，内循环正向遍历直到找到下一个更大元素，复杂度<span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：单调递减栈，正向遍历数组，每一个新元素都会<strong>弹出栈顶比它小的所有元素</strong>（因为其存在会阻断右边所有元素被选中的可能），所有弹出元素都得到答案（<strong>反用</strong>）。然后将新元素入栈。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><h3 id="子数组的最小值之和-m">907. 子数组的最小值之和 (M)</h3><p><ahref="https://leetcode.cn/problems/sum-of-subarray-minimums/">题目描述</a>：给定一个整数数组<code>arr</code>，找到 <code>min(b)</code> 的总和，其中 <code>b</code>的范围为 <code>arr</code> 的<strong>每个连续子数组</strong>。</p><p><strong>方法1</strong>：<strong>两个</strong>单调栈 +乘法原理，用两个<strong>单调递增栈</strong>预处理得到 <spanclass="math inline">\(arr[i]\)</span> 左边第一个 <spanclass="math inline">\(&lt;arr[i]\)</span> 的下标，右边第一个 <spanclass="math inline">\(\leqslant arr[i]\)</span>的下标，得到<strong>左开右闭</strong>区间，两边相乘计算贡献。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：<strong>正反单调栈</strong>，在第一个栈预处理<strong>左边界</strong>时，栈顶元素<span class="math inline">\(\geqslant arr[i]\)</span> 需要<code>pop</code>，此时 <span class="math inline">\(i\)</span>恰好是栈顶元素的<strong>右边界</strong>，可以一起标记。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：进一步地，由于<strong>栈顶下面的元素正好也是栈顶元素的左边界</strong>，所以甚至连<span class="math inline">\(left\)</span> 和 <spanclass="math inline">\(right\)</span>数组都不需要，直接在<strong>出栈的时候计算贡献</strong>，为简化代码逻辑，可以在遍历前在<span class="math inline">\(arr\)</span> 末尾和栈顶分别加一个 <spanclass="math inline">\(−1\)</span>，当作哨兵。空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="接雨水-h">42. 接雨水 (H)</h3><p><ahref="https://leetcode.cn/problems/trapping-rain-water/">题目描述</a>：给定<code>n</code> 个非负整数表示每个宽度为 <code>1</code>的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。</p><p><strong>方法1</strong>：暴力，对于每个坐标，其能接的水量取决于<strong>左右两边最高的柱子中较矮者</strong>，两层循环遍历，复杂度<span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：DP预处理最大值，两次扫描，记录<strong>每个柱子</strong>的<strong>左右两边最大值</strong>，再扫描累计，时空复杂度均为<spanclass="math inline">\(O(n)\)</span>。该方法可以简单优化：从左到右的预处理可以和第三次扫描<strong>合并</strong>，<strong>用一个变量存储</strong>左边最大值，节省一个数组。</p><p><strong>方法3</strong>：<strong>单调递减栈</strong> +模拟，每遍历到一个柱子，如果比栈顶元素<strong>更小则入栈</strong>，如果更大则说明前面的柱子<strong>可以形成水洼</strong>，则依次将所有较小数弹出，并计算<strong>和新柱子的差值</strong>，再把新柱子入栈。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法4</strong>：贪心 +对撞双指针，双指针从两端开始遍历，再用两个变量存储左右两边最大值，每次<strong>较小者向中间收窄一格</strong>，同时计算出当前柱子能接的水量。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>方法5</strong>：分治 +面积法，<strong>最高柱子将全局分为两边</strong>，两边各有各自的次高柱子，<strong>次高柱子与最高柱子中间的区域</strong>显然取值就是「次高柱子- 当前柱子」，同理以此类推到两端。时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。具体计算方法参考 <ahref="https://hwcoder.top/LeetCode-Array">力扣刷题笔记 #1 数组</a>。</p><p><strong>坑点</strong>：最左边、最右边两个柱子<strong>不可能接到水</strong>，要跳过，本题的所有方法可以直接从<code>1</code> 遍历到 <code>n-2</code>。</p><h3 id="柱状图中最大的矩形-h">84. 柱状图中最大的矩形 (H)</h3><p><ahref="https://leetcode.cn/problems/largest-rectangle-in-histogram/">题目描述</a>：给定非负整数数组，表示柱状图中各个柱子的高度。柱子<strong>彼此相邻</strong>且宽度为<spanclass="math inline">\(1\)</span>，求图中<strong>最大矩形</strong>的面积。</p><p><strong>方法1</strong>：暴力枚举宽度，用两层循环表示左、右边界，在内层循环维护区间内最大值，求出面积，时间复杂度<span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：暴力枚举高度，<strong>枚举每个柱子作为最低高度</strong>，找出两侧<strong>第一个小于</strong>该柱子的位置作为边界，时间复杂度<span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法3</strong>：正反单调栈，改进方法2，用单调栈预处理两侧第一个小于的位置，时空复杂度均为 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="最大矩形-h">85. 最大矩形 (H)</h3><p><ahref="https://leetcode.cn/problems/maximal-rectangle/">题目描述</a>：给定一个仅包含0 和 1 的 <span class="math inline">\(m\times n\)</span><strong>二维矩阵</strong>，找出只包含 <code>1</code>的<strong>最大矩形</strong>，并返回其面积。</p><p><strong>方法1</strong>：暴力枚举 +前缀和，枚举矩形的左上角和右下角，并判断矩形内是否全为1（<strong>区间和等于矩形大小</strong>），复杂度 <spanclass="math inline">\(O(m^2n^2)\)</span>。</p><p><strong>方法2</strong>：转化成柱状图，预处理出矩阵<strong>每个位置的上方有多少个连续的1</strong>，再取出每一行，将该行视为<strong>柱状图</strong>，计算图中最大矩形的面积，时间复杂度<span class="math inline">\(O(mn^2)\)</span>。</p><p><strong>方法3</strong>：柱状图 + 单调栈优化，时间复杂度 <spanclass="math inline">\(O(mn)\)</span>。</p><h3 id="下一个更大元素-iv-h">2453. 下一个更大元素 IV (H)</h3><p><ahref="https://leetcode.cn/problems/next-greater-element-iv/">题目描述</a>：给定数组<code>nums</code>，求其中每个元素的<strong>下下个更大元素</strong>（两个数都大于<span class="math inline">\(nums[i]\)</span>，不需要递增）。</p><p><strong>方法1</strong>：两个<strong>反单调栈</strong>，正向遍历，栈<span class="math inline">\(s\)</span> 存放第一轮，栈 <spanclass="math inline">\(t\)</span>存放第二轮，都是单调递减栈，每次出栈代表找到下一个更大元素。每遍历到一个新元素先将<span class="math inline">\(t\)</span>中的数较小数弹出（这些<strong>弹出的元素都得到答案</strong>），再将新元素放入<span class="math inline">\(s\)</span> 中，<spanclass="math inline">\(s\)</span> 弹出的元素按照原顺序放入 <spanclass="math inline">\(t\)</span>。时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><blockquote><p>不能用正向单调栈的原因：在栈 <span class="math inline">\(s\)</span>中找到下一个更大元素后，下下个更大元素可能在 <spanclass="math inline">\(s\)</span> 栈剩余元素中，也可能在 <spanclass="math inline">\(t\)</span>栈中。而使用反向单调栈，元素每次出栈都代表找到了下一个更大元素。</p><p>这题的巧妙之处在于：每新来一个 <spanclass="math inline">\(x\)</span>，第一个 <code>while</code> 使得 <spanclass="math inline">\(t\)</span> 中没有比 <spanclass="math inline">\(x\)</span> 小的数，所以可以让 <spanclass="math inline">\(s\)</span> 中比 <spanclass="math inline">\(x\)</span> 小的数直接加在 <spanclass="math inline">\(t\)</span> 后面，同时维持 <spanclass="math inline">\(t\)</span> 单调递减。而 <spanclass="math inline">\(s\)</span> 又是递减的，所以第二个<code>while</code> 直接截取小于 <span class="math inline">\(x\)</span>的后缀，不改变顺序就能接在 <span class="math inline">\(t\)</span>后面。</p></blockquote><p><strong>方法2</strong>：反单调栈 +优先队列，反单调栈用法同上，优先队列存放栈弹出的元素（已经找到了第一个更大元素的），每遍历到一个新元素，如果这个数<strong>比队首元素大</strong>，则队首元素就<strong>得到答案</strong>。该方法实现比较简单，时间复杂度<span class="math inline">\(O(n\log n)\)</span>。</p>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #10 搜索&amp;剪枝</title>
    <link href="/LeetCode-Search.html"/>
    <url>/LeetCode-Search.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「搜索&amp;剪枝」类型题中的：搜索回溯、优化剪枝、A*搜索、启发式搜索等。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><h2 id="搜索回溯">搜索回溯</h2><p>暴力搜索 +回溯，常见于「<strong>返回所有可能组合、方案、排列</strong>」题目，由于要得到所有可能，就必须暴力搜索，最多加上一定的优化剪枝。</p><p>回溯算法建立在 DFS基础之上的，但不同的是在搜索过程中，达到结束条件后，恢复状态，回溯上一层，再次搜索。因此回溯算法与DFS 的区别就是<strong>有无状态重置</strong>。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c++">function&lt;<span class="hljs-built_in"><span class="hljs-keyword">void</span></span>(<span class="hljs-keyword">int</span>)&gt; dfs = [&amp;](<span class="hljs-keyword">int</span> now)&#123;<br>    <span class="hljs-keyword">if</span>(now == n)&#123;<br>        <span class="hljs-comment">// 生成答案，回溯</span><br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)&#123;<br>        <span class="hljs-keyword">if</span>(vis) <span class="hljs-keyword">continue</span>; <span class="hljs-comment">// 剪枝</span><br>        <span class="hljs-comment">// 此处要进入下一个状态</span><br>        <span class="hljs-built_in">dfs</span>(now + <span class="hljs-number">1</span>); <span class="hljs-comment">// 搜索</span><br>        <span class="hljs-comment">// 此处要恢复上一个状态</span><br>    &#125;<br>&#125;;<br><span class="hljs-built_in">dfs</span>(<span class="hljs-number">0</span>);<br></code></pre></td></tr></table></figure><h3 id="电话号码的字母组合-m">17. 电话号码的字母组合 (M)</h3><p><ahref="https://leetcode.cn/problems/letter-combinations-of-a-phone-number/">题目描述</a>：给定一个仅包含数字<code>2-9</code>的字符串，返回所有它能表示的<strong>九键字母组合</strong>。答案可以按<strong>任意顺序</strong>返回。</p><p><strong>方法1</strong>：DFS 回溯，递归时用参数 <spanclass="math inline">\(now\)</span> 表示遍历到第几个数字；当 <spanclass="math inline">\(now=n\)</span>时递归结束，加入答案数组。时间复杂度 <spanclass="math inline">\(O(3^n)\)</span>，空间复杂度为递归栈的深度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：BFS +队列，每遍历到一个数字，就把队列中所有元素依次弹出，尾部<strong>加上不同字母后再压回队列</strong>。最后整个队列存放的就是答案。</p><h3 id="括号生成-m">22. 括号生成 (M)</h3><p><ahref="https://leetcode.cn/problems/generate-parentheses/">题目描述</a>：数字<code>n</code>代表目标字符串中<strong>合法括号</strong>的对数，生成<strong>所有可能的</strong>并且合法的括号组合字符串。</p><p><strong>方法1</strong>：DFS 回溯，定义用一个 <spanclass="math inline">\(cur\)</span> 全局字符串，每次 DFS 时如果 <spanclass="math inline">\(左括号数&lt;n\)</span>，则在<strong>末尾添加</strong>一个<code>(</code> 继续 DFS；如果 <spanclass="math inline">\(右括号数&lt;左括号数\)</span>，则在<strong>末尾添加</strong>一个<code>)</code> 继续 DFS，当 <spanclass="math inline">\(cur.size()=2n\)</span> 时结束。</p><blockquote><p>时刻保持 <span class="math inline">\(右括号数 \leqslant 左括号数\leqslantn\)</span>，则能够保证<strong>左括号先放置，则序列一定合法</strong>。</p></blockquote><p><strong>方法2</strong>：递归调用自身，递归 <code>f(n-1)</code>得到对数 <span class="math inline">\(n-1\)</span>的答案，并在<strong>每一个位置</strong>插入<code>()</code>，去重后得到答案。</p><blockquote><p>最终答案包含元素个数可以证明是第 <spanclass="math inline">\(n\)</span> 个<strong>卡特兰数</strong> <spanclass="math inline">\(\frac{1}{n+1}C^{n}_{2n}\)</span>，渐进于 <spanclass="math inline">\(\frac{4^n}{n\sqrt{n}}\)</span>。而在回溯过程中，每个字符串需要<span class="math inline">\(O(n)\)</span> 的时间插入新的括号，并 <spanclass="math inline">\(O(1)\)</span> 移动到新数组，所以总时间复杂度为<span class="math inline">\(O(\)</span>$ )$。</p><p>而总共的递归层数为 <spanclass="math inline">\(n\)</span>，每层都需要一个与答案数组同样数量级的临时数组，所以总空间复杂度也为<span class="math inline">\(O(\)</span>$ )$。</p></blockquote><p><strong>方法3</strong>：记忆化 DFS，答案序列的第一个字符一定是<code>(</code>，与之对应的 <code>)</code>可能出现在<strong>右侧任何一个位置</strong>，构成<code>(A)B</code>。枚举 <code>)</code> 的位置 <spanclass="math inline">\(2i+1\)</span>，则 <code>A</code> 就是<code>f(i)</code>，<code>B</code> 就是<code>f(n-i-1)</code>，<strong>记忆化存储</strong>每个结果，遍历<strong>拼接</strong>。</p><h3 id="组合总和-m">39. 组合总和 (M)</h3><p><ahref="https://leetcode.cn/problems/combination-sum/">题目描述</a>：给定一个<strong>无重复整数</strong>数组，和一个目标整数，找出数组中所有<strong>和为目标数</strong>的元素组合，同一个数字可以<strong>无限次重复</strong>选中，如果至少一个数字的被选数量不同，则两种组合是不同的。</p><p><strong>方法1</strong>：DFS回溯，用一个<strong>临时序列</strong>记录。递归时记录当前<strong>距离目标的差值</strong>和当前<strong>可选的数字起点</strong>，每次递归可以选择跳过当前数并<strong>将起点向前移动</strong>，也可以选择当前数且<strong>不移动起点</strong>，递归回溯后要将当前数<strong>移出临时序列</strong>。当起点遍历完时递归结束，当差值为零时得到组合。</p><p><strong>方法2</strong>：DFS +剪枝，先将数组<strong>从小到大排序</strong>，则当一个数字选完可能会溢出时，其后面的数字也不会再选，提前结束。</p><h3 id="全排列-m">46. 全排列 (M)</h3><p><ahref="https://leetcode.cn/problems/permutations/">题目描述</a>：给定一个<strong>无重复整数</strong>数组，返回其所有可能的全排列，可以按<strong>任意顺序</strong>返回。</p><p><strong>方法1</strong>：使用「31.下一个排列」中的方法，每次都「<strong>贪心</strong>」找出下降点，时间复杂度<span class="math inline">\(O(n\times n!)\)</span>。</p><p><strong>方法2</strong>：DFS回溯，用一个<strong>临时序列</strong>记录，由于不按顺序，每次可从<strong>剩余数中放入一个数</strong>继续递归，递归回溯后要将当前数<strong>移出临时序列</strong>。时间复杂度<span class="math inline">\(O(n\times n!)\)</span>。</p><blockquote><p>记录剩余数的方法有很多：</p><ol type="1"><li>枚举下一个数，同时比对其是否出现在临时序列（无重复的好处）；</li><li>用一个 <span class="math inline">\(vis\)</span>数组记录临时序列中的数，作为参数传递，进一步地，可以用<strong>状态压缩</strong>；</li><li>使用一个分割点，左侧的数作为已确定的，右侧的数是将要访问的，每次从右侧swap 一个数到分割点的位置，继续递归，递归回溯后将其 swap 回原位。</li></ol></blockquote><h3 id="全排列-ii-m">47. 全排列 II (M)</h3><p><ahref="https://leetcode.cn/problems/permutations-ii/">题目描述</a>：给定一个<strong>有重复整数</strong>数组，返回其所有<strong>不重复</strong>的全排列，可以按<strong>任意顺序</strong>返回。</p><p><strong>方法1</strong>：DFS回溯，用一个<strong>临时序列</strong>记录，每次从<strong>剩余数中放入一个数</strong>继续递归，但是还需要判断相同数不能多次放置在当前位置。先排序将相同数放到一起，再用以下条件判断：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">if</span>(vis[i] || (i &gt; <span class="hljs-number">0</span> &amp;&amp; nums[i] == nums[i - <span class="hljs-number">1</span>] &amp;&amp; !vis[i - <span class="hljs-number">1</span>])) <span class="hljs-keyword">continue</span>;<br><span class="hljs-comment">// !vis[i - 1] 用于限制两个相邻的重复数字的访问顺序</span><br><span class="hljs-comment">// 例如 1 1，只有当前面的 1 访问过才会访问后面的 1，确保不会出现两次 1 1</span><br></code></pre></td></tr></table></figure><p><strong>坑点</strong>：本题不能用上一题记录剩余数的方法1，因为<strong>有重复</strong>；不能用方法 3，因为 swap会<strong>打乱右侧数的顺序</strong>，使相同数不再放到一起。</p><h3 id="子集-m">78. 子集 (M)</h3><p><ahref="https://leetcode.cn/problems/subsets/">题目描述</a>：给定一个整数数组，数组中的元素<strong>互不相同</strong>。返回该数组所有可能的子集（幂集）。</p><p><strong>方法1</strong>：枚举，每个数有两种选择，总共有 <spanclass="math inline">\(2^n\)</span>个子集，用二进制表示每个子集，则范围是 <span class="math inline">\([0,2^n-1]\)</span>。枚举每个数字并生成对应的子集，时间复杂度 <spanclass="math inline">\(O(n \cdot 2^n)\)</span>。</p><p><strong>方法2</strong>：DFS回溯，用一个<strong>临时序列</strong>记录，每次将 <spanclass="math inline">\(now\)</span>指向的数「放入或不放入」后继续递归，递归回溯后要将放入的数<strong>移出临时序列</strong>。时间复杂度<span class="math inline">\(O(n \cdot 2^n)\)</span>。</p><p><strong>方法3</strong>：线性 DP，<spanclass="math inline">\(dp[i]\)</span> 表示前 <spanclass="math inline">\(i\)</span> 个数的子集，初始 <spanclass="math inline">\(dp[0]\)</span>为空集，每遍历到一个数就<strong>将前面的所有子集拼接上这个数</strong>，然后放入<span class="math inline">\(ans\)</span>。时间复杂度 <spanclass="math inline">\(O(n \cdot 2^n)\)</span>。 <spanclass="math display">\[dp[i]=dp\left[ i-1 \right] +dp\left[ i-1 \right] \oplus nums\left[ i\right]\]</span></p><h2 id="优化剪枝">优化剪枝</h2><p>有的搜索问题涉及排列组合、卡特兰数等内容，暴力复杂度达到 <spanclass="math inline">\(O(n!)\)</span> 或 <spanclass="math inline">\(O(k^n)\)</span>，必须使用优化、剪枝技巧。二者有一定的相似性。</p><p>所谓<strong>优化</strong>，就是降低单轮判断的复杂度，常用的优化技巧有：</p><ul><li>记忆化，通过记录<strong>出现过</strong>的子状态，来快速判断当前状态的有效性；</li><li>辅助数组，以空间换时间，通常记录影响当前局面的<strong>历史信息</strong>；</li><li>编排<strong>分支顺序</strong>，先解决相对简单的子问题，使其他尚未解决的问题得到简化；</li><li>状态压缩，在小范围数据中使用，加快<strong>多维数组</strong>的访问效率、或减少辅助数组的<strong>空间占用</strong>。</li></ul><p>所谓<strong>剪枝</strong>，就是提早退出某些搜索分支，常用的剪枝技巧有：</p><ul><li>记忆化，对于<strong>出现过</strong>的子状态，标记 <spanclass="math inline">\(vis\)</span>，防止多次访问；</li><li>可行性判断，根据题意提前退出<strong>不可能的分支</strong>（无法转向目标状态），避免无用的搜索；</li><li>最优性判断，每次搜索前判断当前解<strong>是否可能超过当前最优解</strong>，避免无用的搜索。</li></ul><h3 id="n-皇后-h">51. N 皇后 (H)</h3><p><ahref="https://leetcode.cn/problems/n-queens/">题目描述</a>：国际象中，皇后可以攻击与之处在<strong>同一行、同一列、同一斜线</strong>上的棋子。将<span class="math inline">\(N\)</span> 个皇后放在 <spanclass="math inline">\(N\times N\)</span>的棋盘上，使其彼此之间不能相互攻击，返回所有解决方案。</p><p><strong>方法1</strong>：暴力，按行搜索，第一行有 <spanclass="math inline">\(N\)</span> 种选择，第二行有 <spanclass="math inline">\(N-1\)</span>种选择，以此类推。对于每个选择，逐个判断和之前放置过的皇后是否冲突，如果<strong>不冲突则搜索下一行</strong>。时间复杂度<span class="math inline">\(O(N\times N!)\)</span>。</p><p><strong>方法2</strong>：暴力 +<strong>辅助数组</strong>，使用额外的三个<strong>数组或哈希集合</strong>标记<strong>该列、主对角线、副对角线有无皇后</strong>，每次可以<span class="math inline">\(O(1)\)</span> 判断冲突，时间复杂度 <spanclass="math inline">\(O(N!)\)</span>，空间复杂度 <spanclass="math inline">\(O(N)\)</span>。</p><blockquote><p>对角线可以通过存<strong>行下标与列下标之差</strong>、<strong>行下标与列下标之和</strong>来表示，注意数组下标不能是负数，需要偏移<span class="math inline">\(N\)</span>，但是哈希集合不需要。</p></blockquote><p><strong>方法3</strong>：暴力 + 辅助数组 +<strong>状态压缩</strong>，用三个整数的<strong>二进制位</strong>实现标志，每次先三个数进行<strong>或运算</strong>，得到0 的位置就是<strong>可以放置的位置</strong>，时间复杂度 <spanclass="math inline">\(O(N!)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>（不考虑递归的空间占用）。</p><blockquote><p>二进制位的标志 0 表示的是「该位可以放置」，而不是方法 2中的「列、主对角线、副对角线」，因此在搜索时，下一行的应为<code>dfs(n, row+1, col|pos, (ld|pos)&lt;&lt;1, (rd|pos)&gt;&gt;1);</code>，最后两个参数表示两条对角线带来的影响会左移、右移一位。</p><p>求 pos 需要用到位运算技巧 <code>n &amp; (-n)</code> 和<code>n &amp; (n-1)</code>。</p></blockquote><h3 id="删除无效的括号-h">301. 删除无效的括号 (H)</h3><p><ahref="">题目描述</a>：给定一个由括号和字母组成的字符串，删除<strong>最小数量</strong>的无效括号，使得字符串有效。返回<strong>所有可能</strong>的结果。</p><p><strong>方法1</strong>：DFS回溯，合法的方案左右括号数量相等，记左括号为 <spanclass="math inline">\(+1\)</span>，右括号为 <spanclass="math inline">\(-1\)</span>，<strong>最终得分</strong>必然为 <spanclass="math inline">\(0\)</span>。DFS正向遍历字符串，考虑每个元素<strong>是否放入</strong>子串，当遍历结束时，<strong>判断最终得分</strong>是否为<spanclass="math inline">\(0\)</span>，如果合法，则考虑将其加入<strong>集合去重</strong>。由于要求删除最小数量，剩余的字符串应该越长越好，因此维护一个<spanclass="math inline">\(maxlen\)</span>，<strong>仅当子串长度大于等于</strong><span class="math inline">\(maxlen\)</span>时将其放入，并且<strong>只保留长度等于</strong> <spanclass="math inline">\(maxlen\)</span> 的结果。时间复杂度 <spanclass="math inline">\(O(n\cdot 2^n)\)</span>。</p><p><strong>方法2</strong>：分数剪枝，在 DFS 传入参数 <spanclass="math inline">\(score\)</span>，则根据 <spanclass="math inline">\(score\)</span>的取值范围可以进行剪枝，提前排除不可能的子串。</p><blockquote><p>在搜索过程中，两种情况可以<strong>提前剪枝</strong>：</p><ul><li>得分为负数，即在子串前缀中右括号数量大于左括号，此时不可能是合法方案。</li><li>得分超过上限，这里的上限指的是<code>min(左括号的数量, 右括号的数量)</code>，上限只有在「<strong>合法左括号先全部出现在左边，右边全是右括号</strong>」时才出现。因此当子串前缀中出现大量左括号时，不可能是合法方案。</li></ul><p>于是，我们可以得到分数的限制范围 <span class="math inline">\([0,limit]\)</span>，<span class="math inline">\(limit\)</span>可以预处理。</p></blockquote><p><strong>方法3</strong>：预处理 <spanclass="math inline">\(maxlen\)</span>剪枝，最终目标串的长度可以提前确定为 <spanclass="math inline">\(n-l-r\)</span>，<spanclass="math inline">\(l\)</span> 为<strong>失配左括号数</strong>，<spanclass="math inline">\(r\)</span>为失<strong>配右括号数</strong>。正向遍历到左括号时 <spanclass="math inline">\(l++\)</span>，遍历到右括号时 <spanclass="math inline">\(l--\)</span>，如果 <spanclass="math inline">\(l\)</span> 为零，则 <spanclass="math inline">\(r++\)</span> 表示右括号失配。</p><h3 id="todo-1240.-铺瓷砖-h">Todo 1240. 铺瓷砖 (H)</h3><h2 id="a-搜索">A* 搜索</h2><h2 id="启发式搜索">启发式搜索</h2>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #09 数学</title>
    <link href="/LeetCode-Math.html"/>
    <url>/LeetCode-Math.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「数学」类型题中的：乘法原理、乘法逆元、素数筛、组合数公式、不等式等。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><h2 id="乘法原理">乘法原理</h2><p>通常场景为「<strong>所有子数组</strong>的 XX之和」、「<strong>所有子串</strong>的 XX之和」，并且会有<strong>一个元素被多个子数组/子串统计多次</strong>的情况。枚举每个子数组必定超时，正确的做法是枚举<strong>每个元素作为XX的最远边界</strong>，再利用乘法原理计算该元素能作为多少个「<strong>子数组的XX</strong>」，即每个元素的<strong>贡献度</strong>。 <spanclass="math display">\[\sum_{i=0}^{n-1} \operatorname{arr}[i] \times \operatorname{left}[i]\times \operatorname{right}[i]\]</span>计算最远边界时通常采用<strong>单调栈</strong>预处理，并记录下标距离。如果原数组中有重复元素，为了防止<strong>包含重复元素的子数组被重复考虑</strong>，需要修改边界定义，采用<strong>不对称</strong>区间，即一侧可以跨越<span class="math inline">\(x\)</span>，另一侧不可以。</p><h3 id="统计子串中的唯一字符-h">828. 统计子串中的唯一字符 (H)</h3><p><ahref="https://leetcode.cn/problems/count-unique-characters-of-all-substrings-of-a-given-string/">题目描述</a>：定义函数<code>countUniqueChars(t)</code> 来统计字符串 <code>t</code>中的唯一字符个数，给定字符串<code>s</code>，要求返回<strong>所有连续子串</strong>的函数值之和。</p><p><strong>方法1</strong>：乘法原理，每个元素产生贡献<strong>仅当其只出现一次</strong>时，所以两侧边界就是该元素本身，预处理得到每个元素<strong>两侧第一个相同元素</strong>，再遍历一次算出总和。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：空间优化，只需记录每个字母<strong>近两次出现的位置</strong>，当遇到第三个次时即可算中间的字母，空间复杂度<span class="math inline">\(O(1)\)</span>。</p><h3 id="子数组的最小值之和-m">907. 子数组的最小值之和 (M)</h3><p><ahref="https://leetcode.cn/problems/sum-of-subarray-minimums/">题目描述</a>：给定一个整数数组<code>arr</code>，找到 <code>min(b)</code> 的总和，其中 <code>b</code>的范围为 <code>arr</code> 的<strong>每个连续子数组</strong>。</p><p><strong>方法1</strong>：单调栈 +乘法原理，用两个<strong>单调递增栈</strong>预处理得到 <spanclass="math inline">\(arr[i]\)</span> 左边第一个 <spanclass="math inline">\(&lt;arr[i]\)</span> 的下标，右边第一个 <spanclass="math inline">\(\leqslant arr[i]\)</span>的下标，得到<strong>不对称</strong>区间，两边相乘计算贡献。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：一个单调栈做法，在第一个栈预处理<strong>左边界</strong>时，栈顶元素<span class="math inline">\(\geqslant arr[i]\)</span> 需要<code>pop</code>，此时 <span class="math inline">\(i\)</span>恰好是栈顶元素的<strong>右边界</strong>，可以一起标记。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：进一步地，由于<strong>栈顶下面的元素正好也是栈顶元素的左边界</strong>，所以甚至连<span class="math inline">\(left\)</span> 和 <spanclass="math inline">\(right\)</span>数组都不需要，直接在栈的时候计算贡献，为简化代码逻辑，可以在遍历前往<span class="math inline">\(arr\)</span> 末尾和栈顶分别加一个 <spanclass="math inline">\(−1\)</span>，当作哨兵。</p><h3 id="子数组最小乘积的最大值-m">1856. 子数组最小乘积的最大值 (M)</h3><p><ahref="https://leetcode.cn/problems/maximum-subarray-min-product/">题目描述</a>：一个数组的<strong>最小乘积</strong>定义为这个<strong>数组中最小值</strong>乘以<strong>数组的和</strong>，给定一个<strong>正整数</strong>数组，求子数组中最小乘积的最大值。</p><p><strong>方法</strong>：单调栈 + 前缀和 +乘法原理，单调栈处理得到不对称区间，预处理前缀和，每个数算出最小乘积，复杂度<span class="math inline">\(O(n)\)</span>。</p><p><strong>坑点</strong>：题目明确「保证最小乘积的最大值在<strong>不取余</strong>的情况下可以用<strong>64 位有符号整数</strong>保存」，取 <code>max</code>前千万不能取模，必须在<strong>返回值处取模</strong>，否则答案会出错。</p><h3 id="thuoj-最强战力-h">THUOJ 最强战力 (H)</h3><p><strong>题目描述</strong>：有一个 HERO数组，存放每一名英雄的战力值，从中取出<strong>连续的一组</strong>英雄组成一个小队，小队的战力值定义为「小队中<strong>最弱战力</strong><span class="math inline">\(\times\)</span>小队<strong>总战力</strong>」，计算所有可能小队的小队战力值<strong>之和</strong>。</p><p><strong>方法</strong>：单调栈 + 乘法原理，一个单调栈处理得到 <spanclass="math inline">\(hero[i]\)</span> 左边第一个 <spanclass="math inline">\(&lt;hero[i]\)</span> 的下标，右边第一个 <spanclass="math inline">\(\leqslant hero[i]\)</span>的下标，得到不对称区间。记一个元素会被乘 <spanclass="math inline">\(times[i]\)</span>次，遍历<strong>每一个元素对周围元素</strong>的 <spanclass="math inline">\(times\)</span> 影响，时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。</p><blockquote><p>对于每个元素 <spanclass="math inline">\(hero[i]\)</span>，其左边的元素 <spanclass="math inline">\(hero[x]\)</span> 会被 <spanclass="math inline">\(hero[i]\)</span> 影响 <spanclass="math inline">\(hero[i]\times (l[i]-i)\times (x-r[i])\)</span>次，右边的元素同理。根据两边是否有元素，需要分四种情况讨论。</p><p>最后算出每个元素被两侧元素的累计影响（包括自身对自身的影响）的 <spanclass="math inline">\(times\)</span>，则其贡献就是 <spanclass="math inline">\(hero[i]\times times[i]\)</span>。</p></blockquote><h3 id="子数组范围和-m">2104. 子数组范围和 (M)</h3><p><ahref="https://leetcode.cn/problems/sum-of-subarray-ranges/">题目描述</a>：一个整数数组，定义<strong>子数组</strong>的<strong>范围</strong>是子数组中<strong>最大元素和最小元素</strong>的差值，求所有子数组的范围和。</p><p><strong>方法1</strong>：暴力，两层循环，外循环枚举左端点，内循环枚举右端点并<strong>DP 处理前缀最值</strong>，时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：乘法原理 +交换律，要求<strong>「最大元素减去最小元素」之和</strong>，等价于求「<strong>最大元素之和</strong>」减去「<strong>最小元素之和</strong>」，用两个单调栈分别计算两个子问题，最后求差值即可。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><h3 id="子序列宽度之和-h">891. 子序列宽度之和 (H)</h3><p><ahref="https://leetcode.cn/problems/sum-of-subsequence-widths/">题目描述</a>：一个整数数组，定义<strong>子序列</strong>的<strong>宽度</strong>是子序列中<strong>最大元素和最小元素</strong>的差值，求所有子序列的宽度和。</p><p><strong>方法1</strong>：排序 +数学，由于子序列的<strong>顺序对结果不会产生影响</strong>，所以可以对数组先排序。此时每个元素都是其<strong>左边的元素的最大值，右边元素的最小值</strong>，可以算出每个数作为最大值的次数和最小值的次数（贡献）。时间复杂度<span class="math inline">\(O(n \log n)\)</span>。</p><blockquote><p>以排序后的数字 <span class="math inline">\(nums[i]\)</span>为例，其左边有 <span class="math inline">\([0,i-1]\)</span> 共 <spanclass="math inline">\(i\)</span> 个元素，每个元素按照是否出现，总共有<span class="math inline">\(2^i\)</span> 种子序列；右边有 <spanclass="math inline">\([i+1, n-1]\)</span> 共 <spanclass="math inline">\(n-i-1\)</span> 个元素，总共有 <spanclass="math inline">\(2^{n-i-1}\)</span> 种子序列。需要 <spanclass="math inline">\(O(n)\)</span> 预处理 <spanclass="math inline">\(2\)</span> 的所有幂次才可直接求和。</p></blockquote><p><strong>方法2</strong>：递推处理幂次，方法 1 需要额外存储 <spanclass="math inline">\(2\)</span> 的每次，但其实可以直接递推计算 <spanclass="math inline">\(2^i\)</span> 和 <spanclass="math inline">\(2^{n-i-1}\)</span>，后者需要用到 <spanclass="math inline">\(2\)</span> 关于 <spanclass="math inline">\(10^9+7\)</span>的<strong>逆元</strong>，空间复杂度优化到 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>方法3</strong>：另一种思路，计算 <spanclass="math inline">\(2^i\)</span> 对答案的贡献度，即 <spanclass="math inline">\((nums[i]-nums[n-1-i])\times2^i\)</span>，空间复杂度也是 <spanclass="math inline">\(O(1)\)</span>。</p><h2 id="乘法逆元">乘法逆元</h2><p>定义：对于一个实数 <spanclass="math inline">\(a\)</span>，如果存在一个 <spanclass="math inline">\(x\)</span> 使得 <span class="math inline">\(ax=1\textrm{ mod } p\)</span>，则称 <span class="math inline">\(x\)</span>是 <span class="math inline">\(a\)</span> 关于 <spanclass="math inline">\(p\)</span> 的逆元。此处要求 <spanclass="math inline">\(p\)</span> 与 <spanclass="math inline">\(a\)</span> <strong>互质</strong>，即满足 <spanclass="math inline">\(\mathrm{gcd}(a,p)=1\)</span>。</p><p>常见问法：返回分数 <span class="math inline">\(\frac{a}{b}\)</span>关于 <span class="math inline">\(p\)</span>的模，由于<strong>除法不满足模不变性</strong>，此时需要用 <spanclass="math inline">\(a\)</span> 乘以 <spanclass="math inline">\(b\)</span> 关于 <spanclass="math inline">\(p\)</span> 的逆元。</p><p>最简单的方法，由<strong>费马小定理</strong>得到 <spanclass="math inline">\(a\)</span> 关于 <spanclass="math inline">\(p\)</span> 的逆元为 <spanclass="math inline">\(a^{p-2}\)</span>，要求 <spanclass="math inline">\(p\)</span> 是质数，结合快速幂 <spanclass="math inline">\(O(\log n)\)</span> 模板：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 非递归快速幂，带 mod 运算</span><br><span class="hljs-function">ll <span class="hljs-title">qpowmod</span><span class="hljs-params">(ll a, ll n, ll mod)</span> </span>&#123;<br>ll ans = <span class="hljs-number">1</span>;<br><span class="hljs-keyword">while</span>(n)&#123;<br><span class="hljs-keyword">if</span>(n &amp; <span class="hljs-number">1</span>) ans = (ans * a) % mod;<br>a = (a * a) % mod;<br>n &gt;&gt;= <span class="hljs-number">1</span>;<br>&#125;<br><span class="hljs-keyword">return</span> ans % mod;<br>&#125;<br>ll x = <span class="hljs-built_in">qpowmod</span>(a, p - <span class="hljs-number">2</span>, p);<br></code></pre></td></tr></table></figure><p><strong>扩展欧几里得</strong>：<spanclass="math inline">\(ax+py=\textrm{gcd}(a,p)\)</span>一定有解，此时不要求 <span class="math inline">\(p\)</span> 为质数，但<span class="math inline">\(\mathrm{gcd}(a,p)=1\)</span>必须满足。复杂度 <span class="math inline">\(O(\log n)\)</span>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++">ll x, y; <span class="hljs-comment">// 用于递归的全局变量</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">Exgcd</span><span class="hljs-params">(ll a, ll b, ll &amp;x, ll &amp;y)</span> </span>&#123;<br><span class="hljs-keyword">if</span> (!b) x = <span class="hljs-number">1</span>, y = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">else</span> <span class="hljs-built_in">Exgcd</span>(b, a % b, y, x), y -= a / b * x;<br>&#125;<br><span class="hljs-built_in">Exgcd</span>(a, p, x, y); <span class="hljs-comment">// 返回的逆元存放在 x 中</span><br></code></pre></td></tr></table></figure><p>最后是线性递推 <span class="math inline">\(O(n)\)</span>求<strong>连续数的逆元</strong>，注意 <spanclass="math inline">\(p\)</span> 必须为质数，否则 <spanclass="math inline">\(p\;\%\; i\)</span> 可能为零：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++">ll inv[<span class="hljs-number">100001</span>], p;<br><span class="hljs-function">ll <span class="hljs-title">Inv</span><span class="hljs-params">(ll i)</span> </span>&#123;<br><span class="hljs-keyword">if</span>(i == <span class="hljs-number">1</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;<br><span class="hljs-keyword">return</span> (p - p / i) * <span class="hljs-built_in">Inv</span>(p % i) % p;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="素数筛">素数筛</h2><p>埃氏筛，<span class="math inline">\(O(n \log(\logn))\)</span>，空间占用小，返回<strong>布尔数组</strong>，适用于<strong>快速判断素数</strong>，最高支持<span class="math inline">\(1e8\)</span>，模板如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> maxn = <span class="hljs-number">1e8</span> + <span class="hljs-number">10</span>;<br><span class="hljs-keyword">bool</span> isprime[maxn]; <span class="hljs-comment">//布尔数组，1表示质数</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">Esieve</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n)</span></span>&#123;<br>    <span class="hljs-comment">// 所有偶数先赋值 0，奇数还是 1</span><br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>, d = <span class="hljs-number">0</span>; i &lt;= n; i++, d ^= <span class="hljs-number">1</span>) isprime[i] = d; <br>isprime[<span class="hljs-number">1</span>] = <span class="hljs-number">0</span>, isprime[<span class="hljs-number">2</span>] = <span class="hljs-number">1</span>;<span class="hljs-comment">// 特判</span><br>    <span class="hljs-comment">// 从 3 开始排除，因为偶数已经全被排除，2没用</span><br><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">3</span>; i &lt;= <span class="hljs-built_in">sqrt</span>(n) + <span class="hljs-number">1</span>; i++)&#123;<br><span class="hljs-keyword">if</span> (isprime[i])&#123;<br>            <span class="hljs-comment">// i 必为奇数，i*i 也为奇数，偶数跳过</span><br><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = i * i; j &lt;= n; j += <span class="hljs-number">2</span> * i) isprime[j] = <span class="hljs-number">0</span>;<br>&#125;<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>欧式筛，<spanclass="math inline">\(O(n)\)</span>，空间占用大，适用于<strong>从零开始顺序打表</strong>，最高支持<span class="math inline">\(1e6\)</span>，模板如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> maxn = <span class="hljs-number">1e6</span> + <span class="hljs-number">10</span>;<br><span class="hljs-keyword">int</span> v[maxn];<span class="hljs-comment">// 合数标记，存放最小质因数</span><br><span class="hljs-keyword">int</span> prime[maxn];<span class="hljs-comment">// 顺序存储质数，注意末尾有 0</span><br><span class="hljs-keyword">int</span> cnt = <span class="hljs-number">0</span>;<span class="hljs-comment">// 素数个数</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">Osieve</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n)</span> </span>&#123;<br>    <span class="hljs-built_in">memset</span>(v, <span class="hljs-number">0</span>, <span class="hljs-built_in"><span class="hljs-keyword">sizeof</span></span>(v)); <span class="hljs-comment">// 假设全为素数</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">2</span>; i &lt;= n; i++) &#123;<br>        <span class="hljs-keyword">if</span>(!v[i]) &#123;<br>            prime[++cnt] = i;<br>            v[i] = i;<br>        &#125;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = <span class="hljs-number">1</span>; j &lt;= cnt; j++) &#123;<br>            <span class="hljs-keyword">if</span>(prime[j] &gt; v[i] || prime[j] * i &gt; n) <span class="hljs-keyword">break</span>;<br>            v[prime[j] * i] = prime[j];<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="排列数与组合数">排列数与组合数</h2><p>组合数的定义：<spanclass="math inline">\(C_i^j=\frac{i!}{j!(i-j)!}\)</span>，排列数定义：<spanclass="math inline">\(A_i^j=\frac{i!}{(i-j)!}\)</span></p><p>组合数和式：</p><ul><li><spanclass="math inline">\(C_n^0+C_n^1+C_n^2+\cdots+C_n^n=2^n\)</span></li><li><span class="math inline">\(1 C_n^1+2 C_n^2+3 C_n^3+\cdots+n C_n^n=n2^{n-1}\)</span></li><li><span class="math inline">\(1^2 C_n^1+2^2 C_n^2+3^2 C_n^3+\cdots+n^2C_n^n=n(n+1) 2^{n-2}\)</span></li><li><spanclass="math inline">\(\left(C_n^0\right)^2+\left(C_n^1\right)^2+\left(C_n^2\right)^2+\cdots+\left(C_n^n\right)^2=C_{2n}^n\)</span></li></ul><p>排列数和式：</p><ul><li><spanclass="math inline">\(A_n^0+A_n^1+A_n^2+\cdots+A_n^n=\mathrm{INT}(n!\cdote)\)</span></li></ul><p><strong>快速求组合数</strong>：</p><ul><li><p>定义法计算<strong>较小的单值</strong>，只需要 <spanclass="math inline">\(\mathcal{O}(\min(j,i-j))\)</span> 的时间和 <spanclass="math inline">\(\mathcal{O}(1)\)</span>的空间。不能用于<strong>多值、可能溢出的较大值</strong>。</p></li><li><p>递推：可以用 <spanclass="math inline">\(C_i^j=C_{i-1}^j+C_{i-1}^{j-1}\)</span> 的递推式<span class="math inline">\(\mathcal{O}\left(k^2\right)\)</span>求组合数。</p></li><li><p>递推：可以通过 <span class="math inline">\(C_k^i=\frac{k-i+1}{i}C_k^{i-1}\)</span> 的递推式 <spanclass="math inline">\(\mathcal{O}(k)\)</span> 求组合数，分母的 <spanclass="math inline">\(i\)</span>需要用<strong>乘法逆元</strong>计算。</p></li><li><p>线性求逆元：<code>for(int i = 2; i &lt;= K; i++) inv[i] = (MOD - MOD / i) * inv[MOD % i] % MOD;</code></p></li></ul><h3 id="不同路径-m">62. 不同路径 (M)</h3><p><ahref="https://leetcode.cn/problems/unique-paths/">题目描述</a>：一个机器人位于一个<code>m x n</code>网格的<strong>左上角</strong>，试图达到网格的<strong>右下角</strong>，求路径数目。</p><p><strong>方法1</strong>：DP，用 <spanclass="math inline">\(dp[i][j]\)</span> 表示从左上角走到 <spanclass="math inline">\((i,j)\)</span>的路径数，<strong>首行和首列</strong>初始化为 1，时空复杂度均为 <spanclass="math inline">\(O(n^2)\)</span>。 <span class="math display">\[dp[i][j]=dp[i-1][j] + dp[i][j-1]\]</span> <strong>方法2</strong>：DP +滚动数组，下一个状态只由最近两行的状态转移，因此可以用两行数组，空间复杂度为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：<strong>组合数公式</strong>，从左上角到右下角的过程中，总共需要走<span class="math inline">\(m+n-2\)</span> 步，其中必有 <spanclass="math inline">\(m-1\)</span> 步向下，其余向右。因此答案就是 <spanclass="math inline">\(C^{m-1}_{m+n-2}\)</span>。数字较小可以直接用定义计算，时间复杂度<span class="math inline">\(O(\min(m,n))\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="恰好移动-k-步到达某一位置的方法数目-m">2400. 恰好移动 k步到达某一位置的方法数目 (M)</h3><p><ahref="https://leetcode.cn/problems/number-of-ways-to-reach-a-position-after-exactly-k-steps/">题目描述</a>：给定数轴上的两个整数<code>startPos</code> 和 <code>endPos</code>代表起点和终点，每次可以向左或向右<strong>移动一步</strong>，求<code>k</code> 步后从起点恰好移动到终点的<strong>方案数</strong>。</p><p><strong>方法1</strong>：记忆化搜索，即起点到终点的距离为 <spanclass="math inline">\(n\)</span>，则有<code>dfs(k,n)=dfs(k-1,n-1)+dfs(k-1,n+1);</code>，利用 visited数组剪枝，复杂度 <span class="math inline">\(O(k^2)\)</span>。</p><p><strong>方法2</strong>：动态规划，状态转移方程同上，但是从 <spanclass="math inline">\(k=0\)</span> 开始递推，复杂度 <spanclass="math inline">\(O(k^2)\)</span>。</p><p><strong>方法3</strong>：<strong>组合数公式</strong>，假设往正方向走<span class="math inline">\(a\)</span> 步，负方向走 <spanclass="math inline">\(k-a\)</span> 到达，从 <spanclass="math inline">\(k\)</span> 步中选 <spanclass="math inline">\(a\)</span> 步的结果为 <spanclass="math inline">\(C_{k}^{a}\)</span>。由 <spanclass="math inline">\(a-(k-a)=n\)</span> 得 <spanclass="math inline">\(a=\frac{k+n}{2}\)</span>。因此要先判断 <spanclass="math inline">\((n+k)\)</span> 为偶数，然后求组合数。最优复杂度<span class="math inline">\(O(k)\)</span>。</p><p><strong>坑点</strong>：使用 visited 数组、DP 数组时，要注意<code>n-k</code> 可能的下标越界问题，需要整体向右平移。</p><h3 id="到达终点数字-m">754. 到达终点数字 (M)</h3><p><ahref="https://leetcode.cn/problems/reach-a-number/description/">题目描述</a>：给定数轴上的<code>target</code> 表示终点，起点在 <spanclass="math inline">\(0\)</span>。每次可以向左或向右移动，第 <spanclass="math inline">\(i\)</span> 次移动走 <spanclass="math inline">\(i\)</span>步，计算到达终点所需的<strong>最小移动次数</strong>。</p><p><strong>方法</strong>：组合数思想，设 <spanclass="math inline">\(k\)</span> 为最小的满足 <spanclass="math inline">\(s=\sum_{i=1}^k{\geqslant target}\)</span>的正整数，如果 <span class="math inline">\(s=target\)</span> 则答案就是<spanclass="math inline">\(k\)</span>，否则需要在部分整数前<strong>添加负号</strong>来将和凑到目标。如果这个差值$$ 是偶数，则必然可以从 <span class="math inline">\(1\)</span> 到 <spanclass="math inline">\(k\)</span> 中<strong>凑出若干整数</strong>和为<span class="math inline">\(\varDelta /2\)</span>；如果是奇数，则无法整除 <spanclass="math inline">\(2\)</span>，考虑 <spanclass="math inline">\(k+1\)</span> 和 <spanclass="math inline">\(k+2\)</span>，其中<strong>必有一个是奇数</strong>，可以改变$$ 的奇偶性，此时满足题意。</p><h2 id="绝对值中位数不等式">绝对值中位数不等式</h2><p>$( ) $ 问题，当 <span class="math inline">\(x\)</span>取中位数时，总和达到最小值。如果中位数有两个，则这两个数<strong>构成的区间内任意取值</strong>都可以。</p><h3 id="最佳的碰头地点-m">296. 最佳的碰头地点 (M)</h3><p><ahref="https://leetcode.cn/problems/best-meeting-point/">题目描述</a>：在<span class="math inline">\(n\times n\)</span> 网格中有 <spanclass="math inline">\(m\)</span>个人，在网格中选择一个点作为碰头地点，最小化<strong>他们的总行走距离</strong>。距离采用曼哈顿距离，即<code>distance(p1, p2) = |p2.x - p1.x| + |p2.y - p1.y|</code>。</p><p><strong>方法1</strong>：暴力，横纵坐标可以分开处理，遍历所有可选的网格，计算与每个人的距离，时间复杂度<span class="math inline">\(O(nm)\)</span>。</p><p><strong>方法2</strong>：排序，从左往右遍历时，每右移一格，总距离「<strong>加上左边的人数、减去右边的人数</strong>」，维护一个变量记录左边的人数，只需遍历一轮，时间复杂度<span class="math inline">\(O(m \log m+n)\)</span>，遍历复杂度可以优化到 <spanclass="math inline">\(O(m)\)</span>。</p><p><strong>方法3</strong>：<strong>绝对值中位数贪心</strong>，由方法 2可知，当左右两边人数相等时，总距离达到最小值。如果 <spanclass="math inline">\(n\)</span>为奇数，则返回中位数；否则可以在中位数<strong>两侧的数构成的区间内任意取值</strong>。<code>nth_element()</code>求中位数复杂度为 <span class="math inline">\(O(m)\)</span>。</p><h3 id="使数组相等的最小开销-h">2448. 使数组相等的最小开销 (H)</h3><p><ahref="https://leetcode.cn/problems/minimum-cost-to-make-array-equal/">题目描述</a>：给定数组<code>nums</code> 和 <code>cost</code>，分别包含 <spanclass="math inline">\(n\)</span> 个正整数。要将 <code>nums</code>中的所有元素修改为相等，每次可以对一个元素增一或减一，每次操作的代价为<code>cost</code> 中的对应元素，求最小总开销。</p><p><strong>方法1</strong>：排序，将两个数组<strong>捆绑排序</strong>，先计算使所有元素都等于<span class="math inline">\(nums[0]\)</span> 的开销，以及 <spanclass="math inline">\(sumCost\)</span>。目标数每增一，则总开销「加上<span class="math inline">\(cost[0]\)</span>，减去 <spanclass="math inline">\(sumCost-cost[0]\)</span>」，时间复杂度 <spanclass="math inline">\(O(n \log n)\)</span>。</p><p><strong>方法2</strong>：<strong>绝对值中位数贪心</strong>，$( ) $问题，按 <span class="math inline">\(a_i\)</span>拆分，考虑全体的中位数即可。这里先排序，然后遍历时累加 <spanclass="math inline">\(cost\)</span>，直到突破 <spanclass="math inline">\(sumCost/2\)</span> 即可得到中位数。时间复杂度<span class="math inline">\(O(n\log n)\)</span>。</p><blockquote><p>对于 $( ) $ 问题，当 <span class="math inline">\(a_i=1\)</span>时，最优解就是 <span class="math inline">\(x\)</span>取中位数的时候。如果 <span class="math inline">\(a_i\)</span>为任意正整数，则可以将其<strong>拆分</strong>为 <spanclass="math inline">\(a_i\)</span> 个系数为 <spanclass="math inline">\(1\)</span>的绝对值表达式求和，接下来依然考虑中位数即可。</p></blockquote><h2 id="拒绝采样">拒绝采样</h2><p>拒绝采样常用于随机事件的生成：通过多次采样已知的随机事件，并<strong>拒绝（重采样）</strong>部分情况，则可达到目标概率。</p><p>采样已知事件<strong>次数的期望</strong>可以用被拒绝的概率等比数列求和，假设每轮需要采样<span class="math inline">\(n\)</span> 次组合，拒绝概率为 <spanclass="math inline">\(p\)</span>： <span class="math display">\[\mathbb{E} =n+n\cdot p+n\cdot p^2+\cdots +n\cdot p^{\infty}\\=n\sum_{i=0}^{\infty}{p^i}=\frac{n}{1-p}\]</span></p><h3 id="用-rand7-实现-rand10-m">470. 用 Rand7 实现 Rand10 (M)</h3><p><ahref="https://leetcode.cn/problems/implement-rand10-using-rand7/">题目描述</a>：给定方法<code>rand7</code> 可生成 <code>[1,7]</code>范围内的均匀随机整数，将其改写为 <code>rand10</code>。</p><p><strong>方法1</strong>：先组合再拒绝，将两次 <code>rand7</code>组合成 <span class="math inline">\((x,y)\)</span>，共 <spanclass="math inline">\(49\)</span> 种结果，映射 <spanclass="math inline">\(idx=x+(y-1)\times 7\)</span>，则 <spanclass="math inline">\(idx\)</span> 范围 <spanclass="math inline">\([1,49]\)</span>，拒绝 <spanclass="math inline">\(idx&gt;40\)</span> 的情况，返回 <spanclass="math inline">\(1+idx\;\%\;10\)</span> 即可。</p><blockquote><p>该方法调用 <code>rand7</code> 次数的期望是 <spanclass="math inline">\(\frac{2}{1-\frac{9}{49}}=2.45\)</span>，为了减少次数，需要减小随机被拒绝的概率：合理地使用被拒绝的采样。若<span class="math inline">\(idx\)</span> 在 <spanclass="math inline">\([41,49]\)</span> 之间，相当于一个 <spanclass="math inline">\([1,9]\)</span> 的随机数，如果在调用一次<code>rand7</code>，就可以得到 <spanclass="math inline">\([1,63]\)</span>，保留 <spanclass="math inline">\([1,60]\)</span> 并拒绝 <spanclass="math inline">\([61,63]\)</span>，得到一个 <spanclass="math inline">\([1,3]\)</span> 的随机数，以此类推。</p></blockquote><p><strong>方法2</strong>：先拒绝再组合，第一次 <code>rand7</code> 构造<span class="math inline">\(\frac{1}{2}\)</span> 概率（拒绝 <spanclass="math inline">\(7\)</span> 后取奇偶），第二次 <code>rand7</code>构造 <span class="math inline">\(\frac{1}{5}\)</span> 概率（拒绝 <spanclass="math inline">\([6,7]\)</span> 得到 <spanclass="math inline">\(idx\)</span>）。如果第一次是奇数就返回 <spanclass="math inline">\(idx\)</span>，偶数就返回 <spanclass="math inline">\(idx+5\)</span>。</p><p><strong>坑点</strong>：千万不能使用两个 <code>rand7</code><strong>相乘、相加</strong>的做法，否则映射的结果<strong>不均匀</strong>！</p>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #08 链表</title>
    <link href="/LeetCode-LinkList.html"/>
    <url>/LeetCode-LinkList.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「链表」类型题中的：扫描链表、链表模拟等。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><h2 id="扫描链表">扫描链表</h2><p>链表的特点是只能<strong>正向遍历</strong>，通常由<strong>指针</strong>来扫描，个别题目可以用<strong>递归法</strong>解决（逆向思维）。</p><blockquote><p><strong>小技巧</strong>：链表问题通常没有空间复杂度的限制，因为其本身的存储也需要<spanclass="math inline">\(O(n)\)</span>，因此面试题通常要求「<strong>原地操作</strong>」。如果只追求解题，完全可以将链表<span class="math inline">\(O(n)\)</span> 转为数组后再操作，最后再 <spanclass="math inline">\(O(n)\)</span> 转回链表。</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ListNode</span> &#123;</span><br>    <span class="hljs-keyword">int</span> val;<br>    ListNode *next;<br>    <span class="hljs-built_in">ListNode</span>() : <span class="hljs-built_in">val</span>(<span class="hljs-number">0</span>), <span class="hljs-built_in">next</span>(<span class="hljs-literal">nullptr</span>) &#123;&#125;<br>    <span class="hljs-built_in">ListNode</span>(<span class="hljs-keyword">int</span> x) : <span class="hljs-built_in">val</span>(x), <span class="hljs-built_in">next</span>(<span class="hljs-literal">nullptr</span>) &#123;&#125;<br>    <span class="hljs-built_in">ListNode</span>(<span class="hljs-keyword">int</span> x, ListNode *next) : <span class="hljs-built_in">val</span>(x), <span class="hljs-built_in">next</span>(next) &#123;&#125; <br>&#125;;<br><br><span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">to_vector</span><span class="hljs-params">(ListNode *head)</span></span>&#123;<br>    vector&lt;<span class="hljs-keyword">int</span>&gt; ret;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> it = head; it; it = it-&gt;next)<br>        ret.<span class="hljs-built_in">push_back</span>(it-&gt;val);<br>    <span class="hljs-keyword">return</span> ret;<br>&#125;<br><br><span class="hljs-function">ListNode *<span class="hljs-title">to_list</span><span class="hljs-params">(<span class="hljs-keyword">const</span> vector&lt;<span class="hljs-keyword">int</span>&gt; &amp;v)</span></span>&#123;<br>    <span class="hljs-keyword">if</span> (v.<span class="hljs-built_in">empty</span>()) <span class="hljs-keyword">return</span> <span class="hljs-literal">NULL</span>;<br>    ListNode *ret = <span class="hljs-keyword">new</span> <span class="hljs-built_in">ListNode</span>(v[<span class="hljs-number">0</span>]);<br>    ListNode *it = ret;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; v.<span class="hljs-built_in">size</span>(); ++ i)&#123;<br>        it-&gt;next = <span class="hljs-keyword">new</span> <span class="hljs-built_in">ListNode</span>(v[i]);<br>        it = it-&gt;next;<br>    &#125;<br>    <span class="hljs-keyword">return</span> ret;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="两数相加-e">2. 两数相加 (E)</h3><p><ahref="https://leetcode.cn/problems/add-two-numbers/">题目描述</a>：两个非空链表，每个结点代表十进制的一位（表头代表最低位），将其相加，并以相同形式返回一个新链表。</p><p><strong>方法</strong>：直接遍历两个链表，存储进位，复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>坑点</strong>：并不是两个链表都遍历完就结束，还要考虑最后的<strong>进位</strong>。</p><h3 id="回文链表-e">234. 回文链表 (E)</h3><p><ahref="https://leetcode.cn/problems/palindrome-linked-list/">题目描述</a>：给定一个链表，判断链表节点的值是否构成回文序列。</p><p><strong>方法1</strong>：暴力，将链表转为数组，再用对撞双指针遍历，时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：栈，第一次遍历链表将元素<strong>压入栈</strong>中，第二次遍历时逐个<strong>弹出匹配</strong>，可以实现逆向遍历。同理，也可以用递<strong>归反向迭代</strong>，同时用递归<strong>函数外的变量正向迭代</strong>，原理相同。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：修改原链表，使用快慢指针获得链表中点后，将<strong>后半部分反转</strong>，再用双指针进行配对。空间复杂度<span class="math inline">\(O(1)\)</span>。</p><p><strong>方法4</strong>：字符串哈希，将链表当成数字字符串，计算<strong>正向哈希值和反向哈希值</strong>，如果两个值相等则可以认为构成回文。无需修改原链表，时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><blockquote><p>正向值和反向值可以一次遍历算出，二者的方法不一样。需要定义 <spanclass="math inline">\(base\)</span>，<spanclass="math inline">\(mod\)</span>，以及递增的<strong>乘子</strong><span class="math inline">\(mul\)</span>。</p><ul><li>正向哈希值：<span class="math inline">\(pos=(pos*base+val)\%mod\)</span></li><li>反向哈希值：<spanclass="math inline">\(neg=(neg+mul*val)\%mod\)</span>，<spanclass="math inline">\(mul=mul*base \% mod\)</span></li></ul></blockquote><h3 id="合并两个有序链表-e">21. 合并两个有序链表 (E)</h3><p><ahref="https://leetcode.cn/problems/merge-two-sorted-lists/">题目描述</a>：将两个升序链表合并为一个新的<strong>升序</strong>链表并返回。</p><p><strong>方法1</strong>：正向扫描，当两个链表都不为空时取较小者；当<strong>一个为空</strong>时，将<strong>链表末尾指向另一个</strong>即可。在实现时可以用一个虚拟head 作为头，复杂度 <span class="math inline">\(O(n+m)\)</span>。</p><p><strong>方法2</strong>：递归，边界条件是一个为空时，返回另一个。都不为空时，取出较小者，指向「<strong>剩下的两条链表递归合并的结果</strong>」，并返回较小者所在的链表头。</p><h3 id="合并-k-个升序链表-h">23. 合并 k 个升序链表 (H)</h3><p><ahref="https://leetcode.cn/problems/merge-k-sorted-lists/">题目描述</a>：一个链表数组，里面有<span class="math inline">\(k\)</span> 个长为 <spanclass="math inline">\(n\)</span>的链表，每个链表都按升序排列，将所有链表合并成一个升序链表。</p><p><strong>方法1</strong>：顺序合并，每次合并的复杂度是两个链表长度之和，第一次是<span class="math inline">\(2n\)</span>，第二次是 <spanclass="math inline">\(3n\)</span>，以此类推到 <spanclass="math inline">\(k-1\)</span> 次，累加得到总复杂度为 <spanclass="math inline">\(O(k^2 n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>方法2</strong>：<strong>优先队列</strong>，维护每个链表未被合并的<strong>首个元素</strong>，每次取出最小者，再将其下一个元素放入队列。时间复杂度<span class="math inline">\(O(kn\log k)\)</span>，维护队列需要 <spanclass="math inline">\(O(k)\)</span> 的空间。</p><p><strong>方法3</strong>：分治归并，将长度相等的链表<strong>两两归并</strong>，第一轮<span class="math inline">\(k\)</span> 个链表合并为 <spanclass="math inline">\(k/2\)</span> 个，总长度 <spanclass="math inline">\(kn\)</span>；第二轮合并为 <spanclass="math inline">\(k/4\)</span> 个，总长度还是 <spanclass="math inline">\(kn\)</span>，以此类推到 <spanclass="math inline">\(\log k\)</span> 次，时间复杂度 <spanclass="math inline">\(O(kn\log k)\)</span>，递归需要 <spanclass="math inline">\(O(\log k)\)</span> 的栈空间。</p><h3 id="反转链表-e">206. 反转链表 (E)</h3><p><ahref="https://leetcode.cn/problems/reverse-linked-list/">题目描述</a>：给定单链表的头节点<code>head</code>（有值），返回<strong>原地反转</strong>后的链表（不能使用新链表、临时栈）。</p><p><strong>方法1</strong>：迭代（三指针），<code>prev</code> 指向〇，<code>curr</code> 指向 ①，<code>temp</code> 指向 ②。每次将 ①-&gt;②反转，再依次前移 <code>prev</code> 和 <code>curr</code>，结束条件为<code>curr</code> 指向空。复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：头插法，虚拟一个<strong>哑结点</strong>（DummyNode）指向第一个节点，每次将下一个点插到哑结点之后，复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：递归，边界条件是<strong>单结点</strong>时，无需反转。子问题是「<code>head</code>后面所有结点已经反转，只需再反转 <code>head</code> 和<code>head-&gt;next</code> 中间的指针，并将 <code>head</code>指向空」。复杂度 <spanclass="math inline">\(O(n)\)</span>，只用到了单指针。</p><h3 id="反转链表-ii-m">92. 反转链表 II (M)</h3><p><ahref="https://leetcode.cn/problems/reverse-linked-list-ii/">题目描述</a>：给定单链表的头节点<code>head</code>（有值）和两个整数 <code>left</code> 和<code>right</code>，反转从位置 <code>left</code> 到位置<code>right</code> 的链表节点。</p><p><strong>方法1</strong>：头插法，在上一题的基础上，除了<code>dummy</code> 节点，还需要用四个指针分别指向 <code>left</code>及其前序节点、<code>right</code> 及其后继节点。复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>坑点</strong>：当 <code>left</code> 取 <spanclass="math inline">\(1\)</span> 的时候，前序节点就是<code>dummy</code>。此外，如果使用头插法，本题还可能导致<code>left</code> 节点自身成环（在<code>cur-&gt;next = pre-&gt;next;</code> 操作时，不像上一题那样会指向nullptr），这会导致 heap-use-after-free 错误，需要注意在最后将其指向<code>right</code> 的后继节点。</p><h3 id="k-个一组翻转链表-h">25. K 个一组翻转链表 (H)</h3><p><ahref="https://leetcode.cn/problems/reverse-nodes-in-k-group/">题目描述</a>：给定单链表的头节点<code>head</code>（有值），每 K个节点一组<strong>原地反转</strong>，不足 K个<strong>不反转</strong>。</p><p><strong>方法1</strong>：两次遍历，第一次遍历计数链表长度 <spanclass="math inline">\(Len\)</span>，第二次遍历翻转 <spanclass="math inline">\(Len/K\)</span> 次子链表，时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：递归，边界条件是不足 K个时，无需反转。通过循环找到第 <span class="math inline">\(K\)</span>个节点 <code>tail</code>，子问题是「<code>tail</code>后面所有结点已经反转，只需再反转 <code>head</code> 到<code>tail-&gt;next</code> 的指针，并将 <code>head</code> 指向<code>tail</code>」。复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>坑点</strong>：注意<strong>子问题处理完</strong>后<code>head</code> 会变成第 <span class="math inline">\(K\)</span>个节点，<strong>不能直接返回</strong>。</p><h2 id="双指针定位">双指针定位</h2><h3 id="删除链表的倒数第-n-个结点-m">19. 删除链表的倒数第 N 个结点(M)</h3><p><ahref="https://leetcode.cn/problems/remove-nth-node-from-end-of-list/">题目描述</a>：一个链表，删除链表的倒数第<code>n</code> 个结点，并且返回链表的头结点。</p><p><strong>方法1</strong>：强行计算链表长度，至少需要遍历两次。</p><p><strong>方法2</strong>：借助栈，将所有结点依次入栈再弹出，第<code>n</code>个弹出的结点就是要删除的结点，此时栈顶结点就是其前驱结点。</p><p><strong>方法3</strong>：<strong>快慢指针</strong>，快指针先走<code>n</code> 步，随后快慢指针同步前进。遍历一次且空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>坑点</strong>：链表的倒数第 <code>n</code>个结点可能就是头结点，此时快指针已经指向nullptr，慢指针是不需要继续前进的，直接返回 <code>head-&gt;next</code>即可。</p><p><strong>拓展</strong>：如果要删除链表<strong>正中间结点</strong>，只需用步长为2 的快指针和步长为 1 的慢指针遍历即可。</p><h3 id="环形链表-e">141. 环形链表 (E)</h3><p><ahref="https://leetcode.cn/problems/linked-list-cycle/">题目描述</a>：给一个链表的头节点<code>head</code> ，判断链表中是否有环。</p><p><strong>方法1</strong>：哈希集合，将便利到的结点的指针存储到集合中，每次先<code>count</code> 判断。时空复杂度均 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：<strong>快慢指针</strong>，用步长为 2的快指针和步长为 1 的慢指针遍历，如果两指针相遇，则一定有环。空间复杂度<span class="math inline">\(O(1)\)</span>。</p><p><strong>坑点</strong>：快慢指针的初值不应该都设置为<code>head</code>，否则可能会直接退出循环，可以将快指针设为<code>head-&gt;next</code>，或者直接先让两个指针各前进一步再判断。</p><p><strong>扩展</strong>：如何统计环的长度？快慢指针<strong>相遇后继续移动</strong>，直到第二次相遇。<strong>两次相遇间的移动次数</strong>即为环的长度。</p><h3 id="环形链表-ii-m">142. 环形链表 II (M)</h3><p><ahref="https://leetcode.cn/problems/linked-list-cycle-ii/">题目描述</a>：给一个链表的头节点<code>head</code>，返回链表<strong>环的入口</strong>（开始入环的第一个节点）。</p><p><strong>方法1</strong>：哈希集合，同上题，当 <code>count()!=0</code>时即找到入口，返回即可。时空复杂度均 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：快慢指针，用步长为 2 的 <spanclass="math inline">\(fast\)</span> 和步长为 1 的 <spanclass="math inline">\(slow\)</span> 遍历，两指针相遇后，用一个步长为 1的 <span class="math inline">\(pos\)</span> 从 <code>head</code>出发，且 <span class="math inline">\(slow\)</span> 继续以步长 1 前进。当<span class="math inline">\(slow\)</span> 和 <spanclass="math inline">\(pos\)</span>第一次相遇时，相遇处即为环的入口。空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><blockquote><p>快慢指针第一次相遇时，设慢指针路程 <spanclass="math inline">\(s\)</span>，快指针路程 <spanclass="math inline">\(f=2s\)</span>，环的长度为 <spanclass="math inline">\(r\)</span>。显然 <spanclass="math inline">\(f=s+nr\)</span>，<spanclass="math inline">\(n\)</span> 为套圈数。联立两式可知 <spanclass="math inline">\(s=nr\)</span>。</p><p>记 <code>head</code> 到环的入口需要走的路程为 <spanclass="math inline">\(p\)</span>，显然 <spanclass="math inline">\(p+nr\)</span> 也是环的入口，所以 <spanclass="math inline">\(p+s\)</span> 也是环的入口。令一个定位指针从<code>head</code> 出发走 <spanclass="math inline">\(p\)</span>，同时慢指针继续走 <spanclass="math inline">\(p\)</span>，二者就会在环的入口相遇。</p></blockquote><h3 id="寻找重复数-m">287. 寻找重复数 (M)</h3><p><ahref="https://leetcode.cn/problems/find-the-duplicate-number/">题目描述</a>：给定一个包含<span class="math inline">\(n + 1\)</span> 个整数的数组，其数字都在<span class="math inline">\([1, n]\)</span>范围内，假设数组中<strong>有且仅有一个重复的整数</strong>，找出数组中重复的数字，要求空间复杂度<span class="math inline">\(O(1)\)</span>。</p><p><strong>方法1</strong>：二分答案，定义 <spanclass="math inline">\(cnt[i]\)</span>表示数组中<strong>小于等于</strong> <spanclass="math inline">\(i\)</span> 的数的个数，假设重复数是 <spanclass="math inline">\(x\)</span>，则 <spanclass="math inline">\([1,x-1]\)</span> 里所有数都满足 <spanclass="math inline">\(cnt[i] \leq i\)</span>；而 <spanclass="math inline">\([x,n]\)</span> 里的所有数满足 <spanclass="math inline">\(cnt[i]&gt;i\)</span>。因此可以二分枚举 <spanclass="math inline">\(x\)</span> 直到找到。时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法2</strong>：二进制，预处理 <spanclass="math inline">\([1,n]\)</span> 这 <spanclass="math inline">\(n\)</span> 个数「<strong>每一位</strong>为 <spanclass="math inline">\(1\)</span>的<strong>个数之和</strong>」，如果当前数组的「<strong>每一位</strong>为<span class="math inline">\(1\)</span>的<strong>个数之和</strong>」大于期望值，则说明<strong>重复数的这一位</strong>为<span class="math inline">\(1\)</span>。遍历每一位即可，时间复杂度 <spanclass="math inline">\(O(n \log n)\)</span>。</p><p><strong>方法3</strong>：抽象成环形链表寻找入口，快慢指针，先用 <spanclass="math inline">\(fast\)</span> 和 <spanclass="math inline">\(slow\)</span> 相遇，再用 <spanclass="math inline">\(pos\)</span> 和 <spanclass="math inline">\(slow\)</span> 相遇，时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><blockquote><p>将数组和下标抽象为链表，数组的值代表链表下一个节点的下标。由于不存在数字0，可下标为 0的元素作为<strong>链表头节点</strong>。由于数组中存在重复数字，对应链表中有多个指针指向同一个顶点，则链表中<strong>必有环</strong>。找出重复数等价于找出链表<strong>环的入口</strong>。</p></blockquote><h3 id="相交链表-e">160. 相交链表 (E)</h3><p><ahref="https://leetcode.cn/problems/intersection-of-two-linked-lists/">题目描述</a>：给你两个单链表的头节点<code>headA</code> 和<code>headB</code>，请你找出并返回两个单链表相交的<strong>起始节点</strong>。</p><p><strong>方法1</strong>：哈希集合，遍历 <code>headA</code>并将每个节点加入集合，然后遍历<code>headB</code>，对每个结点检查是否在集合中。时间复杂度为 <spanclass="math inline">\(O(m+n)\)</span>，空间复杂度为 <spanclass="math inline">\(O(m)\)</span>。</p><p><strong>方法2</strong>：暴力，<strong>消除两个链表的长度差</strong>，遍历两个链表并计数得到长度，长的先走几步，然后一起走并判断两指针是否相等。时间复杂度为<span class="math inline">\(O(m+n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>方法3</strong>：双指针，<strong>消除两个链表的长度差</strong>，将两个链表拼接到一起就是等长。具体实现：当指针遍历完一个链表时，<strong>再次指向另一个链表</strong>的头节点继续遍历。时间复杂度<span class="math inline">\(O(m+n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h2 id="链表模拟">链表模拟</h2><h3 id="todo-707.-设计链表">Todo 707. 设计链表</h3><h3 id="lru-缓存-m">146. LRU 缓存 (M)</h3><p><ahref="https://leetcode.cn/problems/lru-cache/">题目描述</a>：设计一个<code>LRUCache</code> 类，实现以下功能：</p><ul><li><code>LRUCache(int capacity)</code>：以<strong>正整数</strong>作为容量<code>capacity</code> 初始化 LRU 缓存；</li><li><code>int get(int key)</code>：如果 <code>key</code>存在于缓存中，则返回其值，否则返回 <code>-1</code>，要求 <spanclass="math inline">\(O(1)\)</span>；</li><li><code>void put(int key, int value)</code>：如果 <code>key</code>存在，则变更其值为<code>value</code>；如果不存在，则向缓存中插入键值对。如果插入操作导致数量溢出，则应该<strong>逐出</strong>最久未使用（LRU）的关键字，要求 <spanclass="math inline">\(O(1)\)</span>。</li></ul><p><strong>方法</strong>：双向链表 +哈希表。链表用来存放<strong>关键字序列</strong>，首部表示 MRU，尾部表示LRU，<code>put</code> 关键字默认放在首部，<code>get</code>关键字默认移到首部，溢出时首先移除尾部。时间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><blockquote><p>使用两个哈希表存储<strong>关键字到链表节点</strong>、<strong>关键字到值</strong>的映射：前者用来<span class="math inline">\(O(1)\)</span>找到关键字在链表中的位置，方便移动；后者用来存值，也可以省略并在链表中存pair。</p><p>使用双向链表的原因：<code>get</code> 或 <code>put</code>需要将已有关键字移到首部，从链表中间删除一个节点需要<strong>知道其前序节点的指针</strong>，双向链表可以<span class="math inline">\(O(1)\)</span> 获取。</p><p>注意：移除尾部时<strong>并非</strong>利用了双向链表的特性，维护一个<span class="math inline">\(tail\)</span> 指针同样可以 <spanclass="math inline">\(O(1)\)</span> 移除尾部。</p></blockquote><h3 id="lfu-缓存-h">460. LFU 缓存 (H)</h3><p><ahref="https://leetcode.cn/problems/lfu-cache/">题目描述</a>：设计一个<code>LFUCache</code> 类，实现以下功能：</p><ul><li><code>LFUCache(int capacity)</code> 和 <code>int get(int key)</code>同上；</li><li><code>void put(int key, int value)</code>：如果插入操作导致数量溢出，则应该<strong>逐出</strong>最不经常使用（LFU）的关键字。当存在<strong>平局</strong>（即两个或更多个键具有相同使用频率）时，应该去除<strong>最近最久未使用</strong>的键。</li></ul><p><strong>方法1</strong>：哈希表 + 集合。用集合存储 <spanclass="math inline">\(Node\)</span> 结构体，<spanclass="math inline">\(Node\)</span>中包含键值对和<strong>使用频率、最近使用时间</strong>，并且<strong>自定义结构体比较函数</strong>。每次更新需要把元素从set 取出、修改、再重新插入，时间复杂度 <spanclass="math inline">\(O(\log n)\)</span>。</p><p><strong>方法2</strong>：十字链表 +双哈希表。一个哈希表用<strong>频率为索引</strong>，每个索引存放一个双向链表；另一个哈希表以<strong>关键字为索引</strong>，存放链表节点的指针。维护一个<spanclass="math inline">\(minFreq\)</span>，每次删除操作直接从对应的链表中删，时间复杂度<span class="math inline">\(O(1)\)</span>。</p>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #07 贪心算法</title>
    <link href="/LeetCode-Greedy.html"/>
    <url>/LeetCode-Greedy.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「贪心算法」类型题中的：扫描贪心、置换环贪心、单调队列贪心、排序贪心等。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><p>满足「重叠子问题 +贪心选择性」的问题。通常涉及一定的思维量，以及巧妙的<strong>构造</strong>。</p><h2 id="扫描贪心">扫描贪心</h2><h3 id="下一个排列-m">31. 下一个排列 (M)</h3><p><ahref="https://leetcode.cn/problems/next-permutation/">题目描述</a>：给定整数数组的一个排列，生成<strong>字典序的下一个排列</strong>。数字可能<strong>有重复</strong>，要求<strong>原地修改</strong>。</p><p><strong>方法1</strong>：贪心 +两次反向扫描，第一次扫描找到首个「<strong>下降数</strong>」，将该数与右边的数交换就能将排列变大；第二次找到「<strong>比下降数大的第一个数</strong>」，将两数交换，右边的数字逆序即可。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>由于按字典序的下一个排列会比当前要大，但是要让<strong>变大的幅度尽可能小</strong>。观察排列的变化规律，发现从后往前一直会有递增关系，直到遇到第一个下降数（且<strong>不能是相等的</strong>）。可以证明，如果只改变「下降数」右边的序列，结果不可能更大。</p><p>因此<strong>必须取代下降数</strong>，而每次取代下降数的则是<strong>右边一个比之稍大的数</strong>（同样<strong>不能是相等的</strong>，否则交换没有意义）。将两数交换后，整个排列必然会增大，为了能让变大的幅度尽可能小，右边的序列就需要逆序成<strong>升序序列</strong>。</p><p>但是如果<strong>没有找到下降数</strong>，则说明整个序列已经是降序，此时直接逆转即可。</p></blockquote><p><strong>方法2</strong>：C++ 函数 <code>netx_permutation(a,a+n)</code>实现了方法 1 的功能。</p><h3 id="跳跃游戏-m">55. 跳跃游戏 (M)</h3><p><ahref="https://leetcode.cn/problems/jump-game/">题目描述</a>：给定一个非负整数数组，数组中的每个元素代表在该位置可以跳跃的最大长度，判断<strong>能否</strong>从第一个元素跳到最后一个元素。</p><p><strong>方法1</strong>：贪心，只需考虑向前跳（向后跳意味着循环）。为了不漏掉每个可能的转移，需要扫描跳跃范围内的所有元素，贪心维护「<strong>能跳到的最右范围</strong>」，每次遇到更大的就更新。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>方法2</strong>：贪心，维护「<strong>剩余还能跳的步数</strong>」，逐步递减，每次遇到更大的就更新。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="最大交换-m">670. 最大交换 (M)</h3><p><ahref="https://leetcode.cn/problems/maximum-swap/">题目描述</a>：给定一个非负整数，<strong>至多</strong>可以交换一次数字中的任意两位，返回能得到的最大值。</p><p><strong>方法1</strong>：暴力，遍历所有可能的交换方式，复杂度 <spanclass="math inline">\(O(k^2)\)</span>，<spanclass="math inline">\(k\)</span> 为位数。</p><p><strong>方法2</strong>：贪心，每一个数字只能和其左侧相对小的数字交换，且<strong>不能跨越比它更大的数字</strong>，因为比它<strong>更大的数字往左交换的收益更高</strong>。因此从右往左扫描，记录当前最大的数及其可以交换的最远距离，复杂度<span class="math inline">\(O(k)\)</span>。</p><p><strong>扩展</strong>：这类问题需要把<strong>整数当作序列</strong>来操作，可以结合<code>to_string()</code> 和 <code>stoi()</code> 函数。</p><h3 id="盛最多水的容器-m">11. 盛最多水的容器 (M)</h3><p><ahref="https://leetcode.cn/problems/container-with-most-water/">题目描述</a>：给定一个长度为<code>n</code> 的数组表示 <code>n</code>条垂线的高，找出其中两条线，使它们构成的容器能装最多的水。</p><p><strong>方法1</strong>：暴力，两层循环，先选中左边界，再遍历右边界，时间复杂度<span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：贪心 +对撞双指针，双指针从两端开始遍历，选定<strong>两个边界中的短板</strong>，向中间收窄一格。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>贪心选择性证明：在每个状态下，无论长板或短板向中间收窄一格，都会导致水槽<strong>底边宽度</strong>变短。</p><ul><li>若向内<strong>移动短板</strong>，水槽的短板<strong>可能变大</strong>，因此下个水槽的面积<strong>可能增大</strong>。</li><li>若向内<strong>移动长板</strong>，水槽的短板<strong>不变或变小</strong>，因此下个水槽的面积<strong>一定变小</strong>。</li></ul><p>因此每轮向内移动短板，都会消去「<strong>不可能成为最大值的状态</strong>」。</p></blockquote><h3 id="接雨水-h">42. 接雨水 (H)</h3><p><ahref="https://leetcode.cn/problems/trapping-rain-water/">题目描述</a>：给定<code>n</code> 个非负整数表示每个宽度为 <code>1</code>的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。</p><p><strong>方法1</strong>：暴力，对于每个坐标，其能接的水量取决于<strong>左右两边最高的柱子中较矮者</strong>，两层循环遍历，复杂度<span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：DP预处理最大值，两次扫描，记录<strong>每个柱子</strong>的<strong>左右两边最大值</strong>，再扫描累计，时空复杂度均为<spanclass="math inline">\(O(n)\)</span>。该方法可以简单优化：从左到右的预处理可以和第三次扫描<strong>合并</strong>，<strong>用一个变量存储</strong>左边最大值，节省一个数组。</p><p><strong>方法3</strong>：单调不增栈 +模拟，每遍历到一个柱子，如果<strong>小于等于栈顶元素则入栈</strong>，如果更大则说明前面的柱子<strong>可以形成水洼</strong>，则依次将所有较小数弹出，并计算<strong>栈顶元素和新柱子的距离差值</strong>（表示水洼的左右墙壁，如果此时栈空，则表示没有左墙，该<strong>水洼不成立</strong>），再把新柱子入栈。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法4</strong>：贪心 +对撞双指针，双指针从两端开始遍历，再用两个变量存储左右两边最大值，每次<strong>最值较小者向中间收窄一格</strong>，同时计算出当前柱子能接的水量（<strong>当前柱子与这一侧最值的高度差</strong>，该值就是当前柱子能接到的最大水量）。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><blockquote><p>最值较小侧可以先计算的原因是：每个位置能接的水量取决于左右两边最高的柱子中较矮者，而最值较小侧的指针所指的位置，其所在侧的最值已经是较矮的（确定性），另一侧的最值未知但只会变得更大，所以此时该位置的答案已经可以提前确定。</p></blockquote><h3 id="最短无序连续子数组-m">581. 最短无序连续子数组 (M)</h3><p><ahref="https://leetcode.cn/problems/shortest-unsorted-continuous-subarray/">题目描述</a>：给定一个整数数组，从中找出一个<strong>最短连续子数组</strong>，如果对这个子数组排序，那么<strong>整个数组</strong>都会变为升序。</p><p><strong>方法1</strong>：索引排序，对整个数组进行<strong>索引排序</strong>，再从两边扫描第一个<code>idx[i]!=i</code> 的位置，时间复杂度 <spanclass="math inline">\(O(n\logn)\)</span>。为了防止<strong>重复数导致不稳定</strong>，应采用<code>stable_sort()</code>。</p><p><strong>方法2</strong>：贪心，<strong>从左往右扫描并维护最大值</strong>，如果新遇到一个元素小于最大值，则该元素必定无序，<strong>扩展右边界</strong>；同理，<strong>从右往左维护最小值</strong>，扩展左边界。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>贪心选择性证明：整个数组必定会呈现「<strong>有序 + 无序 +有序</strong>」的排列，而无序段的所有元素都要大于左段的最大值、小于右段的最小值。</p><p>因此，从左往右扫描时如果遇到一个元素小于最大值，则其不可能处于右段（因为右段必须更大、且升序），不可能处于左段（因为左段必须升序），则其必定在无序段中。</p></blockquote><h3 id="任务调度器-m">621. 任务调度器 (M)</h3><p><ahref="https://leetcode.cn/problems/task-scheduler/">题目描述</a>：给定一个字符数组表示任务列表，每个大写母代表一种不同种类的任务，任务可以按<strong>任何顺序</strong>执行，每次执行需要<strong>一个单位时间</strong>。但两个<strong>相同种类的任务之间</strong>需要有<code>n</code> 的冷却时间。计算完成所有任务所需的最短时间。</p><p><strong>方法1</strong>：贪心 +构造，先用桶记录下每种任务出现的次数，再降序排列。最后按照如下方式构造解：</p><blockquote><p>假设出现最多的任务是 <spanclass="math inline">\(A\)</span>，则<strong>至少要形成</strong> <spanclass="math inline">\(A** +A**+A\)</span>的序列，为了尽量利用空间，我们要将<strong>剩余任务插入</strong>到 <spanclass="math inline">\(*\)</span> 位置。此时最短长度为 <spanclass="math inline">\((hash[A]-1)\times (n+1)+1\)</span>。</p><p>注意到如果其他任务的次数和 <span class="math inline">\(A\)</span>一样多，则会形成 <span class="math inline">\(AB*+AB*+AB\)</span>的序列，即<strong>末尾扩充</strong>。而如果其他任务比 <spanclass="math inline">\(A\)</span>少，直接插入到空位即可，必然会满足冷却时间。</p><p>当所有 <span class="math inline">\(*\)</span>位置都插满后，<strong>剩余的任务可以直接补到每一组子序列之后</strong>，例如<span class="math inline">\(ABCD+ABCD+AB\)</span>，此时所有的 <spanclass="math inline">\(D\)</span> 之间间隔都大于<code>n</code>，且无需新的 <span class="math inline">\(*\)</span>占位。此时答案就是 <code>tasks</code> 的总数。</p></blockquote><h3 id="原创.-相邻字母不相同的最长字符串-m">原创.相邻字母不相同的最长字符串 (M)</h3><p><strong>题目描述</strong>：给定 26 个数字，分别表示字母<code>a-z</code>的数量，用这些字符拼接成<strong>相邻字母不相同</strong>的字符串，求最长字符串长度。</p><p><strong>方法</strong>：贪心，制约总长度的关键因素在于<strong>数量最多的字符</strong>，如果它的数量比其余字符的总和多1个则恰好可以交错排列，否则就无法将其全部放入。其次，如果其他字符的总和超过它，则多余字符可以插进序列的前部。因此，扫描维护所有字符的总数<code>sum</code>，找出最多字符数<code>max_cnt</code>。如果最大值超过剩余字母数量加 1，则最长字符串长度为<code>2 * (sum - max_cnt) + 1</code>。否则，可以完全利用所有字母，长度为<code>sum</code>。时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="划分字母区间">763. 划分字母区间</h3><p><ahref="https://leetcode.cn/problems/partition-labels/">题目描述</a>：给定一个由小写字母组成的字符串<code>s</code>，将其按顺序划分为<strong>尽可能多</strong>的片段，同一字母最多出现在一个片段中。返回一个表示每个字符串片段的长度的列表。</p><p><strong>方法</strong>：贪心，由于每个字母都只可能出现在一个片段，当<code>s[i]</code>被放入当前片段后，其最后一次出现的位置就是当前片段的最短长度。因此第一次遍历收集<strong>每个字母的最后出现位置</strong>，第二次遍历时维护当前片段的最短范围<code>now_end</code>，每添加一个字母就检查一次，直到<code>i==now_end</code> 时一个片段结束。时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="分发糖果-h">135. 分发糖果 (H)</h3><p><ahref="https://leetcode.cn/problems/candy/">题目描述</a>：给定大小为<code>n</code> 的数组表示 <code>n</code>个孩子的评分，要求每个孩子<strong>至少分到一个</strong>糖果，且相邻两个孩子<strong>评分更高</strong>的孩子会获得<strong>更多的</strong>糖果，求<strong>最少需要</strong>的糖果数目。</p><p><strong>方法1</strong>：记忆化搜索，DFS终止条件是相邻点评分<strong>大于等于</strong>当前点，否则就先 DFS求出邻居的结果。用 vis 数组记录访问过的点，时空复杂度均为 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：贪心 +两遍扫描，第一遍从左到右扫描，初始化一个全 1 的数组<code>left</code>，再对所有 <code>r[i]&lt;r[i+1]</code>的情况维护，<strong>其他情况不变</strong>；同理从右到左维护一个<code>right</code>，最后<strong>取二者中最大值</strong>即为答案。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>贪心选择性证明：任取序列中相邻的两点 A 和 B，如果 A &lt;B，则从左到右扫描后，B 的糖果一定比 A 多；且从右到左扫描后，A的糖果不会变得更多，因此满足两个规则。同理 A &gt; B 也成立，而当 A = B时，二者的值不相互影响。综上所述，取最大值后两个规则都能成立。</p></blockquote><p><strong>方法3</strong>：贪心 +一遍扫描，同时考虑两个规则，从左到右遍历，如果当前处于递增序列，则持续增一；如果处于递减序列，则每次增加「<strong>递减序列的长度</strong>」的量。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>坑点</strong>：方法 1要注意<strong>相邻的评分如果相等，糖果数可以任意取值</strong>，因此相等时递归就可以停止了（只需看另一边，两边都相等时直接取1）。如果相等时仍进行递归，由于<strong>相等具有对称性</strong>，会互相递归导致死循环！</p><h2 id="置换环贪心">置换环贪心</h2><p>置换环是线性扫描贪心中的一系列经典问题，背景是计算「<strong>使一个无重复数组有序的最小交换次数</strong>」。例如在数组<span class="math inline">\([2,0,1,4,3]\)</span> 中，<spanclass="math inline">\([2,0,1]\)</span> 和 <spanclass="math inline">\([4,3]\)</span>分别是两个置换环，将每个置换环排序的代价是 <spanclass="math inline">\(length-1\)</span>，因此将整体排序的代价就是「<strong>数组长度减去置换环的个数</strong>」。有两种做法：</p><ul><li>将<strong>目标序列</strong>的下标用哈希表映射存储<code>hash[value]=index</code>，遍历原序列，将<strong>不在位元素交换到目标位置</strong>，并使得<strong>当前位置的目标元素归位</strong>。</li><li>计算环的个数，需要将数组离散化到 <spanclass="math inline">\(0-N\)</span>，<strong>标记访问过的元素</strong>，对每个未访问的元素<code>for(auto a: nums)</code>，继续访问<code>a = nums[a]</code>，直到绕环一周。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// nums 中无重复元素</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">getMinSwaps</span><span class="hljs-params">(vector&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums)</span> </span>&#123;<br>    unordered_map&lt;<span class="hljs-keyword">int</span>, <span class="hljs-keyword">int</span>&gt; hash;<br>    <span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">sortedNums</span><span class="hljs-params">(nums)</span></span>;<br>    <span class="hljs-built_in">sort</span>(sortedNums.<span class="hljs-built_in">begin</span>(), sortedNums.<span class="hljs-built_in">end</span>());<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; sortedNums.<span class="hljs-built_in">size</span>(); i++)<br>        hash[sortedNums[i]] = i;<br><br>    <span class="hljs-keyword">int</span> cnt = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; nums.<span class="hljs-built_in">size</span>(); i++) &#123;<br>        <span class="hljs-keyword">while</span>(nums[i] != sortedNums[i])&#123;<br>            <span class="hljs-built_in">swap</span>(nums[i], nums[hash[nums[i]]]);<br>            cnt++;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> cnt;<br>&#125;<br><span class="hljs-comment">// 如果 nums 是 0 到 n-1 的元素，可以直接原地哈希</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">getMinSwaps</span><span class="hljs-params">(vector&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums)</span> </span>&#123;<br>    <span class="hljs-keyword">int</span> cnt = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; nums.<span class="hljs-built_in">size</span>(); i++) &#123;<br>        <span class="hljs-keyword">while</span>(nums[i] != i)&#123;<br>            <span class="hljs-built_in">swap</span>(nums[i], nums[nums[i]]);<br>            cnt++;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> cnt;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="逐层排序二叉树所需的最少操作数目-m">2471.逐层排序二叉树所需的最少操作数目 (M)</h3><p><ahref="https://leetcode.cn/problems/minimum-number-of-operations-to-sort-a-binary-tree-by-level/">题目描述</a>：给定一颗<span class="math inline">\(N\)</span>个节点的二叉树，树上节点的<strong>取值无重复</strong>，计算将每层二叉树节点<strong>排序所需的最少交换次数</strong>。</p><p><strong>方法1</strong>：层序遍历 +哈希表，先遍历取出每一层的元素形成嵌套数组。对每个数组先排序得到目标序列，再用哈希表记录<strong>每个元素的目标位置（无重复）</strong>，将原数组遍历并交换不在位元素，复杂度<span class="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法2</strong>：层序遍历 +置换环，对每个数组先排序，再离散化，使用访问数组<strong>绕环访问</strong>，时间复杂度<span class="math inline">\(O(n\log n)\)</span>。</p><p><strong>拓展</strong>：任何「<strong>取值无重复</strong>的排序所需的<strong>最少交换次数</strong>」其实就是置换环问题！</p><h3 id="情侣牵手-h">765. 情侣牵手 (H)</h3><p><ahref="https://leetcode.cn/problems/couples-holding-hands/">题目描述</a>：<code>n</code>对情侣<strong>随机</strong>坐在连续的 <code>2n</code>个座位上，假设情侣对按顺序编号，第一对是 <code>0,1</code>，第二对是<code>2,3</code>以此类推，求最少交换座位的次数，使得<strong>每对情侣可以并肩坐在一起</strong>。</p><p><strong>方法1</strong>：置换环 +并查集，将数组两两分组，<strong>错误配对的情侣必然位于一个置换环</strong>，而总共需要交换的次数就是「总情侣对数- 置换环的个数」，通过并查集找出置换环，时间复杂度 <spanclass="math inline">\(O(n \log n)\)</span>，空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：置换环 + BFS，将置换环视为连通分量，通过 BFS和访问数组 <span class="math inline">\(vis\)</span>找出所有连通分量，时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：哈希表 +贪心，用哈希表（数组）记录<strong>每个序号所在的位置</strong>，每次遍历一个情侣对，如果错误配对，则<strong>将不在位的序号交换到当前位置</strong>，并更新哈希表，可以证明每次交换都是「<strong>必要的</strong>」，时间复杂度<span class="math inline">\(O(n)\)</span>。</p><h2 id="单调队列贪心">单调队列贪心</h2><p>单调队列贪心是扫描贪心的一种特殊情况，需要在扫描的过程中维护一个单调递增/递减的队列。扫描的同时在队列中进行<strong>二分查找、尾部插入</strong>等操作，最终结果与<strong>队列长度</strong>、<strong>队列元素</strong>相关。</p><p>注意区分单调栈和单调队列，前者包含了<strong>栈的连续弹出</strong>操作，用来排除所有不可能成为下一个最大/最小值的元素；后者则维护一个<strong>可持续</strong>的队列（可以用<strong>数组、栈、双端队列实现</strong>）。</p><h3 id="最长递增子序列-lis-m">300. 最长递增子序列 (LIS) (M)</h3><p><ahref="https://leetcode.cn/problems/longest-increasing-subsequence/">题目描述</a>：一个整数数组<code>nums</code>，找到其中<strong>最长严格递增子序列</strong>的长度。（子序列可以不连续）</p><p><strong>方法1</strong>：DP，定义 <spanclass="math inline">\(dp[i]\)</span> 为<strong>以 <spanclass="math inline">\(nums[i]\)</span> 结尾的 LIS 长度</strong>，每次从<span class="math inline">\(dp[j]\)</span>中找满足条件的最大值转移，复杂度 <spanclass="math inline">\(O(n^2)\)</span>。 <span class="math display">\[d p[i]=\max (d p[j])+1 \text {, 其中 } 0 \leq j&lt;i \text { 且 } n um[j]&lt;n u m[i]\]</span> <strong>方法2</strong>：贪心 +单调队列<strong>二分查找</strong>，要使 LIS尽可能的长，就需要让<strong>序列上升得尽可能慢</strong>，因此我们希望每次在LIS <strong>末尾元素尽可能的小</strong>。因此维护一个数组 <spanclass="math inline">\(d[i]\)</span> 表示长度为 <spanclass="math inline">\(i\)</span> 的 LIS的<strong>末尾元素的最小值</strong>（初始化为正无穷），每新来一个元素就将其<strong>二分查找</strong>到其可以放入的位置，并更新<span class="math inline">\(d\)</span> 数组。复杂度 <spanclass="math inline">\(O(n \log n)\)</span>。</p><blockquote><p>显然 <span class="math inline">\(d[i]\)</span>是<strong>单调严格递增</strong>的，反证：如果 <spanclass="math inline">\(d[i] \geqslant d[j]\)</span> 但 <spanclass="math inline">\(i &lt; j\)</span>，则可以将 <spanclass="math inline">\(d[j]\)</span> 对应的 LIS 删除 <spanclass="math inline">\(j-i\)</span> 个数使其长度变为 <spanclass="math inline">\(i\)</span>，但显然此时末尾元素满足 <spanclass="math inline">\(d[i] \geqslant d[j] \geqslantd[i]&#39;\)</span>，则 <span class="math inline">\(d[i]\)</span>与其定义相矛盾。</p></blockquote><h3 id="最多能完成排序的块-m">769. 最多能完成排序的块 (M)</h3><p><ahref="https://leetcode.cn/problems/max-chunks-to-make-sorted/">题目描述</a>：给定一个长度为<code>n</code> 的数组表示 <span class="math inline">\([0, n-1]\)</span>的整数的一个排列。现将数组分为<strong>若干大小不等的块</strong>，并对每个块<strong>单独排序</strong>，且排序后的结果和整个数组排序的结果相同。计算数组能划分的<strong>最多块数</strong>。</p><p><strong>方法1</strong>：贪心 +一次遍历，由于数组是一个<strong>无重复的全排列</strong>，每个数就是其排序后对应的下标。因此维护一个变量<code>mx</code>表示<strong>已遍历过的数中的最大值</strong>（分块的上限），如果<code>mx</code> 与当前遍历到的下标 <code>i</code>相等，说明可以进行一次分块。时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>方法2</strong>：贪心 + 单调队列，方法 1要求数组是无重复的全排列，如果<strong>数字不连续、有重复</strong>就无法实现下标的对应关系。观察发现从左到右，<strong>每个分块都有一个最大值</strong>，并且这些最大值呈<strong>单调递增</strong>。因此可以用一个单调队列，时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>当栈为空，或遍历到的数 <code>x</code><strong>大于等于</strong>栈顶元素时直接入栈；</p><p>如果 <code>x</code> 小于栈顶元素 <code>t</code>，则需要把栈中大于<code>x</code> 的元素弹出，再<strong>放回</strong><code>t</code>，表示「介于 <code>x</code> 与 <code>t</code>之间」的分块融合。由于有放回的操作，所以本题实质上是单调队列而非单调栈。</p><p>最终返回栈的大小。</p></blockquote><h3 id="最多能完成排序的块-ii-h">768. 最多能完成排序的块 II (H)</h3><p><ahref="https://leetcode.cn/problems/max-chunks-to-make-sorted-ii/">题目描述</a>：同上一题类似，但数组大小、元素取值范围更大，且可能出现<strong>重复元素</strong>。</p><p><strong>方法1</strong>：排序 +哈希表，将原数组复制并排序，同时遍历两个数组，用<strong>哈希表记录出现过的元素及频次</strong>，如果在原数组出现过，则频次加一；在排序数组出现过，则频次减一。当<strong>频次为0 时 erase元素</strong>，如果<strong>哈希表为空，则表示划分出了一个新的块</strong>。时间复杂度<span class="math inline">\(O(n\log n)\)</span>，空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：索引排序，将原数组进行<strong>索引排序</strong>得到<span class="math inline">\(idx\)</span>，此时 <spanclass="math inline">\(idx\)</span> 即为 <spanclass="math inline">\([0,n-1]\)</span>的<strong>全排列</strong>，因此可以用一次遍历完成。时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>，空间复杂度 <spanclass="math inline">\(O(\logn)\)</span>，均为<strong>快排的开销</strong>。</p><p><strong>方法3</strong>：贪心 + 单调队列，同上一题的方法2，时空复杂度均为 <span class="math inline">\(O(n)\)</span>。</p><h3 id="滑动窗口最大值-h">239. 滑动窗口最大值 (H)</h3><p><ahref="https://leetcode.cn/problems/sliding-window-maximum/">题目描述</a>：给定一个整数数组<code>nums</code>，一个大小为 <code>k</code>的滑动窗口从数组的<strong>最左侧移动到数组的最右侧</strong>。每次移动一位，返回<strong>每次</strong>滑动窗口中的<strong>最大值</strong>。</p><p><strong>方法1</strong>：优先队列，窗口内维护一个<strong>大顶堆</strong>，每次将堆顶元素作为答案。当窗口移动时，新元素放入，<strong>旧元素懒弹出</strong>，即<strong>当旧元素成为堆顶时再弹出</strong>。因此优先队列需要放入元素值和<strong>下标</strong>，用于判断旧元素是否不在窗口内。由于队列长度最长为<span class="math inline">\(n\)</span>，时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>，空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：贪心 +单调队列，窗口内维护一个单调队列，存放<strong>可能成为下一个最大值</strong>的元素下标。因此这些下标按照<strong>从小到大</strong>的顺序被存储，并且它们对应的值是<strong>严格单调递减</strong>的。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(k)\)</span>。</p><blockquote><p>如果队列中有两个下标，其对应的值相等或递增，后者一定比前者存在的时间更久，因此前者不可能成为最大值，可以弹出。</p><p>因此当窗口右侧进入新元素时，要先弹出比之小的所有元素。而窗口左侧的元素则在离开窗口时就可弹出。两侧都需要弹出元素，因此使用<strong>双端队列</strong>。</p></blockquote><h3 id="最大宽度坡-m">962. 最大宽度坡 (M)</h3><p><ahref="https://leetcode.cn/problems/maximum-width-ramp/">题目描述</a>：给定一个整数数组，定义坡是元组<code>(i,j)</code>，其中 <code>i&lt;j</code> 且<code>A[i]&lt;=A[j]</code>，此时坡的宽度为<code>j-i</code>。求坡的<strong>最大</strong>宽度。</p><p><strong>方法1</strong>：贪心 + 单调队列 +二分，能成为<strong>最长子数组的左端点</strong>的元素必然构成一个<strong>递减序列</strong>，因为如果来了一个<strong>大于等于栈顶</strong>的元素，则栈顶元素一定能比新元素构成更长的区间。先<strong>正向遍历</strong>获得单调队列，再枚举每个点作为右端点，<strong>二分查找第一个</strong>满足<code>A[i]&lt;=A[j]</code> 的左端点，时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法2</strong>：贪心 + 单调队列 +栈，上述单调队列<strong>用栈实现</strong>。先<strong>正向遍历</strong>获得单调递减栈，再<strong>反向遍历</strong>枚举<strong>右端点</strong>，每遇到比栈顶更大的元素就更新答案并<strong>弹出元素</strong>，因为不会再用到。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><h2 id="排序贪心">排序贪心</h2><p>排序贪心问题通常涉及一组任务、事件或对象，需要按照某种顺序排列，目标是优化某个全局指标（如总时间、总成本、总收益等）。其核心解法是确定一个<strong>局部最优的选择顺序</strong>，最终达到全局最优。</p><p>因此，需要找到<strong>对象 A 排在对象 B前的充分必要条件</strong>，并证明这个条件具有贪心选择性。</p><h3 id="原创.-排队接水-e">原创. 排队接水 (E)</h3><p><strong>题目描述</strong>：有 <span class="math inline">\(n\)</span>个人在一个水龙头前排队接水，数组 <code>times</code>表示每个人接水所需时间，安排一个顺序使得所有人的<strong>等待时间总和</strong>最小。</p><blockquote><p>直觉：让时间短的人先接水，可以减少后面人的等待时间。</p><p>证明：对于任意相邻的两个人 <spanclass="math inline">\(t_i&lt;t_j\)</span>，他们的顺序不会影响前面和后面人的等待时间。如果<span class="math inline">\(i\)</span> 排在 <spanclass="math inline">\(j\)</span> 前，则两人的等待时间为 <spanclass="math inline">\(t_i+(t_i+t_j)\)</span>，反之则为 <spanclass="math inline">\(t_j+(t_j+t_i)\)</span>，显然前者更小，因此要让更小者排在前面。</p></blockquote><p><strong>方法</strong>：排序 +贪心，按照接水时间从小到大排序，累计前缀和即可。时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>。</p><h3 id="原创.-打怪兽-m">原创. 打怪兽 (M)</h3><p><strong>题目描述</strong>：有 <span class="math inline">\(n\)</span>个怪兽，每个怪兽有自己的血量 HP 和它的攻击力DPS。你可以选择其中一只，在挨了所有存活怪兽的一次攻击后攻击它。你只有 1点攻击力，求最少会受到多少伤害。</p><p><strong>方法</strong>：排序 + 贪心，除了要考虑到会被一只怪物打 <spanclass="math inline">\(DPS \times HP\)</span>的伤害，还要考虑到被剩余所有怪物打 <spanclass="math inline">\(HP\)</span>下，因此需要排序来减少剩余怪物的总伤害。此外，本题还有个隐含线索，就是当选中一只怪物，就必须打到底。所以就是一个贪心排序问题。时间复杂度<span class="math inline">\(O(n\log n)\)</span>。</p><blockquote><p>直觉 1：先打 DPS 高的怪兽。反例：一个怪兽 HP 1 DPS 99，另一个怪兽 HP100 DPS 100。如先打后者，就会被前者一直打很久。</p><p>直觉 2：先打 DPS / HP 高的怪兽。</p><p>证明：对于任意的两个相邻的怪兽 <span class="math inline">\(i\)</span>和 <spanclass="math inline">\(j\)</span>，他们的顺序不会影响前面和后面的伤害。如果先打<span class="math inline">\(i\)</span> 则会受到： <spanclass="math display">\[DPS[i]\times HP[i] + DPS[j]\times (HP[i]+ HP[j])\]</span> 反之如果先打 <span class="math inline">\(j\)</span> 则会受到：<span class="math display">\[DPS[j]\times HP[j] + DPS[i]\times (HP[i]+ HP[j])\]</span> 如果 <span class="math inline">\(i\)</span>要排在前面，则上式要小于下式，则有： <span class="math display">\[DPS[j] \times HP [i] &lt; DPS[i] \times HP[j]\]</span>自定义排序：<code>return i.dps * j.hp &lt; j.dps * i.hp;</code></p></blockquote><h3 id="合并区间-m">56. 合并区间 (M)</h3><p><ahref="https://leetcode.cn/problems/merge-intervals/">题目描述</a>：给定N个数对表示若干个区间，将所有重叠的区间合并为一个大区间，返回合并后的数对集合。</p><p><strong>方法</strong>：排序 +贪心，将区间<strong>按照左端点排序</strong>，维护一个当前的 <spanclass="math inline">\(start\)</span> 和 <spanclass="math inline">\(end\)</span>，如果下一个区间左端点大于 <spanclass="math inline">\(end\)</span>，则另起一个大区间；否则将其合并，更新<span class="math inline">\(end\)</span> 即可。 时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>。</p><h3 id="最长数对链-活动安排问题-m">646. 最长数对链 (活动安排问题)(M)</h3><p><ahref="https://leetcode.cn/problems/maximum-length-of-pair-chain/">题目描述</a>：N个数对（第一个数字总是比第二个数字小）中，选择部分构造数对链，使得<code>(a,b)-&gt;(c,d)</code> 满足<code>b&lt;c</code>。找出并返回能够形成的<strong>最长数对链的长度</strong>。</p><p><strong>方法1</strong>：当成 LIS 问题，也是要找一个递增序列，有 DP和贪心 + 二分查找解法，复杂度 <spanclass="math inline">\(O(n^2)\)</span> 或 <span class="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法2</strong>：排序 +贪心，优先挑选<strong>第二个数字最小</strong>的，这样能给挑选后续的数对留下更多的空间。因此按照第二个数字排序，排序复杂度<span class="math inline">\(O(n \log n)\)</span>，再遍历一遍即可。</p><h3 id="优势洗牌-m">870. 优势洗牌 (M)</h3><p><ahref="https://leetcode.cn/problems/advantage-shuffle/">题目描述</a>：给定两个大小相等的数组<code>nums1</code> 和 <code>nums2</code>，如果 <code>nums1</code>中的数大于 <code>nums2</code>中<strong>相同位置</strong>的数，则具有<strong>优势</strong>，返回<code>num1</code> 的<strong>一个排列</strong>，使其优势最大化。</p><p><strong>方法1</strong>：贪心 + multiset，每次在 <code>nums1</code>中寻找<strong>大于</strong> <code>nums2[i]</code>的<strong>最小值</strong>；若没有，则返回 <code>nums1</code>中的最小值。为了防止重复使用，每次要在 <code>nums1</code>中<strong>删除该数字且保持有序</strong>，因此采用 multiset，时间复杂度<span class="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法2</strong>：贪心 + <strong>索引排序</strong> +对撞双指针，将两个数组都进行索引排序。每次取<strong>首个元素</strong>，如果前者大于后者，则<strong>进行配对</strong>；如果不大于，则前者和<code>nums2</code> 的<strong>最大数进行配对</strong>，用双指针指向<code>nums2</code> 的首尾。时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>。</p><blockquote><p><span class="math inline">\(nums1\)</span>可以改变原有的顺序，因此可以直接在其上进行排序，不需要索引化。</p><p>但 <span class="math inline">\(nums2\)</span>不能改变原有的顺序（否则就没法对应放置数字），只能新建一个 <spanclass="math inline">\(idx\)</span>数组并初始化为索引序列，再<strong>自定义排序函数</strong>实现映射：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">idx</span><span class="hljs-params">(n)</span></span>;<br><span class="hljs-built_in">iota</span>(idx.<span class="hljs-built_in">begin</span>(), idx.<span class="hljs-built_in">end</span>(), <span class="hljs-number">0</span>);<br><span class="hljs-built_in">sort</span>(idx.<span class="hljs-built_in">begin</span>(), idx.<span class="hljs-built_in">end</span>(), [&amp;](<span class="hljs-keyword">int</span> i, <span class="hljs-keyword">int</span> j)&#123; <span class="hljs-keyword">return</span> nums[i] &lt; nums[j]; &#125;);<br></code></pre></td></tr></table></figure><p>之后就可以通过 <code>nums2[idx[i]]</code>来访问升序的数组，答案也可以直接放在 <code>nums2</code>。</p></blockquote><p><strong>坑点</strong>：方法 1 中如果不采用 multiset而是用<strong>排序的数组</strong>，每次删除操作的复杂度是 <spanclass="math inline">\(O(n)\)</span>，整体复杂度就是 <spanclass="math inline">\(O(n^2)\)</span>。</p><h3 id="根据身高重建队列-m">406. 根据身高重建队列 (M)</h3><p><ahref="https://leetcode.cn/problems/queue-reconstruction-by-height/">题目描述</a>：有打乱顺序的一群人站成一个队列，每人提供两个属性：他的身高、他前面<strong>应该</strong>有多少个<strong>身高大于等于他</strong>的人。要求将这群人<strong>排回原来的顺序</strong>。</p><blockquote><p>直觉：矮个子排在哪都对高个子没有影响，但是高个子排在矮个子前面就会造成影响。所以，<strong>高个子可以先排</strong>，矮个子再插队。因此要先按身高<strong>从大到小</strong>排序，身高相同再按第二关键字<strong>从小到大</strong>排序。</p><p>自定义排序：<code>return u[0] &gt; v[0] || (u[0] == v[0] &amp;&amp; u[1] &lt; v[1]);</code></p></blockquote><p><strong>方法</strong>：自定义排序 +贪心，遍历排序后的数组，后来的矮个子<strong>直接插入到第二关键字</strong>的位置，能满足自身属性且不会影响到之前的队列属性，时间复杂度<span class="math inline">\(O(n^2)\)</span>。</p><h3 id="使数组相似的最少操作次数-h">2449. 使数组相似的最少操作次数(H)</h3><p><ahref="https://leetcode.cn/problems/minimum-number-of-operations-to-make-arrays-similar/">题目描述</a>：给定长度相等的两个正整数数组<code>nums</code> 和 <code>target</code>，操作 <code>nums</code>，使其与<code>target</code>中每个元素<strong>出现次数相等</strong>，但<strong>位置可以不同</strong>。每次操作需<strong>选中两个数</strong>，其中一个数<span class="math inline">\(+2\)</span>，另一个数 <spanclass="math inline">\(-2\)</span>。计算最少操作次数。</p><p><strong>方法</strong>：排序 +贪心，要使操作次数最小化，可以证明应该<strong>让最小的一对、次小的一对、以此类推</strong>。由于<span class="math inline">\(\pm 2\)</span>不会改变奇偶性，因此配对时考虑<strong>奇偶数分离</strong>，再进行排序（由于原数组是正数，可以考虑将奇数翻转为负来分离）。累加每对元素的差的绝对值，时间复杂度<span class="math inline">\(O(n\log n)\)</span>。</p><p><strong>坑点</strong>：本题保证能使两个数组相似，因此最终累加的结果除以<span class="math inline">\(4\)</span> 一定能得到整数。</p>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #06 图论</title>
    <link href="/LeetCode-Graph.html"/>
    <url>/LeetCode-Graph.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「图论」类型题中的：遍历图、矩阵图模拟、拓扑排序、最短路、最小生成树、网络流等。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><p>图的表示通常有：邻接矩阵、邻接表（数组或链表）、链式前向星。在这里提到的「图」通常以<strong>邻接表数组</strong>存储。</p><h2 id="遍历图">遍历图</h2><p>BFS &amp;DFS，遍历图比遍历树麻烦，需要<strong>防止多次访问同一个节点</strong>，陷入死循环。通常是采用<strong>哈希表或哈希集合</strong>存储访问过的点，下次访问时判断即可。</p><p>常用<strong>有向图</strong>代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 一维数组 nums 存点, 二维数组 edges 存边</span><br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">g</span>(nums.<span class="hljs-built_in">size</span>());<br><span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">in</span><span class="hljs-params">(nums.size())</span>, <span class="hljs-title">out</span><span class="hljs-params">(nums.size())</span></span>;<br><span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> &amp;e : edges) &#123;<br>    g[e[<span class="hljs-number">0</span>]].<span class="hljs-built_in">push_back</span>(e[<span class="hljs-number">1</span>]); <span class="hljs-comment">// 有向图，只存一次</span><br>    in[e[<span class="hljs-number">1</span>]]++, out[e[<span class="hljs-number">0</span>]++];<br>&#125;<br></code></pre></td></tr></table></figure><p>常用<strong>无向图</strong>代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 一维数组 nums 存点, 二维数组 edges 存边</span><br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">g</span>(nums.<span class="hljs-built_in">size</span>());<br><span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> &amp;e : edges) &#123;<br>    <span class="hljs-keyword">int</span> x = e[<span class="hljs-number">0</span>], y = e[<span class="hljs-number">1</span>];<br>    g[x].<span class="hljs-built_in">push_back</span>(y);<br>    g[y].<span class="hljs-built_in">push_back</span>(x); <span class="hljs-comment">// 无向图，存两次</span><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="克隆图-m">133. 克隆图 (M)</h3><p><ahref="https://leetcode.cn/problems/clone-graph/">题目描述</a>：给出无向连通图中一个节点的引用，请你返回该图的<strong>深拷贝</strong>（克隆），原图以<strong>链式邻接表</strong>存储。</p><p><strong>方法1</strong>：DFS，用哈希表 <spanclass="math inline">\(vis\)</span> 记录 <code>node</code> 和<code>cloneNode</code>的映射关系，同时标记访问过的节点，当遇到重复节点时<strong>直接返回哈希表中的记录</strong>。递归调用主函数，遍历邻接表中的所有点。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：BFS，哈希表 <spanclass="math inline">\(vis\)</span> 和方法 1类似，使用一个队列来存放即将遍历的点，时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h2 id="矩阵图模拟">矩阵图模拟</h2><p>给定一个矩阵，矩阵上的点可以视为<strong>四向连通的图节点</strong>（除边界外）。常见的有岛屿问题、火势蔓延问题等。常用技巧：</p><ul><li>采用 DFS遍历时，终止的条件之一为<strong>数组下标越界</strong>，可以独立一个valid 函数判断。</li><li>全局定义四个方向，然后再用 <code>for(auto &amp;d : dir)</code>循环。</li><li>矩阵中的<strong>连通块可以用不同的数字标记</strong>（在一个等大的数组中），再用<strong>哈希表</strong>记录数字对应的属性。</li><li>用数字记录连通块还有一个好处：在特定时候可以用<code>set&lt;int&gt;</code> 去重、或者判断是否已经可达。</li></ul><p>常用代码段：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 在全局定义</span><br><span class="hljs-keyword">int</span> n, m;<br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; dir = &#123;&#123;<span class="hljs-number">1</span>, <span class="hljs-number">0</span>&#125;, &#123;<span class="hljs-number">-1</span>, <span class="hljs-number">0</span>&#125;, &#123;<span class="hljs-number">0</span>, <span class="hljs-number">1</span>&#125;, &#123;<span class="hljs-number">0</span>, <span class="hljs-number">-1</span>&#125;&#125;;<br><br><span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">valid</span><span class="hljs-params">(<span class="hljs-keyword">int</span> row, <span class="hljs-keyword">int</span> col)</span></span>&#123;<br>    <span class="hljs-keyword">return</span> row &gt;= <span class="hljs-number">0</span> &amp;&amp; row &lt; n &amp;&amp; col &gt;= <span class="hljs-number">0</span> &amp;&amp; col &lt; m;<br>&#125;<br><span class="hljs-comment">// 记忆化搜索，传入矩阵 + 记忆化数组 + 当前坐标</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">dfs</span><span class="hljs-params">(vector&lt;vector&lt;<span class="hljs-keyword">char</span>&gt;&gt;&amp; grid, vector&lt;vector&lt;<span class="hljs-keyword">bool</span>&gt;&gt;&amp; vis, <span class="hljs-keyword">int</span> x, <span class="hljs-keyword">int</span> y)</span></span>&#123;<br>    vis[x][y] = <span class="hljs-literal">true</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> &amp;d : dir)&#123;<br>        <span class="hljs-keyword">int</span> nx = x + d[<span class="hljs-number">0</span>], ny = y + d[<span class="hljs-number">1</span>];<br>        <span class="hljs-keyword">if</span>(<span class="hljs-built_in">valid</span>(nx, ny) &amp;&amp; grid[nx][ny] == <span class="hljs-string">&#x27;1&#x27;</span> &amp;&amp; !vis[nx][ny])&#123;<br>            <span class="hljs-built_in">dfs</span>(grid, vis, nx, ny);<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> ;<br>&#125;<br><span class="hljs-comment">// 主函数</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">(vector&lt;vector&lt;<span class="hljs-keyword">char</span>&gt;&gt;&amp; grid)</span></span>&#123;<br>    n = grid.<span class="hljs-built_in">size</span>(), m = grid[<span class="hljs-number">0</span>].<span class="hljs-built_in">size</span>();<br>    vector&lt;vector&lt;<span class="hljs-keyword">bool</span>&gt;&gt; <span class="hljs-built_in">vis</span>(n, vector&lt;<span class="hljs-keyword">bool</span>&gt;(m, <span class="hljs-literal">false</span>));<br>    <span class="hljs-comment">// TODO</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; m; j++)&#123;<br>            <span class="hljs-keyword">if</span>(grid[i][j] == <span class="hljs-string">&#x27;1&#x27;</span> &amp;&amp; !vis[i][j])&#123;<br>                <span class="hljs-comment">// TODO</span><br>                <span class="hljs-built_in">dfs</span>(grid, vis, i, j);<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="岛屿数量-m">200. 岛屿数量 (M)</h3><p><ahref="https://leetcode.cn/problems/number-of-islands/">题目描述</a>：给定<span class="math inline">\(m\times n\)</span> 矩阵，矩阵上的 <spanclass="math inline">\(1\)</span> 代表陆地，<spanclass="math inline">\(0\)</span> 代表海洋，计算网格中岛屿的数量。</p><p><strong>方法1</strong>：DFS，遍历矩阵，当遇到陆地时 DFS标记整个岛屿访问过的点，时空复杂度均为 <spanclass="math inline">\(O(mn)\)</span>。</p><p><strong>方法2</strong>：并查集，相邻的 <spanclass="math inline">\(1\)</span> 合并到同一个集合，遍历每个 <spanclass="math inline">\(0\)</span>，最终答案就是<strong>并查集中连通分量</strong>的数目。时间复杂度为<span class="math inline">\(O(mn \times \alpha(mn))\)</span>。</p><h3 id="被围绕的区域-m">130. 被围绕的区域 (M)</h3><p><ahref="https://leetcode.cn/problems/surrounded-regions/">题目描述</a>：：给定<span class="math inline">\(m\times n\)</span> 矩阵，由字符<code>X</code> 和 <code>O</code> 构成，找到所有被 <code>X</code> 包围的<code>O</code>，并将其替换为 <code>X</code>。</p><p><strong>方法</strong>：DFS，已知没有被 <code>X</code> 包围的<code>O</code> 必然与边界相连，因此<strong>从边界上</strong>的<code>O</code> 开始 DFS 搜索，可以找到最<strong>终保留</strong>的<code>O</code>。将所有保留的 <code>O</code>替换成<strong>临时字母</strong> <code>A</code>，最后将剩余<code>O</code> 修改为 <code>X</code>，再将 <code>A</code><strong>恢复</strong>即可。</p><h3 id="最短的桥-m">934. 最短的桥 (M)</h3><p><ahref="https://leetcode.cn/problems/shortest-bridge/">题目描述</a>：给定<span class="math inline">\(n\times n\)</span> 矩阵，矩阵上的 <spanclass="math inline">\(1\)</span> 代表陆地，<spanclass="math inline">\(0\)</span>代表海洋，网格中恰好有两座岛，可以将任意数量的 <spanclass="math inline">\(0\)</span> 修改为 <spanclass="math inline">\(1\)</span>，计算最少的修改次数。</p><p><strong>方法1</strong>：先 DFS标记第一个岛，并将其周围的一圈水域<strong>入队</strong>，再进行记忆化BFS，搜索水域并记录路径长度，直到搜索到第二个岛，时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：双向 BFS，用 DFS先标记两个岛，用两个队列分别存储两个岛周围的水域，<strong>每次从队列较小的进行扩展</strong>，比方法1 更快一些。</p><h3 id="最大人工岛-h">827. 最大人工岛 (H)</h3><p><ahref="https://leetcode.cn/problems/making-a-large-island/">题目描述</a>：给定<span class="math inline">\(n\times n\)</span> 矩阵，矩阵上的 <spanclass="math inline">\(1\)</span> 代表陆地，<spanclass="math inline">\(0\)</span> 代表海洋，最多可将一个 <spanclass="math inline">\(0\)</span> 变成 <spanclass="math inline">\(1\)</span>，返回可能的最大岛屿面积。</p><p><strong>方法1</strong>：DFS，第一次遍历用<strong>不同的数字标记</strong>岛屿，再用<strong>哈希表统计</strong>每个数字对应的面积。第二次遍历所有<spanclass="math inline">\(0\)</span>，将<strong>海洋的四个方向</strong>的岛屿面积相加。时空复杂度均为<span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：并查集，相邻的 <spanclass="math inline">\(1\)</span> 合并到同一个集合，遍历每个 <spanclass="math inline">\(0\)</span>，查找其相邻四个点所在集合，累加<strong>去重后的岛屿面积</strong>。时间复杂度为<span class="math inline">\(O(n^2 \alpha(n^2 ))\)</span>。</p><h3 id="todo水位上升">Todo水位上升</h3><h2 id="拓扑排序">拓扑排序</h2><p>拓扑排序发生在有向无环图（DAG）中，存在两种<strong>线性时间</strong>算法：</p><ul><li>入度表 BFS：用一个数组<strong>存储入度</strong>，将 0入度的点放入队列，依次删除队列中的点，删除后再找 0入度的点，<strong>删除顺序</strong>即为拓扑排序。</li><li>DFS判圈：用一个<strong>访问数组</strong>存储三种状态：<strong>未访问、已访问、已入栈</strong>，用<strong>栈</strong>来保存拓扑排序的顶点序列。<ul><li>从任意一个未访问过的点开始 DFS，先标记为已访问，再遍历 DFS邻接点。</li><li>如果遍历到的邻接点也是已访问的状态，则找到环；否则在遍历 DFS完所有邻接点后该点入栈。</li><li>保证在某顶点入栈前，其所有邻接点已入栈，<strong>栈顶到栈底</strong>即为拓扑排序。</li></ul></li></ul><h3 id="课程表-m">207. 课程表 (M)</h3><p><ahref="https://leetcode.cn/problems/course-schedule/">题目描述</a>：给定<span class="math inline">\(n\)</span>门课程，以及课程之间的<strong>先修关系</strong>，判断是否可能完成所有课程的学习。</p><p><strong>方法1</strong>：入度表BFS，存储<strong>有向图</strong>时记录每个节点的<strong>入度</strong>，将入度为<span class="math inline">\(0\)</span>的点放入队列，依次将队列里的每个节点删除（实际只要将<strong>其指向的点入度减一</strong>），并将入度变为<span class="math inline">\(0\)</span> 的点也放入队列。时间复杂度 <spanclass="math inline">\(O(V+E)\)</span>。</p><p><strong>方法2</strong>：DFS判圈，<strong>有向无环图一定存在拓扑排序</strong>，因此只要判断图中是否有圈。用<span class="math inline">\(vis\)</span> 数组记录三种状态，时间复杂度<span class="math inline">\(O(V+E)\)</span>。</p><h3 id="矩阵中的最长递增路径-h">329. 矩阵中的最长递增路径 (H)</h3><p><ahref="https://leetcode.cn/problems/longest-increasing-path-in-a-matrix/">题目描述</a>：给定<span class="math inline">\(m\times n\)</span>整数矩阵，每个单元格可以往四个方向移动，找出其中<strong>最长递增路径</strong>的长度。</p><p><strong>方法1</strong>：矩阵图模拟 + <strong>记忆化搜索</strong>，DFS终止条件是相邻点<strong>没有更大值</strong>，否则就先 DFS求出邻居的结果。用 vis 数组记录访问过的点，每次 DFS时<strong>剪枝</strong>，时空复杂度均为 <spanclass="math inline">\(O(mn)\)</span>。</p><p><strong>方法2</strong>：拓扑 + 入度表BFS，把矩阵看作一个<strong>有向图</strong>，有向边从较小数指向较大数，问题转化为「<strong>求图中最长路径</strong>」，从所有<strong>出度为零</strong>的点开始BFS，当搜索结束（队列空）时<strong>搜索的层数</strong>即为答案。时间复杂度为<span class="math inline">\(O(V+E)\approx O(mn)\)</span>。</p><h2 id="最短路问题">最短路问题</h2><p>最短路问题通常出现在<strong>带权图</strong>中，分为<strong>单源最短路</strong>（SSSP）和<strong>全源最短路</strong>（APSP）。前者包括Dijkstra（无法处理负权）、Bellman-Ford（可以处理负权，支持负圈检测）、DAGSSSP、SPFA 算法。后者常用 Floyd-Warshall算法。注意，最短路算法<strong>适用于有向图和无向图</strong>，只是在建图时用单向边或双向边的区别。</p><p>Dijksta的关键操作在于「<strong>松弛</strong>」，即对新加入顶点的可达顶点进行更新，使用了<strong>贪心</strong>思想。此外，<spanclass="math inline">\(getMin\)</span>操作可以用堆优化，适合边较少的<strong>稀疏图</strong>，复杂度 <spanclass="math inline">\(O(E\log E)\)</span>。堆优化模板如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">typedef</span> pair&lt;<span class="hljs-keyword">int</span>, <span class="hljs-keyword">int</span>&gt; pii;<br><span class="hljs-comment">// 建图</span><br>vector&lt;vector&lt;pii&gt;&gt; <span class="hljs-built_in">g</span>(n); <span class="hljs-comment">// n 是顶点数</span><br><span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> &amp;e: edges) &#123;<br>    <span class="hljs-keyword">int</span> u = e[<span class="hljs-number">0</span>], v = e[<span class="hljs-number">1</span>], w = e[<span class="hljs-number">2</span>];<br>    g[u].<span class="hljs-built_in">emplace_back</span>(v, w);<br>    g[v].<span class="hljs-built_in">emplace_back</span>(u, w); <span class="hljs-comment">// 建无向图，有向图需删掉</span><br>&#125;<br><span class="hljs-comment">// Dijkstra 算法，返回从 start 到每个点的最短路</span><br><span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">dist</span><span class="hljs-params">(n, INT_MAX)</span></span>; <span class="hljs-comment">// 初始所有节点都不可达</span><br>dist[start] = <span class="hljs-number">0</span>;<br>priority_queue&lt;pii, vector&lt;pii&gt;, greater&lt;pii&gt;&gt; pq; <span class="hljs-comment">// &#123;距离, 目标点&#125; 先按距离排序</span><br>pq.<span class="hljs-built_in">emplace</span>(<span class="hljs-number">0</span>, start);<br><span class="hljs-keyword">while</span> (!pq.<span class="hljs-built_in">empty</span>()) &#123;<br>    <span class="hljs-keyword">auto</span> [d, x] = pq.<span class="hljs-built_in">top</span>();<br>    pq.<span class="hljs-built_in">pop</span>();<br>    <span class="hljs-keyword">if</span> (d &gt; dist[x]) <span class="hljs-keyword">continue</span>; <span class="hljs-comment">// 堆里存的可能是已经访问过的点，这里相当于 vis</span><br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> &amp;[y, wt] : g[x]) &#123;<br>        <span class="hljs-keyword">int</span> new_d = dist[x] + wt;<br>        <span class="hljs-keyword">if</span> (new_d &lt; dist[y]) &#123;<br>            dist[y] = new_d;<br>            pq.<span class="hljs-built_in">emplace</span>(new_d, y); <span class="hljs-comment">// 将可达点放入堆，等待访问</span><br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>Floyd算法的本质也是<strong>贪心</strong>，常用<strong>邻接矩阵</strong>存图，最终矩阵中就是两两间的最短路，时间复杂度<span class="math inline">\(O(V^3)\)</span>，模板如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> inf = <span class="hljs-number">0x3f3f3f3f</span>;<br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">g</span>(n, vector&lt;<span class="hljs-keyword">int</span>&gt;(n, inf));<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++) g[i][i] = <span class="hljs-number">0</span>; <span class="hljs-comment">// 到自身的距离为 0</span><br><span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> &amp;e: edges) &#123;<br>    <span class="hljs-keyword">int</span> u = e[<span class="hljs-number">0</span>], v = e[<span class="hljs-number">1</span>], w = e[<span class="hljs-number">2</span>];<br>    g[u][v] = w;<br>    g[v][u] = w; <span class="hljs-comment">// 建无向图，有向图需删掉</span><br>&#125;<br><span class="hljs-comment">// Floyd 算法，k 为中间点，最终 g 中存储两两间的最短路</span><br><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> k = <span class="hljs-number">0</span>; k &lt; n; k++) &#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++) &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; n; j++) &#123;<br>            g[i][j] = <span class="hljs-built_in">min</span>(g[i][j], g[i][k] + g[k][j]);<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="网络延迟时间-m">743. 网络延迟时间 (M)</h3><p><ahref="https://leetcode.cn/problems/network-delay-time/">题目描述</a>：给定一个有向带权图，权重表示信号经过边的传递时间，求从节点<span class="math inline">\(k\)</span>发出信号后多久，所有节点都能收到。</p><p><strong>方法</strong>：单源最短路，用 Dijkstra 找出距离节点 <spanclass="math inline">\(k\)</span>最远的点，则该点收到信号时就是所有点都能收到信号。时间复杂度 <spanclass="math inline">\(O(E\log E )\)</span>。</p><p><strong>坑点</strong>：<span class="math inline">\(n\)</span>个节点的编号 <spanclass="math inline">\([1,n]\)</span>，建图时最好手动转为 <spanclass="math inline">\([0,n-1]\)</span>，否则会多一个<strong>不可能到达</strong>的节点<span class="math inline">\(0\)</span>。</p><h3 id="细分图中的可到达节点-h">882. 细分图中的可到达节点 (H)</h3><p><ahref="https://leetcode.cn/problems/reachable-nodes-in-subdivided-graph/">题目描述</a>：给定一个无向图，现在将<strong>每条边分裂</strong>出<span class="math inline">\(cnt_i\)</span>个细分点，得到新的一个细分图。计算此时从节点 <spanclass="math inline">\(0\)</span> 出发，经过 <spanclass="math inline">\(maxMoves\)</span> 步可以到达的节点数。</p><p><strong>方法</strong>：单源最短路，边上的细分点 <spanclass="math inline">\(cnt_i+1\)</span>可以看作<strong>边的权重</strong>，用 Dijkstra先在原图中找出起点能到达的所有顶点。对顶点 <spanclass="math inline">\(u\)</span>而言，到达后<strong>还剩的步数</strong>记为 <spanclass="math inline">\(resU\)</span>。则<strong>枚举所有边</strong> <spanclass="math inline">\((u,v)\)</span>，其上的细分点最多能访问 <spanclass="math inline">\(\max(resU+resV, cnt)\)</span> 个。时间复杂度 <spanclass="math inline">\(O(E\log E)\)</span>。</p><h3 id="todo-1334.-阈值距离内邻居最少的城市-m">Todo 1334.阈值距离内邻居最少的城市 (M)</h3><p><ahref="https://leetcode.cn/problems/find-the-city-with-the-smallest-number-of-neighbors-at-a-threshold-distance/">题目描述</a>：</p><p><strong>方法1</strong>：全源最短路</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><h3 id="最小体力消耗路径-m">1631. 最小体力消耗路径 (M)</h3><p><ahref="https://leetcode.cn/problems/path-with-minimum-effort/">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><h3 id="todo-407.-接雨水-ii-h">Todo 407. 接雨水 II (H)</h3><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p><h2 id="最小生成树">最小生成树</h2><p>两大算法：Prim（类似 Dijk）&amp; Kruskal（并查集）。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">prim</span><span class="hljs-params">(vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt; &gt;&amp; points, <span class="hljs-keyword">int</span> start)</span> </span>&#123;<br>    <span class="hljs-keyword">int</span> n = points.<span class="hljs-built_in">size</span>();<br>    <span class="hljs-keyword">int</span> res = <span class="hljs-number">0</span>;<br><br>    <span class="hljs-comment">// 1. 邻接矩阵</span><br>    vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt; &gt; <span class="hljs-built_in">g</span>(n, vector&lt;<span class="hljs-keyword">int</span>&gt;(n));<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++) &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = i + <span class="hljs-number">1</span>; j &lt; n; j++) &#123;<br>            <span class="hljs-keyword">int</span> dist = <span class="hljs-built_in">abs</span>(points[i][<span class="hljs-number">0</span>] - points[j][<span class="hljs-number">0</span>]) + <span class="hljs-built_in">abs</span>(points[i][<span class="hljs-number">1</span>] - points[j][<span class="hljs-number">1</span>]);<br>            g[i][j] = g[j][i] = dist;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">// 记录V中的点到Vnew的最近距离</span><br>    <span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">lowcost</span><span class="hljs-params">(n, INT_MAX)</span></span>;<br>    <span class="hljs-comment">// 记录V中的点是否加入到了Vnew</span><br>    <span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">v</span><span class="hljs-params">(n)</span></span>;<br>    <span class="hljs-comment">// lowcost 和 v 可以优化成一个数组</span><br><br>    <span class="hljs-comment">// 2. 先将start加入到Vnew</span><br>    v[start] = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++) &#123;<br>        <span class="hljs-keyword">if</span> (i == start) <span class="hljs-keyword">continue</span>;<br>        lowcost[i] = g[i][start];<br>    &#125;<br><br>    <span class="hljs-comment">// 3. 遍历剩余若干个未加入到Vnew的节点</span><br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> _ = <span class="hljs-number">1</span>; _ &lt; n; _++) &#123;<br>        <span class="hljs-comment">// 找出此时V中，离Vnew最近的点</span><br>        <span class="hljs-keyword">int</span> minIdx = <span class="hljs-number">-1</span>;<br>        <span class="hljs-keyword">int</span> minVal = INT_MAX;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; n; j++) &#123;<br>            <span class="hljs-keyword">if</span> (v[j] == <span class="hljs-number">0</span> &amp;&amp; lowcost[j] &lt; minVal) &#123;<br>                minIdx = j;<br>                minVal = lowcost[j];<br>            &#125;<br>        &#125;<br>        <span class="hljs-comment">// 将最近的点加入Vnew</span><br>        v[minIdx] = <span class="hljs-number">1</span>;<br>        res += minVal;<br><br>        <span class="hljs-comment">// 更新集合V中剩余所有点的lowcost</span><br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; n; j++) &#123;<br>            <span class="hljs-keyword">if</span> (v[j] == <span class="hljs-number">0</span> &amp;&amp; g[j][minIdx] &lt; lowcost[j]) &#123;<br>                lowcost[j] = g[j][minIdx];<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="连通性问题">连通性问题</h2><p>Tarjan 算法是基于 DFS的算法，可以在线性时间内求出<strong>无向图的割点与桥</strong>，进一步地可以求解<strong>无向图的双连通分量</strong>；同时，也可以求解<strong>有向图的强连通分量、必经点与必经边</strong>。</p><p>https://zhuanlan.zhihu.com/p/101923309</p><h2 id="网络流">网络流</h2><p>最大流最小割定理。</p>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #05-3 复杂动态规划</title>
    <link href="/LeetCode-DP-3.html"/>
    <url>/LeetCode-DP-3.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「动态规划」类型题中的：树形 DP、区间 DP、状压 DP、数位 DP等。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><p>动态规划（DP）适用于满足<strong>重叠子问题 +最优子结构</strong>的问题，常用的方法有「记忆化搜索」、「递推」等。DP的时间复杂度通常是「状态总数 <span class="math inline">\(\times\)</span>每个状态向其他状态转移的次数」。</p><p>动态规划（DP）的一般步骤：</p><ol type="1"><li>列出题目给的<strong>整体规模变量</strong>，例如 <spanclass="math inline">\(n,m\)</span></li><li>用局部变量 <span class="math inline">\(i,j,k\)</span>描述<strong>一般状态</strong>，例如 <spanclass="math inline">\(dp[i][j]\)</span></li><li>观察上述一般状态的<strong>最后一步</strong>，例如判断 <spanclass="math inline">\(s[i]==s[j]\)</span></li><li>去掉最后一步，问题规模缩小，变成<strong>子问题</strong>，例如 <spanclass="math inline">\(dp[i-1][j-1]\)</span></li><li>得到<strong>状态转移方程</strong>，例如 <spanclass="math inline">\(dp[i][j]=dp[i-1][j-1]+1\)</span></li><li><strong>初始值</strong>和<strong>最终状态</strong>，例如 <spanclass="math inline">\(dp[i][0]\)</span>、<spanclass="math inline">\(dp[0][j]\)</span> 和 <spanclass="math inline">\(dp[n][m]\)</span></li><li>可选的<strong>转移优化</strong>，例如记忆化、前缀和等</li></ol><h2 id="树形-dp">树形 DP</h2><p>树形 DP 的本质是「树上的一维线性DP」，其约束和转移通常来自<strong>父节点或子节点</strong>（很少有跨层），常用的套路也是：线性递推和记忆化搜索。而区别在于，树上的操作必须依赖DFS 进行：</p><ul><li>线性递推：适用于每个节点<strong>只有一种状态</strong>且<strong>只由相邻节点转移</strong>时，此时无论是从「根到叶」还是「叶到根」，每个节点都只会被访问一次，因此可以直接DFS。前者从根节点<strong>直接向下转移</strong>，后者需要到叶节点再<strong>回溯转移</strong>。</li><li>记忆化搜索：适用于每个节点<strong>有多个状态</strong>或<strong>可能受到祖先节点影响</strong>时，例如状态机DP，此时需要的 vis数组，可以通过<strong>哈希表映射</strong>存储每个节点对应的 DP值，同时标记是否访问过。</li></ul><h3 id="打家劫舍-iii-m">337. 打家劫舍 III (M)</h3><p><ahref="https://leetcode.cn/problems/house-robber-iii/">题目描述</a>：同其他两题，但是房子排成<strong>二叉树形</strong>，除根节点外，每栋房子有且只有一个父房子与之相连。</p><p><strong>方法1</strong>：树形 DP +记忆化搜索，当前节点的值<strong>由子节点转移</strong>，从根节点往下DFS。每个节点有<strong>两种操作，对应两种状态</strong>，用哈希表记录。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：树形 DP +递推，观察发现每个节点<strong>只受相邻节点影响</strong>，而且两个状态可以同时求出，利用一个<code>pair</code> <strong>捆绑作为 DFS的返回值</strong>，无需哈希表。空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="lcp-64.-二叉树灯饰-h">LCP 64. 二叉树灯饰 (H)</h3><p><ahref="https://leetcode.cn/problems/U7WvvU/">题目描述</a>：给定一个二叉树，树上每个节点均有一盏灯和三个开关，灯通过<span class="math inline">\(0/1\)</span> 表示关闭、开启状态。开关 1切换<strong>当前灯</strong>；开关 2切换<strong>当前点为子树的所有灯</strong>；开关 3切换<strong>当前灯和左右子节点灯</strong>。计算最少操作几次可以关闭所有灯。</p><p><strong>方法</strong>：树形 DP + 记忆化搜索，从根节点往下DFS，每个节点会受到祖先节点、父节点选择的影响，变成<strong>开灯或关灯</strong>状态，而不同状态的当前节点，也会有不同操作<strong>使其最终关闭</strong>并且<strong>继续影响</strong>子节点。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>定义状态(<strong>当前节点</strong>原始状态亮/灭，<strong>祖先节点</strong>开关 2的切换次数的奇偶性，<strong>父节点</strong>是否切换了开关3)，每个状态表示从当前状态出发，最少需要操作多少次开关，可以关闭子树所有节点的灯。</p><p>如果 DFS 到当前节点时，其「原始状态 + 祖先节点 +父节点」共同影响下为亮时，有四种操作：</p><ul><li>操作开关 1；操作开关 2；操作开关 3；操作开关 123。</li><li>四种操作都能使灯最终为灭，且影响到子树，因此取最小值。</li></ul><p>为灭时，也有四种操作：</p><ul><li>不操作任何一个开关；操作开关 12；操作开关 13；操作开关 23。</li></ul></blockquote><p><strong>坑点</strong>：每个节点虽然都是由其父节点转移，但是可能有<strong>四种转移方式</strong>，在递归深处的节点可能被反复计算了非常多次，因此需要记忆化。Python中在函数前用 <code>@cache</code> 即可对函数的每次调用进行记忆化，如果是C++ 则需要 vis数组，对于树结构可以先用<strong>哈希表映射</strong>得到连续数组下标。</p><h3 id="移除子树后的二叉树高度-h">2831. 移除子树后的二叉树高度 (H)</h3><p><ahref="https://leetcode.cn/problems/height-of-binary-tree-after-subtree-removal-queries/">题目描述</a>：二叉树的中有<span class="math inline">\(n&lt;1e5\)</span> 个节点，每个节点被分配<span class="math inline">\(1\)</span> 到 <spanclass="math inline">\(n\)</span> 的不同值，给定 <spanclass="math inline">\(m&lt;1e4\)</span>个查询，每次查询<strong>删除掉某个节点的所有子树</strong>后，整个树的<strong>高度</strong>（一个节点高度为0）。</p><p><strong>方法1</strong>：DFS 暴力，第一次 DFS 维护每个节点的 <spanclass="math inline">\(height\)</span>，父节点及兄弟节点编号。每次查询时再自底向上回溯，先看有无兄弟，如果有兄弟再看兄弟的高度分类讨论。时间复杂度<span class="math inline">\(O(nm)\)</span>，超时。</p><p><strong>方法2</strong>：两次 DFS + 树形 DP 线性递推，第一次 DFS<strong>自底向上回溯</strong>算出 <spanclass="math inline">\(height\)</span>，第二次 DFS<strong>自顶向下</strong>算出 <spanclass="math inline">\(depth\)</span>，同时在第二次 DFS中，还可以<strong>自顶向下处理出答案</strong>，最后查询时直接取答案即可。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>设删掉节点 <code>root</code> 的所有子树后的高度为 <spanclass="math inline">\(restH\)</span>，则删掉其左子树的所有子树后整个树的高度有两种可能：</p><ul><li>不包含 <code>root</code> 节点的路径最长：则这条节点贡献的高度就是<span class="math inline">\(restH\)</span>；</li><li>包含 <code>root</code> 节点的路径最长：则这条路径肯定来自<code>root</code> 的右子树，因此就是此时 <code>root</code> 的 <spanclass="math inline">\(depth\)</span> 加上右子树的 <spanclass="math inline">\(height\)</span>。</li></ul><p>其中 <span class="math inline">\(restH\)</span>只与父节点有关，因此可以自顶向下计算，这时一种 DP 的思想。</p></blockquote><p><strong>方法3</strong>：转 DFS 序 + 前缀处理，先 DFS遍历并<strong>按顺序存下遍历过的节点编号</strong>（DFS序），并预处理出每个节点的深度，并存储每个节点<strong>管辖的子树区间</strong>。每次删除点就相当于删除一段连续区间，并将原数组分为两段，分别代表子树的左侧和右侧。两端区间中每个节点深度的最大值即为答案，因此先预处理<strong>前缀和后缀MAX</strong>，总时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h2 id="区间-dp">区间 DP</h2><p>求解在一个区间上的最优解，可以将这个<strong>区间分割</strong>成一个个小区间，求解每个小区间的最优解，再<strong>合并小区间</strong>得到大区间。通常需要<spanclass="math inline">\(O(n^3)\)</span>，分别用三层循环来枚举：长度、起点、分割点。模板如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 枚举长度，从小区间开始</span><br><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> len = <span class="hljs-number">1</span>; len &lt;= n; len++) &#123; <br>    <span class="hljs-comment">// 枚举起点，ends &lt;= n，区间为 [i, ends)</span><br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i + len &lt;= n; i++) &#123; <br>        <span class="hljs-keyword">int</span> j = i + len;<br>        <span class="hljs-comment">// 枚举分割点，更新当前区间最优解，注意 dp[i][i] = 0</span><br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> k = i; k &lt; j; j++) &#123; <br>            dp[i][j] = <span class="hljs-built_in">min</span>(dp[i][j], dp[i][k] + dp[k+<span class="hljs-number">1</span>][j] + something);<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="oj-合并石子-h">OJ 合并石子 (H)</h3><p><a href="https://www.luogu.com.cn/problem/P1090">题目描述一</a>：N堆石子排成一条线，要将石子<strong>有序的</strong>合并成一堆，每次可以合并<strong>任意的</strong>两堆，代价为一次合并的石子总数，求N 堆石子合并成一堆的最小代价。</p><p><strong>方法</strong>：贪心 +优先队列，每次选择最小的两堆合并，合并后将新的结果放回队列，复杂度 <spanclass="math inline">\(O(n\logn)\)</span>。本题实际上是求解哈夫曼树的变形。</p><p><ahref="https://www.luogu.com.cn/problem/P1775">题目描述二</a>：同上，但每次只能合并<strong>相邻的</strong>两堆，求最小代价。</p><p><strong>方法1</strong>：区间 DP，<spanclass="math inline">\(\text{dp}[i][j]\)</span> 表示第 <spanclass="math inline">\(i\)</span> 堆<strong>到</strong>第 <spanclass="math inline">\(j\)</span> 堆石子合并的最小代价，<spanclass="math inline">\(\operatorname{sum}[i][j]\)</span> 表示第 <spanclass="math inline">\(i\)</span> 堆到第 <spanclass="math inline">\(j\)</span> 堆石子的总数（前缀和），复杂度为 <spanclass="math inline">\(O(n^3)\)</span>。状态转移方程如下： <spanclass="math display">\[\text{dp}[i][j]=\left\{\begin{array}{cc}0 &amp; i=j \\\min \left(\text{dp}[i][k], \text{dp}[k+1][j] \right)+\text{sum}[i][j]&amp; i \neq j\end{array}\right.\]</span> <strong>方法2</strong>：区间 DP +<strong>四边形不等式优化</strong>（交叉小于包含），复杂度可优化为 <spanclass="math inline">\(O(n^2)\)</span>。</p><p><ahref="https://www.luogu.com.cn/problem/P1880">题目描述三</a>：同上，但是N 堆石子排成<strong>环形</strong>！求最小代价。</p><p><strong>方法1</strong>：区间 DP，采用 mod N方式实现环形，但是要注意跨边界求解时前缀和、状态转移方程的书写。复杂度<span class="math inline">\(O(n^3)\)</span>。</p><p><strong>方法2</strong>：区间 DP +复制环，将原数组<strong>复制一遍</strong>扩展为 2N长度，但此时最外层循环<strong>枚举最大长度仍为 N</strong>。复杂度 <spanclass="math inline">\(O(n^3)\)</span>。</p><h3 id="戳气球-h">312. 戳气球 (H)</h3><p><ahref="https://leetcode.cn/problems/burst-balloons/">题目描述</a>：<spanclass="math inline">\(n\)</span>个气球排成一排，每个气球上都标有一个数，存在数组 <code>nums</code>中。扎破第 <span class="math inline">\(i\)</span> 个气球，可以获得<code>nums[i-1] * nums[i] * nums[i+1]</code>枚硬币。求所能获得硬币的最大数量。</p><p><strong>方法1</strong>：记忆化搜索，逆向思维，将整个过程看作<strong>每次添加一个</strong>气球直到填满，则<span class="math inline">\(dfs(i,j)\)</span> 表示将区间 <spanclass="math inline">\([i,j)\)</span> 填满的答案，目标是 <spanclass="math inline">\(dfs(0,n)\)</span>，每次需要<strong>枚举分割点</strong>并递归求解 <spanclass="math inline">\(dfs(i, mid)\)</span> 和 <spanclass="math inline">\(dfs(mid+1,j)\)</span>，采用<strong>记忆化</strong>防止重复搜索，<strong>状态数</strong>有<span class="math inline">\(n^2\)</span>，<strong>转移方向</strong>有<span class="math inline">\(n\)</span>，因此时间复杂度 <spanclass="math inline">\(O(n^3)\)</span>，空间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：区间 DP，正向思维，<spanclass="math inline">\(dp[i][j]\)</span> 表示区间 <spanclass="math inline">\([i,j)\)</span> 的答案，由于是半开半闭区间，起始时<span class="math inline">\(dp[i][i]=0\)</span>，目标是 <spanclass="math inline">\(dp[0][n]\)</span>，依次枚举<strong>长度、起点、分割点</strong>，时间复杂度<span class="math inline">\(O(n^3)\)</span>，空间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。 <span class="math display">\[dp\left[ i \right] \left[ j \right] =\max \left( dp\left[ i \right]\left[ k \right] +dp\left[ k+1 \right] \left[ j \right] +nums\left[ i-1\right] \times nums\left[ k \right] \times nums\left[ j \right] \right)\]</span><strong>坑点</strong>：为了减少分类讨论，一开始可以<strong>对数组进行处理</strong>，在两边各加上<span class="math inline">\(1\)</span>，目标转化为 <spanclass="math inline">\(dfs(1, n+1)\)</span> 和 <spanclass="math inline">\(dp[1][n+1]\)</span>，<strong>防止下标越界</strong>。在vector 起始位置插入元素采用 <code>v.insert(v.begin(), 1)</code>。</p><h3 id="合并石头的最低成本-h">1000. 合并石头的最低成本 (H)</h3><p><ahref="https://leetcode.cn/problems/minimum-cost-to-merge-stones/">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><h2 id="状压-dp">状压 DP</h2><p>也称为多维线性 DP，此类题的特点是<strong>数组很小</strong>，通常在 32个数以内，可以将<strong>每一个元素用一个二进制位标记进行状态压缩</strong>，数组下标0 的元素对应第 0 个二进制位，复杂度通常在指数级。</p><p>尽管数据范围很小，但在指数级复杂度下依然可能会溢出，如果直接用多维数组存储状态，匹配每个状态的时间要<spanclass="math inline">\(O(\Sigma)\)</span>，如果进行状态压缩，则只需要<spanclass="math inline">\(O(1)\)</span>，可以达到<strong>一个量级的优化效果</strong>。</p><p>状态压缩后的 DP 通常用<strong>一层循环递推求解</strong>，每个<code>dp[i]</code> 可能转移到多个 <code>dp[next]</code>、也可能由多个<code>dp[before]</code> 转移过来，其中 <spanclass="math inline">\(next\)</span>、<spanclass="math inline">\(before\)</span> 通过位运算求出。</p><h3 id="划分为-k-个相等的子集-h">698. 划分为 k 个相等的子集 (H)</h3><p><ahref="https://leetcode.cn/problems/partition-to-k-equal-sum-subsets/">题目描述</a>：给定一个<strong>长度不超过16</strong> 的整数数组 <code>nums</code> 和一个正整数<code>k</code>，找出是否有可能把这个数组分成 <code>k</code>个非空子集，各子集的总和都相等。</p><p><strong>方法1</strong>：暴力，如果元素总和不为 <code>k</code>的倍数，失败；设有 <code>k</code>个桶，每个元素都可能分进任何一个，复杂度 <spanclass="math inline">\(O(k^n)\)</span>。</p><p><strong>方法2</strong>：暴力 +剪枝，先进行排序，<strong>优先使用剩余的最大数值</strong>搜索；依旧对每个元素判断是否可以进当前桶，如果无法放入，则搜索下一个桶；如果<strong>相邻两个桶</strong>内的总量相同，则第二个桶可以跳过。</p><p><strong>方法3</strong>：状压 + DFS，用一个大小<code>1&lt;&lt;n</code> 的 <span class="math inline">\(vis\)</span>数组表示状态，<code>vis[s]</code> 表示在数字 <code>s</code>下<strong>是否可能成功</strong>，<code>s</code> 的二进制位为 1代表<strong>该元素未被使用</strong>。共有 <spanclass="math inline">\(2^n\)</span> 个状态，时间复杂度 <spanclass="math inline">\(O(2^n)\)</span>。</p><blockquote><p>初始将所有状态设为 true（表示所有状态都有可能），<code>s</code> 从全1 开始 DFS（表示每个元素都未使用），递归结束的条件是 <code>s</code> 全0（表示所有元素都被使用），直接返回 true。</p><p>DFS 前<strong>先将当前状态置false</strong>，因为如果这个状态可能成功，那就必然直接成功；如果不能成功，则下次再访问到也只是顺序调换了，直接剪枝。</p><p>DFS 时，<strong>根据当前状态还有剩余 1的个数进行分支</strong>（前提是加入该数后不会溢出），但是大部分分支不会访问到，实际访问不会超过<span class="math inline">\(2^n\)</span> 个状态。</p></blockquote><p><strong>方法4</strong>：状压 + DP，用一个大小 <code>1&lt;&lt;n</code>的 <span class="math inline">\(dp\)</span> 和 <spanclass="math inline">\(sum\)</span>数组表示状态及当前桶内总和，<code>dp[s]</code> 表示在状态 <code>s</code>下<strong>是否可能成功</strong>，<code>s</code> 的二进制位为 0代表<strong>该元素未被使用</strong>。递推求解，<strong>每个状态只会被求解一次</strong>，时间复杂度<span class="math inline">\(O(2^n)\)</span>。</p><blockquote><p>初始将所有状态设为 false（表示所有状态都未知），只记 <spanclass="math inline">\(dp[0]=ture\)</span>（表示每个元素都未使用时可能可行）。从<code>s</code> 从全 0 开始递推，最终返回<code>dp[(1&lt;&lt;n)-1]</code>。</p><p>DP 转移时，<strong>根据当前状态还有剩余 1 的个数进行转移</strong>得到<spanclass="math inline">\(dp[next]\)</span>，若加入该数后不会溢出，则该状态可能可行，标记为true，否则保持 false。</p><p>DP 线性递推时，新来的每个 <code>dp[i]</code> 都会由比 <code>i</code>小的若干状态转移过来，如果仍为 false，则表示不可能发生，直接剪枝。</p></blockquote><h3 id="n-次操作后的最大分数和-h">1799. N 次操作后的最大分数和 (H)</h3><p><ahref="https://leetcode.cn/problems/maximize-score-after-n-operations/">题目描述</a>：给定一个<strong>长度不超过14</strong> 的整数数组，要求将其中数字<strong>两两配对</strong>，第<code>i</code> 对可以获得 <code>i*gcd(x,y)</code>积分，计算配对结束后能获得的最大分数和。</p><p><strong>方法</strong>：状压 + DP，由于数据很少，先处理出<code>gcd</code> 值。用大小 <code>1&lt;&lt;n</code> 的 <spanclass="math inline">\(dp\)</span>存储每个数被选中的状态，<code>dp[s]</code> 表示选中 <code>s</code>时的分数和。<strong>顺序求解</strong> <spanclass="math inline">\(2^n\)</span> 个状态，每个状态有 <spanclass="math inline">\(n\)</span>种可能转移，且<strong>转移的来源必定求解过</strong>，复杂度 <spanclass="math inline">\(O(n\cdot 2^n)\)</span>。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++">dp[s] = <span class="hljs-built_in">max</span>(dp[s], dp[s ^ (<span class="hljs-number">1</span> &lt;&lt; i) ^ (<span class="hljs-number">1</span> &lt;&lt; j)] + cnt / <span class="hljs-number">2</span> * g[i][j]);<br></code></pre></td></tr></table></figure><blockquote><p>用一层循环递推求 <code>dp[s]</code>，初始 <code>dp[0]=0</code>，返回<code>dp[(1&lt;&lt;n)-1]</code>。由于每次的转移都是将两个 <spanclass="math inline">\(1\)</span> 替换成 <spanclass="math inline">\(0\)</span>，因此转移的来源必定求解过，所以直接访问<code>dp[s ^ (1 &lt;&lt; i) ^ (1 &lt;&lt; j)]</code>。</p></blockquote><h2 id="数位-dp">数位 DP</h2>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #05-2 二维动态规划</title>
    <link href="/LeetCode-DP-2.html"/>
    <url>/LeetCode-DP-2.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「动态规划」类型题中的：二维线性 DP、字符串匹配 DP、背包 DP等。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><p>动态规划（DP）适用于满足<strong>重叠子问题 +最优子结构</strong>的问题，常用的方法有「记忆化搜索」、「递推」等。DP的时间复杂度通常是「状态总数 <span class="math inline">\(\times\)</span>每个状态向其他状态转移的次数」。</p><p>动态规划（DP）的一般步骤：</p><ol type="1"><li>列出题目给的<strong>整体规模变量</strong>，例如 <spanclass="math inline">\(n,m\)</span></li><li>用局部变量 <span class="math inline">\(i,j,k\)</span>描述<strong>一般状态</strong>，例如 <spanclass="math inline">\(dp[i][j]\)</span></li><li>观察上述一般状态的<strong>最后一步</strong>，例如判断 <spanclass="math inline">\(s[i]==s[j]\)</span></li><li>去掉最后一步，问题规模缩小，变成<strong>子问题</strong>，例如 <spanclass="math inline">\(dp[i-1][j-1]\)</span></li><li>得到<strong>状态转移方程</strong>，例如 <spanclass="math inline">\(dp[i][j]=dp[i-1][j-1]+1\)</span></li><li><strong>初始边界值</strong>和<strong>最终状态</strong>，例如 <spanclass="math inline">\(dp[i][0]\)</span>、<spanclass="math inline">\(dp[0][j]\)</span> 和 <spanclass="math inline">\(dp[n][m]\)</span></li><li>可选的<strong>转移优化</strong>，例如记忆化、前缀和等</li></ol><h2 id="二维线性-dp">二维线性 DP</h2><p>需要用到一个二维数组进行转移，而转移的方向通常是相邻的左方、左上方、上方状态，因此可以<strong>采用滚动数组优化空间复杂度</strong>。</p><h3 id="不同路径-m">62. 不同路径 (M)</h3><p><ahref="https://leetcode.cn/problems/unique-paths/">题目描述</a>：一个机器人位于一个<code>m x n</code>网格的<strong>左上角</strong>，试图达到网格的<strong>右下角</strong>，求路径数目。</p><p><strong>方法1</strong>：DP，用 <spanclass="math inline">\(dp[i][j]\)</span> 表示从左上角走到 <spanclass="math inline">\((i,j)\)</span>的路径数，<strong>首行和首列</strong>初始化为 1，时空复杂度均为 <spanclass="math inline">\(O(n^2)\)</span>。 <span class="math display">\[dp[i][j]=dp[i-1][j] + dp[i][j-1]\]</span> <strong>方法2</strong>：DP +滚动数组，下一个状态只由最近两行的状态转移，因此可以用两行数组，空间复杂度为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：<strong>组合数公式</strong>，从左上角到右下角的过程中，总共需要走<span class="math inline">\(m+n-2\)</span> 步，其中必有 <spanclass="math inline">\(m-1\)</span> 步向下，其余向右。因此答案就是 <spanclass="math inline">\(C^{m-1}_{m+n-2}\)</span>。数字较小可以直接用定义计算，时间复杂度<span class="math inline">\(O(\min(m,n))\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>拓展</strong>：<ahref="https://leetcode.cn/problems/unique-paths-ii/">63. 不同路径II</a>，路径中加入了障碍物，此时在递推时如果<strong>遇到障碍物</strong>，则直接将该点的状态<strong>置为零</strong>。需要注意的是，在初始化首行和首列时，<strong>如果遇到障碍物，则后续的位置都不可访问</strong>！</p><h3 id="最大正方形-m">221. 最大正方形 (M)</h3><p><ahref="https://leetcode.cn/problems/maximal-square/">题目描述</a>：在一个由<code>'0'</code> 和 <code>'1'</code> 组成的二维矩阵内，找到只包含<code>'1'</code> 的<strong>最大正方形</strong>，并返回其面积。</p><p><strong>方法1</strong>：暴力，对于每个元素 <spanclass="math inline">\(1\)</span>，将其作为右下角，<strong>扫描</strong>确定其能构成的最大正方形，时间复杂度<span class="math inline">\(O(n^4)\)</span>。</p><p><strong>方法2</strong>：二维线性 DP，<spanclass="math inline">\(dp[i][j]\)</span> 表示 <spanclass="math inline">\((i,j)\)</span><strong>作为右下角能构成的最大</strong>正方形，状态转移时应从周边三个中取最小值，<strong>确保剩下三个角都能取到</strong>，时空复杂度均为<span class="math inline">\(O(n^2)\)</span>。 <spanclass="math display">\[dp\left[ i \right] \left[ j \right] =\min \left( dp\left[ i-1 \right]\left[ j \right] ,\; dp\left[ i \right] \left[ j-1 \right] ,\; dp\left[i-1 \right] \left[ j-1 \right] \right) +1\]</span><strong>方法3</strong>：滚动数组，前三个状态都可以计算好，左上角的状态用<strong>临时变量</strong>存储，空间复杂度降到<span class="math inline">\(O(n)\)</span>。</p><h3 id="香槟塔-m">799. 香槟塔 (M)</h3><p><ahref="https://leetcode.cn/problems/champagne-tower/">题目描述</a>：把玻璃杯摆成金字塔的形状，其中第<span class="math inline">\(i\)</span> 层有 <spanclass="math inline">\(i\)</span> 个玻璃杯，从最上方倒入 <spanclass="math inline">\(k\)</span> 杯香槟，求第 <spanclass="math inline">\(n\)</span> 层第 <spanclass="math inline">\(m\)</span> 杯的量。</p><p><strong>方法1</strong>：二维 DP 递推，记 <spanclass="math inline">\(dp[i][j]\)</span> 为第 <spanclass="math inline">\(i\)</span> 行第 <spanclass="math inline">\(j\)</span>杯<strong>所经过的水的流量</strong>（而<strong>不是最终剩余的水量</strong>），则<span class="math inline">\(dp[0][0]=k\)</span>，答案为 <spanclass="math inline">\(\min(dp[n][m], 1)\)</span>。当 <spanclass="math inline">\(dp[i][j]\leqslant 1\)</span>时，不会有水从杯子里溢出，则该状态<strong>不能转移到其他状态</strong>，否则会有<span class="math inline">\(dp[i][j]-1\)</span>的水等量地转移到下方两个杯子。时间复杂度 <spanclass="math inline">\(O(nm)\)</span>，空间复杂度为 <spanclass="math inline">\(O(n^2)\)</span>。 <span class="math display">\[\left\{ \begin{array}{c}    dp\left[ i+1 \right] \left[ j \right] +=\frac{dp\left[ i \right]\left[ j \right] -1}{2}\\    dp\left[ i+1 \right] \left[ j+1 \right] +=\frac{dp\left[ i \right]\left[ j \right] -1}{2}\\\end{array} \right.\]</span> <strong>方法2</strong>：二维 DP + 滚动数组，空间优化到 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="oj-数组弹出乘积的最大和-m">OJ 数组弹出乘积的最大和 (M)</h3><p><strong>题目描述</strong>：给定两个等长的数组，其中一个支持<code>pop_front</code>，另一个支持 <code>pop_front</code> 和<code>pop_back</code>。每次<strong>各弹出一个</strong>元素，并将两个元素相乘后累加，求数组<strong>全部弹出</strong>后累加的最大值。</p><p><strong>方法1</strong>：二维DP，第一个数组弹出的<strong>顺序是固定</strong>的，用 <spanclass="math inline">\(now\)</span> 表示。<spanclass="math inline">\(dp[i][j]\)</span>表示<strong>第二个数组</strong>剩下 <spanclass="math inline">\([i,j)\)</span> 时的最大和，则初始 <spanclass="math inline">\(dp[0][n]=0\)</span>，最终返回 <spanclass="math inline">\(dp[i][i]\)</span>中的最大值（对角线）。每个状态有两种转移可能，时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。 <span class="math display">\[dp\left[ i \right] \left[ j \right] =\max \left\{ dp\left[ i \right]\left[ j+1 \right] +now\times nums\left[ j+1 \right] ,\;\; dp\left[ i-1\right] \left[ j \right] +now\times nums\left[ i-1 \right] \right\}\]</span> <strong>方法2</strong>：反向二维DP，由于第一个数组弹出顺序固定，可以将题意逆向为 <code>push</code>，则<span class="math inline">\(dp[i][j]\)</span> 表示压入 <spanclass="math inline">\([i,j)\)</span> 时的最大值，则初始 <spanclass="math inline">\(dp[i][i]=0\)</span>，最终返回 <spanclass="math inline">\(dp[0][n]\)</span>。沿着对角线遍历，时间复杂度<span class="math inline">\(O(n^2)\)</span>。</p><h3 id="分汤-h">808. 分汤 (H)</h3><p><a href="">题目描述</a>：有 A 和 B 两种汤各 <spanclass="math inline">\(n\)</span> 毫升，现以等概率进行四种操作：<spanclass="math inline">\((4,0),(3,1),(2,2),(1,3)\)</span>，其中 <spanclass="math inline">\((a,b)\)</span> 表示分走 <spanclass="math inline">\(a\)</span> 毫升的 A 汤和 <spanclass="math inline">\(b\)</span> 毫升的 B 汤。求 <strong>A先分配完</strong>的概率 + <strong>AB 同时分配完</strong>的概率 / 2。</p><p><strong>方法1</strong>：二维 DP + 浮点近似，用 <spanclass="math inline">\(dp[i][j]\)</span> 表示汤剩下 <spanclass="math inline">\((i,j)\)</span> 时的答案，时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>，但是可以提前<strong>特判</strong>。<span class="math display">\[dp\left[ i \right] \left[ j \right] =\frac{1}{4}\times \left( dp\left[i-4 \right] \left[ j \right] +dp\left[ i-3 \right] \left[ j-1 \right]+dp\left[ i-2 \right] \left[ j-2 \right] +dp\left[ i-1 \right] \left[j-3 \right] \right)\]</span></p><blockquote><p>边界值 <span class="math inline">\(dp[i][0]\)</span> 表示 B汤先分配完，此时答案为 <span class="math inline">\(0\)</span>；<spanclass="math inline">\(dp[0][0]\)</span> 表示同时分配完，此时答案为 <spanclass="math inline">\(1/2=0.5\)</span>；<spanclass="math inline">\(dp[0][j]\)</span> 表示 A 汤先分配完，答案为 <spanclass="math inline">\(1\)</span>。最终结果时 <spanclass="math inline">\(dp[n][n]\)</span>。</p><p>本题 <span class="math inline">\(n&lt;10^9\)</span>，按照 <spanclass="math inline">\(O(n^2)\)</span>的复杂度不可能完成，但是由于返回值在正确答案 <spanclass="math inline">\(10^{-5}\)</span>的<strong>范围内将被认为是正确</strong>的，而本题的结果显然随着 <spanclass="math inline">\(n\)</span> 递增而递增，测算发现在 <spanclass="math inline">\(n\geqslant 179\)</span> 时只需输出 <spanclass="math inline">\(1.0\)</span> 作为答案即可，可以特判。</p></blockquote><p><strong>方法2</strong>：记忆化搜索，依然需要先进行特判，再用 <spanclass="math inline">\(vis\)</span> 数组记录访问过的结果，时间复杂度<span class="math inline">\(O(n^2)\)</span>。</p><h3 id="矩阵中和能被-k-整除的路径-h">2435. 矩阵中和能被 K 整除的路径(H)</h3><p><ahref="https://leetcode.cn/problems/paths-in-matrix-whose-sum-is-divisible-by-k/">题目描述</a>：给定<code>m x n</code> 的整数矩阵 <code>grid</code> 和一个整数<code>k</code>。从<strong>左上角</strong>出发，每一步只能往<strong>下</strong>或者往<strong>右</strong>，到达<strong>右下角</strong>。返回<strong>路径和</strong>能被<code>k</code> 整除的路径数目。</p><p><strong>方法1</strong>：记忆化搜索，显然<strong>要将路径和作为一个维度</strong>，用一个<code>vis[i][j][num]</code> 表示从 <spanclass="math inline">\((i,j)\)</span>走到<strong>右下角</strong>且路径和模 <spanclass="math inline">\(k\)</span> 为 <spanclass="math inline">\(num\)</span> 的路径数，从左上角开始DFS，右下角结束。最后返回 <code>vis[0][0][0]</code>，时空复杂度均为<span class="math inline">\(O(mnk)\)</span>。</p><p><strong>方法2</strong>：DP，发现搜索的方向具有<strong>无后效性</strong>，因此改用<strong>常数更小</strong>的DP，<span class="math inline">\(dp[i][j][num]\)</span>表示从<strong>左上角</strong>走到 <spanclass="math inline">\((i,j)\)</span> 且路径和模 <spanclass="math inline">\(k\)</span> 为 <spanclass="math inline">\(num\)</span> 的路径数，初始值 <spanclass="math inline">\(dp[0][0][grid[0][0]\;\%\;k]=1\)</span>，时空复杂度均为<span class="math inline">\(O(mnk)\)</span>。 <spanclass="math display">\[dp[i][j][(num+\operatorname{grid}[i][j]) \;\%\;k]+=dp[i][j-1][num]+dp[i-1][j][num],\;\; num\in[0,k)\]</span> 或者： <span class="math display">\[\begin{aligned}dp[i][j][num]=dp[i][j-1][((num-\mathrm{grid[}i][j])\;\%\;k+k)\;\%\;k]\\+dp[i-1][j][((num-\mathrm{grid[}i][j])\;\%\;k+k)\;\%\;k]\end{aligned}\]</span> <strong>方法3</strong>：DP +滚动数组，前两个维度中，<strong>每组</strong>状态只由<strong>左边、上面两组</strong>状态转移过来，因此可以用两个<span class="math inline">\(n\times k\)</span>的数组交替存储。空间复杂度优化到 <spanclass="math inline">\(O(nk)\)</span>。</p><p><strong>坑点</strong>：采用第二个状态转移方程，更符合直觉，但是要注意<strong>负数取模防止越界</strong>！</p><h2 id="字符串匹配-dp">字符串匹配 DP</h2><p>字符串匹配是常见的 DP 问题背景，具体可分为下面两类：</p><ul><li><p>给定两个字符串进行<strong>某种规则匹配</strong>，通常是二维线性DP 表示两个子串。</p></li><li><p>给定一个字符串，从<strong>短序列到长序列</strong>扩展（如回文串），通常用一个二维数组表示区间的状态，但只会用到上三角。</p></li></ul><h3 id="最长回文子串-m">5. 最长回文子串 (M)</h3><p><ahref="https://leetcode.cn/problems/longest-palindromic-substring">题目描述</a>：给一个字符串<code>s</code>，找到 <code>s</code> 中最长的回文子串。</p><p><strong>方法1</strong>：暴力，两层循环，遍历区间起始位置和终止位置，然后判断这个区间是不是回文，复杂度<span class="math inline">\(O(n^3)\)</span>。</p><p><strong>方法2</strong>：二维 DP，<spanclass="math inline">\(P(i,j)\)</span> 表示 <spanclass="math inline">\(s[i\cdots j]\)</span>是否为回文串（<strong>布尔值</strong>），复杂度 <spanclass="math inline">\(O(n^2)\)</span>。 <span class="math display">\[\left\{\begin{array}{l}P(i, i)&amp;=\,\,\text {true} \\P(i, i+1)&amp;=\,\,\left(S_i==S_{i+1}\right) \\P(i, j)&amp;=\,\,P(i+1, j-1) \wedge\left(S_i==S_j\right)\end{array}\right.\]</span> &gt; 在转移过程中，<spanclass="math inline">\(dp[i][j]\)</span>仅为二维数组的上三角矩阵，且要注意<strong>扫描的顺序</strong>，<strong>从长度较短的字符串向长度较长的字符串进行转移</strong>。因此外层循环是<code>j++</code>，内层循环是 <code>i--</code>，才能确保无后效性。</p><p><strong>方法3</strong>：中心扩散算法（回文串算法），如果画出方法 2 的DP矩阵，则会发现状态转移的路径都是沿着<strong>反对角线</strong>的直线，即<strong>所有状态在转移时的可能性都是唯一的</strong>，因此可以从每一种边界情况<span class="math inline">\(P(i, i)\)</span>开始扩散，尝试每种边界情况的状态转移最大步数。复杂度 <spanclass="math inline">\(O(n^2)\)</span>。</p><p><strong>方法4</strong>：Manacher算法，利用回文串的对称性，将复杂度降低到 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="回文子串-m">647. 回文子串 (M)</h3><p><ahref="https://leetcode.cn/problems/palindromic-substrings/">题目描述</a>：给一个字符串<code>s</code>，统计并返回这个字符串中<strong>回文子串</strong>的数目。</p><p><strong>方法1</strong>：二维 DP，同上题，先判断再累计求和，时间复杂度<span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：中心扩散算法，两层循环，外层遍历<strong>回文中心点</strong>，内层<strong>向两边扩散</strong>，需要注意中心可能是1 个字符或 2 个字符，因此<strong>内层循环有两次</strong>。时间复杂度<span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法3</strong>：Manacher算法，利用回文串的对称性，将复杂度降低到 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="分割回文串-ii-h">132. 分割回文串 II (H)</h3><p><ahref="https://leetcode.cn/problems/palindrome-partitioning-ii/">题目描述</a>：给一个字符串<code>s</code>，请你将 <code>s</code>分割成若干回文串。返回符合要求的<strong>最少分割次数</strong> 。</p><p><strong>方法1</strong>：二维 DP + 一维 DP，同上题，先得到 <spanclass="math inline">\(dp[i][j]\)</span> 数组表示 <spanclass="math inline">\(s[i\cdots j]\)</span>是否为回文串（<strong>布尔值</strong>），再线性扫描字符串，用 <spanclass="math inline">\(res[j]\)</span> 表示 <spanclass="math inline">\(s[0\cdots j]\)</span>最少可分回文串数量。两次操作的时间复杂度都是 <spanclass="math inline">\(O(n^2)\)</span>。 <span class="math display">\[res[j]=\left\{ \begin{array}{c}    1 \,\,, &amp; \mathrm{if} (dp[0][j]==true) \\     \min(res[i]+1) \,\,, &amp;\mathrm{otherwise\; if}(dp[i+1][j]==true)\\\end{array} \right.\]</span></p><h3 id="todo-516.-最长回文子序列-m">Todo 516. 最长回文子序列 (M)</h3><p><ahref="https://leetcode.cn/problems/longest-palindromic-subsequence/">题目描述</a>：给一个字符串<code>s</code>，找到 <code>s</code>中最长的回文子序列，返回长度，<strong>子序列不要求连续</strong>。</p><p><strong>方法1</strong>：二维 DP，<spanclass="math inline">\(dp[i][j]\)</span> 表示下标范围 <spanclass="math inline">\([i,j]\)</span> 内的最长回文子序列的长度，</p><p><strong>方法2</strong>：注意到，<code>s</code> 和<code>s.reverse()</code>的<strong>最长公共子序列</strong>（LCS）就是最长回文子序列，最快 <spanclass="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法3</strong>：</p><h3 id="最长公共子序列-lcs-m">1143. 最长公共子序列 (LCS) (M)</h3><p><ahref="https://leetcode.cn/problems/longest-common-subsequence/">题目描述</a>：给定两个字符串，返回两个字符串的最长公共子序列长度，<strong>子序列不需要连续</strong>。</p><p><strong>方法1</strong>：DP，用 <spanclass="math inline">\(dp[i][j]\)</span> 存储 <spanclass="math inline">\(s1\)</span> 的前 <spanclass="math inline">\(i\)</span> 个字符与 <spanclass="math inline">\(s2\)</span> 中的前 <spanclass="math inline">\(j\)</span> 个字符，返回 <spanclass="math inline">\(dp[n][m]\)</span>，时空复杂度均为 <spanclass="math inline">\(O(nm)\)</span>。 <span class="math display">\[dp[i][j]=\left\{ \begin{array}{c}    dp\left[ i-1 \right] \left[ j-1 \right] +1 \,\,, &amp;\mathrm{if}\left( s\left[ i-1 \right] =s\left[ j-1 \right] \right)\,\,\\    \max \left( dp\left[ i \right] \left[ j-1 \right] , dp\left[ i-1\right] \left[ j \right] \right) \,\,, &amp;\mathrm{otherwise}\\\end{array} \right.\]</span> <strong>方法2</strong>：DP +滚动数组，用单行数组记录，再用一个<strong>额外变量</strong>存储 <spanclass="math inline">\(dp[i-1][j-1]\)</span> 即可，空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：<strong>下标化</strong>，用哈希表 + 向量记录<code>s2</code>中每个字符出现的下标，并用这些<strong>下标的倒序</strong>替换<code>s1</code>中对应字符，得到一个下标序列，求其<strong>最长上升子序列</strong>（LIS）即可得到答案。平均复杂度<span class="math inline">\(O(n \logn)\)</span>，当字符串<strong>稠密重复</strong>时退化到 <spanclass="math inline">\(O(n^2 \log n)\)</span>。</p><blockquote><p>举例：<span class="math inline">\(s1 = \{a,b,c,d,b\}\)</span>，<spanclass="math inline">\(s2 =\{b,c,a,b\}\)</span>，则 <spanclass="math inline">\(a\)</span> 对应在 <spanclass="math inline">\(s2\)</span> 的下标为 <spanclass="math inline">\(2\)</span>，<span class="math inline">\(b\)</span>对应下标为 <span class="math inline">\(\{0,3\}\)</span>，<spanclass="math inline">\(c\)</span> 对应序号为 <spanclass="math inline">\(1\)</span>，<span class="math inline">\(d\)</span>对应为空，<strong>逆序</strong>后生成的新序列为 <spanclass="math inline">\(\{2, 3, 0, 1, 3, 0\}\)</span>，其最长上升子序列为<span class="math inline">\(\{0,1,3\}\)</span>，对应的公共子序列为 <spanclass="math inline">\(\{b, c, b\}\)</span>。</p><p>此处可以证明：s1、s2的一个公共子序列和新序列的一个严格递增子序列一一对应。任意两个不同字符，如果能被选中作为公共子序列的一部分，其下标必严格递增。而之所以要逆序就是<strong>防止同一个位置的字符被多次选中</strong>。</p><p>退化：<span class="math inline">\(s1 = \{a,a,a,a,a\}\)</span>，<spanclass="math inline">\(s2 =\{a,a,a,a\}\)</span>，则生成的新序列长度为<span class="math inline">\(n\times m\)</span>，还要再乘上 <spanclass="math inline">\(\log(n\timesm)\)</span>。当然如果确保无重复，则可以保证复杂度不退化。</p></blockquote><h3 id="编辑距离-h">72. 编辑距离 (H)</h3><p><ahref="https://leetcode.cn/problems/edit-distance/">题目描述</a>：给定两个字符串，返回将<code>s1</code> 修改成 <code>s2</code>需要的最少操作数，一次操作可以是<strong>插入、删除、替换</strong>一个字符。</p><p><strong>方法1</strong>：DP，用 <spanclass="math inline">\(dp[i][j]\)</span> 存储 <spanclass="math inline">\(s1\)</span> 的前 <spanclass="math inline">\(i\)</span> 个字符与 <spanclass="math inline">\(s2\)</span> 中的前 <spanclass="math inline">\(j\)</span> 个字符，返回 <spanclass="math inline">\(dp[n][m]\)</span>，时空复杂度均为 <spanclass="math inline">\(O(nm)\)</span>。 <span class="math display">\[dp[i][j]=\left\{ \begin{array}{c}    dp\left[ i-1 \right] \left[ j-1 \right] \,\,, &amp;\mathrm{if}\left( s\left[ i-1 \right] =s\left[ j-1 \right] \right)\,\,\\    \min \left( dp\left[ i-1 \right] \left[ j-1 \right], \; dp\left[ i\right] \left[ j-1 \right] ,\; dp\left[ i-1 \right] \left[ j \right]\right)+1 \,\,, &amp;\mathrm{otherwise}\\\end{array} \right.\]</span></p><blockquote><p>当新来的两个字符相等时，不会增加编辑距离，因此由左上角转移。当两个字符不相等时，有三种转移可能：</p><ul><li>通过替换一个 <span class="math inline">\(s1\)</span> 字符到 <spanclass="math inline">\(s2\)</span> 字符（或反之），代价为 <spanclass="math inline">\(dp[i-1][j-1] + 1\)</span></li><li>通过删除一个 <span class="math inline">\(s1\)</span> 字符，代价为<span class="math inline">\(dp[i-1][j]+1\)</span></li><li>通过删除一个 <span class="math inline">\(s2\)</span> 字符，代价为<span class="math inline">\(dp[i][j-1]+1\)</span></li></ul></blockquote><p><strong>方法2</strong>：DP +滚动数组，用单行数组记录，再用一个<strong>额外变量</strong>存储 <spanclass="math inline">\(dp[i-1][j-1]\)</span> 即可，空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="正则表达式匹配-h">10. 正则表达式匹配 (H)</h3><p><ahref="https://leetcode.cn/problems/regular-expression-matching">题目描述</a>：给定字符串<code>s</code> 和模式 <code>p</code>，实现一个支持 <code>.</code> 和<code>*</code> 的正则表达式匹配，其中 <code>.</code>匹配任意单个字符，<code>*</code>匹配<strong>零或多个</strong>前一个字符。</p><p><strong>方法1</strong>：正向扫描，但是不知道 <code>*</code>该替换成多少个字符，例如：<code>aaaa</code> 和<code>a*</code>、<code>a*a</code>、<code>a*aa</code>，要考虑后面跟着的字符。同理反向扫描也得考虑<code>a*</code>、<code>aa*</code>、<code>aaa*</code>。实际操作中可以用DFS + 记忆化处理，复杂度为 <spanclass="math inline">\(O(mn)\)</span>。</p><p><strong>方法2</strong>：DP，用 <spanclass="math inline">\(f[i][j]\)</span> 表示 <spanclass="math inline">\(s\)</span> 的前 <spanclass="math inline">\(i\)</span> 个字符与 <spanclass="math inline">\(p\)</span> 中的前 <spanclass="math inline">\(j\)</span> 个字符是否能够匹配。复杂度为 <spanclass="math inline">\(O(mn)\)</span>。</p><blockquote><p>字母 + 星号的组合在匹配时，要么匹配掉 <code>s</code>的一个字符，转移到 <span class="math inline">\(f[i-1][j]\)</span> 或<spanclass="math inline">\(f[i][j-2]\)</span>；要么不匹配直接消去，转移到<spanclass="math inline">\(f[i][j-2]\)</span>。此外，匹配成功仅当两方相同或者一方是<code>.</code>，写成函数 <spanclass="math inline">\(\mathrm{matches}\)</span>，状态转移方程如下：<span class="math display">\[f[i][j]=\begin{cases}  \,\,\mathrm{if} \left( p[j]\ne \text{星号} \right) =\begin{cases}  f[i-1][j-1],&amp;       \,\,\mathrm{matches} (s[i],p[j])\\  \mathrm{false},&amp;        \,\,\mathrm{otherwise}\\\end{cases}\\  \,\,\mathrm{otherwise} =\begin{cases}  f[i-1][j]\,\, \mathrm{or}\,\,f[i][j-2],&amp;       \,\,\mathrm{matches} (s[i],p[j-1])\\  f[i][j-2],&amp;     \,\,\mathrm{otherwise}\\\end{cases}\\\end{cases}\]</span></p></blockquote><p><strong>坑点</strong>：</p><ol type="1"><li><span class="math inline">\(f[0][0]\)</span>不是表示首个字符，而是两个空字符串，因此初始化为 1；DP数组初始化的时候也要注意维数 +1。</li><li>字母 + 星号匹配时，依然有可能转移到 <spanclass="math inline">\(f[i][j-2]\)</span>，这是因为存在「字母 +星号的组合假匹配，<strong>直接消失才能成功</strong>」的情况，例如<code>a</code> 和 <code>aa*</code>。</li></ol><h2 id="背包-dp">背包 DP</h2><p>背包问题是<strong>二维</strong>线性 DP的分支，其常见类型有<strong>：01背包、完全背包、多重背包</strong>等问题。注意部分背包问题本质是贪心算法，此处不展开讨论。</p><p>首先介绍是 <strong>01背包</strong>问题，这类问题中「<strong>每个物品最多只能放一次</strong>」。这类问题中题目包含：</p><ul><li><p>背景：共有 <span class="math inline">\(n\)</span>件物品，最大容量为 <span class="math inline">\(m\)</span> 的背包。第<span class="math inline">\(i\)</span> 件物品的价值是 <spanclass="math inline">\(v[i]\)</span>，重量为 <spanclass="math inline">\(w[i]\)</span>。</p></li><li><p>状态：<span class="math inline">\(dp[i][j]\)</span>表示<strong>前</strong> <span class="math inline">\(i\)</span>件物品<strong>以某种组合能够放进</strong>容量为 <spanclass="math inline">\(j\)</span> 的背包的最大价值，考虑第 <spanclass="math inline">\(i\)</span>件物品<strong>放或不放</strong>。</p></li><li><p><span class="math display">\[dp[i][j]=\max \{dp[i-1][j], \; dp[i-1][j-w[i]]+v[i]\}\]</span></p></li><li><p>初始化：数组大小至少为 <span class="math inline">\(n \times(m+1)\)</span>，初始所有元素都为零，首行 <spanclass="math inline">\(dp[0][w[0]\cdots m]=v[0]\)</span>。</p></li><li><p>优化思路：可以用<strong>滚动数组</strong>优化空间复杂度，注意此时需要<strong>倒推</strong>防止覆盖，倒推的下限也可以优化。</p></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 原始数组，无优化</span><br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">dp</span>(n, vector&lt;<span class="hljs-keyword">int</span>&gt;(m + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>));<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = w[<span class="hljs-number">0</span>]; i &lt;= m; i++) dp[<span class="hljs-number">0</span>][i] = v[<span class="hljs-number">0</span>];<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; n; i++)&#123;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt;= m; j++)&#123;<br>       dp[i][j] = dp[i - <span class="hljs-number">1</span>][j];<br>       <span class="hljs-keyword">if</span>(j &gt;= w[i]) dp[i][j] = <span class="hljs-built_in">max</span>(dp[i][j], dp[i - <span class="hljs-number">1</span>][j - w[i]] + v[i]);<br>    &#125;<br>&#125;<br><span class="hljs-keyword">return</span> dp[n - <span class="hljs-number">1</span>][m];<br><br><span class="hljs-comment">// 滚动数组，倒推防止覆盖</span><br><span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">dp</span><span class="hljs-params">(m + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)</span></span>;<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = m; j &gt;= w[i]; j--)<br>        dp[j] = <span class="hljs-built_in">max</span>(dp[j], dp[j - w[i]] + v[i]);<br><span class="hljs-keyword">return</span> dp[m];<br><br><span class="hljs-comment">// 多维 01 背包，还是采用滚动数组 + 倒推，w1 上限为 m，w2上限为 t</span><br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">dp</span>(m + <span class="hljs-number">1</span>, vector&lt;<span class="hljs-keyword">int</span>&gt;(t + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>));<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = m; j &gt;= w1[i]; j--)<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> k = t; k &gt;= w2[i]; k--)<br>        dp[j][k] = <span class="hljs-built_in">max</span>(dp[j][k], dp[j - w1[i]][k - w2[i]] +  v[i]);<br><span class="hljs-keyword">return</span> dp[m][t];<br></code></pre></td></tr></table></figure><h3 id="分割等和子集-m">416. 分割等和子集 (M)</h3><p><ahref="https://leetcode.cn/problems/partition-equal-subset-sum/">题目描述</a>：给定一个<strong>只包含正整数</strong>的数组。判断是否可以将这个数组<strong>分割成两个子集</strong>，使得两个子集的元素和相等。</p><p><strong>方法1</strong>：排序 +贪心？先划分大数再划分小数，但无法得到最优解，因为此题<strong>不满足贪心选择性</strong>，先划分的数不一定属于该子集，会被后面的数影响。</p><p><strong>方法2</strong>：记忆化搜索，目标是找到 <spanclass="math inline">\(sum/2\)</span> 的元素，对每个数有两种分支。用<span class="math inline">\(vis\)</span> 数组记忆，时间复杂度 <spanclass="math inline">\((n\cdot sum)\)</span>。</p><p><strong>方法3</strong>：01 背包，<spanclass="math inline">\(dp[i][j]\)</span> 表示前 <spanclass="math inline">\(i\)</span> 个元素<strong>能否组成</strong> <spanclass="math inline">\(j\)</span> 的和，目标是 <spanclass="math inline">\(dp[n][sum/2]\)</span>。也可以更直接点，用 <spanclass="math inline">\(dp[i][j]\)</span> 表示前 <spanclass="math inline">\(i\)</span> 个元素放进容量 <spanclass="math inline">\(j\)</span> 的<strong>背包的最大值</strong>，目标是<span class="math inline">\(dp[n][sum/2]==sum/2\)</span>。时间复杂度<span class="math inline">\((n\cdot sum)\)</span>。</p><h3 id="最后一块石头的重量-ii-h">1049. 最后一块石头的重量 II (H)</h3><p><ahref="https://leetcode.cn/problems/last-stone-weight-ii/">题目描述</a>：给定整数数组表示一堆石头的重量，每次可从中<strong>选取两块石头</strong>进行粉碎，如果石头重量相等，则两块石头都被完全粉碎；否则<strong>较小者将完全粉碎</strong>，<strong>较大者重量变为二者之差</strong>，最后至多剩下一块石头，返回其<strong>最小的</strong>可能重量。</p><blockquote><p>贪心分析：如果将石头分为两堆，每次各取出一个石头进行粉碎，则两边会<strong>减少相同重量</strong>。为此，我们尽量将石头平均分到两边，使得<strong>两边重量之差的绝对值最小化</strong>。最后剩余的一堆中的那块石头，就是最小的可能重量。</p><p>贪心选择性证明：会不会出现一堆没有石头，而另一堆不止一块石头的情况呢？如果会这样，则要继续粉碎，从多余的那堆中取出较小的一块石头移入另一堆，继续粉碎。但<strong>移入操作让重量之差的绝对值变得更小</strong>，则与一开始的划分<strong>矛盾</strong>，不可能出现。</p></blockquote><p><strong>方法</strong>：01 背包，本题可转化为在<strong>容量</strong>为<span class="math inline">\(sum/2\)</span>的背包中，放入物品<strong>重量和价值均为</strong> <spanclass="math inline">\(stones_i\)</span> 的问题，时间复杂度 <spanclass="math inline">\(O(n\cdot sum)\)</span>，空间复杂度最低为 <spanclass="math inline">\(O(sum)\)</span>。</p><h3 id="目标和-m">494. 目标和 (M)</h3><p><ahref="https://leetcode.cn/problems/target-sum/">题目描述</a>：给定一个<strong>正整数</strong>数组与目标，在每个整数前面添加<strong>正负号</strong>，求运算结果等于目标的<strong>不同表达式</strong>数目。</p><p><strong>方法1</strong>：记忆化搜索，用 <spanclass="math inline">\(vis[i][j]\)</span> 表示考虑前 <spanclass="math inline">\(i\)</span> 个数结果为 <spanclass="math inline">\(j\)</span> 的数目，从 <spanclass="math inline">\(vis[n-1][target]\)</span>开始搜索，注意数组的第二维<strong>可能是负数</strong>，可以采用<strong>哈希表</strong>实现。复杂度<span class="math inline">\(O(n\cdot sum)\)</span>。</p><p><strong>方法2</strong>：二维 DP，<spanclass="math inline">\(dp[i][j]\)</span> 同上，直接递推，时空复杂度均为<span class="math inline">\(O(n\cdot sum)\)</span>。 <spanclass="math display">\[dp[i][j] = dp[i - 1][j - nums[i]] + dp[i - 1][j + nums[i]]\]</span> <strong>方法3</strong>：01背包，记<strong>所有正数</strong>的和为 <spanclass="math inline">\(x\)</span>，负数为 <spanclass="math inline">\(sum-x\)</span>，要求 <spanclass="math inline">\(x-(sum-x)=target\)</span>，则 <spanclass="math inline">\(x=(target+sum)/2\)</span>。该值<strong>必定非负</strong>。提前<strong>特判</strong>奇数情况、<spanclass="math inline">\(target&gt;sum\)</span>情况，剩下的就是求<strong>装满容量为 <spanclass="math inline">\(x\)</span> 的背包，有几种方法</strong>。 <spanclass="math display">\[dp[j] = dp[j] + dp[j - nums[i]]\]</span></p><h3 id="一和零-m">474. 一和零 (M)</h3><p><ahref="https://leetcode.cn/problems/ones-and-zeroes/">题目描述</a>：给定一组<strong>二进制字符串</strong>，从中选取一个子集，子集中<strong>最多</strong>有<code>m</code> 个 <code>0</code> 和 <code>n</code> 个<code>1</code>。求最大子集长度。</p><blockquote><p>本题易混淆为多重背包问题，实际上每个物品还是只能选一次，还是 01背包，只是<strong>背包的容量有两个维度</strong>。</p></blockquote><p><strong>方法</strong>：多维 01 背包，<spanclass="math inline">\(dp[i][j]\)</span>表示<strong>两种物品的容量</strong>分别为 <spanclass="math inline">\(i\)</span> 和 <spanclass="math inline">\(j\)</span>时的最大子集，由于涉及高维，这里直接采用滚动数组节约空间，倒着递推。每个串的价值都是<span class="math inline">\(1\)</span>，转移方程如下。复杂度 <spanclass="math inline">\(O(mn)\)</span>。 <span class="math display">\[dp[i][j]=\max \left( dp[i][j], dp[i-zeros][j-ones]+1 \right)\]</span></p><h3 id="todo-2518.-好分区的数目-h">Todo 2518. 好分区的数目 (H)</h3><p><ahref="https://leetcode.cn/problems/number-of-great-partitions/">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><hr /><p>以下是<strong>完全背包</strong>问题，这类问题中「<strong>每种物品可以放无限多次</strong>」。这类问题中题目包含：</p><ul><li><p>背景：共有 <span class="math inline">\(n\)</span>件物品，最大容量为 <span class="math inline">\(m\)</span> 的背包。第<span class="math inline">\(i\)</span> 件物品的价值是 <spanclass="math inline">\(v[i]\)</span>，重量为 <spanclass="math inline">\(w[i]\)</span>。</p></li><li><p>状态：<span class="math inline">\(dp[i][j]\)</span>表示<strong>前</strong> <span class="math inline">\(i\)</span>件物品<strong>以某种组合能够放进</strong>容量为 <spanclass="math inline">\(j\)</span> 的背包的最大价值，考虑第 <spanclass="math inline">\(i\)</span>件物品<strong>放多少个</strong>。</p></li><li><p><span class="math display">\[dp[i][j]=\max \{dp[i-1][j-k\times w[i]]+k\times v[i]\} \;\; 0\leqslantk\times w[i]\leqslant j\]</span></p></li><li><p>初始化：数组大小至少为 <span class="math inline">\(n \times(m+1)\)</span>，初始所有元素都为零，<strong>首行要分段处理</strong>。</p></li><li><p>复杂度分析：每个状态可以由<strong>若干个状态转移</strong>，时间复杂度<span class="math inline">\(O(n\cdot \sum(m/w[i]))\)</span>。</p></li><li><p>优化思路：用<strong>滚动数组</strong>优化空间复杂度，但此时<strong>正推</strong>，这样就保证当访问<span class="math inline">\(dp[j-w[i]]\)</span>时，该值已经考虑过加入物品 <spanclass="math inline">\(i\)</span>，反映了完全背包<strong>可以重复选择</strong>的特点！时间复杂度<span class="math inline">\(O(nm)\)</span>。</p></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 原始数组，无优化，多了一层循环遍历个数</span><br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">dp</span>(n, vector&lt;<span class="hljs-keyword">int</span>&gt;(m + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>));<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = w[<span class="hljs-number">0</span>]; i &lt;= m; i++) dp[<span class="hljs-number">0</span>][i] = i / w[<span class="hljs-number">0</span>] * v[<span class="hljs-number">0</span>];<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; n; i++)<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt;= m; j++)<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> k = <span class="hljs-number">0</span>; k * w[i] &lt;= j; k++)<br>            dp[i][j] = <span class="hljs-built_in">max</span>(dp[i][j], dp[i - <span class="hljs-number">1</span>][j - k * w[i]] + k * v[i]);<br><span class="hljs-keyword">return</span> dp[n - <span class="hljs-number">1</span>][m];<br><br><span class="hljs-comment">// 滚动数组最终版，正推，考虑重复选择</span><br><span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">dp</span><span class="hljs-params">(m + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)</span></span>;<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = w[i]; j &lt;= m; j++)<br>        dp[j] = <span class="hljs-built_in">max</span>(dp[j], dp[j - w[i]] + v[i]);<br><span class="hljs-keyword">return</span> dp[m];<br></code></pre></td></tr></table></figure><p>一种常见的变体是<strong>要求背包恰好凑满</strong>，求<strong>物品组合数</strong>或<strong>最少物品个数</strong>，这里也给出模板：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 最少物品个数，原始数组</span><br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">dp</span>(n, vector&lt;<span class="hljs-keyword">int</span>&gt;(m + <span class="hljs-number">1</span>, <span class="hljs-number">0x3f3f3f3f</span>));<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i * w[<span class="hljs-number">0</span>] &lt;= m; i++) dp[<span class="hljs-number">0</span>][i * w[<span class="hljs-number">0</span>]] = i;  <span class="hljs-comment">// 间隔赋值</span><br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; n; i++)<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt;= m; j++)<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> k = <span class="hljs-number">0</span>; k * w[i] &lt;= j; k++)<br>            dp[i][j] = <span class="hljs-built_in">min</span>(dp[i][j], dp[i - <span class="hljs-number">1</span>][j - k * w[i]] + k);<br><span class="hljs-keyword">return</span> dp[n - <span class="hljs-number">1</span>][m] == <span class="hljs-number">0x3f3f3f3f</span> ? <span class="hljs-number">-1</span> : dp[n - <span class="hljs-number">1</span>][m];<br><br><span class="hljs-comment">// 最少物品个数，滚动数组</span><br><span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">dp</span><span class="hljs-params">(m + <span class="hljs-number">1</span>, <span class="hljs-number">0x3f3f3f3f</span>)</span></span>;<br>dp[<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = w[i]; j &lt;= m; j++)<br>        dp[j] = <span class="hljs-built_in">min</span>(dp[j], dp[j - w[i]] + <span class="hljs-number">1</span>);<br><span class="hljs-keyword">return</span> dp[m] == <span class="hljs-number">0x3f3f3f3f</span> ? <span class="hljs-number">-1</span> : dp[m];<br><br><span class="hljs-comment">// 物品组合数，原始数组</span><br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">dp</span>(n, vector&lt;<span class="hljs-keyword">int</span>&gt;(m + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>));<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i * w[<span class="hljs-number">0</span>] &lt;= m; i++) dp[<span class="hljs-number">0</span>][i * w[<span class="hljs-number">0</span>]] = <span class="hljs-number">1</span>;<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; n; i++)<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt;= m; j++)<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> k = <span class="hljs-number">0</span>; k * w[i] &lt;= j; k++)<br>            dp[i][j] += dp[i - <span class="hljs-number">1</span>][j - k * w[i]];<br><span class="hljs-keyword">return</span> dp[n - <span class="hljs-number">1</span>][m];<br><br><span class="hljs-comment">// 物品组合数，滚动数组</span><br><span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">dp</span><span class="hljs-params">(m + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)</span></span>;<br>dp[<span class="hljs-number">0</span>] = <span class="hljs-number">1</span>;<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = w[i]; j &lt;= m; j++)<br>        dp[j] += dp[j - w[i]];<br><span class="hljs-keyword">return</span> dp[m];<br></code></pre></td></tr></table></figure><h3 id="零钱兑换-ii-m">518. 零钱兑换 II (M)</h3><p><ahref="https://leetcode.cn/problems/coin-change-ii/">题目描述</a>：给一个整数数组<code>coins</code>表示不同硬币面额，一个整数表示总金额。计算可以凑成总金额的<strong>硬币组合数</strong>。假设每一种面额的硬币有<strong>无限个</strong>，硬币面额<strong>不重复</strong>。</p><p><strong>方法1</strong>：完全背包，<spanclass="math inline">\(dp[i][j]\)</span> 表示用前 <spanclass="math inline">\(i\)</span> 种硬币凑成 <spanclass="math inline">\(j\)</span>的组合数，本题的特点在于<strong>不凑满的背包是不合法的</strong>，因此初始化首行时要<strong>间隔</strong>，状态转移方程也要修改。<span class="math display">\[dp[i][j] += dp[i - 1][j - k \times coins[i]], \;\; 0\leqslant k\timescoins[i]\leqslant j\]</span> <strong>方法2</strong>：完全背包 +滚动数组，注意要<strong>初始化</strong> <spanclass="math inline">\(dp[0]=1\)</span>，理解为<strong>没有硬币时也有一种组合</strong>，再开始考虑第一种。<span class="math display">\[dp[j] += [j - coins[i]]\]</span></p><blockquote><p>由于外层循环先遍历 <span class="math inline">\(coins\)</span>数组，在内层循环中，可以确保硬币面额在排列中的顺序，<strong>不会重复考虑</strong>一个组合。</p></blockquote><h3 id="零钱兑换-m">322. 零钱兑换 (M)</h3><p><ahref="https://leetcode.cn/problems/coin-change/">题目描述</a>：同上，但返回的是<strong>可以凑成</strong>总金额所需的<strong>最少的硬币个数</strong>，如果不存在则返回<span class="math inline">\(-1\)</span>。</p><p><strong>方法1</strong>：完全背包。<spanclass="math inline">\(dp[i][j]\)</span> 表示用前 <spanclass="math inline">\(i\)</span> 种硬币凑成 <spanclass="math inline">\(j\)</span> 的最少个数，本题由于转移方程用到了<spanclass="math inline">\(\min\)</span>，所以<strong>初始值</strong>应该设为<code>0x3f</code>，只有<strong>间隔位置的元素可以凑成</strong>，且 <spanclass="math inline">\(dp[0][0]=0\)</span>。但是由于总金额过大，该方法会超时。<span class="math display">\[dp[i][j]=\min \left( dp[i][j], \;dp[i-1][j-k\times coins[i]]+k \right)\]</span> <strong>方法2</strong>：完全背包 +滚动数组，注意要<strong>初始化</strong> <spanclass="math inline">\(dp[0]=0\)</span>，理解为<strong>没有硬币时也可以凑齐零元</strong>。<span class="math display">\[dp[j]=\min \left( dp[j],\;dp[j-coins[i]]+1 \right)\]</span></p><h3 id="todo-377.-组合总和-iv-m">TODO 377. 组合总和 IV (M)</h3><p><ahref="https://leetcode.cn/problems/combination-sum-iv/">题目描述</a>：给定由<strong>不同</strong>整数组成的数组<code>nums</code> 和一个目标数。从 <code>nums</code>中找出总和为目标数的元素组合的个数。<strong>顺序不同序列被视为不同的组合</strong>。</p><p><strong>方法1</strong>：完全背包。</p><p><strong>方法2</strong>：</p><hr /><p>以下是<strong>多重背包</strong>问题，这类问题中「<strong>每种物品各有一个指定的次数上限</strong>」。这类问题中题目包含：</p><ul><li><p>背景：共有 <span class="math inline">\(n\)</span>件物品，最大容量为 <span class="math inline">\(m\)</span> 的背包。第<span class="math inline">\(i\)</span> 件物品的价值是 <spanclass="math inline">\(v[i]\)</span>，重量为 <spanclass="math inline">\(w[i]\)</span>，其<strong>最多可装</strong> <spanclass="math inline">\(s[i]\)</span> 件。</p></li><li><p>状态：<span class="math inline">\(dp[i][j]\)</span>表示<strong>前</strong> <span class="math inline">\(i\)</span>件物品<strong>以某种组合能够放进</strong>容量为 <spanclass="math inline">\(j\)</span> 的背包的最大价值，考虑第 <spanclass="math inline">\(i\)</span>件物品<strong>放多少个</strong>。</p></li><li><p><span class="math display">\[dp[i][j]=\max \{dp[i-1][j-k\times w[i]]+k\times v[i]\}\;\;0\leqslantk\times w[i]\leqslant j,\; 0\leqslant k\leqslant s\left[ i \right]\]</span></p></li><li><p>初始化：数组大小至少为 <span class="math inline">\(n \times(m+1)\)</span>，初始所有元素都为零，<strong>首行要分段处理</strong>。</p></li><li><p>复杂度分析：每个状态可以由<strong>若干个状态转移</strong>，时间复杂度<span class="math inline">\(O(n\cdot \sums[i])\)</span>。注意此时即使用了<strong>滚动数组正推</strong>，也<strong>不能省略</strong>内层循环<span class="math inline">\(0\leqslant k\leqslant s\left[ i\right]\)</span>，因为在 <span class="math inline">\(dp[j]\)</span>的时候<strong>不知道是否达到上限</strong>。</p></li><li><p>优化思路：换个思路，我们可以将多重背包<strong>转化为 01背包</strong>问题，只需要将每个物品<strong>扁平化</strong>为 <spanclass="math inline">\(s[i]\)</span> 件相同物品。但这样复杂度还是 <spanclass="math inline">\(O(n\cdot\sums[i])\)</span>。一个巧妙的办法是采用<strong>二进制压缩</strong>，将物品拆分为<span class="math inline">\(1,2,4\cdots\)</span>个，使其能够表达完整物品又压缩了空间，复杂度 <spanclass="math inline">\(O(n\cdot \sum\log s[i])\)</span>。</p></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 原始数组，无优化，多了一层循环遍历个数</span><br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">dp</span>(n, vector&lt;<span class="hljs-keyword">int</span>&gt;(m + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>));<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = w[<span class="hljs-number">0</span>]; i &lt;= m; i++) <br>dp[<span class="hljs-number">0</span>][i] = (i / w[<span class="hljs-number">0</span>] &lt; s[<span class="hljs-number">0</span>] ? i / w[<span class="hljs-number">0</span>] : s[<span class="hljs-number">0</span>]) * v[<span class="hljs-number">0</span>];<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; n; i++)<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt;= m; j++)<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> k = <span class="hljs-number">0</span>; k * w[i] &lt;= j &amp;&amp; k &lt;= s[i]; k++)<br>            dp[i][j] = <span class="hljs-built_in">max</span>(dp[i][j], dp[i - <span class="hljs-number">1</span>][j - k * w[i]] + k * v[i]);<br><span class="hljs-keyword">return</span> dp[n - <span class="hljs-number">1</span>][m];<br><br><span class="hljs-comment">// 扁平化，用 W 和 V 存储扁平化数组，再接 01 背包</span><br>vector&lt;<span class="hljs-keyword">int</span>&gt; W, V;<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)&#123;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> t = <span class="hljs-number">1</span>; t &lt;= s[i]; t &lt;&lt;= <span class="hljs-number">1</span>)&#123;<br>        s[i] -= t;<br>        W.<span class="hljs-built_in">push_back</span>(w[i] * t);<br>        V.<span class="hljs-built_in">push_back</span>(v[i] * t);<br>    &#125;<br>    <span class="hljs-keyword">if</span>(s[i] &gt; <span class="hljs-number">0</span>)&#123;<br>        W.<span class="hljs-built_in">push_back</span>(w[i] * s[i]);<br>        V.<span class="hljs-built_in">push_back</span>(v[i] * s[i]);<br>    &#125;<br>&#125;<br><span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">dp</span><span class="hljs-params">(m + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)</span></span>;<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; W.<span class="hljs-built_in">size</span>(); i++)<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = m; j &gt;= W[i]; j--)<br>        dp[j] = <span class="hljs-built_in">max</span>(dp[j], dp[j - W[i]] + V[i]);<br><span class="hljs-keyword">return</span> dp[m];<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #05-1 一维动态规划</title>
    <link href="/LeetCode-DP-1.html"/>
    <url>/LeetCode-DP-1.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「动态规划」类型题中的：一维线性 DP、状态机DP。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><p>动态规划（DP）适用于满足<strong>重叠子问题 +最优子结构</strong>的问题，常用的解法有<strong>自顶向下</strong>的「记忆化搜索」、<strong>自底向上</strong>的「递推」。一旦一个子问题的求解得到结果，以后的计算过程就不会修改它，这样的特点叫做<strong>无后效性</strong>。DP的时间复杂度通常是「状态总数 <span class="math inline">\(\times\)</span>每个状态向其他状态转移的次数」。</p><p>动态规划（DP）的一般步骤：</p><ol type="1"><li>列出题目给的<strong>整体规模变量</strong>，例如 <spanclass="math inline">\(n,m\)</span></li><li>用局部变量 <span class="math inline">\(i,j,k\)</span>描述<strong>一般状态</strong>，例如 <spanclass="math inline">\(dp[i][j]\)</span></li><li>观察上述一般状态的<strong>最后一步</strong>，例如判断 <spanclass="math inline">\(s[i]==s[j]\)</span></li><li>去掉最后一步，问题规模缩小，变成<strong>子问题</strong>，例如 <spanclass="math inline">\(dp[i-1][j-1]\)</span></li><li>得到<strong>状态转移方程</strong>，例如 <spanclass="math inline">\(dp[i][j]=dp[i-1][j-1]+1\)</span></li><li><strong>初始值</strong>和<strong>最终状态</strong>，例如 <spanclass="math inline">\(dp[i][0]\)</span>、<spanclass="math inline">\(dp[0][j]\)</span> 和 <spanclass="math inline">\(dp[n][m]\)</span></li><li>可选的<strong>转移优化</strong>，例如记忆化、前缀和等</li></ol><h2 id="一维线性-dp">一维线性 DP</h2><p>简单的一维线性DP，就是一个递推的序列，当前值由<strong>之前的某些值</strong>转移而来，如果<strong>只由前一个值</strong>转移还可以空间优化。</p><h3 id="爬楼梯-e">70. 爬楼梯 (E)</h3><p><ahref="https://leetcode.cn/problems/climbing-stairs/">题目描述</a>：每次可以爬<code>1</code> 或 <code>2</code>个台阶，计算有多少种不同的方法可以爬到第 <code>n</code> 阶。</p><p><strong>方法1</strong>：DP，<spanclass="math inline">\(dp[i]\)</span> 表示爬到第 <spanclass="math inline">\(i\)</span>阶的方法数，线性递推即可。时空复杂度均为 <spanclass="math inline">\(O(n)\)</span>。 <span class="math display">\[dp[i]=dp[i-1] + dp[i-2]\]</span> <strong>方法2</strong>：DP +空间优化，每个状态只由前两个状态转移，因此可以采用<strong>三个变量交替</strong>，空间复杂度为<span class="math inline">\(O(1)\)</span>。</p><p><strong>方法3</strong>：矩阵快速幂，原状态转移方程是<strong>齐次线性递推式</strong>，因此可以转化为<strong>矩阵的递推关系</strong>，构造出参数矩阵的<span class="math inline">\(n\)</span>次方，使用快速幂加速计算，时间复杂度 <span class="math inline">\(O(\logn)\)</span>。</p><blockquote><p>如果一个递归式的形式如 $f(n)=_{i=1}^m{a_i}f( n-i )$，即为齐次线性递推式。本题中的原式可以构造出： <spanclass="math display">\[\left[ \begin{array}{c}  f(n+1)\\  f(n)\\\end{array} \right] =\left[ \begin{array}{c}  f(n)+f(n-1)\\  f(n)\\\end{array} \right] =\left[ \begin{matrix}  1&amp;      1\\  1&amp;      0\\\end{matrix} \right] \left[ \begin{array}{c}  f(n)\\  f(n-1)\\\end{array} \right]\]</span> 因此： <span class="math display">\[\left[\begin{array}{c}f(n+1) \\f(n)\end{array}\right]=\left[\begin{array}{ll}1 &amp; 1 \\1 &amp; 0\end{array}\right]^n\left[\begin{array}{l}f(1) \\f(0)\end{array}\right]\]</span></p></blockquote><p><strong>方法4</strong>：斐波那契通项公式，解出矩阵的两个<strong>特征值</strong>，代入得通项公式。时间复杂度为<strong>幂运算</strong>复杂度。<span class="math display">\[f(n)=\frac{1}{\sqrt{5}}\left[\left(\frac{1+\sqrt{5}}{2}\right)^n-\left(\frac{1-\sqrt{5}}{2}\right)^n\right]\]</span></p><p><strong>拓展</strong>：<ahref="https://leetcode.cn/problems/min-cost-climbing-stairs/">746.使用最小花费爬楼梯</a>，题目类似，但是给定一个数组表示<strong>从</strong>第<code>i</code>个台阶<strong>向上爬</strong>的代价，求爬到楼梯顶的最小代价。此时 <spanclass="math inline">\(dp[i]\)</span> 就应该表示爬到第 <spanclass="math inline">\(i\)</span>阶的<strong>最小花费</strong>，也是线性递推即可。 <spanclass="math display">\[dp[i]=\min(dp[i-1] + cost[i-1],\; dp[i-2] + cost[i-2])\]</span></p><h3 id="买卖股票的最佳时机-e">121. 买卖股票的最佳时机 (E)</h3><p><ahref="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock/">题目描述</a>：给定<code>prices[]</code>价格数组，选择某一天购买，并在未来某一天出售，使得利润最大。</p><p><strong>方法1</strong>：DP，<spanclass="math inline">\(dp[i]\)</span> 表示前 <spanclass="math inline">\(i\)</span>天的最大利润，用一个变量维护<strong>此前的最小值</strong>，则时空复杂度均为<span class="math inline">\(O(n)\)</span>。 <spanclass="math display">\[dp[i]=\max (dp[i-1],\mathrm{prices}[i]-\mathrm{minPrice})\]</span> <strong>方法2</strong>：DP +空间优化，<strong>每个状态只由前一个状态转移过来</strong>，因此只需维护一个<span class="math inline">\(\mathrm{maxProfit}\)</span>，空间复杂度<span class="math inline">\(O(1)\)</span>。</p><h3 id="不同的二叉搜索树-m">96. 不同的二叉搜索树 (M)</h3><p><ahref="https://leetcode.cn/problems/unique-binary-search-trees/">题目描述</a>：给定<span class="math inline">\(n\)</span>个<strong>互不相同</strong>的节点值构成二叉搜索树，返回<strong>可能的</strong>二叉搜索树的种数。</p><p><strong>方法1</strong>：DP，当数字 <spanclass="math inline">\(i\)</span> 作为根时，<spanclass="math inline">\(1\)</span> 到 <spanclass="math inline">\(i-1\)</span> 的序列作为左子树， <spanclass="math inline">\(i+1\)</span> 到 <spanclass="math inline">\(n\)</span>的序列作为右子树。原问题可以分解成规模较小的两个子问题。初始 <spanclass="math inline">\(f[0]=f[1]=1\)</span>，时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。 <span class="math display">\[f\left[ n \right] =\sum_{i=1}^n{f\left[ i-1 \right] \times f\left[ n-i\right]}\]</span> <strong>方法2</strong>：卡特兰数，<spanclass="math inline">\(C_0 = 1\)</span>，递推式 <spanclass="math inline">\(C_{n+1}=\frac{2\left( 2n+1\right)}{n+2}C_n\)</span>，时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="丑数-ii-m">264. 丑数 II (M)</h3><p><ahref="https://leetcode.cn/problems/ugly-number-ii/">题目描述</a>：<strong>丑数</strong>是只包含质因数<code>2</code>、<code>3</code> 和 <code>5</code> 的正整数，返回第<code>n</code> 个丑数。</p><p><strong>方法1</strong>：优先队列，维护一个最小堆<strong>存储丑数列表</strong>，每次弹出堆顶元素后可以生成三个元素（<code>2x</code>、<code>3x</code>、<code>5x</code>）加入最小堆，时间复杂度为<span class="math inline">\(O(n \log n)\)</span>。</p><p><strong>方法2</strong>：贪心 +DP，每个丑数必然会转移到三个丑数，单看任一种转移方式，小丑数一定先于大丑数转移，因此可以<strong>用三个指针来记录三种转移方式当前转移到的丑数</strong>。下一个弹出的丑数一定是三者中的最小值。时间复杂度<span class="math inline">\(O(n)\)</span>。 <spanclass="math display">\[dp\left[ i \right] =\min \left\{ dp\left[ p2 \right] \times 2,dp\left[p3 \right] \times 3,dp\left[ p5 \right] \times 5 \right\}\]</span> <strong>坑点</strong>：一个丑数可能会被不同的丑数经过<code>2x</code>、<code>3x</code>、<code>5x</code>转移到，因此两种方法都会出现重复数字的情况。方法一在入队前要用unorder_set 来<strong>去除重复数字</strong>，方法二中的三种转移方式<span class="math inline">\(dp\)</span>值可能相等，此时<strong>所有相等的指针都要移动</strong>。</p><h3 id="整数拆分-m">343. 整数拆分 (M)</h3><p><ahref="https://leetcode.cn/problems/integer-break/">题目描述</a>：给定一个正整数<code>n</code>，将其拆分为若干个<strong>正整数</strong>的和，并使这些整数的<strong>乘积最大化</strong>，返回最大乘积。</p><p><strong>方法1</strong>：线性 DP，当 <span class="math inline">\(n\geqslant 2\)</span> 时，可以进行拆分，设 <spanclass="math inline">\(dp[i]\)</span> 表示整数 <spanclass="math inline">\(i\)</span> 能拆分的最大乘积，则除了 <spanclass="math inline">\(2\)</span> 和 <spanclass="math inline">\(3\)</span>要被初始化为特殊值，其他数都可以由更小的数转移过来。时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。 <span class="math display">\[dp\left[ i \right] =\underset{2\leqslant j&lt;i}{\max}\left( dp\left[i-j \right] \times j \right)\]</span> <strong>方法2</strong>：优化 DP，注意到 <spanclass="math inline">\(i&gt;3\)</span> 时，每个数仅在拆分出 <spanclass="math inline">\(2\)</span> 或 <spanclass="math inline">\(3\)</span>时可能取最优解，因此<strong>无需完全遍历，</strong>复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：数学，通过<strong>函数极值</strong>证明，当<span class="math inline">\(n \leqslant 3\)</span> 时，最大乘积只能是<span class="math inline">\(n-1\)</span>；当 <spanclass="math inline">\(n \geqslant 4\)</span>时，<strong>尽可能将因子三等分时，乘积最大</strong>。当被三整除时，直接返回<span class="math inline">\(3^a\)</span>；余数为 <spanclass="math inline">\(1\)</span> 时，返回 <spanclass="math inline">\(3^{a-1}\times 4\)</span>；余数为 <spanclass="math inline">\(2\)</span> 时，返回 <spanclass="math inline">\(3^a \times 2\)</span>。时间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="单词拆分-m">139. 单词拆分 (M)</h3><p><ahref="https://leetcode.cn/problems/word-break/">题目描述</a>：给定字符串<code>s</code> 和单词字典<code>wordDict</code>。判断是否可以利用<strong>字典中出现的单词拼接</strong>出<code>s</code>。<strong>可重复利用</strong>。</p><p><strong>方法1</strong>：线性 DP + 枚举字典，<spanclass="math inline">\(dp[i]\)</span> 表示到第 <spanclass="math inline">\(i\)</span>个字符位置的子串<strong>是否能拼接成功</strong>，则初始 <spanclass="math inline">\(dp[0]=True\)</span>，目标是 <spanclass="math inline">\(dp[n]\)</span>。对每个位置都<strong>枚举整个字典</strong>，直到能使<span class="math inline">\(dp[i]=True\)</span>，时间复杂度 <spanclass="math inline">\(O(nm)\)</span>。 <span class="math display">\[dp\left[ i \right] =dp\left[ j \right] \,\,\&amp;\,\,\mathrm{check}\left( s\left[ j\cdots i-1 \right] \right)\]</span> <strong>方法2</strong>：线性 DP + 枚举子串，<spanclass="math inline">\(dp[i]\)</span>同上，但用<strong>哈希表存储单词字典</strong>，对每个位置都枚举 <spanclass="math inline">\(0&lt;j&lt;i\)</span>，看是否能找到子串 <spanclass="math inline">\(s[j\cdots i-1]\)</span><strong>存在于哈希表中</strong>，时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。</p><h3 id="约瑟夫环问题-m">约瑟夫环问题 (M)</h3><p><a href="https://leetcode.cn/problems/W7yuXW/">题目描述</a>：有 n个人排成一个环，他们的编号为 <code>0</code> 到<code>n-1</code>。轮流报数，每次喊到 <code>k</code>的倍数的人出去，求最后剩下的人。</p><p><strong>方法1</strong>：模拟，用链表或数组存储编号，用一个计数器表示环，一个计数器数<code>k</code>。淘汰的时候可以用链表或数组的<code>erase(nums.begin() + i)</code>。时间复杂度 <spanclass="math inline">\(O(nk)\)</span>。</p><p><strong>方法2</strong>：线性 DP，记 <spanclass="math inline">\(dp[i]\)</span> 为 <spanclass="math inline">\(i\)</span>个人的时候<strong>最后剩下的人的编号</strong>，<spanclass="math inline">\(dp[1]=0\)</span> 表示一个人的时候必然是<code>0</code> 号。如果有 <span class="math inline">\(i\)</span>个人，则一个人淘汰后问题规模缩小为 <spanclass="math inline">\(i−1\)</span>，且此时会从编号 <code>k%i</code>开始报数。递推时间复杂度 <span class="math inline">\(O(n)\)</span>。<span class="math display">\[dp[i]=(dp[i-1]+k)\;\%\; i\]</span></p><h3 id="不同的子序列-ii-h">940. 不同的子序列 II (H)</h3><p><ahref="https://leetcode.cn/problems/distinct-subsequences-ii/">题目描述</a>：给定一个字符串<code>s</code>，计算 <code>s</code>的<strong>不同非空子序列</strong>的个数，子序列不要求连续。</p><p><strong>方法1</strong>：线性 DP，用一个长度为 26的数组存放以<strong>每个字母结尾的子序列</strong>个数，当遇到新字符时更新对应值，时间复杂度为<span class="math inline">\(O(n\left| \varSigma\right|)\)</span>，空间复杂度为 <span class="math inline">\(O(\left|\varSigma \right|)\)</span>，<span class="math inline">\(\left|\varSigma \right|\)</span> 为字符集的大小。 <spanclass="math display">\[vec[c]=1+\sum{vec[i]}\]</span></p><blockquote><p>观察序列生成，得出以下规律：</p><ul><li>当遇到一个新字符的时候，将其加到所有已有序列的末尾，即可生成一批<strong>完全不重复</strong>的序列Set，Set 内都是以该字符结尾的；</li><li>如果这个新字符不是第一次出现，那么上次以这个字符为末尾的子序列oldSet，会被 Set完全包含，考虑该字符上次出现的位置，也是将其加到所有已有序列的末尾，而这些已有序列会一直存在；</li><li>但是「这个字符本身」，不会出现在 Set 中，因为 Set 至少长度为2，因此还要加 1。</li></ul></blockquote><p><strong>方法2</strong>：线性 DP + 时间优化，每次更新时都要把 <spanclass="math inline">\(vec\)</span> 求和，因此用一个变量 <spanclass="math inline">\(tot\)</span> 维护 <spanclass="math inline">\(\sum{vec[i]}\)</span>，每次将 <spanclass="math inline">\(vec[c]\)</span> 更新后，再将 <spanclass="math inline">\(tot\)</span> 的值增加 <spanclass="math inline">\(vec[c]\)</span> 的变化量即可。时间复杂度优化为<span class="math inline">\(O(n + \left| \varSigma\right|)\)</span>。</p><h3 id="规划兼职工作-h">1235. 规划兼职工作 (H)</h3><p><ahref="https://leetcode.cn/problems/maximum-profit-in-job-scheduling/">题目描述</a>：有<code>n</code> 份兼职工作，每份工作预计从 <code>startTime[i]</code>开始到 <code>endTime[i]</code> 结束，报酬为<code>profit[i]</code>。计算可以获得的最大报酬。时间上出现重叠的两份工作不能同时进行。</p><p><strong>方法</strong>：线性 DP +二分，这题可以视为<strong>活动安排问题</strong>的复杂版，多了报酬维度，因此<strong>不能贪心</strong>，但同样可以按照结束时间排序。设<span class="math inline">\(dp[i]\)</span> 为第 <spanclass="math inline">\(i\)</span> 早结束的工作，<spanclass="math inline">\(dp[0]=0\)</span>表示一开始没有任何工作可做，时间复杂度 <spanclass="math inline">\(O(n)\)</span>。 <span class="math display">\[dp[i] = \max \{dp[i-1], dp[k] + profit[i]\}\]</span></p><blockquote><p>本题隐含了离散化的思想，因为时间取值跨度大且不连续，而只有当该时刻有工作结束时才可能更新，其他时刻都会从上一时刻转移。</p><p>如果当前时刻 <span class="math inline">\(i\)</span>有工作结束，要做该工作，则从 <code>startTime[i]</code>以后就不能再做其他工作，因此要找到比该工作开始时间<strong>早结束的工作中最晚的</strong>，因此使用<code>upper_bound</code>。</p></blockquote><p><strong>坑点</strong>：分开给的三个数组，如果要合并排序，只需再用一个嵌套数组<strong>充当结构体</strong>，用下标排序太麻烦了！</p><h2 id="状态机-dp">状态机 DP</h2><p>状态机 DP，一维线性 DP的特例，当前值可能涉及到<strong>多个状态序列</strong>（持有股票/现金、买卖股票次数、是否交换等），<strong>根据当前操作会发生状态序列的切换</strong>，如果不操作则<strong>保持原序列的上一个状态</strong>。由于通常只与前一个值有关，可以进行空间优化。</p><h3 id="买卖股票的最佳时机-ii-m">122. 买卖股票的最佳时机 II (M)</h3><p><ahref="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-ii//">题目描述</a>：给定<code>prices[]</code>价格数组，同上，但是可以<strong>无限次</strong>购买出售，同一时间最多持有<strong>一股</strong>股票。</p><p><strong>方法1</strong>：暴力DFS，每天可以选择是否操作，操作会<strong>转变当前状态</strong><code>cash</code> 或 <code>stock</code>，时间复杂度 <spanclass="math inline">\(O(2^n)\)</span>，超时。</p><p><strong>方法2</strong>：状态机 DP，<spanclass="math inline">\(dp[i][j]\)</span> 表示到下标 <spanclass="math inline">\(i\)</span> 的这天，持股<strong>状态</strong>为<span class="math inline">\(j\)</span>时，手上拥有的最大现金/盈利数，<span class="math inline">\(j=0\)</span>表示持有现金，<span class="math inline">\(j=1\)</span>表示持有股票。显然，最终结果为 <spanclass="math inline">\(dp[n-1][0]\)</span>，初始值为 <spanclass="math inline">\(dp[0][0]=0\)</span> 和 <spanclass="math inline">\(dp[0][1]=-\mathrm{prices[0]}\)</span>。时空复杂度均为<span class="math inline">\(O(n)\)</span>。 <spanclass="math display">\[\begin{cases}    dp\left[ i \right] \left[ 0 \right] =\max \left( dp\left[ i-1\right] \left[ 1 \right] +\mathrm{prices}\left[ i \right] , \;\;dp\left[i-1 \right] \left[ 0 \right] \right)\\    dp\left[ i \right] \left[ 1 \right] =\max \left( dp\left[ i-1\right] \left[ 0 \right] -\mathrm{prices}\left[ i \right] , \;\;dp\left[i-1 \right] \left[ 1 \right] \right)\\\end{cases}\]</span> <strong>方法3</strong>：状态机 DP +滚动数组，<strong>每个状态只由前一维状态转移过来</strong>，因此只需维护一个<span class="math inline">\(\mathrm{preCash}\)</span> 代替 <spanclass="math inline">\(dp[i-1][0]\)</span>，一个 <spanclass="math inline">\(\mathrm{preStock}\)</span> 替代 <spanclass="math inline">\(dp[i-1][1]\)</span>，最后再<strong>同时将二者更新</strong>。空间复杂度<span class="math inline">\(O(1)\)</span>。</p><p><strong>方法4</strong>：贪心，若「day0 买入，day2卖出」是最优解，拆分成「day0 买入，day1 卖出，day1 买入，day2卖出」也是最优解。而对于「今天的股价 -昨天的股价」，贪心<strong>选择正数累加</strong>即可得到最大值。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><blockquote><p>贪心算法是选择那些所有差分（严格）大于 0的数，把它们相加即可。贪心选择性也很好证明，加上负数的结果一定更小。</p></blockquote><p><strong>坑点</strong>：滚动数组中，最好先<strong>计算好完整一维再同时更新</strong>，除非确保没有依赖，此时二维的滚动数组还能<strong>再压缩到一维</strong>。</p><h3 id="最佳买卖股票时机含冷冻期-m">309. 最佳买卖股票时机含冷冻期(M)</h3><p><ahref="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-with-cooldown/">题目描述</a>：给定<code>prices[]</code>价格数组，同上，可以<strong>无限次</strong>购买出售，但每次售出后都会有一天<strong>冷冻期</strong>，即第二天无法立即买入。同一时间最多持有<strong>一股</strong>股票。</p><p><strong>方法1</strong>：状态机 DP，<spanclass="math inline">\(dp[i][j]\)</span> 表示到下标 <spanclass="math inline">\(i\)</span> 的这天，持股<strong>状态</strong>为<span class="math inline">\(j\)</span> 时，手上拥有的最大现金数，<spanclass="math inline">\(j=0\)</span>表示持有现金且<strong>非冷冻期</strong>，<spanclass="math inline">\(j=1\)</span> 表示持有股票，<spanclass="math inline">\(j=2\)</span>表示持有现金且<strong>处于冷冻期</strong>。此时，最终结果为 <spanclass="math inline">\(\max(dp[n][0],dp[n][2])\)</span>，初始状态除了<span class="math inline">\(dp[0][1]=-\mathrm{prices[0]}\)</span>其他都是零。 <span class="math display">\[\begin{cases}    dp\left[ i \right] \left[ 0 \right] =\max \left( dp\left[ i-1\right] \left[ 0 \right] ,\;\;dp\left[ i-1 \right] \left[ 2 \right]\right)\\    dp\left[ i \right] \left[ 1 \right] =\max \left( dp\left[ i-1\right] \left[ 0 \right] -\mathrm{prices}\left[ i \right] ,\;\;dp\left[i-1 \right] \left[ 1 \right] \right)\\    dp\left[ i \right] \left[ 2 \right] =dp\left[ i-1 \right] \left[ 1\right] +\mathrm{prices}\left[ i \right]\\\end{cases}\]</span> <strong>方法2</strong>：状态机 DP +滚动数组，<strong>每个状态只由前一维状态转移过来</strong>，因此只需维护三个变量，最后再<strong>同时将三者更新</strong>。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="买卖股票的最佳时机-iii-m">123. 买卖股票的最佳时机 III (M)</h3><p><ahref="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-iii/">题目描述</a>：给定<code>prices[]</code>价格数组，同上，但是<strong>最多</strong>可以完成<strong>两笔</strong>交易，同一时间最多持有<strong>一股</strong>股票。</p><p><strong>方法1</strong>：DP + 贪心，用两个 <spanclass="math inline">\(dp[i]\)</span> 数组分别计算到下标 <spanclass="math inline">\(i\)</span> 为止、从下标 <spanclass="math inline">\(i\)</span>开始到结束，完成<strong>一笔</strong>交易的最大利润。将这两个数组相加，得到的<span class="math inline">\(dp[i]\)</span> 表示以下标 <spanclass="math inline">\(i\)</span>天<strong>为分界进行两笔</strong>交易的最大利润。遗憾的是这种做法<strong>仅适用</strong>于「两笔交易」。</p><p><strong>方法2</strong>：状态机 DP +滚动数组，共有<strong>五种状态</strong>：未操作、买一次、买卖各一次、买两次卖一次、买卖各两次。第一个状态利润显然为0，因此可以不用记录。剩下四个状态的最大利润分别记为 <spanclass="math inline">\(buy1\)</span>、<spanclass="math inline">\(sell1\)</span>、<spanclass="math inline">\(buy2\)</span>、<spanclass="math inline">\(sell2\)</span>。复杂度 <spanclass="math inline">\(O(n)\)</span>。 <span class="math display">\[\left\{ \begin{array}{l}    buy_1=\max \left\{ buy_1,\;\;-\mathrm{prices[}i] \right\}\\    sell_1=\max \left\{ sell_1,\;\; buy_1+\mathrm{prices[}i] \right\}\\    buy_2=\max \left\{ buy_2, \;\;sell_1-\mathrm{prices[}i] \right\}\\    sell_2=\max \left\{ sell_2,\;\; buy_2+\mathrm{prices[}i] \right\}\\\end{array} \right.\]</span></p><h3 id="打家劫舍-m">198. 打家劫舍 (M)</h3><p><ahref="https://leetcode.cn/problems/house-robber/">题目描述</a>：给定一个非负整数数组<code>nums</code>表示每个房子存放的现金，计算小偷在<strong>不偷相邻房子</strong>的条件下最大偷窃金额。</p><p><strong>方法1</strong>：状态机 DP，<spanclass="math inline">\(dp[i][j]\)</span> 表示到下标 <spanclass="math inline">\(i\)</span> 的房子为止，偷窃<strong>状态</strong>为<span class="math inline">\(j\)</span> 时的最大偷窃金额，<spanclass="math inline">\(j=0\)</span> 表示偷，<spanclass="math inline">\(j=1\)</span> 表示不偷。时空复杂度均为 <spanclass="math inline">\(O(n)\)</span>。 <span class="math display">\[\begin{cases}    dp\left[ i \right] \left[ 0 \right] =\max \left( dp\left[ i-1\right] \left[ 0 \right] ,\;\;dp\left[ i-1 \right] \left[ 1 \right]\right)\\    dp\left[ i \right] \left[ 1 \right] =dp\left[ i-1 \right] \left[ 0\right] +nums\left[ i \right]\\\end{cases}\]</span> <strong>方法2</strong>：状态机 DP +滚动数组，<strong>每个状态只由前一维状态转移过来</strong>，因此只需维护两个状态变量，空间复杂度<span class="math inline">\(O(1)\)</span>。</p><p><strong>方法3</strong>：简化状态方程，利用<strong>递归</strong>的思想，<spanclass="math inline">\(dp[i]\)</span> 表示到下标 <spanclass="math inline">\(i\)</span> 的房子为止的最大偷窃金额，时间复杂度<span class="math inline">\(O(n)\)</span>。 <spanclass="math display">\[dp\left[ i \right] =\max \left( dp\left[ i-2 \right]+nums[i],\;\;dp\left[ i-1 \right] \right)\]</span></p><h3 id="打家劫舍-ii-m">213. 打家劫舍 II (M)</h3><p><ahref="https://leetcode.cn/problems/house-robber-ii/">题目描述</a>：同上，但是N 个房子排成<strong>环形</strong>。意味着 <spanclass="math inline">\(nums[0]\)</span> 和 <spanclass="math inline">\(nums[n-1]\)</span> 是相邻的。</p><p><strong>方法1</strong>：状态机 DP，由于 <spanclass="math inline">\(nums[0]\)</span> 和 <spanclass="math inline">\(nums[n-1]\)</span> 不会同时被偷，本题可以拆成<span class="math inline">\([0,n-2]\)</span> 和 <spanclass="math inline">\([1, n-1]\)</span>两个偷窃区间，在两个区间内计算，状态转移方程同上题。时空复杂度均为 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：状态机 DP + 滚动数组，同上题，空间复杂度<span class="math inline">\(O(1)\)</span>。</p><h3 id="乘积最大子数组-m">152. 乘积最大子数组 (M)</h3><p><ahref="https://leetcode.cn/problems/maximum-product-subarray/">题目描述</a>：给定一个包含<strong>正负整数</strong>的数组，找出数组中<strong>乘积最大</strong>的非空连续子数组，返回最大乘积。</p><p><strong>方法1</strong>：状态机 DP。<spanclass="math inline">\(f_{\max}\)</span> 和 <spanclass="math inline">\(f_{\min}\)</span> 表示以第 <spanclass="math inline">\(i\)</span>个元素为结尾的最大、最小乘积，均<strong>初始化</strong>为 <spanclass="math inline">\(1\)</span>，每个元素可以加入前一段或者单独成段，时空复杂度均为<span class="math inline">\(O(n)\)</span>。 <spanclass="math display">\[\begin{aligned}f_{\max }(i) &amp;=\max _{i=1}^n\left\{f_{\max }(i-1) \times a_i,f_{\min }(i-1) \times a_i, a_i\right\} \\f_{\min }(i) &amp;=\min _{i=1}^n\left\{f_{\max }(i-1) \times a_i,f_{\min }(i-1) \times a_i, a_i\right\}\end{aligned}\]</span></p><blockquote><p>考虑只维护一个 <spanclass="math inline">\(f_{\max}\)</span>，如果全为正数，则以第 <spanclass="math inline">\(i\)</span>个元素为结尾的状态<strong>必然由前一个状态转移</strong>（加入或单独成段）。但本题包含正负数，如果当前位置是负数，则当前位置是否单独成段还要考虑接下来还有没有负数。</p><p>而从下一个负数的角度出发，我们反而希望维护的前一个状态值也是负数，并且尽可能小，这样负负得正之后<span class="math inline">\(f_{\max}\)</span>才能最大。因此必须维护两个状态值，当<strong>遇到负数时，两个序列会自动相互转换</strong>。</p></blockquote><p><strong>方法2</strong>：状态机 DP +滚动数组，<strong>每个状态只由前一维状态转移过来</strong>，因此只需维护<strong>两个变量</strong>，最后再<strong>同时将二者更新</strong>（用临时变量先存储）。空间复杂度<span class="math inline">\(O(1)\)</span>。</p><h3 id="多米诺和托米诺平铺-m">790. 多米诺和托米诺平铺 (M)</h3><p><ahref="https://leetcode.cn/problems/domino-and-tromino-tiling/">题目描述</a>：有两种形状的瓷砖：<spanclass="math inline">\(2\times 1\)</span> 的长方形，和形如 <code>∟</code>的直角型，两种形状都可以旋转。给定整数 <spanclass="math inline">\(n\)</span>，计算用这两种瓷砖铺满 <spanclass="math inline">\(2\times n\)</span> 的平板的方法数。</p><p><strong>方法1</strong>：状态机 DP，设 <spanclass="math inline">\([1,i-1]\)</span> 列均已全部覆盖，第 <spanclass="math inline">\(i\)</span>列状态可能有四种：<strong>无覆盖、上方覆盖、下方覆盖、全覆盖</strong>。初始时<spanclass="math inline">\(dp[0][0]=0,dp[0][1]=0,dp[0][2]=0,dp[0][3]=1\)</span>，最后返回<span class="math inline">\(dp[n][3]\)</span>，时间复杂度 <spanclass="math inline">\(O(n)\)</span>。 <span class="math display">\[\begin{aligned}    dp[i][0]&amp;=  dp[i-1][3]\\    dp[i][1]&amp;=  dp[i-1][0]+dp[i-1][2]\\    dp[i][2]&amp;=  dp[i-1][0]+dp[i-1][1]\\    dp[i][3]&amp;=  dp[i-1][0]+dp[i-1][1]+dp[i-1][2]+dp[i-1][3]\\\end{aligned}\]</span> <strong>方法2</strong>：构造一维线性DP，找规律，枚举前几项，从中找规律得到递推式，时间复杂度 <spanclass="math inline">\(O(n)\)</span>。 <span class="math display">\[f[n]=2\times f[n-1]+f[n-3]\]</span></p><blockquote><p>显然，画出排列方式可知，<spanclass="math inline">\(f[1]=1,f[2]=2\)</span>。而 <spanclass="math inline">\(f[3]\)</span> 可以转化为 <spanclass="math inline">\(f[2]+f[1]+2=5\)</span>。</p><p>同理，<spanclass="math inline">\(f[4]=f[3]+f[2]+2(f[1]+1)\)</span>，这里的 <spanclass="math inline">\(2\)</span> 作为系数是因为 <code>∟</code>具有上下对称的特性。</p><p><spanclass="math inline">\(f[5]=f[4]+f[3]+2(f[2]+f[1]+1)=24\)</span>，这里发现每次都会加上最近两项，因为有长方形块。</p><p>以此类推写出 <span class="math inline">\(f[n-1]\)</span> 和 <spanclass="math inline">\(f[n]\)</span>，两式作差就可以得到递推式。</p></blockquote><p><strong>方法3</strong>：滚动数组，空间压缩到 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="toj-数数-m">TOJ 数数 (M)</h3><p><strong>题目描述</strong>：给定 <spanclass="math inline">\(n\)</span>，求长度为 <spanclass="math inline">\(n\)</span> 的数字串的个数。要求：每一位为 <spanclass="math inline">\(1\)</span>、<span class="math inline">\(2\)</span>或 <spanclass="math inline">\(3\)</span>，且不得连续出现三个相同的数字。</p><p><strong>方法1</strong>：状态机DP，每一个位置有<strong>三种选择</strong>，对应以下三种状态，时间复杂度<span class="math inline">\(O(n)\)</span>。 <spanclass="math display">\[dp\left[ i \right] \left[ 1 \right] =dp\left[ i-1 \right] \left[ 2\right] +dp\left[ i-1 \right] \left[ 3 \right] +\left( dp\left[ i-1\right] \left[ 1 \right] -dp\left[ i-3 \right] \left[ 2 \right]-dp\left[ i-3 \right] \left[ 3 \right] \right)\\dp\left[ i \right] \left[ 2 \right] =dp\left[ i-1 \right] \left[ 1\right] +dp\left[ i-1 \right] \left[ 3 \right] +\left( dp\left[ i-1\right] \left[ 2 \right] -dp\left[ i-3 \right] \left[ 1 \right]-dp\left[ i-3 \right] \left[ 3 \right] \right)\\dp\left[ i \right] \left[ 3 \right] =dp\left[ i-1 \right] \left[ 1\right] +dp\left[ i-1 \right] \left[ 2 \right] +\left( dp\left[ i-1\right] \left[ 3 \right] -dp\left[ i-3 \right] \left[ 1 \right]-dp\left[ i-3 \right] \left[ 2 \right] \right)\]</span></p><blockquote><p>当 <span class="math inline">\(num[i]=1\)</span> 时，<spanclass="math inline">\(num[i-1]\)</span> 显然可以「取 <spanclass="math inline">\(2\)</span>」、「取 <spanclass="math inline">\(3\)</span>」、「取 <spanclass="math inline">\(1\)</span> 且 <spanclass="math inline">\(num[i-2]\)</span> 取 <spanclass="math inline">\(2\)</span> 或 <spanclass="math inline">\(3\)</span>」。前两者显然就是 <spanclass="math inline">\(dp\left[ i-1 \right] \left[ 2 \right] +dp\left[i-1 \right] \left[ 3\right]\)</span>，现在我们想求出<strong>第三个项</strong>。</p><p>注意到当 <span class="math inline">\(num[i-1]=1\)</span> 时，<spanclass="math inline">\(num[i-2]\)</span> 显然可以「取 <spanclass="math inline">\(2\)</span>」、「取 <spanclass="math inline">\(3\)</span>」、「取 <spanclass="math inline">\(1\)</span> 且 <spanclass="math inline">\(num[i-3]\)</span> 取 <spanclass="math inline">\(2\)</span> 或 <spanclass="math inline">\(3\)</span>」。显然前两者就是我们想要的<strong>第三个项</strong>，因此用<span class="math inline">\(dp[i-1][1]\)</span> <strong>扣除</strong><span class="math inline">\((dp[i-3][2]+dp[i-3][3])\)</span>就可以得到目标。</p></blockquote><p><strong>方法2</strong>：构造一维线性 DP，将上述状态机 DP的三个状态相加，得到以下状态转移方程： <span class="math display">\[dp[i] = 3 \times dp[i - 1] - 2 \times dp[i - 3]\]</span> <strong>方法3</strong>：滚动数组，空间压缩到 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="使序列递增的最小交换次数-h">801. 使序列递增的最小交换次数(H)</h3><p><ahref="https://leetcode.cn/problems/minimum-swaps-to-make-sequences-increasing/">题目描述</a>：给定两个长度相等的整数数组，每次操作可以<strong>交换两数组相同位置</strong>的元素，计算使两个数组<strong>均严格递增</strong>所需的最小操作次数。（用例保证可以实现严格递增）</p><p><strong>方法1</strong>：暴力DFS，有时候必须交换才能满足，有时候可换可不换，此时产生<strong>两个分支</strong>，时间复杂度<span class="math inline">\(O(2^n)\)</span>，超时。</p><p><strong>方法2</strong>：状态机 DP，<spanclass="math inline">\(dp[i][j]\)</span> 表示到下标 <spanclass="math inline">\(i\)</span>位置使数组均严格递增的最小操作次数，<spanclass="math inline">\(j=0\)</span> 表示不换，<spanclass="math inline">\(j=1\)</span>表示换。对于每个位置，换和不换<strong>两种操作可能都行</strong>，取决于<strong>相邻两个位置</strong>的四个元素以及<strong>上一个位置是否发生交换</strong>。初始值为<span class="math inline">\(dp[0][0]=0\)</span> 和 <spanclass="math inline">\(dp[0][1]=1\)</span>，最终返回二者中的较小值。时空复杂度均<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>如果<strong>相邻两个位置</strong>的四个元素不换还行、换了不行，则位置<span class="math inline">\(i\)</span> 的交换情况要和位置 <spanclass="math inline">\(i-1\)</span> 保持一致： <spanclass="math display">\[\left\{\begin{array}{l}d p[i][0]=d p[i-1][0] \\d p[i][1]=d p[i-1][1]+1\end{array}\right.\]</span> 如果必须要换、不换不行，则位置 <spanclass="math inline">\(i\)</span> 的交换情况要和位置 <spanclass="math inline">\(i-1\)</span> 相反： <span class="math display">\[\left\{\begin{array}{l}d p[i][0]=d p[i-1][1] \\d p[i][1]=d p[i-1][0]+1\end{array}\right.\]</span> 如果换不换都行，则取两种情况中的较小值即可： <spanclass="math display">\[\left\{\begin{array}{l}d p[i][0]=\min \{d p[i-1][0], d p[i-1][1]\} \\d p[i][1]=\min \{d p[i-1][1], d p[i-1][0]\}+1\end{array}\right.\]</span></p></blockquote><p><strong>方法3</strong>：状态机 DP +滚动数组，<strong>每个状态只由前一维状态转移过来</strong>，因此只需维护<strong>两个变量</strong>，最后再<strong>同时将二者更新</strong>（用临时变量先存储）。空间复杂度<span class="math inline">\(O(1)\)</span>。</p>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #04 二分&amp;分治</title>
    <link href="/LeetCode-Divide-Conquer.html"/>
    <url>/LeetCode-Divide-Conquer.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「二分&amp;分治」类型题中的：二分查找、快速排序、归并排序、分块、倍增等。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><h2 id="二分查找">二分查找</h2><p>二分查找及其衍生题目通常会在一个「<strong>广义有序</strong>」背景下，很难直接使用<code>lower_bound()</code>解决，需要根据题意手写二分查找。通常要求时间复杂度 <spanclass="math inline">\(O(\log n )\)</span> 或 <spanclass="math inline">\(O(n\log n)\)</span>，数据量在 <spanclass="math inline">\(1e5\)</span> 以上。</p><p>这里给出有序数组的二分查找模板：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">binarySearch</span><span class="hljs-params">(vector&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums, <span class="hljs-keyword">int</span> target)</span> </span>&#123;<br><span class="hljs-keyword">int</span> l = <span class="hljs-number">0</span>, r = nums.<span class="hljs-built_in">size</span>() - <span class="hljs-number">1</span>, mid;<br>    <span class="hljs-keyword">while</span>(l &lt;= r)&#123;<br>        mid = (l + r) &gt;&gt; <span class="hljs-number">1</span>; <span class="hljs-comment">// 位运算优先级最低，该方法不防溢出</span><br>        <span class="hljs-keyword">if</span>(nums[mid] == target) <span class="hljs-keyword">return</span> mid;<br>        <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(nums[mid] &lt; target) l = mid + <span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">else</span> r = mid - <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>如果需要找出大于等于、大于目标的第一个数的下标（手写<code>lower_bound</code> 和 <code>upper_bound</code>），模板如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">boundSearch</span><span class="hljs-params">(vector&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums, <span class="hljs-keyword">int</span> target, <span class="hljs-keyword">bool</span> lower)</span></span>&#123;<br>    <span class="hljs-keyword">int</span> l = <span class="hljs-number">0</span>, r = nums.<span class="hljs-built_in">size</span>() - <span class="hljs-number">1</span>, mid, res = nums.<span class="hljs-built_in">size</span>();<br>    <span class="hljs-keyword">while</span> (l &lt;= r) &#123;<br>        mid = (l + r) &gt;&gt; <span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">if</span>(nums[mid] &gt; target || (lower &amp;&amp; nums[mid] &gt;= target))&#123;<br>            r = mid - <span class="hljs-number">1</span>;<br>            res = mid;<br>        &#125;<span class="hljs-keyword">else</span><br>            l = mid + <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="寻找旋转排序数组中的最小值-m">153. 寻找旋转排序数组中的最小值(M)</h3><p><ahref="https://leetcode.cn/problems/find-minimum-in-rotated-sorted-array/">题目描述</a>：将一个升序数组<strong>旋转若干个单位</strong>（轮转平移），距离未知，找出数组中最小值的下标。</p><p><strong>方法1</strong>：二分查找变种，按照如下规则二分，最终返回<span class="math inline">\(nums[mid]\)</span>，时间复杂度 <spanclass="math inline">\(O(\log n )\)</span>。</p><blockquote><ul><li>如果 <span class="math inline">\(mid\)</span>大于等于两端，说明左侧升序，最小值在右侧，<spanclass="math inline">\(l=mid+1\)</span>；</li><li>如果 <span class="math inline">\(mid\)</span>小于等于两端，说明右侧升序，最小值在左侧，<strong>也可能就是</strong><span class="math inline">\(mid\)</span>，<spanclass="math inline">\(r=mid\)</span>；</li><li>如果 <span class="math inline">\(mid\)</span><strong>介于两者之间</strong>，说明此时已按顺序排列，<spanclass="math inline">\(r=mid - 1\)</span>，也可以直接返回 <spanclass="math inline">\(nums[l]\)</span>。</li></ul></blockquote><p><strong>方法2</strong>：简化方法1，观察发现后<strong>两种情况可以合并</strong>为「<spanclass="math inline">\(mid\)</span> 小于右端」，此时令 <spanclass="math inline">\(r=mid\)</span> 就行，最终返回 <spanclass="math inline">\(nums[l]\)</span>。</p><h3 id="搜索旋转排序数组-m">33. 搜索旋转排序数组 (M)</h3><p><ahref="https://leetcode.cn/problems/search-in-rotated-sorted-array/">题目描述</a>：将一个升序数组<strong>旋转若干个单位</strong>（轮转平移），距离未知，设计算法查找一个给定值<code>target</code>。</p><p><strong>方法1</strong>：二分找到旋转排序数组中的最小值，即可将其<strong>分为两个升序数组</strong>，根据<span class="math inline">\(nums[0]\)</span> 与 <spanclass="math inline">\(target\)</span>大小判断目标在左段还是右段，再进行二分查找即可。时间复杂度 <spanclass="math inline">\(O(\log n )\)</span>。</p><p><strong>方法2</strong>：一次二分查找即可，按照如下规则二分，时间复杂度<span class="math inline">\(O(\log n )\)</span>。</p><blockquote><ul><li>如果 <span class="math inline">\(mid\)</span> 就是 <spanclass="math inline">\(target\)</span>，直接返回下标</li><li>如果 <span class="math inline">\(mid\)</span><strong>大于等于</strong>左端（这里取等是因为 <spanclass="math inline">\(mid=l\)</span>时<strong>必须往右</strong>），说明左侧升序，此时比较 <spanclass="math inline">\(target\)</span> 的大小：<ul><li>如果 <span class="math inline">\(target\in[nums[l],nums[mid])\)</span>，说明目标位于左侧，则 <spanclass="math inline">\(r=mid-1\)</span></li><li>否则说明位于右侧，则 <spanclass="math inline">\(l=mid+1\)</span></li></ul></li><li>如果 <span class="math inline">\(mid\)</span><strong>小于</strong>左端，说明右侧升序，此时比较 <spanclass="math inline">\(target\)</span> 的大小：<ul><li>如果 <span class="math inline">\(target\in (nums[mid],nums[r]]\)</span>，说明目标位于右侧，则 <spanclass="math inline">\(l=mid+1\)</span></li><li>否则说明位于左侧，则 <spanclass="math inline">\(r=mid-1\)</span></li></ul></li></ul></blockquote><h3 id="在排序数组中查找元素的第一个和最后一个位置-m">34.在排序数组中查找元素的第一个和最后一个位置 (M)</h3><p><ahref="https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/">题目描述</a>：给定一个升序数组和一个目标值，找出目标值的开始和结束位置，如果不存在则返回<code>[-1, -1]</code>。</p><p><strong>方法1</strong>：二分查找，在用 <code>lower_bound</code> 和<code>upper_bound</code>分别找出两个下标，判断下标指向的元素<strong>是否为末尾</strong>（<strong>防止越界</strong>）、<strong>是否为目标</strong>，如果是目标则返回<code>[l, r - 1]</code>。时间复杂度 <span class="math inline">\(O(\logn)\)</span>。</p><p><strong>方法2</strong>：二分查找，在用 <code>lower_bound</code> 和<code>upper_bound</code> 找出位置后，不需要额外判断，如果<code>l==r</code>，则说明<strong>目标不存在</strong>，两个都指向了<code>end()</code>。</p><p><strong>方法3</strong>：封装，用 <code>equal_range</code>封装上述两个函数，直接返回 <spanclass="math inline">\((l,r)\)</span>。</p><h3 id="寻找峰值-m">162. 寻找峰值 (M)</h3><p><ahref="https://leetcode.cn/problems/find-peak-element/">题目描述</a>：给定一个未排序数组，满足<strong>相邻元素不相等</strong>，找出其中的任意一个峰值元素。峰值元素是指严格大于左右相邻值的元素。</p><p><strong>方法1</strong>：直接找最大值，由于相邻元素不相等，那么最大值两侧的元素一定严格小于最大值本身。时间复杂度<span class="math inline">\(O(n )\)</span>。</p><p><strong>方法2</strong>：二分查找，从一个位置开始，<strong>不断地向高处走</strong>，那么一定可以到达一个峰值。走的方向可以根据<span class="math inline">\(nums[i-1], nums[i], nums[i+1]\)</span>三者的关系决定。如果<strong>规定最后一种情况往右走</strong>，那么可以简化为两种情况：如果<span class="math inline">\(nums[i]&lt;nums[i+1]\)</span>，往右走；如果<spanclass="math inline">\(nums[i]&gt;nums[i+1]\)</span>，往左走。时间复杂度<span class="math inline">\(O(\log n)\)</span>。</p><blockquote><ul><li>如果 <spanclass="math inline">\(nums[i−1]&lt;nums[i]&gt;nums[i+1]\)</span>，那么位置<span class="math inline">\(i\)</span> 就是峰值位置，可以直接返回；</li><li>如果 <spanclass="math inline">\(nums[i−1]&lt;nums[i]&lt;nums[i+1]\)</span>，那么位置<span class="math inline">\(i\)</span> 处于上坡，需要往右走；</li><li>如果 <spanclass="math inline">\(nums[i−1]&gt;nums[i]&gt;nums[i+1]\)</span>，那么位置<span class="math inline">\(i\)</span> 处于下坡，需要往左走；</li><li>如果 <spanclass="math inline">\(nums[i−1]&gt;nums[i]&lt;nums[i+1]\)</span>，那么位置<span class="math inline">\(i\)</span>位于山谷，两侧都是上坡，可以朝任意方向走。</li></ul></blockquote><h3 id="搜索二维矩阵-m">74. 搜索二维矩阵 (M)</h3><p><ahref="https://leetcode.cn/problems/search-a-2d-matrix/">题目描述</a>：给定一个矩阵，每行的元素<strong>从左到右升序</strong>排列、每行的第一个整数<strong>大于上一行</strong>的最后一个整数，判断目标值是否存在。</p><p><strong>方法1</strong>：Z字线性查找，从左上角开始向下搜索，直到遇到<strong>第一个大于</strong>目标值的数，开始向右搜素。复杂度<span class="math inline">\(O(m+n)\)</span>。</p><p><strong>方法2</strong>：两次二分查找，先对<strong>第一列</strong>进行<code>upper_bound</code>找到<strong>第一个大于</strong>目标值的数（注意自定义比较函数），再在其上一行进行<code>binary_search</code> 判断目标值是否存在。复杂度 <spanclass="math inline">\(O(\log m +\log n)=O(\log mn)\)</span>。</p><p><strong>方法3</strong>：一次二分查找，手写 <spanclass="math inline">\([0, mn-1]\)</span>的二分查找，将<strong>一维索引映射到二维矩阵</strong>，<code>x=mat[mid/n][mid%n]</code>。</p><h3 id="搜索二维矩阵-ii-m">240. 搜索二维矩阵 II (M)</h3><p><ahref="https://leetcode.cn/problems/search-a-2d-matrix-ii/">题目描述</a>：给定一个矩阵，每行的元素<strong>从左到右升序</strong>排列、每列的元素<strong>从上到下升序</strong>排列，从中找出目标值。</p><p><strong>方法1</strong>：本题<strong>不能使用两次二分查找</strong>，因为不能确保目标值就出现在指定行，因此考虑<strong>对每行都进行二分</strong>，时间复杂度<span class="math inline">\(O(n \log m)\)</span>。</p><p><strong>方法2</strong>：Z 字线性查找，从矩阵的<strong>右上角</strong><span class="math inline">\((0,n-1)\)</span>开始，每次将当前位置与目标进行比较，可以排除掉一行或一列，从而向左、向下走。如果超出了矩阵的边界，则说明目标值不存在，时间复杂度<span class="math inline">\(O(n+m)\)</span>。</p><blockquote><p>假设当前位置为 <spanclass="math inline">\((x,y)\)</span>，则目标是搜素以 <spanclass="math inline">\((x,y)\)</span>为<strong>右上角</strong>的矩形，其他位置已经排除：</p><ul><li>如果 <spanclass="math inline">\(matrix[x][y]=target\)</span>，说明搜素完成；</li><li>如果 <spanclass="math inline">\(matrix[x][y]&gt;target\)</span>，则<strong>当前元素及其下方</strong>的所有元素（第<span class="math inline">\(y\)</span> 列）都可以排除，向左走；</li><li>如果 <spanclass="math inline">\(matrix[x][y]&lt;target\)</span>，则<strong>当前元素及其左侧</strong>的所有元素（第<span class="math inline">\(x\)</span> 行）都可以排除，向下走。</li></ul></blockquote><p><strong>方法3</strong>：抽象BST，将<strong>右上角</strong>看作根节点，向左和向下的指针看作左、右子树，则可以将矩阵抽象为<strong>二叉搜索树</strong>。因此，如果当前节点大于目标，则应<strong>搜索左子树</strong>，即向左走；反之同理。本质与方法2 相同。</p><h2 id="二分答案">二分答案</h2><p>二分答案是二分查找里面较难的题型。当<strong>直接求一个最值很难</strong>，但我们可以很容易地<strong>在解空间做判定性问题</strong>，比如：能不能、偏大还是偏小，返回一个布尔值，从而缩小解空间。</p><p>这类题要求答案具有「<strong>单调性</strong>」且「<strong>范围已知</strong>」，最常见的类型是「<strong>最小值最大</strong>/<strong>最大值最小</strong>」题，随着答案的增大，解空间逐渐呈「能到不能」，具有单调性。</p><p>其他的类型比较隐蔽，但都有一个特点，要求一个<strong>最大/小值</strong>，且「<strong>小于该值的都True、大于该值的都False</strong>」具有明显的单调性，这类题往往伴随着<strong>贪心选择性</strong>。</p><p><strong>整数域</strong>上的二分模板如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">int</span> l = <span class="hljs-number">0</span>, r = <span class="hljs-number">1e9</span> + <span class="hljs-number">3</span>, mid;<br><span class="hljs-keyword">auto</span> check = [&amp;](<span class="hljs-keyword">int</span> x) &#123;<br>    <span class="hljs-comment">// 根据实际情况填写</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">bool</span>;<br>&#125;;<br><span class="hljs-comment">// 最大化最小模板</span><br><span class="hljs-keyword">while</span>(l &lt;= r)&#123;<br>    mid = (l + r) &gt;&gt; <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">if</span>(<span class="hljs-built_in">check</span>(mid))<br>        l = mid + <span class="hljs-number">1</span>; <span class="hljs-comment">// 如果是最小化最大，则交换这两行</span><br>   <span class="hljs-keyword">else</span><br>        r = mid - <span class="hljs-number">1</span>; <span class="hljs-comment">// 如果是最小化最大，则交换这两行</span><br>&#125;<br><span class="hljs-keyword">return</span> r;<br></code></pre></td></tr></table></figure><p><strong>浮点数域</strong>上的二分模板如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">double</span> l = <span class="hljs-number">0</span>, r = <span class="hljs-number">2</span>, mid;<br><span class="hljs-keyword">auto</span> check = [&amp;](<span class="hljs-keyword">double</span> x) &#123;<br>    <span class="hljs-comment">// 根据实际情况填写</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">bool</span>;<br>&#125;;<br><span class="hljs-comment">// 下方逼近模板</span><br><span class="hljs-keyword">while</span>(r - l &gt; <span class="hljs-number">1e-6</span>)&#123;<br>    mid = l + (r - l) / <span class="hljs-number">2</span>;<br>    <span class="hljs-keyword">if</span>(<span class="hljs-built_in">check</span>(mid))<br>        l = mid; <span class="hljs-comment">// 如果是上方逼近，则交换这两行</span><br>   <span class="hljs-keyword">else</span><br>        r = mid; <span class="hljs-comment">// 如果是上方逼近，则交换这两行</span><br>&#125;<br><span class="hljs-keyword">return</span> r;<br></code></pre></td></tr></table></figure><h3 id="x-的平方根-e">69. x 的平方根 (E)</h3><p><ahref="https://leetcode.cn/problems/sqrtx/">题目描述</a>：给定非负整数<code>x</code>，在不使用 <code>sqrt()</code>函数的情况下求平方根，只需返回整数部分。</p><p><strong>方法1</strong>：公式转换，通过有限的可以使用的数学函数，得到想要计算的结果，时间复杂度<spanclass="math inline">\(O(1)\)</span>。具体实现时要注意浮点误差，需要对结果<span class="math inline">\(\pm 1\)</span> 的进行检查。 <spanclass="math display">\[\sqrt{x}=x^{1 / 2}=\left(e^{\ln x}\right)^{1 / 2}=e^{\frac{1}{2} \ln x}\]</span> <strong>方法2</strong>：二分答案，<spanclass="math inline">\(mid\)</span> 可以逐渐逼近 <spanclass="math inline">\(x\)</span>，每次 <code>check(x)</code> 比较 <spanclass="math inline">\(mid^2\)</span> 与 <spanclass="math inline">\(x\)</span> 的大小即可。时间复杂度 <spanclass="math inline">\(O(\log x)\)</span>。</p><p><strong>方法3</strong>：牛顿迭代法，具体证明过程不展开。时间复杂度<span class="math inline">\(O(\logx)\)</span>，常数可以证明小于二分答案。</p><h3 id="礼盒的最大甜蜜度-m">2517. 礼盒的最大甜蜜度 (M)</h3><p><ahref="https://leetcode.cn/problems/maximum-tastiness-of-candy-basket/">题目描述</a>：给定一个整数数组，每个元素代表一类糖果的价格，要求从中选出<code>k</code>类糖果打包成礼盒，定义甜蜜度为礼盒中<strong>任选两类糖果价格差的最小值</strong>，求可能的<strong>最大</strong>甜蜜度。</p><p><strong>方法1</strong>：暴力枚举，当所选糖果价格<strong>越离散</strong>，甜蜜度越大。先排序再逐个枚举，难以实现，复杂度过高。</p><p><strong>方法2</strong>：二分答案 +贪心，由于<strong>随着甜蜜度的增大，可行选法变少</strong>，有单调性且答案取<span class="math inline">\([0,\frac{Pmax-Pmin}{k}]\)</span>。先将价格<strong>排序</strong>，每次<code>check(x)</code> 时<strong>贪心选择</strong>间距大于 <spanclass="math inline">\(x\)</span> 的糖果，如果能选完，则可行。时间复杂度<span class="math inline">\(O(n\log n)\)</span>。</p><h3 id="打家劫舍-iv-m">2560. 打家劫舍 IV (M)</h3><p><ahref="https://leetcode.cn/problems/house-robber-iv/">题目描述</a>：给定一个非负整数数组<code>nums</code>表示每个房子的钱，小偷<strong>不会窃取相邻的房屋</strong>，<strong>窃取能力</strong>定义为整个窃取过程中能从<strong>单间房屋窃取的最大金额</strong>。另给一个整数<code>k</code>表示至少窃取的房间数，求<strong>最小</strong>窃取能力。</p><p><strong>方法1</strong>：二分答案 +DP，由于随着窃取能力的增加，能窃取的房间数<strong>单调不减</strong>，且答案取<span class="math inline">\([0,MaxNum]\)</span>。二分窃取能力，每次<code>check(x)</code> 时用一维线性 DP，<spanclass="math inline">\(dp[i]\)</span> 表示到下标 <spanclass="math inline">\(i\)</span> 能偷窃的金额不超过 <spanclass="math inline">\(x\)</span> 的<strong>最大房间数</strong>，如果<span class="math inline">\(dp[n] \geqslant k\)</span>则可行，时间复杂度 <span class="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法2</strong>：二分答案 + 贪心，每次 <code>check(x)</code>时<strong>贪心窃取尽量密集的房子</strong>，<spanclass="math inline">\(cnt \geqslant k\)</span> 则可行。时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>。</p><h3 id="寻找重复数-m">287. 寻找重复数 (M)</h3><p><ahref="https://leetcode.cn/problems/find-the-duplicate-number/">题目描述</a>：给定一个包含<span class="math inline">\(n + 1\)</span> 个整数的数组，其数字都在<span class="math inline">\([1, n]\)</span>范围内，假设数组中<strong>有且仅有一个重复的整数</strong>，找出数组中重复的数字，要求空间复杂度<span class="math inline">\(O(1)\)</span>。</p><p><strong>方法1</strong>：二分答案，定义 <spanclass="math inline">\(cnt[i]\)</span>表示数组中<strong>小于等于</strong> <spanclass="math inline">\(i\)</span> 的数的个数，假设重复数是 <spanclass="math inline">\(x\)</span>，则 <spanclass="math inline">\([1,x-1]\)</span> 里所有数都满足 <spanclass="math inline">\(cnt[i] \leq i\)</span>；而 <spanclass="math inline">\([x,n]\)</span> 里的所有数满足 <spanclass="math inline">\(cnt[i]&gt;i\)</span>。因此可以二分枚举 <spanclass="math inline">\(x\)</span> 直到找到。时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法2</strong>：二进制，预处理 <spanclass="math inline">\([1,n]\)</span> 这 <spanclass="math inline">\(n\)</span> 个数「<strong>每一位</strong>为 <spanclass="math inline">\(1\)</span>的<strong>个数之和</strong>」，如果当前数组的「<strong>每一位</strong>为<span class="math inline">\(1\)</span>的<strong>个数之和</strong>」大于期望值，则说明<strong>重复数的这一位</strong>为<span class="math inline">\(1\)</span>。遍历每一位即可，时间复杂度 <spanclass="math inline">\(O(n \log n)\)</span>。</p><p><strong>方法3</strong>：抽象成环形链表寻找入口，快慢指针，先用 <spanclass="math inline">\(fast\)</span> 和 <spanclass="math inline">\(slow\)</span> 相遇，再用 <spanclass="math inline">\(pos\)</span> 和 <spanclass="math inline">\(slow\)</span> 相遇，时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="求出最多标记下标-m">2576. 求出最多标记下标 (M)</h3><p><ahref="https://leetcode.cn/problems/find-the-maximum-number-of-marked-indices/">题目描述</a>：给定一个整数数组，一开始所有下标都没有标记。选择<strong>两个未标记的下标</strong>，满足<code>2*nums[i]&lt;=nums[j]</code>，标记 <code>i</code> 和<code>j</code>。执行上述操作<strong>任意</strong>次，求<strong>最多</strong>可以标记的下标数。</p><blockquote><p>下标与顺序无关，因此先对数组排序处理，排序后<strong>配对的数应当分布于两侧</strong>。</p></blockquote><p><strong>方法1</strong>：二分答案，假设答案为 <spanclass="math inline">\(x\)</span> 对，则小于 <spanclass="math inline">\(x\)</span> 对肯定可以，大于 <spanclass="math inline">\(x\)</span>对肯定不行，具有<strong>单调性</strong>。二分枚举 <spanclass="math inline">\(x\)</span>，每次 <code>check(x)</code>时只需判断最左和最右 <span class="math inline">\(x\)</span>个数是否<strong>一一配对</strong>即可。时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法2</strong>：贪心 +双指针，由于配对的数分布于两侧，最多可有 <spanclass="math inline">\(n/2\)</span>组配对，且左边匹配成功的数<strong>必定是最小的</strong> <spanclass="math inline">\(x\)</span> 个数。因此<strong>顺序枚举左边</strong><spanclass="math inline">\(i\)</span>，并从<strong>右边找到第一个满足条件</strong>的<spanclass="math inline">\(j\)</span>，一直到右边数遍历完。排序时间复杂度<span class="math inline">\(O(n\log n)\)</span>。</p><h2 id="分治">分治</h2><h3 id="最大子数组和-m">53. 最大子数组和 (M)</h3><p><ahref="https://leetcode.cn/problems/maximum-subarray/">题目描述</a>：给定包含<strong>正负整数</strong>的数组，找出具有最大和的<strong>连续子数组</strong>（子数组最少包含一个元素），返回最大和。</p><p><strong>方法1</strong>：滑动窗口 +贪心，累计窗口内的总和，当<strong>和为负数</strong>时，说明窗口<strong>整体不会对后来的数字有贡献</strong>（但里面的元素可以有贡献），可以将左指针向前移动。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>方法2</strong>：DP，用 <spanclass="math inline">\(dp[i]\)</span> 表示以第 <spanclass="math inline">\(i\)</span>个数<strong>结尾</strong>的「连续子数组的最大和」，对于每个数<strong>考虑单独成段还是加入</strong><span class="math inline">\(dp[i-1]\)</span> 对应的段。最终答案为 <spanclass="math inline">\(\max \{ dp[1\cdots n]\}\)</span>。时空复杂度均为<span class="math inline">\(O(n)\)</span>。 <spanclass="math display">\[dp\left[ i \right] =\max \left\{ dp\left[ i-1 \right] +nums\left[ i\right] , nums\left[ i \right] \right\}\]</span> <strong>方法3</strong>：DP +空间优化，<strong>每个状态只由前一个状态转移过来</strong>，空间复杂度<span class="math inline">\(O(1)\)</span>。</p><p><strong>方法4</strong>：分治 +暴力扫描，将数组从中间割开，则最大子段可能在<strong>左边、右边、过中点的两边</strong>，对于第三种可能，<strong>从中点开始扫到两边</strong>，得到的两个最大值相加即可。递归<span class="math inline">\(O(\log n)\)</span>层，每层扫一遍，时间复杂度 <span class="math inline">\(O(n\logn)\)</span>。</p><p><strong>方法5</strong>：分治 +<strong>线段树思想</strong>。第三种可能可以<strong>继续递归计算</strong>，递归<span class="math inline">\(O(\log n)\)</span> 层，每层 <spanclass="math inline">\(2^{i-1}\)</span> 个节点，时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><blockquote><p>首先定义一个操作 <code>get(a, l, r)</code> 表示查询序列 <spanclass="math inline">\(a\)</span> 中 <spanclass="math inline">\([l,r]\)</span> 区间的最大子段和，对于长度大于<span class="math inline">\(1\)</span> 的区间，取 <spanclass="math inline">\(m=(l+r)/2\)</span>，分治求解 <spanclass="math inline">\([l,m]\)</span> 和 <spanclass="math inline">\([m+1,r]\)</span>。这是基础的分治操作，方法 4、5都一样。</p><p>但是如果想<strong>优化扫描过程</strong>，每次递归需要维护的状态量有哪些呢？首先肯定要有<span class="math inline">\(mSum\)</span>表示区间内的最大子段和，当最大子段跨越 <spanclass="math inline">\(m\)</span>时，还需要「<strong>左子区间包含右端点</strong>的最大子段和」与「<strong>右子区间包含左端点</strong>的最大子段和」，因此还要维护<span class="math inline">\(lSum\)</span> 和 <spanclass="math inline">\(rSum\)</span> 表示包含 <spanclass="math inline">\(l\)</span> 和包含 <spanclass="math inline">\(r\)</span> 的最大子段和。</p><p>对于 <span class="math inline">\([l,r]\)</span> 的 <spanclass="math inline">\(lSum\)</span>，要么等于「<strong>左子区间</strong>的<span class="math inline">\(lSum\)</span>」，要么等于「左子区间的总和 +<strong>右子区间</strong>的 <spanclass="math inline">\(lSum\)</span>」，<spanclass="math inline">\(rSum\)</span>同理。因此还要维护<strong>每个区间的总和</strong> <spanclass="math inline">\(iSum\)</span>，等于「左子区间的 <spanclass="math inline">\(iSum\)</span> + 右子区间的 <spanclass="math inline">\(iSum\)</span>」。</p><p>这其实是线段树 PushUp的思想，如果将每个节点的信息用建树存储，则可以在 <spanclass="math inline">\(O(\log n)\)</span> 时间内求到<strong>任意</strong><span class="math inline">\([l,r]\)</span>区间的答案，适用于<strong>多次查询</strong>。</p></blockquote><h3 id="环形子数组的最大和-m">918. 环形子数组的最大和 (M)</h3><p><ahref="https://leetcode.cn/problems/maximum-sum-circular-subarray/">题目描述</a>：给定包含<strong>正负整数</strong>的环形数组，找出具有最大和的<strong>连续子数组</strong>（子数组最少包含一个元素），返回最大和。</p><p><strong>方法1</strong>：滑动窗口，拼接重复数组化环为链，用滑动窗口累计总和，窗口长度上限为<spanclass="math inline">\(n\)</span>。当和为负数或窗口达到上限时滑动左指针，但<strong>因为有两个条件，无法贪心前移</strong>，必须每次找出最优滑动距离，可能退化到<span class="math inline">\(O(n^2)\)</span>。</p><blockquote><p>贪心前移的反例：<code>[-5,-2,5,6,-2,-7,0,2,8]</code></p></blockquote><p><strong>方法2</strong>：DP +正难则反，子数组没有跨过边界的时候与前一题一样，<strong>子数组跨过边界的时候可以转化为找出最小和再取反</strong>。分类讨论，使用类似的状态转移，时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>坑点</strong>：方法 2中，由于找出最小子数组后剩余部分才是目标，当<strong>最小子数组就是整个数组</strong>（<spanclass="math inline">\(total=min\)</span>）时，取反后没有剩余部分，因此要排除。</p><h2 id="快速排序">快速排序</h2><p>时间复杂度 <span class="math inline">\(O(n\logn)\)</span>，递归需要考虑栈空间，空间复杂度 <spanclass="math inline">\(O(\log n)\)</span>，模板如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 极简快排，待排序数组 a[N] 为全局变量</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">qs</span><span class="hljs-params">(<span class="hljs-keyword">int</span> l, <span class="hljs-keyword">int</span> r)</span></span>&#123;<br><span class="hljs-keyword">int</span> i=l, j=r, x=a[(l+r)&gt;&gt;<span class="hljs-number">1</span>];<br><span class="hljs-keyword">while</span>(i&lt;=j)&#123;<br><span class="hljs-keyword">while</span>(a[i]&lt;x) ++i;<br><span class="hljs-keyword">while</span>(a[j]&gt;x) --j;<br><span class="hljs-keyword">if</span>(i&lt;=j) <span class="hljs-built_in">swap</span>(a[i++],a[j--]);<br>&#125;<br><span class="hljs-keyword">if</span>(l&lt;j) <span class="hljs-built_in">qs</span>(l,j);<br><span class="hljs-keyword">if</span>(i&lt;r) <span class="hljs-built_in">qs</span>(i,r);<br>&#125;<br><br><span class="hljs-comment">// partition 函数式写法，方便修改，但会被全相同数字样例卡</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">QuickSort</span><span class="hljs-params">(vector&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums, <span class="hljs-keyword">int</span> low, <span class="hljs-keyword">int</span> high)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(low &lt; high) &#123;<br>        <span class="hljs-keyword">int</span> pivotpos = <span class="hljs-built_in">RandomizedPartition</span>(nums, low, high);<br>        <span class="hljs-built_in">QuickSort</span>(nums, low, pivotpos - <span class="hljs-number">1</span>);<br>        <span class="hljs-built_in">QuickSort</span>(nums, pivotpos + <span class="hljs-number">1</span>, high);<br>    &#125;<br>&#125;<br><span class="hljs-comment">// 随机选枢轴，防止被接近有序的样例卡，大部分时候不需要</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">RandomizedPartition</span><span class="hljs-params">(vector&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums, <span class="hljs-keyword">int</span> low, <span class="hljs-keyword">int</span> high)</span> </span>&#123;<br>    <span class="hljs-keyword">int</span> pos = <span class="hljs-built_in">rand</span>() % (high - low + <span class="hljs-number">1</span>) + low;<br>    <span class="hljs-built_in">swap</span>(nums[low], nums[pos]);<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">Partition</span>(nums, low, high);<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Partition</span><span class="hljs-params">(vector&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums, <span class="hljs-keyword">int</span> low, <span class="hljs-keyword">int</span> high)</span></span>&#123;<br>    <span class="hljs-keyword">int</span> pivot = nums[low];<br>    <span class="hljs-keyword">while</span>(low &lt; high) &#123;<br>        <span class="hljs-keyword">while</span>(low &lt; high &amp;&amp; nums[high] &gt;= pivot) --high;<br>        nums[low] = nums[high];<br>        <span class="hljs-keyword">while</span>(low &lt; high &amp;&amp; nums[low] &lt;= pivot) ++low;<br>        nums[high] = nums[low];<br>    &#125;<br>    nums[low] = pivot;<br>    <span class="hljs-keyword">return</span> low;<br>&#125;<br></code></pre></td></tr></table></figure><p>快排的优化思路有很多：</p><ol type="1"><li>枢轴选取：Median3、随机抽取（尽可能使分割均匀，防止退化到 <spanclass="math inline">\(O(n^2)\)</span>；</li><li>扫描顺序：从两侧以<strong>双指针</strong>的形式靠拢，可以加大均匀分割的概率；</li><li>优化剪枝：当长度 <span class="math inline">\(n&lt;16\)</span>时使用<strong>非递归</strong>排序、或当递归深度达到指定值时采用非递归排序；</li><li>尾递归：在处理两个子问题时，可以先处理更小规模的问题，减小递归深度，节约栈开销；</li><li>三向切分：划分出<strong>小于 pivot、等于 pivot、大于 pivot三个分区</strong>，来避免大量相同元素时退化。</li></ol><h3 id="数组中的第k个最大元素-m">215. 数组中的第K个最大元素 (M)</h3><p><ahref="https://leetcode.cn/problems/kth-largest-element-in-an-array/">题目描述</a>：给定整数数组<code>nums</code> 和整数 <code>k</code>，请返回数组中第 <code>k</code>个最大的元素。</p><p><strong>方法1</strong>：直接排序，快排时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>，空间复杂度 <spanclass="math inline">\(O(\log n)\)</span>。</p><p><strong>方法2</strong>：快速选择 Kth，每次 Partition都将数组分为两部分，并<strong>确定一个元素的最终位置</strong>。通过判断<strong>两侧元素的个数</strong>就可以知道第<code>k</code>大元素位于哪一侧，再<strong>递归一侧即可</strong>。时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(\log n)\)</span>。</p><blockquote><p>修改 <code>QuickSort()</code> 主函数，传入参数 <code>k</code>，先进行<code>Partition</code> 操作，无需随机。</p><p>Partition 后记<strong>枢轴右边的元素个数</strong>为<code>x</code>，如果 <code>x==k-1</code>，直接返回枢轴；如果<code>x&gt;k-1</code> 则递归右边，传入参数<code>k</code>；否则递归左边，传入参数 <code>k-x-1</code>。</p><p>也可以直接使用 C++ 的 <code>nth_element()</code> 函数，可以将第 k大的元素放至对应位置，并确保左侧小、右侧大。</p></blockquote><p><strong>方法3</strong>：堆排序，维护一个大小为 <spanclass="math inline">\(k\)</span>的<strong>小顶堆</strong>，每遍历到一个元素，如果<strong>小于堆顶元素</strong>则不考虑；否则将堆顶元素弹出，<strong>放入新数下滤</strong>。最后堆顶元素就是目标值，时间复杂度为<span class="math inline">\(O(n\log k)\)</span>，空间复杂度 <spanclass="math inline">\(O(k)\)</span>。</p><p><strong>坑点</strong>：很多人以为找第 K大元素就要用<strong>大顶堆</strong>，实则不然。第 K 大元素在<strong>前 K大</strong>元素中就是<strong>最小</strong>元素，因此要用小顶堆维护。</p><h3 id="前-k-个高频元素-m">347. 前 K 个高频元素 (M)</h3><p><ahref="https://leetcode.cn/problems/top-k-frequent-elements/">题目描述</a>：给定整数数组<code>nums</code> 和整数 <code>k</code>，请返回数组中出现频率前<code>k</code> 高的元素，顺序任意。</p><p><strong>方法1</strong>：哈希表 + 快速选择Kth，哈希表预处理得到频率数组。由于题目要求<strong>列出前 K个</strong>元素，因此不建议自己实现二分，直接 <code>nth_element()</code>解决，放在左边的就是前，时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(\log n)\)</span>。</p><p><strong>方法2</strong>：哈希表 +优先队列，哈希表预处理得到频率数组。优先队列，维护一个大小为 <spanclass="math inline">\(k\)</span> 的<strong>小顶堆</strong>，时间复杂度为<span class="math inline">\(O(n\log k)\)</span>，空间复杂度 <spanclass="math inline">\(O(k)\)</span>。</p><h3 id="删除某些元素后的数组均值-e">1619. 删除某些元素后的数组均值(E)</h3><p><ahref="https://leetcode.cn/problems/mean-of-array-after-removing-some-elements/">题目描述</a>：给定一个整数数组<code>arr</code>，删除最小的 <code>5%</code> 个数和最大的<code>5%</code> 个数后，返回剩余数的平均值。</p><p><strong>方法1</strong>：优先队列，用最大堆和最小堆分别找出两部分，但在数据量小的情况下复杂度<strong>常数略大</strong>。</p><p><strong>方法2</strong>：排序，使用 STL 里的 <code>sort</code> 和<code>accumulate</code>，复杂度 <span class="math inline">\(O(n \logn)\)</span> 但是比上一个方法优化更快。</p><p><strong>方法3</strong>：<strong>快速选择</strong>，当成 <strong>TopK问题</strong>，使用 STL 里的 <code>nth_element</code>，将第 K小的元素、第 K大的元素找出，并且两边分别是更小和更大的数，只需将中间部分求和即可。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><h3 id="寻找两个正序数组的中位数-h">4. 寻找两个正序数组的中位数 (H)</h3><p><ahref="https://leetcode.cn/problems/median-of-two-sorted-arrays/">题目描述</a>：给定两个长为m 和 n的有序数组，要求找到两个有序数组<strong>合并后</strong>数组的中位数。（也可以说是第<span class="math inline">\(k\)</span> 小的数）</p><p><strong>方法1</strong>：直接归并，时间复杂度为 <spanclass="math inline">\(O(m+n)\)</span>。</p><p><strong>方法2</strong>：二分 + 递归，类似 Kth 问题，但是省去Partition 的过程（数组本身有序），时间复杂度为 <spanclass="math inline">\(O\left(\log (m+n)\right)\)</span>。</p><blockquote><p>看成 Kth 问题，当 <span class="math inline">\(m+n\)</span>为奇数时，<spanclass="math inline">\(k=(m+n)/2+1\)</span>；为偶数时，<spanclass="math inline">\(k=(m+n)/2\)</span> 和 <spanclass="math inline">\((m+n)/2+1\)</span>。每次从两个数组中取第 <spanclass="math inline">\(k/2\)</span> 个数进行比较，将小的那一方的前 <spanclass="math inline">\(k/2\)</span> 个数全部排除。</p><p>证明：<span class="math inline">\(A[k/2-1]\)</span> 和 <spanclass="math inline">\(B[k/2-1]\)</span> 要想成为第 <spanclass="math inline">\(k\)</span> 小的数，那必须要比 <spanclass="math inline">\(k-1\)</span> 个数大才行。如果 <spanclass="math inline">\(A[k/2-1]＜B[k/2-1]\)</span>，则 <spanclass="math inline">\(A[k/2-1]\)</span> 至多只能比 <spanclass="math inline">\(k-2\)</span> 个数大，因此 <spanclass="math inline">\(A\)</span> 数组可以<strong>全部丢弃</strong>，但是<span class="math inline">\(B\)</span>数组则<strong>都有可能</strong>。</p><p>递归的结束条件为：如果一个数组已经被丢弃为空，则可以直接返回另一个数组的第<span class="math inline">\(k\)</span> 小数；否则如果 <spanclass="math inline">\(k=1\)</span>，此时只要返回两个数组首元素的最小值即可。</p></blockquote><p><strong>方法3</strong>：划分数组，中位线两边的数目相等，且左边最大值小于右边最小值。时间复杂度为<span class="math inline">\(O\left(\log \min (m,n)\right)\)</span>。</p><p><strong>坑点</strong>：方法 2 中，如果取第 <spanclass="math inline">\(k/2\)</span>个数时<strong>数组越界</strong>（没那么多数可选），则需要选取对应数组中的最后一个元素，此时循环过后不能直接将<span class="math inline">\(k\)</span> 减去 <spanclass="math inline">\(k/2\)</span>。</p><h2 id="归并排序">归并排序</h2><p>时间复杂度 <span class="math inline">\(O(n\logn)\)</span>，递归写法，且用到辅助数组，空间复杂度 <spanclass="math inline">\(O(n)\)</span>，模板如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 极简归排</span><br><span class="hljs-keyword">int</span> t[maxn];<br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">ms</span><span class="hljs-params">(<span class="hljs-keyword">int</span> l, <span class="hljs-keyword">int</span> r)</span></span>&#123;<br><span class="hljs-keyword">if</span>(l&gt;=r) <span class="hljs-keyword">return</span>;<br><span class="hljs-keyword">int</span> mid=(l+r)&gt;&gt;<span class="hljs-number">1</span>, p=l, q=mid+<span class="hljs-number">1</span>, ct=l;<br><span class="hljs-built_in">ms</span>(l,mid);<br><span class="hljs-built_in">ms</span>(mid+<span class="hljs-number">1</span>,r);<br><span class="hljs-keyword">while</span>(p&lt;=mid || q&lt;=r)&#123;<br><span class="hljs-keyword">if</span>((p&gt;mid) || (q&lt;=r &amp;&amp; a[q]&lt;a[p])) t[ct++]=a[q++];<br><span class="hljs-keyword">else</span> t[ct++]=a[p++];<br>&#125;<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i=l; i&lt;=r; i++) a[i]=t[i];<br>&#125;<br><span class="hljs-comment">// merge 函数式写法，方便修改</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">MergeSort</span><span class="hljs-params">(vector&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums, <span class="hljs-keyword">int</span> low, <span class="hljs-keyword">int</span> high)</span> </span>&#123;<br>    <span class="hljs-keyword">if</span> (low &lt; high) &#123;<br>        <span class="hljs-keyword">int</span> mid = low + (high - low) / <span class="hljs-number">2</span>;<br>        <span class="hljs-built_in">MergeSort</span>(nums, low, mid);<br>        <span class="hljs-built_in">MergeSort</span>(nums, mid + <span class="hljs-number">1</span>, high);<br>        <span class="hljs-built_in">Merge</span>(nums, low, mid, high);<br>    &#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">Merge</span><span class="hljs-params">(vector&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums, <span class="hljs-keyword">int</span> low, <span class="hljs-keyword">int</span> mid, <span class="hljs-keyword">int</span> high)</span> </span>&#123;<br>    <span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">temp</span><span class="hljs-params">(high - low + <span class="hljs-number">1</span>)</span></span>; <span class="hljs-comment">// 临时数组</span><br>    <span class="hljs-keyword">int</span> i = low, j = mid + <span class="hljs-number">1</span>, k = <span class="hljs-number">0</span>;  <span class="hljs-comment">// k 指向临时数组</span><br>    <br>    <span class="hljs-keyword">while</span> (i &lt;= mid &amp;&amp; j &lt;= high) &#123;<br>        <span class="hljs-keyword">if</span> (nums[i] &lt;= nums[j]) &#123;<br>            temp[k++] = nums[i++];<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            temp[k++] = nums[j++];<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">while</span> (i &lt;= mid) &#123;<br>        temp[k++] = nums[i++];<br>    &#125;<br>    <span class="hljs-keyword">while</span> (j &lt;= high) &#123;<br>        temp[k++] = nums[j++];<br>    &#125;<br>    <span class="hljs-comment">// 将临时数组中的元素复制回原数组</span><br>    <span class="hljs-keyword">for</span> (i = low, k = <span class="hljs-number">0</span>; i &lt;= high; ++i, ++k) &#123;<br>        nums[i] = temp[k];<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="lcr-170.-交易逆序对的总数-h">LCR 170. 交易逆序对的总数 (H)</h3><p><ahref="https://leetcode.cn/problems/shu-zu-zhong-de-ni-xu-dui-lcof/">题目描述</a>：给定一个无序数组，求其中的逆序对个数。</p><p><strong>方法</strong>：归并排序，每次 Merge 时如果将<code>high</code> 先放进临时数组，则一次性会消除 <code>mid-low+1</code>个逆序对（左侧数组的剩余元素个数），其他时候不会消除。时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>，临时数组空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="排序链表-m">148. 排序链表 (M)</h3><p><ahref="https://leetcode.cn/problems/sort-list/">题目描述</a>：给定链表的头结点<code>head</code>，请将其按<strong>升序</strong>排列并返回<strong>排序后的链表</strong>。</p><p><strong>方法1</strong>：自顶向下归并排序，以链表中点为界将其拆分为两个子链表，对两个子链表递归排序后，合并两个有序链表，时间复杂度<span class="math inline">\(O(n\log n)\)</span>，空间复杂度为递归栈深度<span class="math inline">\(O(\log n)\)</span>。</p><p><strong>方法2</strong>：自底向上归并排序，记每次要排序的长度为 <spanclass="math inline">\(len\)</span>，初始 <spanclass="math inline">\(len=1\)</span>，将子链表两两归并后，<spanclass="math inline">\(len\)</span> 倍增。无需递归，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="全局倒置与局部倒置-m">775. 全局倒置与局部倒置 (M)</h3><p><ahref="https://leetcode.cn/problems/global-and-local-inversions/">题目描述</a>：给定一个<span class="math inline">\([0,n-1]\)</span><strong>随机排列</strong>的数组，定义两个相邻元素的逆序对为<strong>局部倒置</strong>，任意两个元素的逆序对为<strong>全局倒置</strong>，判断该数组中两种倒置的数量是否相等。</p><p><strong>方法1</strong>：暴力，全局倒置可以用归并获取，局部倒置可以冒泡扫描一遍获取，时间复杂度<span class="math inline">\(O(n \log n)\)</span>，空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：转换思维，只需判断是否存在「<strong>跨元素的逆序对</strong>」即可，如果一次<strong>冒泡扫描</strong>能让数组变得有序，则必然不存在跨元素的逆序对，时间复杂度<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：贪心，维护<strong>前缀最大值</strong>，如果<span class="math inline">\(maxPre\)</span> 大于 <spanclass="math inline">\(nums[i+2]\)</span>，则存在跨元素的逆序对，时间复杂度<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法4</strong>：归纳，可以证明满足题意的充要条件是「每个元素<span class="math inline">\(x\)</span> 都要满足 <spanclass="math inline">\(\left| nums\left[ x \right] -x \right|\leqslant1\)</span>」，时间复杂度 <span class="math inline">\(O(n)\)</span>。</p><blockquote><p>对于 <span class="math inline">\([0,n-1]\)</span>的原始问题，如果满足题意，<strong>最小元素</strong> <spanclass="math inline">\(0\)</span> <strong>必然在前两个位置</strong>：</p><ul><li>若 <span class="math inline">\(nums[0]=0\)</span>，则转换为 <spanclass="math inline">\([1,n-1]\)</span> 的子问题；</li><li>若 <span class="math inline">\(nums[1]=0\)</span>，则次小元素 <spanclass="math inline">\(1\)</span> <strong>必须在第一个位置</strong>，转为<span class="math inline">\([2, n-1]\)</span> 的子问题。</li></ul><p>以此类推，元素 <span class="math inline">\(1\)</span> 必须在下标为<span class="math inline">\(0,1,2\)</span> 的位置，元素 <spanclass="math inline">\(2\)</span> 必须在下标为 <spanclass="math inline">\(1,2,3\)</span>的位置，<strong>每个数偏离目标位置不能超过一个单位</strong>。</p></blockquote><h2 id="分块">分块</h2><h2 id="倍增">倍增</h2><p>二分的逆过程是倍增，每次根据已经得到的信息，将考虑的范围增加一倍，从而加速操作。</p><p>快速幂就是经典的倍增算法。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 非递归快速幂，最常用</span><br><span class="hljs-function">ll <span class="hljs-title">qpow</span><span class="hljs-params">(ll a, ll n)</span> </span>&#123;<br>    ll ans = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">while</span> (n) &#123;<br>        <span class="hljs-keyword">if</span> (n &amp; <span class="hljs-number">1</span>)     <span class="hljs-comment">// 如果 n 的当前末位为 1</span><br>            ans *= a;  <span class="hljs-comment">// ans 乘上当前的 a</span><br>        a *= a;        <span class="hljs-comment">// a 倍增</span><br>        n &gt;&gt;= <span class="hljs-number">1</span>;       <span class="hljs-comment">// n 往右移一位</span><br>    &#125;<br>    <span class="hljs-keyword">return</span> ans;<br>&#125;<br><br><span class="hljs-comment">// 非递归快速幂，带 mod 运算，常用</span><br><span class="hljs-function">ll <span class="hljs-title">qpowmod</span><span class="hljs-params">(ll a, ll n, ll mod)</span> </span>&#123;<br>    ll ans = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">while</span> (n) &#123;<br>        <span class="hljs-keyword">if</span> (n &amp; <span class="hljs-number">1</span>)<br>            ans = (ans * a) % mod;<br>        a = (a * a) % mod;<br>        n &gt;&gt;= <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> ans % mod;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="powx-n">50. Pow(x, n)</h3><p><ahref="https://leetcode.cn/problems/powx-n/">题目描述</a>：计算浮点数<code>x</code> 的整数 <code>n</code> 次幂函数（即 <spanclass="math inline">\(x^n\)</span>），<code>n</code> 可为负数。</p><p><strong>方法</strong>：快速幂模板题，浮点数不会影响，但是当<code>n</code> 为负数的时候最好先转为正数再处理。时间复杂度 <spanclass="math inline">\(O(\log n)\)</span>。</p><p><strong>坑点</strong>：负数 int 为 -2147483648时取反会溢出，此时可以直接将<strong>入参处</strong>改为<code>long</code>。</p>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #03 数据结构</title>
    <link href="/LeetCode-Data-Structure.html"/>
    <url>/LeetCode-Data-Structure.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「数据结构」类型题中的：线段树、树状数组、并查集等。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><h2 id="线段树">线段树</h2><p>将区间 <span class="math inline">\([1,n]\)</span>逐层二分成子区间（总数不超过 <spanclass="math inline">\(4n\)</span>），从而实现对区间 <spanclass="math inline">\([L,R]\)</span> 的 <spanclass="math inline">\(O(\log n)\)</span>的单点修改、区间修改、区间和查询、区间最值查询等操作。</p><p>懒加载用于<strong>多次更新，少量查询</strong>时，节约时间。</p><p>线段树单点修改，区间求和模板：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> N = <span class="hljs-number">1e5</span>;<br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">node</span> &#123;</span> <span class="hljs-comment">//区间 [l,r] 的和为sum</span><br>    <span class="hljs-keyword">int</span> l, r, sum;<br>&#125;;<br><span class="hljs-function">vector&lt;node&gt; <span class="hljs-title">tree</span><span class="hljs-params">(N)</span></span>;<br>vector&lt;<span class="hljs-keyword">int</span>&gt; nums;<br><span class="hljs-comment">/*  递归建树</span><br><span class="hljs-comment">    每次将区间[l,r]从中划分为两段</span><br><span class="hljs-comment">    id的左孩子为2*id指向左半区间</span><br><span class="hljs-comment">    id的右孩子为2*id+1指向右半区间</span><br><span class="hljs-comment">    当l==r时到达叶子节点，返回即可</span><br><span class="hljs-comment">    id：当前节点</span><br><span class="hljs-comment">    l：区间左边界</span><br><span class="hljs-comment">    r：区间右边界</span><br><span class="hljs-comment">    */</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">build</span><span class="hljs-params">(<span class="hljs-keyword">int</span> id, <span class="hljs-keyword">int</span> l, <span class="hljs-keyword">int</span> r)</span></span>&#123;<br>    tree[id].l = l;<br>    tree[id].r = r;<br>    <span class="hljs-keyword">if</span>(l == r)&#123;<br>        tree[id].sum = nums[l];<br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-keyword">int</span> mid = (l + r) &gt;&gt; <span class="hljs-number">1</span>;<br>    <span class="hljs-built_in">build</span>(<span class="hljs-number">2</span> * id, l, mid);<br>    <span class="hljs-built_in">build</span>(<span class="hljs-number">2</span> * id + <span class="hljs-number">1</span>, mid + <span class="hljs-number">1</span>, r);<br>    tree[id].sum = tree[<span class="hljs-number">2</span> * id].sum + tree[<span class="hljs-number">2</span> * id + <span class="hljs-number">1</span>].sum;<br>    <span class="hljs-keyword">return</span>;<br>&#125;<br><span class="hljs-comment">/*  区间查询：查询给定区间 [l,r] 的和</span><br><span class="hljs-comment">    从根节点开始查询，节点区间与给定区间之间存在四种情况</span><br><span class="hljs-comment">    1.节点区间完全包含于给定区间，此时查询结果加上节点区间和，直接返回</span><br><span class="hljs-comment">    2.节点区间与给定区间完全不相交，此时直接返回0</span><br><span class="hljs-comment">    3.节点区间的左儿子与给定区间相交，此时递归查询左儿子</span><br><span class="hljs-comment">    4.节点区间的右儿子与给定区间相交，此时递归查询右儿子</span><br><span class="hljs-comment">    id：当前节点</span><br><span class="hljs-comment">    l：区间左边界</span><br><span class="hljs-comment">    r：区间右边界</span><br><span class="hljs-comment">    */</span><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">search</span><span class="hljs-params">(<span class="hljs-keyword">int</span> id, <span class="hljs-keyword">int</span> l, <span class="hljs-keyword">int</span> r)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(tree[id].l &gt;= l &amp;&amp; tree[id].r &lt;= r)<br>        <span class="hljs-keyword">return</span> tree[id].sum;<br>    <span class="hljs-keyword">if</span>(tree[id].r &lt; l || tree[id].l &gt; r)<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">int</span> s = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">if</span>(tree[<span class="hljs-number">2</span> * id].r &gt;= l)<br>        s += <span class="hljs-built_in">search</span>(<span class="hljs-number">2</span> * id, l, r);<br>    <span class="hljs-keyword">if</span>(tree[<span class="hljs-number">2</span> * id + <span class="hljs-number">1</span>].l &lt;= r)<br>        s += <span class="hljs-built_in">search</span>(<span class="hljs-number">2</span> * id + <span class="hljs-number">1</span>, l, r);<br>    <span class="hljs-keyword">return</span> s;<br>&#125;<br><span class="hljs-comment">/*  单点修改</span><br><span class="hljs-comment">    一直搜索到叶子节点，首先修改叶子结点的sum，然后逐层返回，修改对应的sum</span><br><span class="hljs-comment">    id：当前节点</span><br><span class="hljs-comment">    i：要修改的节点</span><br><span class="hljs-comment">    k：修改的值</span><br><span class="hljs-comment">    */</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">change</span><span class="hljs-params">(<span class="hljs-keyword">int</span> id, <span class="hljs-keyword">int</span> i, <span class="hljs-keyword">int</span> k)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(tree[id].l == i &amp;&amp; tree[id].r == i)&#123;<br>        tree[id].sum = k;<br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-keyword">int</span> mid = (tree[id].l + tree[id].r) &gt;&gt; <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">if</span>(i &lt;= mid)<br>        <span class="hljs-built_in">change</span>(<span class="hljs-number">2</span> * id, i, k);<br>    <span class="hljs-keyword">else</span><br>        <span class="hljs-built_in">change</span>(<span class="hljs-number">2</span> * id + <span class="hljs-number">1</span>, i, k);<br>    tree[id].sum = tree[<span class="hljs-number">2</span> * id].sum + tree[<span class="hljs-number">2</span> * id + <span class="hljs-number">1</span>].sum;<br>    <span class="hljs-keyword">return</span>;<br>&#125;<br><br><span class="hljs-built_in">NumArray</span>(vector&lt;<span class="hljs-keyword">int</span>&gt;&amp; num) &#123;<br>    nums = num;<br>    <span class="hljs-built_in">build</span>(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, num.<span class="hljs-built_in">size</span>() - <span class="hljs-number">1</span>);<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">update</span><span class="hljs-params">(<span class="hljs-keyword">int</span> index, <span class="hljs-keyword">int</span> val)</span> </span>&#123;<br>    <span class="hljs-built_in">change</span>(<span class="hljs-number">1</span>, index, val);<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">sumRange</span><span class="hljs-params">(<span class="hljs-keyword">int</span> left, <span class="hljs-keyword">int</span> right)</span> </span>&#123;<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">search</span>(<span class="hljs-number">1</span>, left, right);<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="区域和检索---数组可修改-h">307. 区域和检索 - 数组可修改 (H)</h3><p><ahref="https://leetcode.cn/problems/range-sum-query-mutable/">题目描述</a>：给定一个数组，要求实现两个操作：单点更新、区间和查询。</p><p><strong>方法1</strong>：分块，</p><p><strong>方法2</strong>：线段树，</p><p><strong>方法3</strong>：树状数组，</p><h3 id="最长递增子序列-ii-h">2407. 最长递增子序列 II (H)</h3><p><ahref="https://leetcode.cn/problems/longest-increasing-subsequence-ii/">题目描述</a>：一个整数数组<code>nums</code>，找到其中<strong>最长严格递增子序列</strong>的长度，要求子序列中<strong>相邻元素的差值不超过</strong><code>k</code>。</p><p><strong>方法1</strong>：DP，定义 <spanclass="math inline">\(dp[i]\)</span> 为<strong>以 <spanclass="math inline">\(nums[i]\)</span> 结尾的 LIS 长度</strong>，每次从<span class="math inline">\(dp[j]\)</span>中找满足条件的最大值转移，复杂度 <spanclass="math inline">\(O(n^2)\)</span>。 <span class="math display">\[dp[i]=\max (dp[j])+1 \text {, 其中 } 0 \leq j&lt;i \text { 且 }num[i]-k\leqslant num[j]&lt;num[i]\]</span> <strong>方法2</strong>：DP + 线段树，定义 <spanclass="math inline">\(dp[i]\)</span> 为<strong>以 <spanclass="math inline">\(i\)</span> 结尾的 LIS 长度</strong>，用线段树表示<span class="math inline">\(dp\)</span>数组，在<strong>区间内查询最大值</strong>，然后更新目标值、更新区间最大值。复杂度<span class="math inline">\(O(n \log n)\)</span>。 <spanclass="math display">\[dp[nums[i]]=\max (dp[j])+1 \text {, 其中 } num[i]-k \leq j&lt;num[i]\]</span></p><h2 id="树状数组">树状数组</h2><p>线段树更适合区间修改问题，单点修改问题建议用树状数组。使用时要注意，下标从1 开始。</p><p><strong>单点修改，求区间和</strong>模板：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">long</span> <span class="hljs-keyword">long</span> tree[<span class="hljs-number">30050</span>];<br>vector&lt;<span class="hljs-keyword">int</span>&gt; num;<br><span class="hljs-keyword">int</span> n;<br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">lowbit</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x)</span> </span>&#123;<br>    <span class="hljs-keyword">return</span> x &amp; -x;<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">long</span> <span class="hljs-keyword">long</span> <span class="hljs-title">query</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x)</span> </span>&#123;<br>    <span class="hljs-keyword">long</span> <span class="hljs-keyword">long</span> ans = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = x; i &gt; <span class="hljs-number">0</span>; i -= <span class="hljs-built_in">lowbit</span>(i)) <br>        ans += tree[i];<br>    <span class="hljs-keyword">return</span> ans;<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x, <span class="hljs-keyword">int</span> u)</span> </span>&#123;      <br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = x; i &lt;= n; i += <span class="hljs-built_in">lowbit</span>(i)) <br>        tree[i] += u;<br>&#125;<br><span class="hljs-comment">//初始化，tree 下标从 1 开始</span><br><span class="hljs-built_in">NumArray</span>(vector&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums) &#123;<br>    num = nums;<br>    <span class="hljs-built_in">memset</span>(tree, <span class="hljs-number">0</span>, <span class="hljs-built_in"><span class="hljs-keyword">sizeof</span></span>(tree));<br>    n = nums.<span class="hljs-built_in">size</span>();<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt;= n; i++)&#123;<br>        <span class="hljs-built_in">add</span>(i, nums[i - <span class="hljs-number">1</span>]);<br>    &#125;<br>&#125;<br><span class="hljs-comment">// 单点修改</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">update</span><span class="hljs-params">(<span class="hljs-keyword">int</span> index, <span class="hljs-keyword">int</span> val)</span> </span>&#123; <br>    <span class="hljs-built_in">add</span>(index + <span class="hljs-number">1</span>, val - num[index]);<br>    num[index] = val;<br>&#125;<br><span class="hljs-comment">// 查询 [left,right] 区间和 （left，right 下标从 0 开始）</span><br><span class="hljs-function"><span class="hljs-keyword">long</span> <span class="hljs-keyword">long</span> <span class="hljs-title">sumRange</span><span class="hljs-params">(<span class="hljs-keyword">int</span> left, <span class="hljs-keyword">int</span> right)</span> </span>&#123; <br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">query</span>(right + <span class="hljs-number">1</span>) - <span class="hljs-built_in">query</span>(left);<br>&#125;<br><br></code></pre></td></tr></table></figure><p><strong>单点修改，求最值</strong>模板：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">long</span> <span class="hljs-keyword">long</span> tree[<span class="hljs-number">100050</span>];<br><span class="hljs-keyword">long</span> <span class="hljs-keyword">long</span> dp[<span class="hljs-number">100050</span>];  <span class="hljs-comment">//dp[i] 表示以 i 结尾的最长序列长度</span><br><span class="hljs-keyword">int</span> n;<br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">lowbit</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x)</span> </span>&#123;<br>    <span class="hljs-keyword">return</span> x &amp; -x;<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">long</span> <span class="hljs-keyword">long</span> <span class="hljs-title">query</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x, <span class="hljs-keyword">int</span> y)</span></span>&#123;<br>    <span class="hljs-keyword">long</span> <span class="hljs-keyword">long</span> ans = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">while</span>(y &gt;= x)&#123;<br>        ans = <span class="hljs-built_in">max</span>(dp[y], ans);<br>        y--;<br>        <span class="hljs-keyword">for</span>(; y - <span class="hljs-built_in">lowbit</span>(y) &gt;= x; y -= <span class="hljs-built_in">lowbit</span>(y)) <br>            ans = <span class="hljs-built_in">max</span>(tree[y], ans);<br>    &#125;<br>    <span class="hljs-keyword">return</span> ans;<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">update</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x, <span class="hljs-keyword">long</span> <span class="hljs-keyword">long</span> u)</span> </span>&#123;      <br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = x; i &lt;= n; i += <span class="hljs-built_in">lowbit</span>(i)) <br>        tree[i] = <span class="hljs-built_in">max</span>(u, tree[i]);<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="并查集">并查集</h2><p>并查集主要用于处理<strong>不相交集合的合并</strong>问题，常用于快速判断两个点是否在同一个集合。在使用<strong>路径压缩</strong>优化后，合并和查找操作都<strong>近似</strong><span class="math inline">\(O(1)\)</span> 时间复杂度。</p><p>并查集极简模板（在函数内定义即可）：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">int</span> fa[n], size[n], setCnt = n;<br><span class="hljs-built_in">iota</span>(fa, fa + n, <span class="hljs-number">0</span>);<br><span class="hljs-built_in">fill</span>(size, size + n, <span class="hljs-number">1</span>);<br>function&lt;<span class="hljs-built_in"><span class="hljs-keyword">int</span></span>(<span class="hljs-keyword">int</span>)&gt; find = [&amp;](<span class="hljs-keyword">int</span> x)&#123; <br>    <span class="hljs-keyword">return</span> fa[x] == x ? x : fa[x] = <span class="hljs-built_in">find</span>(fa[x]); <span class="hljs-comment">// 路径压缩</span><br>&#125;;<br><span class="hljs-keyword">auto</span> merge = [&amp;](<span class="hljs-keyword">int</span> from, <span class="hljs-keyword">int</span> to)&#123;<br>    from = <span class="hljs-built_in">find</span>(from);<br>    to = <span class="hljs-built_in">find</span>(to);<br>    <span class="hljs-keyword">if</span> (from != to) &#123;<br>        fa[from] = to;<br>        size[to] += size[from]; <span class="hljs-comment">// size 只有在 fa[x] == x 处才有意义</span><br>        setCnt--;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><p>并查集完整类模板：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">UnionFind</span> &#123;</span><br><span class="hljs-keyword">public</span>:<br>    vector&lt;<span class="hljs-keyword">int</span>&gt; parent;<br>    vector&lt;<span class="hljs-keyword">int</span>&gt; size;<br>    <span class="hljs-keyword">int</span> n;<br>    <span class="hljs-keyword">int</span> setCount;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-built_in">UnionFind</span>(<span class="hljs-keyword">int</span> _n): <span class="hljs-built_in">n</span>(_n), <span class="hljs-built_in">setCount</span>(_n), <span class="hljs-built_in">parent</span>(_n), <span class="hljs-built_in">size</span>(_n, <span class="hljs-number">1</span>) &#123;<br>        <span class="hljs-built_in">iota</span>(parent.<span class="hljs-built_in">begin</span>(), parent.<span class="hljs-built_in">end</span>(), <span class="hljs-number">0</span>);<br>    &#125;<br>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">find</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x)</span> </span>&#123;<br>        <span class="hljs-keyword">return</span> parent[x] == x ? x : parent[x] = <span class="hljs-built_in">find</span>(parent[x]);<br>    &#125;<br>    <span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">merge</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x, <span class="hljs-keyword">int</span> y)</span> </span>&#123;<br>        x = <span class="hljs-built_in">find</span>(x);<br>        y = <span class="hljs-built_in">find</span>(y);<br>        <span class="hljs-keyword">if</span> (x == y) <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>        <span class="hljs-keyword">if</span> (size[x] &lt; size[y]) <span class="hljs-built_in">swap</span>(x, y);<br>        parent[y] = x;<br>        size[x] += size[y];<br>        --setCount;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>    &#125;<br>    <span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">connected</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x, <span class="hljs-keyword">int</span> y)</span> </span>&#123;<br>        x = <span class="hljs-built_in">find</span>(x);<br>        y = <span class="hljs-built_in">find</span>(y);<br>        <span class="hljs-keyword">return</span> x == y;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><p>相关题目：</p><ul><li>「力扣」第 547 题：<ahref="https://leetcode-cn.com/problems/number-of-provinces">省份数量</a>（中等）；</li><li>「力扣」第 684 题：<ahref="https://leetcode-cn.com/problems/redundant-connection">冗余连接</a>（中等）；</li><li>「力扣」第 1319 题：<ahref="https://leetcode-cn.com/problems/number-of-operations-to-make-network-connected">连通网络的操作次数</a>（中等）；</li><li>「力扣」第 1631 题：<ahref="https://leetcode-cn.com/problems/path-with-minimum-effort">最小体力消耗路径</a>（中等）；</li><li>「力扣」第 959 题：<ahref="https://leetcode-cn.com/problems/regions-cut-by-slashes">由斜杠划分区域</a>（中等）；</li><li>「力扣」第 1202 题：<ahref="https://leetcode-cn.com/problems/smallest-string-with-swaps">交换字符串中的元素</a>（中等）；</li><li>「力扣」第 947 题：<ahref="https://leetcode-cn.com/problems/most-stones-removed-with-same-row-or-column">移除最多的同行或同列石头</a>（中等）；</li><li>「力扣」第 721 题：<ahref="https://leetcode-cn.com/problems/accounts-merge">账户合并</a>（中等）；</li><li>「力扣」第 803 题：<ahref="https://leetcode-cn.com/problems/bricks-falling-when-hit">打砖块</a>（困难）；</li><li>「力扣」第 1579 题：<ahref="https://leetcode-cn.com/problems/remove-max-number-of-edges-to-keep-graph-fully-traversable">保证图可完全遍历</a>（困难）;</li><li>「力扣」第 778 题：<ahref="https://leetcode-cn.com/problems/swim-in-rising-water">水位上升的泳池中游泳</a>（困难）。</li></ul><h3 id="寻找图中是否存在路径-e">1971. 寻找图中是否存在路径 (E)</h3><p><ahref="https://leetcode.cn/problems/find-if-path-exists-in-graph/">题目描述</a>：给定一张具有<span class="math inline">\(n\)</span>个顶点的双向图，判断是否存在从顶点 A 到顶点 B 的有效路径。</p><p><strong>方法1</strong>：BFS，建图后，将顶点 A先放入队列，每次将其邻居入队，<strong>直到队列为空</strong>或访问到顶点B 时结束。时空复杂度均为 <spanclass="math inline">\(O(n+m)\)</span>。</p><p><strong>方法2</strong>：DFS，建图后，从顶点 A 开始递归搜索，用 <spanclass="math inline">\(vis\)</span>记忆每个访问过的顶点<strong>防止陷入循环</strong>，遍历完所有邻居后如果还没访问到B，则返回 false。时空复杂度均为 <spanclass="math inline">\(O(n+m)\)</span>。</p><p><strong>方法3</strong>：并查集，将图中<strong>每个连通分量视为一个集合</strong>，用并查集维护，判断A 和 B 是否属于一个集合即可。空间复杂度 <spanclass="math inline">\(O(n)\)</span>，时间复杂度 <spanclass="math inline">\(O(n+m)\)</span>。</p><h3 id="省份数量-m">547. 省份数量 (M)</h3><p><ahref="https://leetcode.cn/problems/number-of-provinces/">题目描述</a>：有<span class="math inline">\(n\)</span>个城市，其中一些彼此相连，用一个邻接矩阵存储。省份是一组<strong>直接或间接相连</strong>的城市，求省份数量。</p><p><strong>方法1</strong>：DFS，使用 <spanclass="math inline">\(vis\)</span>数组记录访问过的城市，以每个未访问的城市为起点开始搜索，并标记访问。搜索起点的个数就是省份数量。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：并查集，维护集合数 <spanclass="math inline">\(cntSet\)</span>，合并每个相连的城市，最后的 <spanclass="math inline">\(cntSet\)</span> 就是答案。时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="冗余连接-m">684. 冗余连接 (M)</h3><p><ahref="https://leetcode.cn/problems/redundant-connection/">题目描述</a>：给定往一棵<span class="math inline">\(n\)</span>个节点的树中<strong>添加一条边后的图</strong>，从中找出<strong>一条可删去的边</strong>，删除后可使得剩余部分<strong>恢复</strong>成树，如果有多个答案，返回数组<code>edges</code> 中<strong>最后出现的边</strong>。</p><p><strong>方法1</strong>：并查集，<strong>最后出现的边就是第一个导致环的边</strong>，起初每个点属于不同的连通分量，遍历每条边将节点相连。如果某一条边的两端顶点<strong>已经属于</strong>同一个集合，则说明它就是冗余边。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><h3 id="除法求值-h">399. 除法求值 (H)</h3><p><ahref="https://leetcode.cn/problems/evaluate-division/">题目描述</a>：给定一个<strong>字符串分式</strong>数组和分式的<strong>浮点数值</strong>，再给出一组查询分式数组，要求<strong>根据已知条件计算出值</strong>，如果没有答案则返回<code>-1.0</code>。</p><blockquote><p>变量之间的倍数关系具有传递性，因此可以看作<strong>图论</strong>问题，分子和分母各为图中的节点，分式的值作为两点之间的权重。要查询到分式首先要判断分子与分母<strong>是否有联系</strong>，有联系才可能计算出结果。</p><p>由于分子、分母本身是字符串，如果要建图，就需要用哈希表将每个出现过的字符串<strong>映射成数字索引</strong>，用连续的数字表示节点编号。</p></blockquote><p><strong>方法1</strong>：BFS，建图后，对于每个查询，从起点出发，通过BFS的方式，<strong>不断更新起点与当前点之间的路径长度</strong>，直到搜索到终点为止。</p><p><strong>方法2</strong>：Floyd算法，对于查询数量很多的情形，如果为每次查询都独立搜索一次，则效率会变低。可以使用Floyd 算法，<strong>预先计算出任意两点之间的距离</strong>。</p><p><strong>方法3</strong>：<strong>带权</strong>并查集，不需要建图，用<strong>哈希表</strong>维护并查集。只需将出现过的分子、分母放到同一个集合，即可<strong>快速判断</strong>两个字符串是否有联系。并查集同时维护每个字符串<strong>到其父节点的倍数</strong>，路径压缩、合并时也要更新倍数。每次查询时只需判断是否在同一个集合，再将其通分成<strong>同一个祖先的倍数相除</strong>即可。</p>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #02 数位&amp;二进制</title>
    <link href="/LeetCode-Bitwise.html"/>
    <url>/LeetCode-Bitwise.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「数位&amp;二进制」类型题中的：位运算技巧、与运算应用、异或应用。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><h2 id="位运算技巧">位运算技巧</h2><p>常用技巧如下：</p><ul><li>末一置零：<code>n &amp; (n-1)</code>，<code>n &amp;= n - 1;</code></li><li>末零置一：<code>n | (n+1)</code>，<code>n |= n + 1;</code></li><li>LowBit 运算：<code>int t = n &amp; (-n);</code></li><li>HighBit运算：只能按位枚举，<code>int t = 1&lt;&lt;31; while(!(n &amp; t)) t&gt;&gt;=1;</code></li><li>1 的个数：<code>__builtin_popcount(n)</code></li></ul><h3 id="的幂-e">231. 2 的幂 (E)</h3><p><ahref="https://leetcode.cn/problems/power-of-two/">题目描述</a>：给定一个int 值（可为负），判断该整数是否是 2 的幂次。</p><p><strong>方法1</strong>：位运算，向右移位，当末位第一次出现 1的时候，如果整体为 <span class="math inline">\(1\)</span> 则true，复杂度为 <span class="math inline">\(O(k)\)</span>。</p><p><strong>方法2</strong>：<strong>末一置零</strong>，<code>n &amp; (n-1)</code>可以将二进制的最低位 1 <strong>移除</strong>，判断<code>n&gt;0 &amp;&amp; (n&amp;(n-1))==0</code> 即可。</p><p><strong>方法3</strong>：<strong>Lowbit运算</strong>，<code>n &amp; (-n)</code>可以<strong>仅保留</strong>二进制的最低位 1，判断<code>n&gt;0 &amp;&amp; (n&amp;(-n))==n</code> 即可。</p><p><strong>坑点</strong>：等价运算符 <code>==</code>的优先级高于位运算符，<strong>位运算必须加括号</strong>！</p><h3 id="位1的个数-e">191. 位1的个数 (E)</h3><p><ahref="https://leetcode.cn/problems/number-of-1-bits/">题目描述</a>：给出一个32 位二进制整数，返回其<strong>二进制表达式</strong>中 1的个数（也称<strong>汉明权重</strong>）。</p><p><strong>方法1</strong>：位运算，向右以为，每次末位出现 1的时候，答案增加，复杂度为 <spanclass="math inline">\(O(k)\)</span>。</p><p><strong>方法2</strong>：末一置零，多次进行末位清零直到整数归零，操作次数就是答案，复杂度为<spanclass="math inline">\(O(k)\)</span>，但总次数会少于方法一。该方法也可以通过<strong>递归</strong>实现，即：<span class="math display">\[f(n)=f(n \;\&amp;\;(n-1)) +1\]</span> <strong>方法3</strong>：<strong>二分法</strong>，C++ 中内置<code>__builtin_popcount(n)</code> 实现，依次每 2、4、8、16 位取 1累计，复杂度 <span class="math inline">\(O(\log k)\)</span>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">unsigned</span> __builtin_popcount(<span class="hljs-keyword">unsigned</span> u) &#123;<br>    u = (u &amp; <span class="hljs-number">0x55555555</span>) + ((u &gt;&gt; <span class="hljs-number">1</span>) &amp; <span class="hljs-number">0x55555555</span>);<br>    u = (u &amp; <span class="hljs-number">0x33333333</span>) + ((u &gt;&gt; <span class="hljs-number">2</span>) &amp; <span class="hljs-number">0x33333333</span>);<br>    u = (u &amp; <span class="hljs-number">0x0F0F0F0F</span>) + ((u &gt;&gt; <span class="hljs-number">4</span>) &amp; <span class="hljs-number">0x0F0F0F0F</span>);<br>    u = (u &amp; <span class="hljs-number">0x00FF00FF</span>) + ((u &gt;&gt; <span class="hljs-number">8</span>) &amp; <span class="hljs-number">0x00FF00FF</span>);<br>    u = (u &amp; <span class="hljs-number">0x0000FFFF</span>) + ((u &gt;&gt; <span class="hljs-number">16</span>) &amp; <span class="hljs-number">0x0000FFFF</span>);<br>    <span class="hljs-keyword">return</span> u;<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>方法4</strong>：查表法，用长度 256 的数组存放<strong>每 8位数</strong>对应的汉明权重，每次取末尾 8 位查表，移动 4 次累加，复杂度<spanclass="math inline">\(O(1)\)</span>。适合<strong>多次复用函数</strong>时用空间换时间。</p><h3 id="比特位计数-e">338. 比特位计数 (E)</h3><p><ahref="https://leetcode.cn/problems/counting-bits/">题目描述</a>：给定整数<code>n</code>，计算 0 到 n 的所有整数<strong>二进制表达式</strong>中 1的个数，返回一个长度为 <code>n + 1</code> 的数组。</p><p><strong>方法1</strong>：二分法，对每个数调用<code>__builtin_popcount(n)</code>，复杂度 <spanclass="math inline">\(O(n\log k)\)</span>。</p><p><strong>方法2</strong>：查表法，用长度 256 的数组存放<strong>每 8位数</strong>对应的汉明权重，每次进行查表，复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：DP，线性递推，遇到一个新数时可以转移到<strong>任何一个比它小的数</strong>，该数可以保证已经计算过，复杂度<span class="math inline">\(O(n)\)</span>。</p><blockquote><ol type="1"><li>转移到「末位清零」数字：<span class="math inline">\(dp[i]=dp[i\;\&amp;\;(i-1)]+1\)</span></li><li>转移到「首位清零」数字，用一个变量 <spanclass="math inline">\(HighBit\)</span> 记录 <spanclass="math inline">\(100\cdots00\)</span>，即当<code>i&amp;(i-1)==0</code>时更新依次，后面的一连串数字都可以转移：<spanclass="math inline">\(dp[i]=dp[i - HighBit]+1\)</span></li><li>转移到「右移一位」数字，将 <span class="math inline">\(i\)</span>右移一位，得到的数必定比原数小，根据最低位是否为 1 可以进行转移：<spanclass="math inline">\(dp[i]=dp[i &gt;&gt;1]+(i\;\&amp;\;1)\)</span></li></ol></blockquote><h3 id="最小-xor-m">2429. 最小 XOR (M)</h3><p><ahref="https://leetcode.cn/problems/minimize-xor/">题目描述</a>：给定两个正整数<code>num1</code> 和 <code>num2</code>，找出满足下述条件的整数<code>x</code>：<code>x</code> 的<strong>置位数</strong>和<code>num2</code> 相同且 <code>x^num1</code> 的值最小。</p><p><strong>方法</strong>：贪心 + 位运算技巧，先用<code>__builtin_popcount(n)</code>统计出<strong>两个置位数</strong>，分类讨论，贪心地进行<strong>末一置零和末零置一</strong>，复杂度<span class="math inline">\(O(k)\)</span>。</p><blockquote><ul><li><code>cnt1 == cnt2</code>，自身异或结果为零，返回 <code>num1</code>即可。</li><li><code>cnt1 &gt; cnt2</code>，尽量将 <code>num1</code> 的高位 1异或掉，因此答案要保留高位 1，将 <code>num1</code> 末一置零返回。使用<code>num1 &amp;= num1 - 1</code>。</li><li><code>cnt1 &lt; cnt2</code>，将 <code>num1</code>异或完还有剩余，因此答案要把低位 0 异或掉，将 <code>num1</code>末零置一返回。使用 <code>num1 |= num1 + 1</code>。</li></ul></blockquote><h3 id="汉明距离总和-m">477. 汉明距离总和 (M)</h3><p><ahref="https://leetcode.cn/problems/total-hamming-distance/">题目描述</a>：给定一个整数数组，要求计算数组中<strong>任意两两数字</strong>的汉明距离之和。</p><p><strong>方法1</strong>：暴力，枚举每一对数，用<code>__bulitin_popcount(x^y)</code> 获得汉明距离，时间复杂度 <spanclass="math inline">\(O(n^2\log k)\)</span>。</p><p><strong>方法2</strong>：逐位统计，<strong>不同二进制位的取值不会相互影响</strong>，同一二进制位上，如果某个元素取<span class="math inline">\(1\)</span>，则只需要统计其他元素有多少个<span class="math inline">\(0\)</span> 即可。换言之，假设该位上有 <spanclass="math inline">\(c\)</span> 个 <spanclass="math inline">\(1\)</span>、<spanclass="math inline">\(n-c\)</span> 个 <spanclass="math inline">\(0\)</span>，则距离之和为 <spanclass="math inline">\(n\cdot (n-c)\)</span>。时间复杂度 <spanclass="math inline">\(O(n k)\)</span>。</p><h2 id="与运算应用">与运算应用</h2><h3 id="按位与最大的最长子数组-m">2419. 按位与最大的最长子数组 (M)</h3><p><ahref="https://leetcode.cn/problems/longest-subarray-with-maximum-bitwise-and/">题目描述</a>：给定一个数组，找出其中一个<strong>最长</strong>连续子数组，使其满足子数组中元素<strong>按位与的值最大</strong>，返回长度。</p><p><strong>方法</strong>：直接遍历，题目可以转化为「数组中<strong>最大值连续出现</strong>的最大长度」，第一次遍历找出最大值，第二次遍历计算长度，复杂度<span class="math inline">\(O(n)\)</span>。</p><p><strong>坑点</strong>：表面上是「最长连续子 XX」题，但是由于<code>a AND b &lt;= min(a, b)</code>，可以简化题目。</p><h3 id="cf721.a-最大的-k-m">CF#721.A 最大的 k (M)</h3><p><ahref="https://codeforces.com/contest/1527/problem/A">题目描述</a>：给定<span class="math inline">\(n\)</span>，找到最大的 <spanclass="math inline">\(k\)</span> 使得 <spanclass="math inline">\(n\&amp;(n-1)\&amp;\cdots\&amp;k=0\)</span>。</p><p><strong>方法</strong>：脑筋急转弯，找到 <spanclass="math inline">\(n\)</span> 的最高位 1（设为 <spanclass="math inline">\(t\)</span>），显然要想将该位消去，至少得算到 <spanclass="math inline">\(2^t-1\)</span>。而 <spanclass="math inline">\(2^t\&amp;(2^t-1)=0\)</span>，因此最大的 <spanclass="math inline">\(k\)</span> 就是 <spanclass="math inline">\(2^t-1\)</span>。时间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h2 id="异或应用">异或应用</h2><h3 id="只出现一次的数字-e">136. 只出现一次的数字 (E)</h3><p><ahref="https://leetcode.cn/problems/single-number/">题目描述</a>：一个<strong>非空</strong>整数数组，除某个数字只出现了一次外，其余每个数字<strong>均出现两次</strong>。找出那个只出现一次的数字。</p><p><strong>方法1</strong>：哈希表，记录出现过的数字和次数，最后再遍历哈希表，时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：异或，对所有数字进行异或，由于异或具有交换律，两两消除最后剩下的就是答案。空间复杂度<span class="math inline">\(O(1)\)</span>。</p><h3 id="只出现一次的数字-ii-m">137. 只出现一次的数字 II (M)</h3><p><ahref="https://leetcode.cn/problems/single-number-ii/">题目描述</a>：一个<strong>非空</strong>整数数组，除某个数字只出现了一次外，其余每个数字<strong>均出现三次</strong>。找出那个只出现一次的数字。</p><p><strong>方法1</strong>：哈希表，记录出现过的数字和次数，最后再遍历哈希表，时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：<strong>分别考虑每个二进制位</strong>，对第<span class="math inline">\(i\)</span>个二进制位进行累加，对于非答案的元素，必然是 <spanclass="math inline">\(3\)</span> 个 <spanclass="math inline">\(0\)</span> 或 <spanclass="math inline">\(3\)</span> 个 <spanclass="math inline">\(1\)</span>，则最后累加的结果必然是 <spanclass="math inline">\(3\)</span> 的倍数，因此对 <spanclass="math inline">\(3\)</span> 取余就能得到答案。时间复杂度 <spanclass="math inline">\(O(32n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>方法3</strong>：<strong>利用逻辑电路并行化处理每个二进制位</strong>，每个二进制位有<span class="math inline">\(\{0,1,2\}\)</span> 三种可能，用两个寄存器<span class="math inline">\(a,b\)</span> 来存储 <spanclass="math inline">\(\{00,01,10\}\)</span>，画出<strong>真值表</strong>后可以推出表达式。最后<span class="math inline">\(b\)</span> 就是答案。时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><blockquote><p>真值表包含 <span class="math inline">\(a,b,x\)</span> 项，<spanclass="math inline">\(x\)</span> 为下一个遇到的数字，结果为 <spanclass="math inline">\(a_{next}\)</span> 和 <spanclass="math inline">\(b_{next}\)</span>，分别算出表达式： <spanclass="math display">\[\begin{cases}  a_{next}=\bar{a}bx+a\bar{b}\bar{x}\\  b_{next}=\bar{a}\bar{b}x+\bar{a}b\bar{x}=\bar{a}\left( b\oplus x\right)\\\end{cases}\]</span></p></blockquote><p><strong>方法4</strong>：电路化简，观察方法 3 表达式，发现 <spanclass="math inline">\(a\)</span> 的转移比 <spanclass="math inline">\(b\)</span> 复杂，考虑先算出 <spanclass="math inline">\(b_{next}\)</span>，再用其来计算 <spanclass="math inline">\(a_{next}\)</span>，更为简便。时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>坑点</strong>：方法 3 中的表达式右边都是上一刻的 <spanclass="math inline">\(a\)</span> 和 <spanclass="math inline">\(b\)</span>，必须用<strong>临时变量同时更新</strong>，也可以用pair 和 tie 来操作。</p><p><strong>扩展</strong>：此题中的「三次」可以修改为<strong>任意次数</strong>，只要对每个二进制位单独考虑即可。<strong>偶数次数</strong>也可以直接异或。</p><h3 id="只出现一次的数字-iii-m">260. 只出现一次的数字 III (M)</h3><p><ahref="https://leetcode.cn/problems/single-number-iii/">题目描述</a>：一个<strong>非空</strong>整数数组，恰有<strong>两个数字</strong>只出现了一次，其余每个数字<strong>均出现两次</strong>。找出那两个数字。</p><p><strong>方法1</strong>：哈希表，记录出现过的数字和次数，最后再遍历哈希表，时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：异或，对所有数字进行异或，最后的异或和 <spanclass="math inline">\(x=x_1\oplus x_2\)</span>，显然该结果不会为 <spanclass="math inline">\(0\)</span>，否则两数应该相等。用<code>x＆－ｘ</code> 得到最低位 <spanclass="math inline">\(1\)</span>，显然 <spanclass="math inline">\(x_1\)</span> 和 <spanclass="math inline">\(x_2\)</span>中<strong>该位的值不会相等</strong>。于是可以<strong>将所有数字根据该位的取值分为两类，各自进行异或和</strong>，得到的两个结果分别就是<span class="math inline">\(x_1\)</span> 和 <spanclass="math inline">\(x_2\)</span>。</p><p><strong>坑点</strong>：取 lowbit 时，要注意 INT_MIN 为-2147483648，<strong>取反后将溢出</strong>，而其在二进制中表示为 <spanclass="math inline">\(100\cdots00\)</span>，因此本身就可以作为 lowbit使用。</p><h3 id="寻找重复数-m">287. 寻找重复数 (M)</h3><p><ahref="https://leetcode.cn/problems/find-the-duplicate-number/">题目描述</a>：给定一个包含<span class="math inline">\(n + 1\)</span> 个整数的数组，其数字都在<span class="math inline">\([1, n]\)</span>范围内，假设数组中<strong>有且仅有一个重复的整数</strong>，找出数组中重复的数字，要求空间复杂度<span class="math inline">\(O(1)\)</span>。</p><p><strong>方法1</strong>：二分答案，定义 <spanclass="math inline">\(cnt[i]\)</span>表示数组中<strong>小于等于</strong> <spanclass="math inline">\(i\)</span> 的数的个数，假设重复数是 <spanclass="math inline">\(x\)</span>，则 <spanclass="math inline">\([1,x-1]\)</span> 里所有数都满足 <spanclass="math inline">\(cnt[i] \leq i\)</span>；而 <spanclass="math inline">\([x,n]\)</span> 里的所有数满足 <spanclass="math inline">\(cnt[i]&gt;i\)</span>。因此可以二分枚举 <spanclass="math inline">\(x\)</span> 直到找到。时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法2</strong>：二进制，预处理 <spanclass="math inline">\([1,n]\)</span> 这 <spanclass="math inline">\(n\)</span> 个数「<strong>每一位</strong>为 <spanclass="math inline">\(1\)</span>的<strong>个数之和</strong>」，如果当前数组的「<strong>每一位</strong>为<span class="math inline">\(1\)</span>的<strong>个数之和</strong>」大于期望值，则说明<strong>重复数的这一位</strong>为<span class="math inline">\(1\)</span>。遍历每一位即可，时间复杂度 <spanclass="math inline">\(O(n \log n)\)</span>。</p><p><strong>方法3</strong>：抽象成环形链表寻找入口，快慢指针，先用 <spanclass="math inline">\(fast\)</span> 和 <spanclass="math inline">\(slow\)</span> 相遇，再用 <spanclass="math inline">\(pos\)</span> 和 <spanclass="math inline">\(slow\)</span> 相遇，时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法4</strong>：原地置换 + 哈希，跟 <ahref="https://leetcode.cn/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/">剑指Offer 03. 数组中重复的数字</a> 一模一样，比前面的方法都简单。</p><h3 id="金典-17.19.-消失的两个数字-h">金典 17.19. 消失的两个数字(H)</h3><p><ahref="https://leetcode.cn/problems/missing-two-lcci/">题目描述</a>：给定一个数组，包含从1 到 N所有的整数，但其中<strong>缺了两个</strong>数字，找出这两个数字。</p><p><strong>方法1</strong>：放入 1 到 N 所有的整数，求出异或和 <spanclass="math inline">\(x=x_1\oplusx_2\)</span>，可以转化为找出<strong>只出现一次的两个数字</strong>。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>方法2</strong>：累加求出 <span class="math inline">\(x=x_1 +x_2\)</span>，设 <span class="math inline">\(x_1 &lt; x_2\)</span>，则有<span class="math inline">\(x/2\geqslant x_1\)</span> ，将 <spanclass="math inline">\(1\)</span> 到 <spanclass="math inline">\(x/2\)</span>的<strong>所有数累加看差值</strong>就可以找出 <spanclass="math inline">\(x_1\)</span>，从而求出两个数字。时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>方法3</strong>：累加求出 <span class="math inline">\(x_1 +x_2\)</span> 和 <spanclass="math inline">\(x_1^{2}+x_2^{2}\)</span>，从而求出 <spanclass="math inline">\(x_1 - x_2\)</span>，联立即可。时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>坑点</strong>：方法 3计算平方和时<strong>可能溢出</strong>，需要用到计算技巧，包括<code>n * (n+1) / 2 * (2*n+1) / 3</code>。</p><h3 id="最长优雅子数组-m">2401. 最长优雅子数组 (M)</h3><p><ahref="https://leetcode.cn/problems/longest-nice-subarray/">题目描述</a>：<strong>正</strong>整数组成的数组<code>nums</code> 中找出最长连续子数组，使其满足子数组中所有元素<code>&amp;</code> 结果等于 0。</p><p><strong>方法</strong>：滑动窗口 + 位运算，窗口内所有元素取<code>|</code> 使二进制 1 位合并，右侧元素只需和整体进行<code>&amp;</code>就能判断是否加入窗口，如果不能加入，则左侧元素需要弹出，使用<code>^</code> 运算<strong>去除</strong>二进制 1 位。</p>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力扣刷题笔记 #01 数组</title>
    <link href="/LeetCode-Array.html"/>
    <url>/LeetCode-Array.html</url>
    
    <content type="html"><![CDATA[<p>本文包含「数组」类型题中的：哈希表、前缀和/差分、离散化、排序/选择、双指针等技巧。持续更新中。</p><blockquote><p><a href="">题目描述</a>：</p><p><strong>方法1</strong>：</p><p><strong>方法2</strong>：</p><p><strong>方法3</strong>：</p><p><strong>坑点</strong>：</p></blockquote><h2 id="哈希表">哈希表</h2><h3 id="两数之和-e">1. 两数之和 (E+)</h3><p><ahref="https://leetcode.cn/problems/two-sum/">题目描述</a>：给定一个数组，在数组中找出两个整数，使两数之和为指定值，并返回<strong>两数下标</strong>。</p><p><strong>方法1</strong>：暴力枚举，两层循环顺序遍历，复杂度 <spanclass="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：哈希表，注意到内层循环本质是「<strong>快速查找目标值的索引</strong>」，用哈希表可<span class="math inline">\(O(1)\)</span> 查找，总复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>坑点</strong>：当数组中有两个 key相同时，如果先建哈希表，则会发生覆盖，然而由于每种输入只会对应一个答案，因此此时这两个key就必定是结果。解决方法是<strong>边建表边遍历，每次查找的都是遍历过的值</strong>。</p><h3 id="最长连续序列-m">128. 最长连续序列 (M)</h3><p><ahref="https://leetcode.cn/problems/longest-consecutive-sequence/">题目描述</a>：给定一个无序数组<code>nums</code>，找出其中<strong>可以组成的最长连续序列</strong>（不要求序列元素在原数组中连续）的长度。</p><p><strong>方法1</strong>：排序，先进行快速排序，再用一个指针扫描数组即可。时间复杂度<span class="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法2</strong>：哈希集合，先将原数组存入哈希集合，既能<strong>去重</strong>又能<strong>快速定位元素</strong>。遍历哈希集合，对每个<span class="math inline">\(x\)</span> 寻找 <spanclass="math inline">\(x+1\)</span>，直到找到最长序列。为了<strong>防止同一个序列重复判断</strong>，每次先检查<span class="math inline">\(x-1\)</span>是否存在，如果<strong>存在则跳过</strong>。时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：哈希表 +并查集，<strong>维护每个集合的大小</strong>，对于元素 <spanclass="math inline">\(x\)</span> 合并 <span class="math inline">\(x+1\)</span>，并更新最大集合，时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="多数元素-e">169. 多数元素 (E)</h3><p><ahref="https://leetcode.cn/problems/majority-element/">题目描述</a>：给定一个大小为<span class="math inline">\(n\)</span>的数组，返回其中的<strong>多数元素</strong>，即数组中出现次数<strong>大于</strong>$n/2 $ 的元素。</p><p><strong>方法1</strong>：哈希表，直接统计每个元素出现的次数，<strong>同时维护</strong>最大值，时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：排序，由于众数次数超过了一半，下标为 $n/2 $的元素<strong>一定是众数</strong>。时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法3</strong>：随机选取一个数，遍历检查，如果不是再重新选。期望次数为<span class="math inline">\(2\)</span>，时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>方法4</strong>：<strong>半数投票法</strong>，顺序扫描，假设有一个擂台，<spanclass="math inline">\(cnt\)</span>为擂台上的人数。当擂台没人时直接上台；如果有人且是同伙则上台，<spanclass="math inline">\(cnt+1\)</span>；如果不是同伙则打败一个，<spanclass="math inline">\(cnt-1\)</span>。由于<strong>众数超过了一半，最终一定能打败非众数</strong>，最后剩余的就是多数元素。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="表现良好的最长时间段-m">1124. 表现良好的最长时间段 (M)</h3><p><ahref="https://leetcode.cn/problems/longest-well-performing-interval/">题目描述</a>：给定一个仅包含<span class="math inline">\(\pm 1\)</span>的数组，求解<strong>区间分数和</strong>大于 <spanclass="math inline">\(0\)</span> 的<strong>最长区间长度</strong>。</p><p><strong>方法1</strong>：前缀和，预处理得到前缀和之后，暴力枚举区间左右端点，时间复杂度<span class="math inline">\(O(n^2)\)</span>，超时。</p><p><strong>方法2</strong>：前缀和<strong>连续性</strong> +哈希表，<strong>只枚举区间右端点</strong>，此时我们希望找到<strong>最小</strong>的<span class="math inline">\(l&lt;r\)</span> 满足 <spanclass="math inline">\(pre[l]&lt;pre[r]\)</span>，使用哈希表记录<strong>每一个前缀和第一次出现</strong>的位置，可以优化寻找过程，时间复杂度<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>由于 <span class="math inline">\(pre[0]=0\)</span>，所以要根据 <spanclass="math inline">\(pre[r]\)</span> 是否大于零来讨论：</p><ul><li>如果 <spanclass="math inline">\(pre[r]&gt;0\)</span>，则左端点最小就是 <spanclass="math inline">\(l=0\)</span>，此时的最长长度为 <spanclass="math inline">\(r+1\)</span>。</li><li>如果 <span class="math inline">\(pre[r] \leq0\)</span>，则要在其左边找到满足 <span class="math inline">\(pre[l] =pre[r]-1\)</span> 的位置。为什么是 <spanclass="math inline">\(-1\)</span> 而<strong>不是更小的数</strong>？因为<span class="math inline">\(pre[0]=0\)</span>，而前缀数组每次只能 <spanclass="math inline">\(\pm 1\)</span>，所以 <spanclass="math inline">\(pre[r]-1\)</span> 一定比 <spanclass="math inline">\(pre[r]-2\)</span> <strong>更靠左</strong>！</li></ul></blockquote><h2 id="前缀和差分">前缀和/差分</h2><p>前缀和需要 <span class="math inline">\(O(n)\)</span>构建，此后可以多次 <span class="math inline">\(O(1)\)</span>查询<strong>区间和</strong>，但不支持单点或区间的快速修改，每次修改需要<span class="math inline">\(O(n)\)</span>。</p><ul><li>这类题目通常要求「<strong>访问、找到、计数</strong>」一个<strong>连续子数组</strong>，并且统计量具有<strong>前缀性质</strong>。</li><li>此外，前缀和还有其他变式：前缀和的模、前缀积、前缀出现次数等。</li><li>约定：<code>pre[i+1]=pre[i]+nums[i]</code> 表示 <code>nums[0]</code>到 <code>nums[i]</code> 之和，<code>pre[0]</code> 初始化为 0，查询的时候<span class="math inline">\([i,j)\)</span> 用<code>pre[j]-pre[i]</code>。</li></ul><p>差分数组也需要 <span class="math inline">\(O(n)\)</span>构建，此后可以多次 <span class="math inline">\(O(1)\)</span>加减<strong>更新区间值</strong>，但最后需要进行一次前缀和才能恢复原始数组。</p><ul><li>这类题目通常要求「<strong>改变、划分</strong>」多个<strong>连续子数组</strong>，并且每次对多个连续元素执行操作。</li><li>如果要求除了加减外的其他操作，则应该考虑单次操作 <spanclass="math inline">\(O(\log n)\)</span> 的线段树维护。</li><li>约定：<code>diff[i]</code> 表示<code>nums[i]-nums[i-1]</code>，<code>diff[0]</code> 表示<code>nums[0]</code>，还原的时候<code>nums[i]=nums[i-1]+diff[i]</code>。</li></ul><p>有时候需要二维数组的前缀和、差分，用于<strong>求二维数组中给定矩阵区间和</strong>，或者<strong>给矩阵区间的每个值都加上常数</strong>，下面给出常用代码。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 原始数组 nums 转前缀和数组 pre，pre[i][j] 表示 (i,j) 以外的左上角的矩阵之和</span><br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">pre</span>(n + <span class="hljs-number">1</span>, vector&lt;<span class="hljs-keyword">int</span>&gt;(m + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>));<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt;= n; i++)<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = <span class="hljs-number">1</span>; j &lt;= m; j++)<br>        pre[i][j] = nums[i<span class="hljs-number">-1</span>][j<span class="hljs-number">-1</span>] + pre[i<span class="hljs-number">-1</span>][j] + pre[i][j<span class="hljs-number">-1</span>] - pre[i<span class="hljs-number">-1</span>][j<span class="hljs-number">-1</span>];<br><span class="hljs-comment">// 查询左上角为 (x1,y1)，右下角为 (x2,y2) 的矩阵和</span><br>res = pre[x2+<span class="hljs-number">1</span>][y2+<span class="hljs-number">1</span>] - pre[x1][y2+<span class="hljs-number">1</span>] - pre[x2+<span class="hljs-number">1</span>][y1] + pre[x1][y1];<br><br><span class="hljs-comment">// 原始数组 nums 转差分数组 diff, 没有实际意义</span><br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">diff</span>(n + <span class="hljs-number">1</span>, vector&lt;<span class="hljs-keyword">int</span>&gt;(m + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>));<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt;= n; i++)<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = <span class="hljs-number">1</span>; j &lt;= m; j++)<br>        diff[i][j] = nums[i][j] - nums[i<span class="hljs-number">-1</span>][j] - nums[i][j<span class="hljs-number">-1</span>] + nums[i<span class="hljs-number">-1</span>][j<span class="hljs-number">-1</span>];<br><span class="hljs-comment">// 对左上角为 (x1,y1)，右下角为 (x2,y2) 的矩阵加上常数 c</span><br>diff[x1][y1] += c;<br>diff[x1][y2+<span class="hljs-number">1</span>] -= c;<br>diff[x2+<span class="hljs-number">1</span>][y1] -= c;<br>diff[x2+<span class="hljs-number">1</span>][y2+<span class="hljs-number">1</span>] += c;<br><br><span class="hljs-comment">// diff 恢复为原始数组，此时 diff 可以迭代更新，再去除首行和首列</span><br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt;= n; i++)<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = <span class="hljs-number">1</span>; j &lt;= m; j++)<br>        diff[i][j] += diff[i<span class="hljs-number">-1</span>][j] + diff[i][j<span class="hljs-number">-1</span>] - diff[i<span class="hljs-number">-1</span>][j<span class="hljs-number">-1</span>];<br><span class="hljs-comment">// diff 恢复为原始数组，采用另一种方式，得到 n*n 的数组</span><br>vector&lt;vector&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-built_in">ans</span>(n, vector&lt;<span class="hljs-keyword">int</span>&gt;(n));<br><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++) <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; n; j++) ans[i][j] = diff[i][j];<br><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++) <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">1</span>; j &lt; n; j++) ans[i][j] += ans[i][j - <span class="hljs-number">1</span>];<br><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; n; i++) <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; n; j++) ans[i][j] += ans[i - <span class="hljs-number">1</span>][j];<br></code></pre></td></tr></table></figure><h3 id="将区间分为最少组数-m">2406. 将区间分为最少组数 (M)</h3><p><ahref="https://leetcode.cn/problems/divide-intervals-into-minimum-number-of-groups/">题目描述</a>：给定一些整数区间，将其划分为若干区间<strong>组</strong>，使得每组区间都不相交（<strong>区间端点也不能重叠</strong>）。返回最少需要划分的组数。</p><p><strong>方法1</strong>：贪心 + multiset，先按区间起始值排序，在multiset中记录<strong>已有区间组的末尾值</strong>，每次新区间加入时在集合中找到<strong>可以插入的最大值</strong>（最小化碎片，但实际上不影响），并更新该值为新区间的末尾值（multiset自动排序）。时间复杂度为 <span class="math inline">\(O(n\logn)\)</span>。</p><p><strong>方法2</strong>：贪心 +优先队列，由于排过序，每次新区间加入时可以直接插入<strong>任意一个可插入的区间</strong>，因为无论选哪个，下一个新区间<strong>面临的选择还是那些</strong>。因此选择已有的<strong>最小末尾值</strong>（最容易插入的），维护最小堆，时间复杂度为<span class="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法3</strong>：差分 +前缀和，如果有重叠数字则必须分到不同区间组，<strong>最高的重叠次数就是答案</strong>。从前往后枚举，用<strong>差分数组</strong>记录对应每个区间的计数，最后通过<strong>前缀和</strong>还原数组取最值即可。时间复杂度为<span class="math inline">\(O(N)\)</span>，<spanclass="math inline">\(N\)</span> 为<strong>整数范围</strong>。</p><p><strong>拓展</strong>：本题的另一个问法 <ahref="https://leetcode.cn/problems/meeting-rooms-ii">253. 会议室II</a>，给定 <span class="math inline">\(n\)</span>个会议的起止时间，计算<strong>至少分配多少间</strong>会议室才能避免会议冲突。</p><h3 id="除自身以外数组的乘积-m">238. 除自身以外数组的乘积 (M)</h3><p><ahref="https://leetcode.cn/problems/product-of-array-except-self/">题目描述</a>：给定一个整数数组<code>nums</code>，返回另一个答案数组，要求每个位置为 <code>nums</code>中除 <code>nums[i]</code>外<strong>其余各元素</strong>的乘积。保证不会溢出，<strong>禁止使用除法</strong>。</p><p><strong>方法1</strong>：尝试使用除法，直接计算所有元素的乘积再除以对应位置的值，但在数组中有<strong>零元素</strong>时会出错，需要特判。</p><p><strong>方法2</strong>：前后缀乘积，预处理出两个前后缀数组，每个位置直接算出答案。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：前后缀 + 在线计算，正向遍历时用 <spanclass="math inline">\(L\)</span> 维护下标 <spanclass="math inline">\([0,i]\)</span>的<strong>前缀乘积</strong>并放入答案数组，反向遍历时用 <spanclass="math inline">\(R\)</span> 维护 <spanclass="math inline">\([n-1-i,n-1]\)</span>的<strong>后缀乘积</strong>更新答案数组。空间复杂度优化到 <spanclass="math inline">\(O(1)\)</span>，也可以只用一次遍历完成。</p><hr /><p>以下为「<strong>前缀和 +哈希表计数</strong>」类型题，这类题目通常要求统计一类<strong>连续子数组的个数</strong>，需要<strong>枚举一侧端点</strong>。</p><h3 id="和为-k-的子数组-m">560. 和为 K 的子数组 (M)</h3><p><ahref="https://leetcode.cn/problems/subarray-sum-equals-k/">题目描述</a>：给定整数数组<code>nums</code> 和整数 <code>k</code>，统计并返回该数组中和为<code>k</code> 的<strong>连续子数组</strong>的个数 。</p><p><strong>方法1</strong>：暴力，本题不能用滑动窗口的原因是数组<strong>有正有负</strong>，不能提前移动左指针，因此使用两层循环。外层枚举左端点，内层枚举右端点，区间和<strong>累加</strong>计算，复杂度<span class="math inline">\(O(n^2)\)</span>，超时。</p><p><strong>方法2</strong>：前缀和 + 哈希表，枚举左端点 <spanclass="math inline">\(i\)</span>，则 <spanclass="math inline">\(i\)</span> 到<strong>右端点之和</strong>为 <spanclass="math inline">\(k\)</span>，<strong>左侧之和</strong>为 <spanclass="math inline">\(pre[i]\)</span>，两边相加就是右端点的前缀和，因此需要用<strong>哈希表计数前缀和</strong>为<span class="math inline">\(k+pre[i]\)</span> 的个数，时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>注意，由于数组有正有负，前缀和为 <spanclass="math inline">\(k+pre[i]\)</span> 的数组可能在 <spanclass="math inline">\(i\)</span>的左侧，记得要<strong>扣除</strong>这部分。实际操作中只需要在枚举完一个左端点<span class="math inline">\(i\)</span> ，将其前缀和 <spanclass="math inline">\(pre[i+1]\)</span> 的次数减去 <spanclass="math inline">\(1\)</span>即可，确保枚举过的左侧点不再参与计算。</p></blockquote><p><strong>方法3</strong>：空间优化，<strong>先枚举右端点</strong>，在枚举<span class="math inline">\(pre[i] - k\)</span>的前缀和次数，则枚举的过程可以和前缀和一起处理，且不需要再扣除（因为后面的还没遍历到）。前缀和的空间也可以用一个变量存放，空间优化。</p><h3 id="连续的子数组和-m">523. 连续的子数组和 (M)</h3><p><ahref="https://leetcode.cn/problems/continuous-subarray-sum/">题目描述</a>：给定整数数组<code>nums</code> 和整数 <code>k</code>，判断是否含有总和为<code>k</code> 的倍数、长度至少为 <span class="math inline">\(2\)</span>的<strong>连续子数组</strong>。</p><p><strong>方法1</strong>：暴力，两层循环枚举左端点和右端点，区间和累加，时间复杂度<span class="math inline">\(O(n^2)\)</span>，超时。</p><p><strong>方法2</strong>：前缀和 +哈希表，由<strong>同余定理</strong>，哈希表只要存储 <spanclass="math inline">\(sum\%k\)</span>的结果。而本题只要判断不需要计数，因此存储每个值<strong>第一次出现的下标</strong>。同时枚举右端点<span class="math inline">\(i\)</span>，只需看哈希表中 <spanclass="math inline">\(pre[i]\)</span>的下标<strong>距离是否超过</strong> <spanclass="math inline">\(2\)</span>，复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：前缀和 + 哈希集合，由于本题距离下限固定为<spanclass="math inline">\(2\)</span>，在枚举右端点时，<strong>依次进行</strong>「哈希查询、哈希插入、前缀和更新」，就能避免找到长度<span class="math inline">\(&lt;2\)</span>的连续子数组。更容易实现，复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>坑点</strong>：方法 2 中要初始化 <spanclass="math inline">\(hash[0]=-1\)</span>，即没有元素时的前缀和为 <spanclass="math inline">\(0\)</span>。</p><h3 id="统计优美子数组m">1248. 统计「优美子数组」(M)</h3><p><ahref="https://leetcode.cn/problems/count-number-of-nice-subarrays/">题目描述</a>：给定整数数组<code>nums</code> 和整数<code>k</code>，如果某个<strong>连续子数组</strong>中恰好有<code>k</code> 个奇数数字，则称为「优美子数组」。</p><p><strong>方法1</strong>：暴力，两层循环枚举左端点和右端点，区间和累计，时间复杂度<span class="math inline">\(O(n^2)\)</span>，超时。</p><p><strong>方法2</strong>：乘法原理，建立一个纯奇数的 <spanclass="math inline">\(odd\)</span> 数组，存储奇数 <spanclass="math inline">\(i\)</span> 出现的位置，枚举每个奇数 <spanclass="math inline">\(i\)</span>，其对答案的贡献是其<strong>左右边界的范围乘积</strong>，时间复杂度<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：前缀次数 + 哈希表，哈希表记录前缀次数为 <spanclass="math inline">\(k\)</span> 的个数，当统计到 <spanclass="math inline">\(i\)</span> 时，在哈希表中查找 <spanclass="math inline">\(pre[i]-k\)</span> 的个数即可，复杂度 <spanclass="math inline">\(O(n)\)</span>。注意要初始化 <spanclass="math inline">\(hash[0]=1\)</span>，即一开始有 <spanclass="math inline">\(0\)</span> 个奇数数字。</p><p><strong>坑点</strong>：本题可能会让人联想到滑动窗口，因为奇数数字是线性递增的，实际上也是可以的。但对于同一个右端点会存在多个适合的左端点，所以在<strong>枚举完右端点后还需要枚举左端点</strong>——应使用<strong>定界法</strong>。</p><h3 id="路径总和-iii-h">437. 路径总和 III (H)</h3><p><ahref="https://leetcode.cn/problems/path-sum-iii/">题目描述</a>：给定一个二叉树的根节点和一个目标整数，求二叉树中<strong>节点之和</strong>为目标值的<strong>路径数目</strong>。这里的路径不需要从根节点开始，也不需要在叶子节点结束，但是<strong>路径方向必须是向下的</strong>。</p><p><strong>方法1</strong>：暴力 DFS，DFS<strong>枚举每个节点</strong>作为路径头时的情况，再向下 DFS到叶节点，总共有两层 DFS，时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>，空间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：前缀和 + 哈希表，DFS遍历时记录<strong>根节点到当前节点</strong>的和 <spanclass="math inline">\(cur\)</span>，并用哈希表计数，每次查询 <spanclass="math inline">\(cur-target\)</span>的个数。为了<strong>防止不同分支影响计数</strong>，DFS回溯时需要删去哈希表中的记录。注意初始化<code>hash[0]=1</code>，如果从根节点到当前节点<strong>恰好</strong>等于目标，差值就是<span class="math inline">\(0\)</span>。时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>坑点</strong>：本题的方法 1中，<strong>记忆化</strong>数组并不能优化时间复杂度，因为需要存储的关键字有两个，必须用pair，且只能用 map映射，<strong>常数较大</strong>。而且两个关键字组成的<strong>目标域太过稀疏，重复访问的概率很小</strong>。</p><h3 id="统计中位数为-k-的子数组-h">2488. 统计中位数为 K 的子数组(H)</h3><p><ahref="https://leetcode.cn/problems/count-subarrays-with-median-k/">题目描述</a>：给定一个<span class="math inline">\([1,n]\)</span>的排列数组，统计其<strong>中位数</strong>为整数 <spanclass="math inline">\(k\)</span>的<strong>连续子数组</strong>数目，长度为偶数时中位数算靠前者。</p><p><strong>方法1</strong>：暴力，找出中位数，从中间开始向两边遍历，同时<strong>累计</strong>大于、小于<span class="math inline">\(k\)</span> 的个数，复杂度 <spanclass="math inline">\(O(n^2)\)</span>，超时。</p><p><strong>方法2</strong>：前缀次数 + 哈希表，<spanclass="math inline">\(k\)</span> 为中位数意味着「左侧小于 + 右侧小于(+1) = 左侧大于 +右侧大于」，因此需要<strong>计数两侧</strong>大于、小于 <spanclass="math inline">\(k\)</span>的数的个数。利用<strong>前缀次数处理</strong>，再将两侧「大于 -小于」的值出现次数存入<strong>两个哈希表</strong>，遍历其中一个哈希表，用乘法原理算出结果，时间复杂度<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：<strong>正负性转换</strong> + 哈希表，大于<span class="math inline">\(k\)</span> 的数可视为 <spanclass="math inline">\(+1\)</span>，小于 <spanclass="math inline">\(k\)</span> 的数视为 <spanclass="math inline">\(-1\)</span>。从 <spanclass="math inline">\(k\)</span> 所在位置开始，先向左扫描同时累计 <spanclass="math inline">\(lsum\)</span>，并用<strong>哈希表</strong>记录<span class="math inline">\(lsum\)</span> 的出现次数；再向右扫描累计<span class="math inline">\(rsum\)</span>，看哈希表中有多少个 <spanclass="math inline">\(lsum\)</span> 可以配对。时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h2 id="离散化">离散化</h2><p>解决数据范围大但样本点少的情况，将大范围的样本（连续取值）映射到小范围（离散取值）。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">sort</span>(nums.<span class="hljs-built_in">begin</span>(), nums.<span class="hljs-built_in">end</span>());<br>nums.<span class="hljs-built_in">erase</span>(<span class="hljs-built_in">unique</span>(nums.<span class="hljs-built_in">begin</span>(), nums.<span class="hljs-built_in">end</span>()), nums.<span class="hljs-built_in">end</span>());<br><span class="hljs-keyword">int</span> m = nums.<span class="hljs-built_in">size</span>(); <span class="hljs-comment">// 去重后的长度</span><br></code></pre></td></tr></table></figure><h3 id="矩形面积-ii-h">850. 矩形面积 II (H)</h3><p><ahref="https://leetcode.cn/problems/rectangle-area-ii/">题目描述</a>：给定一组X-Y轴对齐的矩形（左上角和右下角坐标），计算平面中<strong>所有矩形</strong>覆盖的<strong>总面积</strong>，重叠部分只算一次。</p><p><strong>方法1</strong>：离散化 +扫描线，对所有矩形的左右边界排序，横坐标离散化成 <spanclass="math inline">\(2n\)</span> 个点，纵坐标去重成 <spanclass="math inline">\(\leqslant2 n-1\)</span>个线段。遍历横坐标，每次对<strong>一批横坐标相同的左右边界</strong>遍历纵坐标线段，复杂度<span class="math inline">\(O(n^2)\)</span>。</p><blockquote><p>扫描线算法：用一条竖直的直线从平面的<strong>最左端扫到最右端</strong>，扫描过程中直线会被给定的矩形覆盖，对覆盖的线段进行积分。每次扫到矩形的<strong>左边界</strong>时，覆盖的长度<strong>可能会增加</strong>；扫到矩形的<strong>右边界</strong>时，覆盖的长度<strong>可能会减少</strong>。</p><p>离散化，分别对横纵坐标处理：</p><ul><li><p>由于矩形的横坐标是连续值（<spanclass="math inline">\(1e9\)</span>），而矩形的个数 <spanclass="math inline">\(n \leqslant 200\)</span>，可以将其转化为 <spanclass="math inline">\(2n\)</span> 个离散的横坐标。</p></li><li><p>纵坐标将扫描线划分作 <span class="math inline">\(2n-1\)</span>个线段（共有 <span class="math inline">\(2n\)</span>个离散的坐标，去掉两端的射线），因此可以用两个长度为 <spanclass="math inline">\(2n-1\)</span> 的数组维护。 <spanclass="math inline">\(seg[i]\)</span> 表示第 <spanclass="math inline">\(i\)</span> 个线段被覆盖的次数，<spanclass="math inline">\(length[i]\)</span> 表示第 <spanclass="math inline">\(i\)</span>个线段的长度。遇到一个左边界时，我们就将左边界覆盖到的线段对应的 <spanclass="math inline">\(seg[i]\)</span> 全部加 <spanclass="math inline">\(1\)</span>；遇到一个右边界时，我们就将右边界覆盖到的线段对应的<span class="math inline">\(seg[i]\)</span> 全部减 <spanclass="math inline">\(1\)</span>。处理完一批后，<spanclass="math inline">\(seg[i]\)</span> 如果大于 <spanclass="math inline">\(0\)</span>，说明它被覆盖，将 <spanclass="math inline">\(length[i]\)</span>累加，即可得到「覆盖的线段长度」。</p></li></ul></blockquote><p><strong>方法2</strong>：离散化 + 扫描线 + 线段树，维护 <spanclass="math inline">\(seg\)</span> 数组的覆盖次数，以及离散化后的 <spanclass="math inline">\(length\)</span>。需要区间加、减操作，对于所有非<span class="math inline">\(0\)</span> 的位置，按照 <spanclass="math inline">\(length\)</span> 求和。复杂度 <spanclass="math inline">\(O(n \log n)\)</span>。</p><h2 id="双指针">双指针</h2><p>常见的有<strong>分段双指针、并行双指针、快慢双指针、对撞双指针、滑动窗口双指针</strong>：</p><ul><li>分段双指针：两个指针将序列分为三段，最右边是未访问的，左边和中间的序列往往具有特定属性。</li><li>并行双指针：两个指针分别遍历两个序列，遍历到的值进行某种操作，并使其中一个指针前进。</li><li>快慢双指针：常用于链表中，解决需要「计数」的问题。</li><li>对撞双指针：两个指针从两端开始向中间靠拢，通常使用<code>while(left &lt; right)</code>。</li><li>滑动窗口双指针：关注窗口内的值，题目中通常会有「<strong>最长连续子</strong>XX」，关键在于「连续」。</li></ul><h3 id="移动零-e">283. 移动零 (E)</h3><p><ahref="https://leetcode.cn/problems/move-zeroes/">题目描述</a>：将数组<code>nums</code> 中的所有 <code>0</code>移动到数组的末尾，同时<strong>保持非零元素的相对顺序</strong>，要求原地操作。</p><p><strong>方法1</strong>：冒泡，不改变相对顺序，将所有 <code>0</code>冒泡移动末尾，复杂度 <span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：分段双指针，左指针左边均为处理过的非零数，<strong>右指针左边直到左指针本身均为零</strong>，右指针右边是未处理的序列。右指针向右移动，每次遇到非零数，就和左指针指向的零交换，同时左指针右移。复杂度<spanclass="math inline">\(O(n)\)</span>，交换的次数是<strong>非零数的个数</strong>。</p><p><strong>方法3</strong>：直接遍历，遍历时计数 <code>0</code>的个数，每次遇到非零数直接往前填充，最后再填充所有零。复杂度 <spanclass="math inline">\(O(n)\)</span>，覆盖次数是非零数的个数 +零的个数，也就是<strong>数组的长度</strong>。</p><h3 id="盛最多水的容器-m">11. 盛最多水的容器 (M)</h3><p><ahref="https://leetcode.cn/problems/container-with-most-water/">题目描述</a>：给定一个长度为<code>n</code> 的数组表示 <code>n</code>条垂线的高，找出其中两条线，使它们构成的容器能装最多的水。</p><p><strong>方法1</strong>：暴力，两层循环，先选中左边界，再遍历右边界，时间复杂度<span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：贪心 +对撞双指针，双指针从两端开始遍历，选定<strong>两个边界中的短板</strong>，向中间收窄一格。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>如果长板向内收窄则不可能得到更优解，因为「短板」只可能更短，而横轴也变短了。</p></blockquote><h3 id="三数之和-m">15. 三数之和 (M)</h3><p><ahref="https://leetcode.cn/problems/3sum/">题目描述</a>：给定一个整数数组，返回其中所有<strong>不重复的三元组</strong>，使其和为零。三元组中可以有相同数。</p><p><strong>方法1</strong>：排序 +暴力，将数组从小到大排序后，三层循环遍历，且<strong>每层循环都大于上一层循环枚举的元素</strong>，可以保证三元组<code>a&lt;=b&lt;=c</code>，不会有其他顺序。同时对<strong>每一层循环跳过相同的数</strong>，否则也会重复。时间复杂度<span class="math inline">\(O(n^3)\)</span>。</p><p><strong>方法2</strong>：排序 + 二分查找，方法 1的第三层遍历中，目标数值是确定的，因此可以直接二分查找<code>binary_search(num+b_idx, num+n, c)</code>，时间复杂度 <spanclass="math inline">\(O(n^2 \log n)\)</span>。</p><p><strong>方法3</strong>：排序 + 对撞双指针，方法 1的第三层遍历中，如果 <code>b</code> 匹配到了一个元素<code>c</code>，则下一次 <code>b&lt;=b'</code> 时一定有<code>c'&lt;=c</code>，因此无需完整遍历，只需用双指针从两端向中靠拢。时间复杂度<span class="math inline">\(O(n^2)\)</span>。</p><p><strong>拓展</strong>：这题的解法也可以照搬到 <ahref="https://leetcode.cn/problems/4sum/">四数之和</a>，只是多了一层循环。</p><h3 id="最接近的三数之和-m">16. 最接近的三数之和 (M)</h3><p><ahref="https://leetcode.cn/problems/3sum-closest/">题目描述</a>：给定一个整数数组和一个<strong>目标值</strong><code>target</code> ，从数组中选出选出三个整数，使它们的和与<code>target</code> 最接近。</p><p><strong>方法1</strong>：暴力，三层循环遍历，时间复杂度 <spanclass="math inline">\(O(n^3)\)</span>，可以通过排序进行一定的剪枝。</p><p><strong>方法2</strong>：排序 +对撞双指针，外层循环不变，内层循环改用对撞双指针，同时维护一个变量 <spanclass="math inline">\(diff\)</span>表示和目标<strong>差值的绝对值</strong>。内层循环时会被 <spanclass="math inline">\(sum \pm diff\)</span>划分为<strong>五种情况</strong>，时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。</p><blockquote><p>五种情况分别为：和小于 <span class="math inline">\(sum-diff\)</span>时左指针移动，和大于 <span class="math inline">\(sum + diff\)</span>时右指针移动；其他情况需要更新 <span class="math inline">\(diff\)</span>值，并且根据和与 <span class="math inline">\(sum\)</span>的关系判断三种情况。</p></blockquote><p><strong>方法3</strong>：排序 + 对撞双指针，不需要方法 2那么麻烦，直接仿照上一题，每次先判断 <spanclass="math inline">\(diff\)</span>是否更新，再正常靠拢双指针即可，时间复杂度 <spanclass="math inline">\(O(n^2)\)</span>。</p><h3 id="颜色分类-m">75. 颜色分类 (M)</h3><p><ahref="https://leetcode.cn/problems/sort-colors/">题目描述</a>：给定一个由数字<code>0 1 2</code>组成的数组，代表三种颜色（<strong>荷兰国旗问题</strong>），按照<code>0&lt;1&lt;2</code> 的顺序<strong>原地排序</strong>。</p><p><strong>方法1</strong>：单指针两次遍历，用指针维护「<strong>已归位</strong>」元素下标，第一次遍历找出数字<span class="math inline">\(0\)</span>放到指针处，并将指针后移；第二次遍历找出数字 <spanclass="math inline">\(1\)</span> 放到指针处。两次遍历完 <spanclass="math inline">\(2\)</span> 都出现在尾部。时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：分段双指针，用指针 <spanclass="math inline">\(p_0\)</span> 维护下一个 <spanclass="math inline">\(0\)</span> 的位置，指针 <spanclass="math inline">\(p_1\)</span> 维护下一个 <spanclass="math inline">\(1\)</span> 的位置。每次遇到 <spanclass="math inline">\(1\)</span> 则将其交换到 <spanclass="math inline">\(p_1\)</span>，并<strong>将 <spanclass="math inline">\(p_1\)</span> 增一</strong>；每次遇到 <spanclass="math inline">\(0\)</span> 则将其交换到 <spanclass="math inline">\(p_0\)</span>，由于 <spanclass="math inline">\(p_0\)</span> 处<strong>可能放着</strong> <spanclass="math inline">\(1\)</span>，所以还要将其放到 <spanclass="math inline">\(p_1\)</span>位置，最后<strong>将两个指针增一</strong>。</p><p><strong>方法3</strong>：对撞双指针，左指针 <spanclass="math inline">\(p_0\)</span> 维护下一个 <spanclass="math inline">\(0\)</span> 的位置，右指针 <spanclass="math inline">\(p_2\)</span> 维护下一个 <spanclass="math inline">\(2\)</span>的位置。<strong>从左往右遍历</strong>，遇到 <spanclass="math inline">\(0\)</span> 直接换，遇到 <spanclass="math inline">\(2\)</span> 交换后，当前位置换来的数可能还是 <spanclass="math inline">\(0\)</span> 或 <spanclass="math inline">\(2\)</span>，需要<strong>一直重复交换直到当前位置为</strong><span class="math inline">\(1\)</span>。</p><blockquote><p>此时遍历用的迭代变量可以理解为 <spanclass="math inline">\(p_1\)</span>，因为本题的目的就是划分三个区间，用三个指针是最符合直觉的。</p></blockquote><h3 id="接雨水-h">42. 接雨水 (H)</h3><p><ahref="https://leetcode.cn/problems/trapping-rain-water/">题目描述</a>：给定<code>n</code> 个非负整数表示每个宽度为 <code>1</code>的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。</p><p><strong>方法1</strong>：暴力，对于每个坐标，其能接的水量取决于<strong>左右两边最高的柱子中较矮者</strong>，两层循环遍历，复杂度<span class="math inline">\(O(n^2)\)</span>。</p><p><strong>方法2</strong>：DP预处理最大值，两次扫描，记录<strong>每个柱子</strong>的<strong>左右两边最大值</strong>，再扫描累计，时空复杂度均为<spanclass="math inline">\(O(n)\)</span>。该方法可以简单优化：从左到右的预处理可以和第三次扫描<strong>合并</strong>，<strong>用一个变量存储</strong>左边最大值，节省一个数组。</p><p><strong>方法3</strong>：单调不增栈 +模拟，每遍历到一个柱子，如果<strong>小于等于栈顶元素则入栈</strong>，如果更大则说明前面的柱子<strong>可以形成水洼</strong>，则依次将所有较小数弹出，并计算<strong>栈顶元素和新柱子的距离差值</strong>（表示水洼的左右墙壁，如果此时栈空，则表示没有左墙，该<strong>水洼不成立</strong>），再把新柱子入栈。时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法4</strong>：贪心 +对撞双指针，双指针从两端开始遍历，再用两个变量存储左右两边最大值，每次<strong>最值较小者向中间收窄一格</strong>，同时计算出当前柱子能接的水量（<strong>当前柱子与这一侧最值的高度差</strong>，该值就是当前柱子能接到的最大水量）。时间复杂度<span class="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><blockquote><p>最值较小侧可以先计算的原因是：每个位置能接的水量取决于左右两边最高的柱子中较矮者，而最值较小侧的指针所指的位置，其所在侧的最值已经是较矮的（确定性），另一侧的最值未知但只会变得更大，所以此时该位置的答案已经可以提前确定。</p></blockquote><p><strong>方法5</strong>：分治 +面积法，<strong>最高柱子将全局分为两边</strong>，两边各有各自的次高柱子，<strong>次高柱子与最高柱子中间的区域</strong>显然取值就是「次高柱子- 当前柱子」，同理以此类推到两端。时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><blockquote><p>因此<strong>只需遍历一次</strong>，从左往右取左边最大值，从右往左取右边最大值，答案增加「<strong>左高+ 右高</strong> -当前柱子」。这里的减去的「当前柱子」只是为了减去所有柱子的面积（合并在一次遍历中完成）。</p><p>最后减去「最高柱子 <span class="math inline">\(\times\)</span>总长度」，因为两次扫描都会<strong>经过最高柱子</strong>，经过后再接着扫描的面积无意义，形成了一个大矩形的面积，需要扣除。</p></blockquote><p><strong>坑点</strong>：最左边、最右边两个柱子<strong>不可能接到水</strong>，要跳过，可以直接从<code>1</code> 遍历到 <code>n-2</code>。</p><h2 id="滑动窗口">滑动窗口</h2><p>双指针中较为困难的一类题：关注窗口内的值，题目中通常会有「<strong>连续子</strong>XX」，关键在于「连续」。</p><ul><li>要求「<strong>最长连续子</strong>XX」的<strong>长度</strong>，此时应该尽量滑动右指针，在窗口不满足要求时滑动左指针，一直到右指针的元素满足要求（如果要求最短则反之）。同时优化判断「<strong>窗口是否满足要求</strong>」的复杂度。此时维护的窗口类型：<ul><li><strong>哈希集合</strong>：维护窗口内有哪些元素；</li><li><strong>哈希表</strong>：维护窗口内元素出现的次数、最后出现的位置；</li><li><strong>双哈希表</strong>：对子串乱序匹配问题，需要一个<code>need</code> 存储目标次数、一个 <code>window</code>维护窗口内出现的次数；</li><li><strong>单个变量</strong>：利用位运算进行存储、或者压位操作。</li></ul></li><li>要求「<strong>连续子</strong>XX」的<strong>个数</strong>，此时应该<strong>枚举右指针并固定</strong>（定界法），然后再枚举左指针，计数左指针的枚举次数。左指针不需要复位，防止退化为<span class="math inline">\(O(n^2)\)</span>。此时需要维护的变量：<ul><li><strong>不该出现</strong>的元素的最后出现的位置，代表窗口左指针的<strong>最左取值</strong>；</li><li><strong>必须出现</strong>的元素的最后出现的位置，代表窗口左指针的<strong>最右取值</strong>。</li></ul></li></ul><h3 id="无重复字符的最长子串-m">3. 无重复字符的最长子串 (M)</h3><p><ahref="https://leetcode.cn/problems/longest-substring-without-repeating-characters/">题目描述</a>：给定一个字符串<code>s</code>，找出其中不含有重复字符的<strong>最长连续子串</strong>的长度。</p><p><strong>方法1</strong>：双指针滑动窗口，右指针每前进一格，在窗口内进行<code>s.find</code>，如果找到重复字符，则左指针快速前移，复杂度 <spanclass="math inline">\(O(n^2)\)</span>。左右指针移动的方式也可以交换。</p><p><strong>方法2</strong>：滑动窗口 +<strong>哈希集合</strong>，左指针多次 <code>erase()</code> 直到<code>hash.count() == 0</code> 代替 <code>find</code>，复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法3</strong>：滑动窗口 +<strong>哈希表</strong>，比上一个方法多用一个数来存储字符在数组中的下标，左指针快速前移时不需要一直多次<code>count()</code>，复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><h3 id="找到字符串中所有字母异位词-m">438. 找到字符串中所有字母异位词(M)</h3><p><ahref="https://leetcode.cn/problems/find-all-anagrams-in-a-string/">题目描述</a>：给定两个字符串<code>s</code> 和 <code>p</code>，找到 <code>s</code> 中所有<code>p</code>的<strong>字母异位词子串</strong>（相同字母重排列形成）。</p><p><strong>方法1</strong>：<strong>定长</strong>滑动窗口，窗口长度限定为<code>p</code> 的长度，枚举窗口的每个位置，用哈希表 +计数来判断窗口内是否匹配所有字母，<strong>每次只考虑一进一出的两个字母</strong>，时间复杂度<span class="math inline">\(O(n+m)\)</span>。</p><p><strong>方法2</strong>：滑动窗口 + <strong>双哈希表</strong>，哈希表<code>need</code>记录<strong>目标字符次数</strong>，用双指针扫描母串并记录<strong>窗口内字符次数</strong><code>window</code>，左指针只在 <spanclass="math inline">\(window[s[r]]&gt;need[s[r]]\)</span>时<strong>前移</strong>。当窗口长度恰好是 <code>p</code>的长度时记录答案（此时两个哈希表必然相等）。时间复杂度 <spanclass="math inline">\(O(n+m)\)</span>。</p><h3 id="串联所有单词的子串-h">30. 串联所有单词的子串 (H)</h3><p><ahref="https://leetcode.cn/problems/substring-with-concatenation-of-all-words//">题目描述</a>：给定一个字符串<code>s</code> 和一个字符串数组<code>words</code><strong>。</strong><code>words</code>中所有字符串<strong>长度相同</strong>，串联子串指的是将<code>words</code>中的所有字符串以<strong>任意顺序排列串联</strong>起来，找到<code>s</code> 中可能出现的串联子串的起始索引。</p><p><strong>方法</strong>：滑动窗口 +双哈希表，跟上一题一模一样的操作，区别在于此时的<strong>单位由字母变成了单词</strong>，因此不能再逐单词前移。考虑到每个字母都有可能充当单词的起点，最好<strong>在外层循环就将起点定好</strong>，内层循环再滑动窗口。时间复杂度<span class="math inline">\(O(nm)\)</span>。</p><h3 id="最长优雅子数组-m">2401. 最长优雅子数组 (M)</h3><p><ahref="https://leetcode.cn/problems/longest-nice-subarray/">题目描述</a>：<strong>正</strong>整数组成的数组<code>nums</code>中找出<strong>最长连续子数组</strong>，使其满足子数组中所有元素<strong>两两</strong><code>&amp;</code> 结果等于 0。</p><p><strong>方法</strong>：滑动窗口 + 位运算，窗口内所有元素取<code>|</code> 使二进制 1 位合并，右侧元素只需和整体进行<code>&amp;</code>就能判断是否加入窗口，如果不能加入，则左侧元素需要弹出，使用<code>^</code> 运算<strong>去除</strong>二进制 1 位。</p><blockquote><p>使用异或必定可以去除：因为窗口内任何二进制位上的 1必定只出现一次（满足优雅子数组）。</p></blockquote><h3 id="最小覆盖子串-h">76. 最小覆盖子串 (H)</h3><p><ahref="https://leetcode.cn/problems/minimum-window-substring/">题目描述</a>：给定字符串<code>s</code> 和 <code>t</code>，找出 <code>s</code> 中涵盖<code>t</code> 所有字符的<strong>最小子串</strong>，顺序可以打乱。</p><p><strong>方法1</strong>：滑动窗口 + <strong>双哈希表</strong>，哈希表<code>need</code>记录<strong>目标字符次数</strong>，用双指针扫描母串并记录<strong>窗口内字符次数</strong><code>window</code>。与前几题不一样的是，本题的目标是满足要求的最小子串，因此要在窗口满足要求时<strong>尽量滑动左指针</strong>，左指针在<span class="math inline">\(window[s[l]]&gt;need[s[l]]\)</span>时<strong>前移</strong>。用 <span class="math inline">\(cnt\)</span>维护已匹配字符数，及时更新答案最小值。复杂度 <spanclass="math inline">\(O(n+m)\)</span>。</p><p><strong>方法2</strong>：滑动窗口 + 单哈希表，哈希表 <code>need</code>记录<strong>距离目标字符剩余次数</strong>，只有遇到 <code>t</code>中存在的字符时才更新窗口，用 <span class="math inline">\(cnt\)</span>维护剩余字符数，及时更新答案。复杂度 <spanclass="math inline">\(O(n+m)\)</span>。</p><hr /><h3 id="区间子数组个数-m">795. 区间子数组个数 (M)</h3><p><ahref="https://leetcode.cn/problems/number-of-subarrays-with-bounded-maximum/">题目描述</a>：给定一个整数数组<code>nums</code> 和两个整数：<code>left</code> 及<code>right</code>。找出连续、非空且<strong>最大值</strong>在<code>[left,right]</code> 内的子数组，统计个数。</p><p><strong>方法1</strong>：单调栈 +乘法原理，需要找到每个数作为最大值时的两边边界（一边大于、一边大于等于，避免重复计算），正好可以用单调栈+ 反用同时满足，乘法计算出个数贡献，时空复杂度均为 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：定界法，显然 <code>&gt;right</code>的数不能包含，记上一个出现的位置为 <spanclass="math inline">\(i_0\)</span>；区间内的数<strong>至少包含一个</strong>，记上一个出现的位置为<spanclass="math inline">\(i_1\)</span>。<strong>枚举右端点</strong>，则当<span class="math inline">\(i_0&lt;i_1\)</span>时，两者的差就是个数。时间复杂度 <spanclass="math inline">\(O(n)\)</span>，空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>坑点</strong>：使用单调栈时，最后栈中剩余的元素可以认为是右侧没有更大的值，因此右边界需要直接取<span class="math inline">\(n\)</span>。</p><h3 id="统计定界子数组的数目-h">2817. 统计定界子数组的数目 (H)</h3><p><ahref="https://leetcode.cn/problems/count-subarrays-with-fixed-bounds/">题目描述</a>：给定一个整数数组<code>nums</code> 和两个整数 <code>minK</code> 以及<code>maxK</code>，若一个<strong>连续子数组</strong>中的<strong>最小值</strong>等于<code>minK</code>，<strong>最大值</strong>等于<code>maxK</code>，则称为定界子数组。返回定界子数组的数目。</p><p><strong>方法1</strong>：分治 +滑动窗口双指针，首先去掉范围之外的数，将原数组<strong>分为若干个子数组</strong>，再对每个子数组讨论。每个子数组采用双指针扫描，时间复杂度<span class="math inline">\(O(n)\)</span>。</p><blockquote><p>对每个子数组，先枚举右端点，右指针滑动并计数 <code>miCnt</code> 和<code>mxCnt</code>，当 <code>miCnt &gt; 0 &amp;&amp; mxCnt &gt; 0</code>时，说明右端点已经满足，此时左端点有多种取值。</p><p>枚举左端点，左指针滑动并减少 <code>miCnt</code> 和<code>mxCnt</code>，当不满足条件时，<strong>左端点枚举过的距离</strong>就是该右端点对应的定界子数组个数。继续枚举下一个右端点。</p><p>对于下一个右端点，即使滑动窗口之间不满足<code>miCnt &gt; 0 &amp;&amp; mxCnt &gt; 0</code>，也可以增加<strong>左端点枚举过的距离</strong>，因为这段距离中任何一个左端点都可以和当前右端点构成定界子数组。</p></blockquote><p><strong>方法2</strong>：定界法，记录<strong>上一个范围之外的数</strong>的下标<spanclass="math inline">\(i_0\)</span>，代表了<strong>左端点的最左取值</strong>。枚举右端点，同时记录遍历过的<strong>距离右端点最近</strong>的<span class="math inline">\(minK\)</span> 和 <spanclass="math inline">\(maxK\)</span>的<strong>下标</strong>，二者中的靠左者就是<strong>左端点的最右取值</strong>。时间复杂度<span class="math inline">\(O(n)\)</span>。</p><p><strong>坑点</strong>：不要试图将两边端点的取值范围找出来后相乘，因为这题每个点有两种角色，需要都满足时才能产生贡献。</p><h2 id="原地置换">原地置换</h2><p>原地置换是数组中经典的套路题，通常要求使用空间复杂度为 <spanclass="math inline">\(O(1)\)</span>的<strong>原地算法</strong>解决，此时应尽量往置换的方向思考。常用模板：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">while</span>(i &lt; nums.<span class="hljs-built_in">size</span>())&#123;<br>    <span class="hljs-keyword">if</span>(nums[i] == i || ...)&#123; <span class="hljs-comment">// 填写不应置换的条件</span><br>        i++;<br>        <span class="hljs-keyword">continue</span>;<br>    &#125;<br>    <span class="hljs-built_in">swap</span>(nums[i], nums[nums[i]]);<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="剑指-offer-03.-数组中重复的数字-e">剑指 Offer 03.数组中重复的数字 (E)</h3><p><ahref="https://leetcode.cn/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/">题目描述</a>：一个含<span class="math inline">\(n\)</span>个整数的数组，其中<strong>每个数</strong>都在区间 <spanclass="math inline">\([0,n-1]\)</span>内。数组中某些数字是重复的，但<strong>不知道有几个</strong>数字重复了，也<strong>不知道每个数字重复了几次</strong>。请找出数组中<strong>任意一个</strong>重复的数字。</p><p><strong>方法1</strong>：哈希集合，记录每个数字，当哈希冲突时即可返回，时空复杂度均为<span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：原地置换 +哈希，由于每个数都在区间内，考虑<strong>利用原数组哈希</strong>节省空间。将每次遇到的数字<code>a</code> 放到 <code>nums[a]</code>，如果该位置已经有<code>a</code> 则说明 <code>a</code> 就是重复的数字。空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="找到所有数组中消失的数字-e">448. 找到所有数组中消失的数字(E)</h3><p><ahref="https://leetcode.cn/problems/find-all-numbers-disappeared-in-an-array/">题目描述</a>：一个含<span class="math inline">\(n\)</span>个整数的数组，其中<strong>每个数</strong>都在区间 <spanclass="math inline">\([1,n]\)</span> 内。请你找出所有在 <spanclass="math inline">\([1,n]\)</span>内但<strong>没有出现在</strong>数组中的数字。</p><p><strong>方法1</strong>：哈希表，用一个长为 <spanclass="math inline">\(n\)</span>的数组记录每个数是否出现，时空复杂度均为 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：原地置换 + 哈希，由于原数组的长度就是 <spanclass="math inline">\(n\)</span>，<strong>利用原数组哈希</strong>可以节省空间。将每次遇到的数字<code>a</code> 放到 <code>nums[a]</code>，如果该位置已经有<code>a</code>则停下。整个数组遍历完后，每个索引上非对应值者就是未出现的数字。空间复杂度<span class="math inline">\(O(1)\)</span>。</p><h3 id="轮转数组-m">189. 轮转数组 (M)</h3><p><ahref="https://leetcode.cn/problems/rotate-array/">题目描述</a>：给定一个整数数组<code>nums</code>，将数组中的元素向右轮转 <code>k</code> 个位置，其中<code>k</code> 是非负数。</p><p><strong>方法1</strong>：辅助数组，用一个辅助数组遍历 + 拷贝，下标<code>i</code> 的元素放到下标 <code>(i+k)%n</code>的位置，时空复杂度均为 <span class="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：原地置换，相距 <spanclass="math inline">\(k\)</span>的元素会构成一个<strong>置换环</strong>，每个元素移动到环中的下一个位置，通过<code>swap</code> 循环即可完成。总共需要 <code>gcd(k,n)</code>个置换环。空间复杂度 <span class="math inline">\(O(1)\)</span>。</p><p><strong>方法3</strong>：镜像翻转，先将所有元素翻转，这样尾部的<code>k%n</code> 个元素就被移至数组头部，然后我们再翻转<code>[0,k%n−1]</code> 区间的元素和 <code>[k%n,n−1]</code>区间的元素即能得到最后的答案。空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><h3 id="寻找重复数-m">287. 寻找重复数 (M)</h3><p><ahref="https://leetcode.cn/problems/find-the-duplicate-number/">题目描述</a>：给定一个包含<span class="math inline">\(n + 1\)</span> 个整数的数组，其数字都在<span class="math inline">\([1, n]\)</span>范围内，假设数组中<strong>有且仅有一个重复的整数</strong>，找出数组中重复的数字，要求空间复杂度<span class="math inline">\(O(1)\)</span>。</p><p><strong>方法1</strong>：二分答案，定义 <spanclass="math inline">\(cnt[i]\)</span>表示数组中<strong>小于等于</strong> <spanclass="math inline">\(i\)</span> 的数的个数，假设重复数是 <spanclass="math inline">\(x\)</span>，则 <spanclass="math inline">\([1,x-1]\)</span> 里所有数都满足 <spanclass="math inline">\(cnt[i] \leq i\)</span>；而 <spanclass="math inline">\([x,n]\)</span> 里的所有数满足 <spanclass="math inline">\(cnt[i]&gt;i\)</span>。因此可以二分枚举 <spanclass="math inline">\(x\)</span> 直到找到。时间复杂度 <spanclass="math inline">\(O(n\log n)\)</span>。</p><p><strong>方法2</strong>：二进制，预处理 <spanclass="math inline">\([1,n]\)</span> 这 <spanclass="math inline">\(n\)</span> 个数「<strong>每一位</strong>为 <spanclass="math inline">\(1\)</span>的<strong>个数之和</strong>」，如果当前数组的「<strong>每一位</strong>为<span class="math inline">\(1\)</span>的<strong>个数之和</strong>」大于期望值，则说明<strong>重复数的这一位</strong>为<span class="math inline">\(1\)</span>。遍历每一位即可，时间复杂度 <spanclass="math inline">\(O(n \log n)\)</span>。</p><p><strong>方法3</strong>：抽象成环形链表寻找入口，快慢指针，先用 <spanclass="math inline">\(fast\)</span> 和 <spanclass="math inline">\(slow\)</span> 相遇，再用 <spanclass="math inline">\(pos\)</span> 和 <spanclass="math inline">\(slow\)</span> 相遇，时间复杂度 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法4</strong>：原地置换 + 哈希，跟 <ahref="https://leetcode.cn/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/">剑指Offer 03. 数组中重复的数字</a> 一模一样，比前面的方法都简单。</p><h3 id="缺失的第一个正数-h">41. 缺失的第一个正数 (H)</h3><p><ahref="https://leetcode.cn/problems/first-missing-positive/">题目描述</a>：给你一个未排序的整数数组<code>nums</code>，找出其中没有出现的最小的正整数。</p><p><strong>方法1</strong>：哈希集合，遍历数组放入集合，由于答案只可能是<span class="math inline">\([1,n+1]\)</span> 之间的数，所以从 <spanclass="math inline">\(1\)</span> 开始一个个访问。时空复杂度均为 <spanclass="math inline">\(O(n)\)</span>。</p><p><strong>方法2</strong>：原地置换 +哈希，将可能是答案的元素交换到对应下标占位，最后顺序访问数组，第一个不与下标对应的元素就是答案。如果都对应，则答案为<span class="math inline">\(n+1\)</span>。空间复杂度 <spanclass="math inline">\(O(1)\)</span>。</p><p><strong>坑点</strong>：本题在判断不应交换时的条件较多，容易遗漏：</p><ul><li>超出 <span class="math inline">\([1,n]\)</span> 的不用管；</li><li>如果当前元素已经在其对应下标，也不应交换；</li><li>如果当前元素的对应下标已经被占位（例如<code>[1,1]</code>，此时第一个位置已经被占），也不应交换。</li></ul>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>力扣刷题</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2022年保研回忆录</title>
    <link href="/BaoYan-Memoir.html"/>
    <url>/BaoYan-Memoir.html</url>
    
    <content type="html"><![CDATA[<h2 id="个人情况">1. 个人情况</h2><ul><li>本科：中流 985 计算机专业</li><li>夏令营排名：6/300+，预推免排名：7/300+，综测排名：2/300+</li><li>荣誉奖项：一次国家奖学金，若干学业奖学金、企业奖学金</li><li>竞赛经历：数学建模国赛一等奖、美赛 H 奖（完全没用）</li><li>科研经历：两篇 NLP 相关论文（非一作，夏令营时在投）</li><li>最终去向：复旦大学计算机学硕</li></ul><h2 id="最初定位">2. 最初定位</h2><ul><li>院校选择：主要冲<strong>华五</strong>，没敢奢求清北本部，本校及同层次学校<strong>不报</strong>（机会留给其他同学）</li></ul><p><span class="math display">\[心仪学校: \mathrm{FDU.nlp}\geqslant  \mathrm{RUC.gl}\geqslant\mathrm{USTC}\approx  \mathrm{CAS.ia}\approx  \mathrm{PKU.rw}\geqslant  \mathrm{NJU.nlp}\approx  \mathrm{ZJU}\]</span></p><p><span class="math display">\[心仪导师: 人品(指导、实习)\geqslant 方向 \approx科研实力(组里资源、论文数量、学生数量) &gt; 补贴、知名度等\]</span></p><ul><li><p>方向：AI 方向，最好与 NLP/RecSys相关，可接受偏软件工程（本科导师方向），也可接受其他 AI 方向；</p></li><li><p>填报学位：倾向<strong>学硕</strong>，可考虑强组直博、科研向的专硕、纯实习的专硕；</p></li><li><p>其他考虑：最好有宿舍，工位，以及大学的人文氛围。</p></li></ul><h2 id="参营情况">3. 参营情况</h2><p>入营大部分都是靠<strong>排名筛选</strong>，优营基本取决于面试，看重<strong>机试能力、科研经历、临场发挥</strong>。这部分先按下不表，下面介绍本人的报名及参营情况：</p><table><thead><tr class="header"><th style="text-align: center;">学校</th><th style="text-align: center;">学院/实验室</th><th style="text-align: center;">类型</th><th style="text-align: center;">入营</th><th style="text-align: center;">优营</th><th style="text-align: center;">备注</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">中科院</td><td style="text-align: center;">软件所中文信息处理</td><td style="text-align: center;">学硕</td><td style="text-align: center;">X</td><td style="text-align: center;">-</td><td style="text-align: center;">需要提前参加机试才能入营</td></tr><tr class="even"><td style="text-align: center;">复旦</td><td style="text-align: center;">计算机学院</td><td style="text-align: center;">学硕</td><td style="text-align: center;">✔</td><td style="text-align: center;">候补</td><td style="text-align: center;">9.19候补到，<strong>最终去向</strong></td></tr><tr class="odd"><td style="text-align: center;">人大</td><td style="text-align: center;">高瓴人工智能学院</td><td style="text-align: center;">学硕</td><td style="text-align: center;">✔</td><td style="text-align: center;">候补</td><td style="text-align: center;">9.28 下午候补到，放弃</td></tr><tr class="even"><td style="text-align: center;">北大</td><td style="text-align: center;">软件与微电子学院</td><td style="text-align: center;">专硕</td><td style="text-align: center;">✔</td><td style="text-align: center;">✔</td><td style="text-align: center;">优营放弃</td></tr><tr class="odd"><td style="text-align: center;">南大</td><td style="text-align: center;">人工智能学院</td><td style="text-align: center;">学硕</td><td style="text-align: center;">✔</td><td style="text-align: center;">X</td><td style="text-align: center;">笔试通过，面试放弃</td></tr><tr class="even"><td style="text-align: center;">南大</td><td style="text-align: center;">计算机学院</td><td style="text-align: center;">学硕</td><td style="text-align: center;">X</td><td style="text-align: center;">-</td><td style="text-align: center;">菜！</td></tr><tr class="odd"><td style="text-align: center;">中科院</td><td style="text-align: center;">自动化所</td><td style="text-align: center;">学硕</td><td style="text-align: center;">X</td><td style="text-align: center;">-</td><td style="text-align: center;">候补入营，根本补不到</td></tr><tr class="even"><td style="text-align: center;">北大</td><td style="text-align: center;">计算机学院</td><td style="text-align: center;">学硕</td><td style="text-align: center;">X</td><td style="text-align: center;">-</td><td style="text-align: center;">菜！</td></tr><tr class="odd"><td style="text-align: center;">北大</td><td style="text-align: center;">智能学院</td><td style="text-align: center;">学硕</td><td style="text-align: center;">X</td><td style="text-align: center;">-</td><td style="text-align: center;">菜！</td></tr><tr class="even"><td style="text-align: center;">上交</td><td style="text-align: center;">电院</td><td style="text-align: center;">学硕</td><td style="text-align: center;">✔</td><td style="text-align: center;">X</td><td style="text-align: center;">时间冲突放弃机试</td></tr><tr class="odd"><td style="text-align: center;">中科大</td><td style="text-align: center;">网安学院</td><td style="text-align: center;">学硕</td><td style="text-align: center;">✔</td><td style="text-align: center;">✔</td><td style="text-align: center;">优营放弃</td></tr></tbody></table><p>预推免：</p><table><thead><tr class="header"><th style="text-align: center;">学校</th><th style="text-align: center;">学院/实验室</th><th style="text-align: center;">类型</th><th style="text-align: center;">入营</th><th style="text-align: center;">优营</th><th style="text-align: center;">备注</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">清华</td><td style="text-align: center;">软件学院</td><td style="text-align: center;">专硕</td><td style="text-align: center;">✔</td><td style="text-align: center;">X</td><td style="text-align: center;">菜！</td></tr><tr class="even"><td style="text-align: center;">清华</td><td style="text-align: center;">深研院人工智能</td><td style="text-align: center;">专硕</td><td style="text-align: center;">X</td><td style="text-align: center;">-</td><td style="text-align: center;">和清软只能入一个</td></tr><tr class="odd"><td style="text-align: center;">中科院</td><td style="text-align: center;">自动化所</td><td style="text-align: center;">学硕</td><td style="text-align: center;">X</td><td style="text-align: center;">-</td><td style="text-align: center;">再次被拒，生气</td></tr><tr class="even"><td style="text-align: center;">浙大</td><td style="text-align: center;">计算机学院</td><td style="text-align: center;">学硕</td><td style="text-align: center;">✔</td><td style="text-align: center;">X</td><td style="text-align: center;">参营前补到复旦，遂放弃</td></tr></tbody></table><h2 id="关键时间点">4. 关键时间点</h2><ol type="1"><li>保研前夕（上一年）：联系导师进组做科研，了解科研的流程，初步阅读论文、跑实验；</li><li>前期准备（2 - 3月）：浏览网上的保研经验贴，整理目标学校及考核内容，准备<strong>简历</strong>；</li><li>陶瓷导师（4月起）：在学校官网、谷歌学术、知乎、导师评价网查找感兴趣老师的信息，发邮件联系（一定要有诚意，至少<strong>不要把导师的基本信息、研究方向说错</strong>），陶瓷老师后很可能需要<strong>提前面试</strong>，特别是强组；</li><li>报名材料（5 月中旬 - 6月初）：证书扫描件、推荐信、成绩单及排名证明、个人陈述等；</li><li>面试准备（5 月初 - 7 月底）：<ul><li><strong>自我介绍</strong>：科研 &gt;&gt; 项目类竞赛 &gt;技能类竞赛</li><li><strong>专业课</strong>：数据结构 &gt;&gt; 操作系统 = 计组 = 计网&gt;&gt; 其他成绩单上的课</li><li><strong>数学课</strong>：线代 = 概率论 &gt;&gt; 高数</li><li><strong>算法题</strong>：力扣 Hot100 &gt; 其他刷题平台</li><li><strong>AI 方向</strong>：机器学习基础 &gt;&gt; 深度学习基础 &gt;NLP 基础 = CV 基础</li><li><strong>其他</strong>：学校特色题（如智力题、编程语言基础）&gt;英语问答 = 性格问答 &gt; 政治问答</li></ul></li><li>夏令营集中进行（7月）：有些学校可能会冲突，需要进行取舍。记得准备好手机支架、安静房间；</li><li>保研间隙（8月）：因为拿了保底就先躺平，几乎没怎么准备预推免。想冲预推免的同学<strong>建议</strong>继续准备上述面试内容，同时根据目标院校的机试难度<strong>适当刷题</strong>，如果不是实在没项目，不建议这时候再花大心思去卷这些，不如多刷面经准备面试题。</li><li>预推免集中进行（9月）：有的预推免实际参营不到入营的一半，机会很大，坚持就是胜利；</li><li>鸽与被鸽（9月中旬起）：<strong>不要因为之前只拿了候补而沮丧</strong>！以下几个时间<strong>鸽子大量起飞</strong>：<ul><li>清深、清软、清华网研院、贵系开奖：清华夏令营不发offer，预推免将吸收一大批华五优营；</li><li>北大预推免开奖：北大夏令营发出部分 offer，但预推免仍会招人；</li><li>比目标院校<strong>高一档</strong>的学校开奖：这里的「<strong>高一档</strong>」可以是title高、地理优势、学硕优势、方向优势甚至各种你想不到原因，但<strong>就是会有人鸽</strong>；</li><li>9.20 - 9.28：内心尚存良知的海王们会放弃一些offer，防止老师招不到学生；</li><li>9.28 中午：学校要求限定时间内接受通知，至此彻底放弃其他offer。<strong>大量学校被鸽穿</strong>，网上流传的包括：上交密院浙大计院 浙大软院 南大计院 南大软院 人大信院 人大高瓴 北航计院 武大计院西交计院 中山计院 吉大计院 东南网安 国科大杭高院 东南软件电子科大深圳。<strong>圣经预言的场景一一应验</strong>。</li></ul></li></ol><blockquote><p>这里的「<strong>鸽穿</strong>」并不是真的有几十个人放鸽子，实际上同一所学校可能只有一两个人放鸽子，但是当名额顺延下来的时候，其他候补同学要么已经接受了待录取通知、要么已经进了其他学校的组不忍心鸽、要么担心候补上只能找到坑导，于是大家纷纷放弃，招生办就一直顺延。</p><p>在这里我 <strong>强烈谴责</strong> 那些<strong>非得等到 928中午才鸽</strong>的人，也许你们只是想晒一张「<strong>拒绝XX</strong>」的截图装逼，但是很可能另一位候补同学就因为在其他学校的催促下仓皇点了接收，从而错失了梦校的offer。</p></blockquote><h2 id="夏令营细节">5. 夏令营细节</h2><p>大部分参营细节在其他人的经验中都提到了，这里仅简单列举部分夏令营。如果有院方或者同学认为文章内容违反了<strong>保密承诺书</strong>，请直接私信我，我会第一时间删除相应内容。</p><h3 id="复旦计算机">复旦计算机</h3><p>入营要求比较高，强制卡 rank，联系老师应该也没用。300人入营，因为和清北夏令营时间冲突最终只来了 200+人，但是发放的名额依然很少。入营后有联络员联系。</p><p>第一天上午线上宣讲，介绍各个实验室情况。晚上进行<strong>志愿的填报</strong>，这个很重要，大型<strong>博弈</strong>现场，后续优营的评选、候补的情况完全都是在你填报的志愿队列排序。系统实时更新，可以看到当前报考人数/招生人数。学硕名额略多于专硕名额。</p><blockquote><p>学硕竞争更为激烈，但是往往拿了学硕 offer的大佬更有可能去「高一档」学校，因此富贵险中求，能进候补队列就是胜利！</p></blockquote><p>第二天上午机考，两小时三题，需要提交代码和思路文档。不设监考，不计入总分（但面试会看，间接影响评分）。往年复旦的题目一般出自力扣，差不多是Easy、Medium 难度，不会评测，但今年不一样，OJ评测，题目难度很大。第一题接近 Hard，第二题Hard++，第三题可能是金牌题（咱也不知道）。</p><p>第二天下午英语面试，6 - 8分钟。需要自我介绍，提问范围较广，会涉及课程和自我介绍中提及的项目等。</p><p>第三天专业面试，15分钟左右，每个会议室都会有一位大佬驻场，<strong>各个方向侧重问的内容不一</strong>。先中文自我介绍。接下来问机试解题思路等，还有Python等语言基础知识。之后的内容根据不同方向有不同问法（人工智能方向侧重问机器学习和科研经历，其他方向侧重408）。最后会问是否专硕和是否调剂，只需要回答是或否。</p><h3 id="人大高瓴">人大高瓴</h3><p>入营要求高，也是卡 rank 的，学硕名额最终只有 24个，比去年缩水了一点。整个流程可能为了杜绝作弊保证公平性，步骤繁杂、有大量等候流程、非常浪费时间、教务老师也比较严格，参营体验一般。</p><p>先进行笔试（75分），1.5h。官方发布的范围是数学、程序设计、数据结构与算法。题目很难，需要准备很多内容。最后需要手写一道算法题的思路，是很经典的题，循序渐进，难度Medium - Hard。</p><p>过几天才是开营仪式，笔试不筛人。各个老师都会来做介绍发言，高瓴有非常多年轻学术能力强的老师，值得一听。可以在这个时候就选定一批<strong>自己能接受的导师</strong>，以防最后候补到了不知道联系谁。</p><p>最后是面试（基础 75 分 + 综合 50分），15min，同一组序号靠后的同学可能要等很久，并且等待过程中需要开着双机位什么都不能做。先翻译一篇英文摘要后进正式面试会议。本人是同组序号最后一个，坐牢了四个小时，心态爆炸，最后面试时老师也急着下班吃饭，体验很差。</p><h3 id="北大软微">北大软微</h3><p>北大各个学院的报名材料是最多的，一向非常繁琐，是今年少数需要推荐信的院校之一。</p><p>需要注意的是，一向以放养、<strong>就业导向</strong>闻名的软微学院，在开营招生会上宣布今年改成<strong>科研导向</strong>！而预推免、考研上岸的同学依然是放实习的<strong>就业导向</strong>！具体的内容其他经验贴应该会介绍，这里就不展开了。</p><p>入营应该是靠 rank初筛，可能会结合简历。初筛过后会给出一个论文清单，需要选择方向并阅读对应的论文，提交一份1500 字的阅读报告。有人卷到了 4000 字最后还是没入营，很惨。</p><p>面试时用 PPT对选择的论文进行汇报和提问，之后提问一些个人的基本情况，几乎不问基础知识，老师可能会对自我介绍中的科研感兴趣。有英语提问。</p><p>面试后可能会有导师电话联系，给<strong>非常短的时间</strong>让你确定来不来，一般和老师确定来的都能给offer，也有不少同学当场放弃，最后优营率可能达 70%以上。值得一提的是，今年<strong>夏令营没有wl</strong>，但在九月底招生办发邮件让放弃的同学回复放弃（不知道有没有同学候补上岸），原来也不打算开预推免（最后还是开了，但方向不同）。</p><blockquote><p>这个阶段需要比较谨慎，因为软微大多数导师不做科研，接的是横向项目，组里并没有能支撑「科研导向」的实力。老师之间差别也比较大，可以直接深入询问老师是否放实习、培养方案、组里毕业去向、横向项目等。如果志在科研或算法岗，建议谨慎选择横向较多的组；如果志在开发岗，今年夏令营选手很可能会失去软微的实习优势；如果志在体制内，无脑冲就完事了。</p></blockquote><h3 id="中科大网安">中科大网安</h3><blockquote><p>这里强烈安利中科大何向南组的 <ahref="https://fulifeng.github.io/">冯福利老师</a>，科研实力非常强，人也非常年轻，沟通起来轻松愉快，给了我很多帮助和鼓励！对<strong>推荐系统、信息检索、自然语言处理</strong>等方向感兴趣的都可以联系。老师在<strong>网安和大数据学院</strong>都有招生名额，通过组里的考核后老师会建议你报名相应的夏令营。</p></blockquote><p>中科大是少有的几所弱 com 学校，同时入营 bar也是华五相对较低的，优营率很高，但是往年都是被鸽惨的学校（地域因素）。</p><p>网安学院无机试无笔试，面试有两轮，<strong>一轮含英语提问，一轮不含</strong>。其他流程一模一样，只是面试官不一样。共享屏幕用PPT自我介绍，抽题号问专业知识，因为我本身没有学过网安知识点，基本都是乱答。<strong>没学过就如实告知</strong>，有的老师会现场出你学过的其他专业课问题，或者翻看简历提问科研。</p><p>优营后需要自己联系导师完成双选，双选后才能向学院确定offer，没有双选则优营不算数。因此<strong>最好提前联系导师</strong>防止名额不够。当时因为已经拿了软微的offer，就私聊老师放弃双选了。</p><h2 id="预推免细节">6. 预推免细节</h2><h3 id="清华软院">清华软院</h3><p>入营 bar较高，和其他院在同一个系统填报，个人推测只能入一个学院，因为我报名的清深材料一直显示未查看。需要两封推荐信，而且要把链接发到推荐老师的邮箱，比较麻烦。推荐报名专硕或直博，留给外校的学硕名额非常少。</p><p>先是机试，外校机试占比 20%，三小时四题，难度 1 Easy，1 Medium，2Hard。题目类型比较常规，跟其他几个院不一样（清深、网研院、贵系是同一套题，据说都是大模拟）。OJ系统，提交后立刻判分，无罚时，可骗分。大部分人应该做出了三题，我比较菜只做出两题多。</p><blockquote><p>值得一提的是，这次的 Easy 居然就是考试前几天 LeetCode的每日一题<strong>原题</strong>，一道 Hard也涉及了每日一题的知识点（虽然我还是不会）。很梦幻。</p></blockquote><p>过几天后面试，占比80%。面试前不知道顺序，需要在腾讯会议等待，如果是序号靠后的同学可能比较煎熬。需要共享屏幕自我介绍，之后常规提问。老师可能会对<strong>软件工程相关</strong>的项目比较感兴趣。</p><h2 id="后记">7. 后记</h2><h3 id="最终去向">最终去向</h3><p>9 月 19日收到了复旦大学打来的候补电话，当天正好是清华除贵系以外的学院开奖，不少大佬选择了清软或清深，于是当即接受。此前一直有在和一位NLP组的梦导保持联系，正好当天看到群里有同学鸽了学硕，于是询问后上岸。</p><p>随后就放弃了软微的offer，因为已经进组学习了一段时间，老师似乎很生气，但最后也无奈放我走了。当初没有offer的时候收到了软微的电话，给考虑的时间不多，一度以为就这样上岸了。我也不喜欢这样的鸽子行为，深感抱歉，只能尽可能降低对老师的影响。</p><p>至此，整个持续好几个月的保研应该完整地结束了，最终的结果（学校、导师、地域、环境）都让我非常满意，也不会和女朋友异地，算是一个圆满的句号。这一路上认识了很多其他本科学校的小伙伴，都非常优秀且人都很有趣，一路上给了我很多帮助，也间接影响了最后的选择。感谢一路帮助过我的本科老师、外校老师、学长学姐、以及历年撰写保研经验的前辈们。最后，感谢女朋友<a href="https://yiwen-ding.github.io/">CindyWen</a>的一路陪伴，宣传一下她的 <ahref="https://yiwen-ding.github.io/Baoyan-essay">保研回忆</a>，未来还请多多指教！</p><h3 id="谨记">谨记</h3><ol type="1"><li>联系导师的最佳时间是「<strong>等到你的简历成型，不会让老师认为你不够优秀</strong>」的时候，越早越好。</li><li>提前规划好刷题、复习时间！不要等到暑假才集中准备，各种材料、系统填报<strong>非常占用时间</strong>。</li><li>复习专业课，与其盲目刷书看课件，不如去<strong>看面经整理常见问题</strong>，然后自己归纳答案。</li><li>线上参营的坏处是大部分 offer 集中在少部分人手里，但是最后 offer都会释放，<strong>每个人都会有学上</strong>！</li><li>富贵险中求！报名人数越多、竞争越激烈的<strong>强组强导，最后越容易被鸽</strong>！优营的大佬大概率会有更好的去向。</li><li>跟老师保持联系时真诚相待，如果<strong>有其他目标院校在报考，最好先跟老师说清楚最后期限</strong>，大部分导师都会理解的。</li><li><strong>警惕有黑历史的学校、导师</strong>，如果不得已选择，最好手中再拿一个保底。</li><li>不可能去的学校就不要报名了，报名时间省下来刷两道题不好吗？<strong>把机会留给其他同学</strong>！</li><li>及时释放手中<strong>不会去</strong>的offer，你的释放可能会让其他同学上岸梦校。</li></ol>]]></content>
    
    
    <categories>
      
      <category>心情随笔</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>算法入门笔记 #2 STL标准库</title>
    <link href="/Algo-Note-2.html"/>
    <url>/Algo-Note-2.html</url>
    
    <content type="html"><![CDATA[<p>STL (Standard Template Library)标准模板库，是一个具有工业强度的，高效的 C++ 程序库。文章开头先附上一个STL 常用模板，方便取用：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;bits/stdc++.h&gt;</span> <span class="hljs-comment">// 万能头文件</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std; <span class="hljs-comment">// 大部分的 STL 保留字位于 std 命名空间</span><br><br><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> pb push_back</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> mp make_pair</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> x first</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> y second  <span class="hljs-comment">// 结合 pair 用</span></span><br><br><span class="hljs-keyword">typedef</span> <span class="hljs-keyword">double</span> db;<br><span class="hljs-keyword">typedef</span> <span class="hljs-keyword">long</span> <span class="hljs-keyword">long</span> ll;<br><span class="hljs-keyword">typedef</span> vector&lt;<span class="hljs-keyword">int</span>&gt; vi;<br><span class="hljs-keyword">typedef</span> pair&lt;<span class="hljs-keyword">int</span>,<span class="hljs-keyword">int</span>&gt; pii; <span class="hljs-comment">// 常用于坐标系</span><br><br>vector&lt;<span class="hljs-keyword">int</span>&gt; a;<br>a.<span class="hljs-built_in">pb</span>(<span class="hljs-number">1</span>);<br><br>vector&lt;pii&gt; p;<br>p.<span class="hljs-built_in">pb</span>(<span class="hljs-built_in">mp</span>(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)); <span class="hljs-comment">// 支持 C++ 11 及以上的平台 mp 可以用 &#123;&#125; 代替</span><br></code></pre></td></tr></table></figure><p>以及一个 ACM 模式的简易模板：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;bits/stdc++.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-keyword">typedef</span> <span class="hljs-keyword">long</span> <span class="hljs-keyword">long</span> ll;<br><br><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> inf = <span class="hljs-number">0x3f3f3f3f</span>;<br><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> mod = <span class="hljs-number">998244353</span>;<br><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> maxn = <span class="hljs-number">1e5</span> + <span class="hljs-number">10</span>;<br><br><span class="hljs-keyword">int</span> n,m,k;<br><span class="hljs-keyword">int</span> dp[maxn][maxn], a[maxn][maxn];<br><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br><br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="algorithm-算法库">Algorithm 算法库</h2><p>头文件 <code>#include&lt;algorithm&gt;</code> 定义了 STL中基础的算法，大部分方法可以在容器中找到对应函数，例如不修改内容的<code>find</code>、<code>count</code> 等操作，修改内容的<code>remove</code>、<code>replace</code>、<code>swap</code>等操作，以及排序、二分查找、两数取最大最小、交换两数等算法。</p><p>这里列出常用的几个操作：</p><ul><li><code>__gcd(1024, 256);</code>：求最大公约数，在 C++ 17 后也可以用<code>gcd(1024, 256);</code>。<ul><li>利用该函数可以实现求最小公倍数：<code>a * b / __gcd(a, b)</code></li></ul></li><li><code>min(&#123;a, b, c, d&#125;)</code>：返回多个数的最值，需要用<strong>大括号</strong>包围（C++11以上）。</li><li><code>sort(a,a+n)</code>：采用快速排序，除了数组指针也支持迭代器<code>sort(v.begin(), v.end())</code>。<ul><li>如果要<strong>自定义比较函数</strong>cmp，遵循以下模板：<code>bool cmp(T &amp;a,T &amp;b)&#123;return a&lt;b;&#125;</code>，在力扣刷题时要在函数前加上<code>static</code>。</li><li>也可以直接用 <strong>Lambda函数</strong>作为比较函数，<code>sort(v.begin(), v.end(), [](auto &amp;a, auto &amp;b)&#123; return a &lt; b;&#125;);</code>这里返回的是「a <strong>必须排在</strong> b 前面的判断条件」。</li><li>默认是升序排列，如果要降序排列则用<code>sort(v.begin(), v.end(), greater&lt;int&gt;());</code></li></ul></li><li><code>stable_sort(a,a+n)</code>：采用归并排序，<strong>不改变相等元素的相对位置</strong>，用法和前者相同。</li><li><code>is_sorted(v.begin(), v.end())</code>：判断容器内部是否有序，返回布尔值。</li><li><code>reverse(v.begin(),v.end())</code>：翻转 vector容器，也可以用于 string 字符串的翻转。</li><li><code>lower_bound(a,a+n,x)</code>：返回<strong>升序数组</strong>中可以插入元素x 的<strong>最低位置</strong>，也就是<strong>大于等于</strong> x的第一个数的地址；</li><li><code>upper_bound(a,a+n,x)</code>：返回<strong>升序数组</strong>中可以插入元素x 的<strong>最高位置</strong>，也就是<strong>大于</strong> x的第一个数的地址；<ul><li>简记：假设有一个数组 <code>1 1 2 2 2 3 3</code>，如果 x 选2，则可以插在第一个 2 的位置，也可以插在第一个 3的位置，此时其他元素后移，不会改变升序。</li><li>将得到地址减<strong>去数组的起始地址</strong>就可以得到下标：<code>pos = lower_bound(a,a+n,x) - a;</code></li><li>此方法在前两个参数构成的<strong>前闭后开</strong>区间内查找，如果<strong>不存在</strong>满足的元素（x比所有元素都大）则返回<code>end()</code>，如果数组是降序的，则需要加上参数<code>greater&lt;int&gt;()</code>。</li><li>也可以自定义 <strong>Lambda函数</strong>作为比较函数，需要注意第一个项是<strong>目标的类型</strong>，第二个项才是<strong>序列的类型</strong>：<code>upper_bound(matrix.begin(), matrix.end(), target, [](const int b, const vector&lt;int&gt; &amp;a) &#123;return b &lt; a[0]; &#125;);</code></li></ul></li><li><code>binary_search(a,a+n,x)</code>：二分查找升序数组中是否存在元素x，返回<strong>布尔值</strong>。</li><li><code>auto [l, r] = equal_range(v.begin(),v.end(),x)</code>：返回升序数组中元素x 出现的第一个位置和最后一个位置 +1，其实就是封装了上述两个二分查找函数。<ul><li>如果返回值 <code>l==r</code>，说明元素 x不存在，二者都<strong>指向了第一个大于 x</strong> 的位置。</li></ul></li><li><code>nth_element(first，kth，end)</code>：将<strong>第 k小</strong>元素放到它该放的位置上，左边元素都小于等于它，右边元素都大于等于它。这是一个<strong>原地算法</strong>，会改变原数组，复杂度<span class="math inline">\(O(n)\)</span>。<ul><li>除了数组指针也支持迭代器<code>nth_element(v.begin(), v.begin() + k, v.end())</code>。</li><li>如果要选择<strong>第 k 大</strong>元素，可以使用<code>nth_element(v.begin(), v.begin() + k, v.end(), greater&lt;int&gt;())</code>。</li></ul></li><li><code>max_element(a,a+n)</code>：遍历并返回数组最大值的位置，可以直接<code>*max_element()</code> 获取最大值。</li><li><code>next_permutation(a,a+n)</code>、<code>next_permutation(v.begin(), v.end())</code>：原地算法，改变原数组、向量、字符串为按字典序的全排列中的下一排列，返回bool 值为 0 时表示结束。</li><li><code>accumulate(a,a+n,0)</code>、<code>accumulate(v.begin(), v.end(), 0)</code>：用加法运算符求出元素序列的和，第三个参数是和的初值。该方法也可以自定义加法运算符，作为第四个参数输入。<ul><li><code>accumulate()</code> 返回值的类型由第三个数确定，如果输入<code>0</code> 则最大值为<code>INT_MAX</code>。在某些场景可能会溢出，需要改成<code>0LL</code>。</li></ul></li><li><code>unique(a,a+n)</code>：去除数组中<strong>相邻的重复</strong>元素（配合排序使用），返回去重后的尾地址。这里的去除并非真正意义的erase，而是将重复的元素放到容器的末尾。<ul><li>将尾地址<strong>减去数组的起始地址</strong>就能得到去重后的个数：<code>n = unique(a,a+n) - a;</code>，然后遍历。</li><li>结合向量的批量删除操作<strong>真正去掉重复</strong>：<code>v.erase(unique(v.begin(), v.end()), v.end());</code>，之后用<code>n = v.size()</code> 就能得到去重后的个数。</li></ul></li></ul><h2 id="字符串-string">字符串 | string</h2><p>string 是 C++ 特有的字符串变量类型。</p><h3 id="基础操作">基础操作</h3><ul><li>头文件 <code>#include&lt;string&gt;</code></li><li>声明 string 变量：<ul><li>直接声明并等号赋值：<code>string str="12345678";</code></li><li>声明一个副本：<code>string s(str);</code></li><li>声明一个字符串数组的复制品：<code>char ch[]="12345678"; string s(ch);</code></li><li>利用迭代器复制区间：<code>string s(str.begin(),str.end()-2);</code></li><li>将 int 变量转string：<code>string strNum=to_string(intNum);</code></li></ul></li><li>运算符（重载后）：<ul><li>比较运算符 <code>==  &gt;  &lt;  &gt;=  &lt;=  !=</code> 用法参考<code>strcmp</code></li><li>加法运算符 <code>+  +=</code> 用于连接两个字符串</li><li>下标运算符 <code>[]</code> 用于获取特定位置</li></ul></li><li>特性函数：<ul><li>返回当前容量，即不必挪动就能存放的字符数：<code>s.capacity();</code></li><li>返回经过挪动后能存放的最大容量：<code>s.max_size();</code></li><li>返回当前在内存空间中的大小（字节数），不计终止符：<code>s.size();</code>或 <code>s.length();</code></li><li>判断当前字符串是否为空：<code>s.empty();</code></li><li>返回当前 string对应的<strong>字符数组</strong>的头指针：<code>printf("%s", s.c_str());</code></li></ul></li><li>查找运算：<ul><li>返回 str 在 s中第一次出现的位置（<strong>整型下标值</strong>），没找到就返回<code>s.npos</code> ：<code>s.find(str);</code></li><li>同上，从下标为 index 处开始查找：<code>s.find(str,idx);</code></li><li>同上，查找对象换成字符：<code>s.find('x');</code></li><li>返回 str 在 s中最后一次出现的位置：<code>s.find_last_of(str);</code></li></ul></li><li>其他运算：<ul><li>在下标 p位置插入，原有的后移，但不能插在结束符后的空间：<code>s.insert(p, "hello");</code></li><li>在末尾插入一个字符：<code>s.push_back('a');</code>（和 vector很相似）</li><li>删除从<strong>下标 i开始到末尾</strong>的所有字符：<code>s.erase(i);</code></li><li>删除从<strong>下标 i 开始连续 n个字符</strong>：<code>s.erase(i, n);</code></li><li>交换当前字符串与 str 的值：<code>s.swap(str);</code></li><li>返回从<strong>下标 i开始到末尾</strong>的子串：<code>s.substr(i)</code></li><li>返回从<strong>下标 i 开始连续 n个字符</strong>的子串：<code>s.substr(i,n)</code>，如果超过总长度则输出到末尾的子串</li><li>将 string 变量转 int：<code>int num = stoi(s);</code>注意该方法<strong>不进行溢出判断</strong>，处理<strong>整数翻转</strong>时容易溢出！</li><li>将 string 变量转 long long：<code>ll num = stoll(s);</code>该方法同样不进行溢出判断，但范围更广。</li></ul></li></ul><h3 id="特殊性质">特殊性质</h3><ul><li>string拥有一个特殊的输入输出流库：<code>#include&lt;sstream&gt;</code>，可以将任意类型的变量输出到流中，再以字符串的形式读出。如int 转 string 操作：</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-meta">#inlcude<span class="hljs-meta-string">&lt;sstream&gt;</span></span><br>stringstream ss;<br>ss &lt;&lt; intNum;<br>string strNum = ss.<span class="hljs-built_in">str</span>(); <span class="hljs-comment">//.str()是&lt;sstream&gt;库中的函数</span><br></code></pre></td></tr></table></figure><ul><li>由于 string也是容器，因此支持迭代器和下标两种访问操作，通常用下标方便处理更复杂的输出。</li></ul><h3 id="易混淆的库">易混淆的库</h3><ul><li>cstring：与 C 兼容的字符串处理库，使用字符串数组、指针操作<ul><li><code>strcpy(a,b)</code>：将 b 字符串拷贝到 a 处，遇到<code>'\0'</code> 停止，可能溢出</li><li><code>strcat(a,b)</code>： 将 b 字符串连接到 a 处，可能溢出</li><li><code>strcmp(a,b)</code>：比较 a,b 字符串，直到遇到不相同的字符或者<code>'\0'</code>，都相同返回 0，首个不同字符 a&lt;b则返回负数，否则返回正数</li><li><code>strlen(a)</code>：返回 a 字符串的长度，不含<code>'\0'</code></li><li><code>strstr(a,b)</code>：在 a 中查找 b字符串，返回第一次出现位置的指针</li><li><code>memset(a,'x',n)</code>：将 a指向的内存空间，逐字节地赋值为字符 x，此处也可替换成 0-255的十进制或十六进制数字</li><li><code>memcpy(a,b,n)</code>：将 b 指向的内存空间拷贝 n 个字节到a，可能溢出</li><li><code>memcmp(a,b,n)</code>：比较两个内存空间的前 n 个字节</li></ul></li><li>cctype：用于字符类型的判别与处理<ul><li><code>isalnum()</code>：如果参数是字母数字，即字母或者数字，函数返回true</li><li><code>isalpha()</code>：如果参数是字母，函数返回 true</li><li><code>isdigit()</code>：如果参数是数字（0－9），函数返回 true</li><li><code>isgraph()</code>：如果参数是除空格之外的打印字符，函数返回true</li><li><code>islower()</code>：如果参数是小写字母，函数返回 true</li><li><code>isprint()</code>：如果参数是打印字符（包括空格），函数返回true</li><li><code>isupper()</code>：如果参数是大写字母，函数返回 true</li><li><code>isxdigit()</code>：如果参数是十六进制数字，即0－9、a－f、A－F，函数返回 true</li><li><code>tolower()</code>：如果参数是大写字符，返回其小写，否则返回该参数</li><li><code>toupper()</code>：如果参数是小写字符，返回其大写，否则返回该参数</li></ul></li></ul><h2 id="向量-vector">向量 | vector</h2><p>vector是一个能够存放任意类型的<strong>动态数组</strong>，能够增加和压缩数据，是<strong>同一种类型</strong>的对象的集合，每个对象都有一个对应的整数索引值。</p><p>向量中的元素按照<strong>严格的线性顺序排序</strong>，可以通过元素在序列中的位置访问对应的元素。使用一个内存分配器对象来动态地处理它的存储需求。</p><h3 id="基础操作-1">基础操作</h3><ul><li><p>头文件 <code>#include&lt;vector&gt;</code></p></li><li><p>创建 vector 对象：</p><ul><li>直接创建：<code>vector&lt;int&gt; v;</code>（实际中通常会<code>typedef vector&lt;int&gt; vi;</code>）</li><li>直接创建并初始化（类似数组）：<code>vector&lt;int&gt; v=&#123;1,2,3&#125;;</code></li><li>拷贝一个<strong>副本</strong>：<code>vector&lt;int&gt; v_b(v_a);</code>，深拷贝，两个向量都会保留</li><li><strong>迁移</strong>对象：<code>vector&lt;int&gt; v_b = move(v_a)</code>，速度最快，只操作指针，但会导致<code>v_a</code> 无法访问</li><li>创建含有 <code>n</code> 个元素 <code>a</code>的对象：<code>vector&lt;int&gt; v(n,a);</code></li><li>创建含有 <code>n</code> 个元素且全 0的对象：<code>vector&lt;int&gt; v(n);</code></li><li>创建二维对象：<code>vector&lt;vector&lt;int&gt;&gt; dp(n, vector&lt;int&gt;(n));</code></li></ul></li><li><p>插入、删除元素：</p><ul><li>尾部插入元素：<code>v.push_back(a);</code></li><li>尾部生成元素：<code>v.emplace_back(a);</code>，效率比前者更高。</li><li>尾部删除元素：<code>v.pop_back();</code>，无返回值。</li><li>任意位置插入元素：<code>v.insert(v.begin()+i, a)</code>，在下标 i的元素前面插入 a。</li><li>删除元素：<code>v.erase(v.begin()+2)</code>，删除下标 2的元素。</li><li>批量删除：<code>v.erase(v.begin()+i, v.begin()+j)</code>，删除左闭右开区间。</li></ul></li><li><p>特性函数：</p><ul><li>向量大小：<code>v.size()</code></li><li>内存中向量能包含的最大元素个数：<code>v.max_size()</code></li><li>清空：<code>v.clear();</code></li><li>判断空：<code>v.empty()</code></li></ul></li><li><p>访问元素：</p><ul><li><p>随机访问成员：<code>v.at(i)</code>，返回元素的引用</p></li><li><p>数组运算符：<code>v[i]</code>，返回元素的引用</p></li><li><p>特定元素访问：<code>v.front()</code> 和 <code>v.back()</code>返回第一个和最后一个元素的引用</p></li><li><p>迭代访问：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 声明迭代器，此时的 it 类似指针</span><br>vector&lt;<span class="hljs-keyword">int</span>&gt;::iterator it; <span class="hljs-comment">// 这里可以换成 auto</span><br><span class="hljs-keyword">for</span>(it=vec.<span class="hljs-built_in">begin</span>(); it!=vec.<span class="hljs-built_in">end</span>(); it++)<br>    cout&lt;&lt;*it&lt;&lt;endl;  <span class="hljs-comment">// 访问指针指向的元素，有 *</span><br><br><span class="hljs-comment">// C++11 新语法，此时的 item 为元素本身</span><br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> item:vec)<br>    cout&lt;&lt;item&lt;&lt;endl;  <span class="hljs-comment">// 不需要 *，且此时修改不会影响原数组（深拷贝）</span><br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> &amp;item:vec)<br>    item++;    <span class="hljs-comment">// 如果带有 &amp; 就可以修改元素（浅拷贝）</span><br><br><span class="hljs-comment">// 不用迭代器直接用下标索引遍历</span><br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; vec.<span class="hljs-built_in">size</span>(); i++)<br>    cout&lt;&lt;vec[i]&lt;&lt;endl;<br></code></pre></td></tr></table></figure></li></ul></li></ul><h3 id="特殊性质-1">特殊性质</h3><ul><li><code>v.begin()</code> 与 <code>v.end()</code>返回的是<strong>指针</strong>，指向第一个元素和最后一个元素的<strong>下一个位置</strong>（无意义），只能赋值给迭代器。<ul><li>与普通指针相同，<code>*(v.begin()+i)</code> 可以访问下标 i的元素。</li><li>与普通数组相同，使用 sort 排序时，必须要用<code>sort(v.begin(),v.end());</code></li></ul></li><li><code>v.front()</code> 与 <code>v.back()</code>返回的是元素的<strong>引用</strong>，可以赋值给<strong>别名变量</strong>（浅拷贝）或普通变量（深拷贝）。<ul><li>引用是 C++ 特有的语法，声明引用变量<code>int &amp;a=v.front();</code> 时，共享同一内存单元。</li><li>类似的还有随机访问 <code>v.at(i)</code> 和数组运算符<code>v[i]</code>，也可以赋值给别名变量或普通变量。</li></ul></li><li>vector 作为函数的参数或者返回值时，必须用 <code>&amp;</code><strong>传引用调用</strong>：<ul><li><code>vector&lt;int&gt;&amp; Func(vector&lt;int&gt; &amp;v)&#123;return v;&#125;</code></li><li>注意<strong>普通数组传引用</strong>调用时需要<strong>带个数</strong>：<code>(int (&amp;a)[10])</code>，传指针不需要：<code>(int *a)</code>或 <code>(int a[])</code></li></ul></li><li>能够存放任意类型，意味着可以嵌套其他数据结构：<ul><li>定义二维动态数组：<code>vector&lt;vector&lt;int&gt;&gt; v;</code></li><li>定义静态数组内一维动态数组：<code>vector&lt;int&gt; a[100];</code>，可用于邻接表。</li><li>定义结构体数组：<code>vector&lt;Student&gt; v;</code>，结构体需要全局定义。</li><li>两个存放相同类型元素的向量可以使用<strong>比较运算符</strong>，依据字典序逐个比较。</li></ul></li><li>vector 的 push_back() 代价虽然是均摊的O(1)，但是当数据量大的时候会很慢。所以如果需要使用的话，可以用<code>vector&lt;int&gt; v(n);</code> 初始化再赋值。</li><li>批量赋<strong>递增值</strong>：<code>iota(v.begin(), v.end(), 0);</code>，其中<code>0</code> 代表首个元素值，赋值后的向量元素为 <spanclass="math inline">\(\{0, 1,2,\cdots\}\)</span>。常用于<strong>不改变原数组顺序的伪排序</strong>。<ul><li><code>sort(idx.begin(), idx.end(), [&amp;](int i, int j) &#123; return nums[i] &lt; nums[j]; &#125;);</code></li></ul></li><li>在 Class 中初始化<strong>成员变量</strong>定长 vector 时，如果用<code>vector&lt;int&gt; v(5);</code>会报错，编译器无法识别语句是成员变量声明还是成员函数声明。必须用<strong>赋值构造</strong>函数<code>vector&lt;int&gt; v = vector&lt;int&gt; (10005, 0);</code>。<ul><li>如果想要动态设置数组节省空间，且不用到 <code>malloc</code>分配，可以先在 Class 中直接创建<code>vector&lt;int&gt; v;</code>，再到函数内<strong>重设</strong>大小<code>v.resize(nums.size())</code>。</li><li>最好的办法是使用<strong>全局静态变量</strong>声明数组，<code>static const int N = 1e5 + 6; int a[N];</code>，这句话如果放到Class 里面则数组是随机初始化，必须<code>memset(a, 0, sizeof(a));</code>。</li></ul></li></ul><h2 id="配对-pair">配对 | pair</h2><p>pair可以将<strong>两个任意类型</strong>的元素绑定成一组元素，其内部实现就是一个<code>template&lt;class T1,class T2&gt;</code>的结构体。可以用来组成更高级的映射map，也可以用来表示<strong>坐标</strong>等双元素的结构体。当一个函数需要返回两个数据的时候，可以选择pair。</p><h3 id="基础操作-2">基础操作</h3><ul><li>头文件 <code>#include&lt;utility&gt;</code></li><li>创建 pair 对象：<ul><li>直接创建：<code>pair&lt;int,int&gt; p;</code>（实际中通常会<code>typedef pair&lt;int,int&gt; pii;</code>）</li><li>创建并赋值：<code>pair&lt;int,int&gt; p(3,4);</code></li><li>使用 C++括号运算符赋值：<code>pair&lt;int,int&gt; p = &#123;3，4&#125;;</code></li><li>先创建后赋值：<code>p = make_pair(3,4);</code></li></ul></li><li>访问对象：<ul><li>等价于结构体变量：<code>cout &lt;&lt; p.first &lt;&lt; ' ' &lt;&lt; p.second;</code>（注意不是指针）</li></ul></li><li>嵌套：<ul><li>三元pair：<code>pair&lt;int, pair&lt;int, int&gt;&gt; p(1,&#123;2,3&#125;);</code></li><li>其他容器：<code>vector&lt;pair&lt;int, int&gt;&gt;</code>（实际中通常会<code>vector&lt;pii&gt; v</code>）</li></ul></li></ul><h3 id="特殊性质-2">特殊性质</h3><ul><li><p>在某些情况函数想要返回两个数据时，可以将 pair对象作为返回值，此时函数外接收的对象可以是 pair，也可以直接通过 std::tie进行接收：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function">pair&lt;string, <span class="hljs-keyword">int</span>&gt; <span class="hljs-title">getPreson</span><span class="hljs-params">()</span> </span>&#123;<br><span class="hljs-keyword">return</span> <span class="hljs-built_in">make_pair</span>(<span class="hljs-string">&quot;Steve&quot;</span>, <span class="hljs-number">25</span>);<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>string name; <span class="hljs-keyword">int</span> ages;<br><span class="hljs-built_in">tie</span>(name, ages) = <span class="hljs-built_in">getPreson</span>(); <span class="hljs-comment">// 类似 Python 中的元组解包</span><br>cout &lt;&lt; <span class="hljs-string">&quot;name: &quot;</span> &lt;&lt; name &lt;&lt; <span class="hljs-string">&quot;, ages: &quot;</span> &lt;&lt; ages &lt;&lt; endl;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure></li></ul><h2 id="元组-tuple">元组 | tuple</h2><p>C++11 引入的 tuple可以将<strong>多个任意类型</strong>的元素绑定成一组元素，是泛化的pair。通常将其当作一个简易的结构体使用，避免复杂的声明，用法与 pair非常类似。</p><h3 id="基础操作-3">基础操作</h3><ul><li>头文件 <code>#include&lt;tuple&gt;</code></li><li>创建 tuple 对象：<ul><li>直接创建：<code>tuple&lt;int,int,int&gt; tup;</code></li><li>创建并赋值：<code>tuple&lt;int,int,int&gt; tup(1, 2, 3);</code></li><li>创建并使用函数赋值：<code>tuple&lt;int,int,int&gt;tup = make_tuple(1, 2, 3);</code>和 pair 不同，这里的<strong>创建步骤不能省略</strong>！</li></ul></li><li>访问对象（注意<strong>不要漏掉括号</strong>）：<ul><li>get函数：<code>cout &lt;&lt; get&lt;0&gt;(tup) &lt;&lt; ' '&lt;&lt; get&lt;1&gt;(tup);</code>，返回的是元素的引用，因此可以修改</li></ul></li><li>元组解包：<ul><li>主动声明变量：<code>tie(a, b, c) = tup;</code></li><li>自动声明变量：<code>auto&amp; [a, b, c] = tup;</code>如果某个元素不需要用到，可以用 <code>_</code> 代替</li></ul></li><li>嵌套到其他容器（以向量为例）：<ul><li>声明：<code>vector&lt;tuple&lt;int, int, int&gt;&gt; tups;</code></li><li>插入：<code>tups.push_back(make_tuple(1, 2, 3))</code> 或<code>tups.emplace_back(1, 2, 3);</code></li></ul></li></ul><h2 id="映射-map">映射 | map</h2><p>map是一个存放<strong>一对一映射</strong>（pair）的关联容器，存储的<strong>关键字和值</strong>可以定义为任意类型，各个键值对的<strong>键互不相同且不允许被修改</strong>，但值可以相同。</p><p>map的内部实现为<strong>红黑树</strong>（弱平衡二叉树），是二叉搜索树的升级版，具有对数据进行<strong>排序</strong>的功能。因此我们可以认为map 内部所有键值对都是按 key 排序的，key必须为<strong>可排序的类型</strong>（包括自定义类型）。</p><p>需要强调的是，map 中对元素增删改查的时间复杂度都是 <spanclass="math inline">\(O(\log n)\)</span>，但使用迭代器遍历 map的复杂度是 <span class="math inline">\(O(n)\)</span>。</p><h3 id="基础操作-4">基础操作</h3><ul><li><p>头文件 <code>#include&lt;map&gt;</code></p></li><li><p>创建 map 对象：</p><ul><li>直接创建：<code>map&lt;string,int&gt; mp;</code></li><li>创建并初始化：<code>map&lt;string,int&gt; mp=&#123;&#123;"A", 10&#125;, &#123;"B", 20&#125;, &#123;"C", 30&#125;&#125;;</code></li></ul></li><li><p>插入元素：</p><ul><li>用 insert 函数插入pair：<code>mp.insert(pair&lt;string, int&gt;("hw", 2001));</code>通常会结合 typedef 简化；</li><li>前者也可以用 make_pair替换：<code>mp.insert(make_pair("hw", 2001));</code></li><li>C++ 11标准支持<strong>花括号初始化</strong>：<code>mp.insert(&#123;"hw", 2001&#125;);</code></li><li>用 insert 函数插入 value_type数据：<code>mp.insert(map&lt;string, int&gt;::value_type("hw", 2001));</code></li><li>用数组运算符<strong>访问并插入</strong>（最常用）：<code>mp[“hw”]=2001;</code>，此时<strong>不受唯一性限制</strong>，可以覆盖已有的键值对；</li></ul></li><li><p>删除元素（erase 函数）：</p><ul><li>删除迭代器指向元素：<code>auto it=mp.find("hw"); mp.erase(it);</code></li><li>删除关键字：<code>bool flag = mp.erase("hw");</code>如果找到并删除则返回 1，否则返回 0</li><li>删除迭代器指向区间元素：<code>mp.erase(++mp.begin(), mp.end());</code></li></ul></li><li><p>特性函数：</p><ul><li>键值对数目：<code>mp.size()</code></li><li>清空：<code>mp.clear();</code></li><li>判断空：<code>mp.empty()</code></li></ul></li><li><p>访问元素：</p><ul><li><p>随机访问成员：<code>mp.at("hw")</code>，返回元素的引用，如果访问到<strong>未知元素会抛出异常</strong>。</p></li><li><p>数组运算符访问：<code>cout &lt;&lt; mp["hw”];</code>，返回元素的引用，如果访问到未知元素会返回一个<strong>全零的空值</strong>！</p></li><li><p>查找元素：<code>auto it=mp.find("hw");</code>返回指向元素的迭代器，如果没找到，则返回<code>mp.end()</code>；此外，如果想<strong>判断一个元素是否存在</strong>，可以直接<code>mp.count("hw") != 0</code>，该值在 map 和 set 中只能为 0 或1。</p></li><li><p>迭代访问：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 声明迭代器，此时的 it 类似结构体指针</span><br>map&lt;string,<span class="hljs-keyword">int</span>&gt;::iterator it; <span class="hljs-comment">// 这里可以换成 auto</span><br><span class="hljs-keyword">for</span>(it=mp.<span class="hljs-built_in">begin</span>(); it!=mp.<span class="hljs-built_in">end</span>(); it++)<br>    cout&lt;&lt;it-&gt;first&lt;&lt;<span class="hljs-string">&#x27; &#x27;</span>&lt;&lt;it-&gt;second&lt;&lt;endl; <span class="hljs-comment">// 不能直接 * 访问</span><br><br><span class="hljs-comment">// C++11 新语法，此时的 item 类似结构体</span><br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> &amp;item: mp)<br>    cout&lt;&lt;item.first&lt;&lt;<span class="hljs-string">&#x27; &#x27;</span>&lt;&lt;item.second&lt;&lt;endl; <span class="hljs-comment">// 结构体直接用 . 访问属性</span><br><br><span class="hljs-comment">// 反向迭代访问</span><br>map&lt;string,<span class="hljs-keyword">int</span>&gt;::reverse_iterator it;<br><span class="hljs-keyword">for</span>(it = mp.<span class="hljs-built_in">rbegin</span>(); it != mp.<span class="hljs-built_in">rend</span>(); it++)<br>    cout&lt;&lt;it-&gt;first&lt;&lt;<span class="hljs-string">&#x27; &#x27;</span>&lt;&lt;it-&gt;second&lt;&lt;endl;<br></code></pre></td></tr></table></figure></li></ul></li></ul><h3 id="特殊性质-3">特殊性质</h3><ul><li>map、set、multimap、multiset 的迭代器是没有加减法的，仅支持自增<code>it++</code>、自减 <code>it--</code> 的操作，不支持<code>it+1</code>、<code>mp.begin()+1</code>操作。这是因为这些容器采用了特殊的数据结构，没有「<strong>两个元素之距离</strong>」的概念。</li><li>采用迭代器遍历 map、set 的复杂度是 <spanclass="math inline">\(O(n)\)</span>，这是因为二叉树的遍历是 <spanclass="math inline">\(O(E)\)</span>，每一条边只会被自上而下、自下而上各访问一次。</li><li>由于内部有序，map 和 set 支持 <code>mp.lower_bound(key)</code>、<code>mp.upper_bound(key)</code>、<code>equal_range(key)</code>运算，返回指向特定结构体的迭代器指针。但是考虑到 map 和 set中都不会有重复的 key，此方法在 multimap、multiset 更常用。</li><li>map 和 set 的插入删除，并不会使已经赋值的 iterator失效，这是因为插入删除<strong>不会改变内部的树结构</strong>，不需要进行内存拷贝和移动；但是对于vector 而言，每次插入删除<strong>都可能使其失效</strong>，即使是调用push_back的尾部插入，除了因为连续存放导致的内存平移，还可能涉及到容量倍增等操作。牢记一个原则：<strong>不要使用过期的迭代器</strong>。</li><li>map 的关键字可以是 pair，例如<code>map&lt;pair&lt;int, int&gt;, int&gt; m</code>，但是 unordered_map却不能使用 pair 当关键字，因为 C++ 没有给 pair的哈希函数，强行使用会报错。</li></ul><h3 id="哈希表-unordered_map">哈希表 | unordered_map</h3><p>如果只是需要一个映射关系，而不需要其有序，可以用 unordered_map。和map 容器相似，unordered_map同样以<strong>键值对</strong>（pair）的形式存储数据，存储的各个键值对的<strong>键互不相同且不允许被修改</strong>。</p><p>无序映射的底层采用<strong>哈希表</strong>存储结构，根据键的 hash值来判断元素是否相同，不具有对数据的排序功能，但可以实现 <spanclass="math inline">\(O(1)\)</span> 查找、插入。由于无序，不支持<code>lower_bound()</code> 和 <code>upper_bound()</code> 等方法。</p><p>常用的操作与 map 类似：</p><ul><li><p>头文件 <code>#include&lt;unordered_map&gt;</code></p></li><li><p>创建 unordered_map对象：<code>unordered_map&lt;string,int&gt; hash;</code></p></li><li><p><span class="math inline">\(O(1)\)</span>插入元素（最常用）：<code>hash["ABC"]=5;</code></p></li><li><p><span class="math inline">\(O(1)\)</span>查询元素（最常用）：<code>int n=hash["ABC"];</code>或先查再取（可以避免访问不到的时候自动新建默认值）：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">auto</span> it = hash.<span class="hljs-built_in">find</span>(value);<br><span class="hljs-keyword">if</span> (it == hash.<span class="hljs-built_in">end</span>()) <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br><span class="hljs-keyword">auto</span>&amp; a = it-&gt;second;<br></code></pre></td></tr></table></figure></li><li><p>判断关键字是否存在：<code>hash.count("ABC") != 0</code> 或<code>hash.find("ABC") != hash.end()</code>，前者更为常用。</p></li><li><p>迭代访问（较少用）：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 声明迭代器，此时的 it 为结构体指针</span><br>unordered_map&lt;string,<span class="hljs-keyword">int</span>&gt;::iterator it;<br><span class="hljs-keyword">for</span>(it=hash.<span class="hljs-built_in">begin</span>(); it!=hash.<span class="hljs-built_in">end</span>(); it++)<br>    cout&lt;&lt;it-&gt;first&lt;&lt;<span class="hljs-string">&#x27; &#x27;</span>&lt;&lt;it-&gt;second&lt;&lt;endl;<br><br><span class="hljs-comment">// C++11 新语法，此时的 item 为结构体</span><br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> item: hash)<br>    cout&lt;&lt;item.first&lt;&lt;<span class="hljs-string">&#x27; &#x27;</span>&lt;&lt;item.second&lt;&lt;endl;<br></code></pre></td></tr></table></figure></li></ul><p>如果想让自定义的 class 作为 key 来使用unordered_map，则还自行需要实现：重载<strong>哈希函数</strong>、判断两个class 变量是否相等的函数（重载<strong>等价运算符</strong>）。</p><p>关于重载哈希函数，有一个 CF 上的经典案例，由于 unordered_map常数较大，且 hash 具有规律，有的人就专门根据 STL 的源码来造 hack数据，导致单次复杂度退化为 <spanclass="math inline">\(O(n)\)</span>。为此，可以重载哈希函数，让其与时间戳有关，就无法被hack 了。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">custom_hash</span> &#123;</span><br><span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">uint64_t</span> <span class="hljs-title">splitmix64</span><span class="hljs-params">(<span class="hljs-keyword">uint64_t</span> x)</span> </span>&#123;<br>x ^= x &lt;&lt; <span class="hljs-number">13</span>;<br>x ^= x &gt;&gt; <span class="hljs-number">7</span>;<br>x ^= x &lt;&lt; <span class="hljs-number">17</span>;<br><span class="hljs-keyword">return</span> x; <br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">size_t</span> <span class="hljs-title">operator</span> <span class="hljs-params">()</span> <span class="hljs-params">(<span class="hljs-keyword">uint64_t</span> x)</span> <span class="hljs-keyword">const</span> </span>&#123;<br><span class="hljs-keyword">static</span> <span class="hljs-keyword">const</span> <span class="hljs-keyword">uint64_t</span> FIXED_RANDOM = chrono::steady_clock::<span class="hljs-built_in">now</span>().<span class="hljs-built_in">time_since_epoch</span>().<span class="hljs-built_in">count</span>(); <span class="hljs-comment">// 时间戳</span><br><span class="hljs-keyword">return</span> <span class="hljs-built_in">splitmix64</span>(x + FIXED_RANDOM);<br>&#125;<br>&#125;;<br>unordered_map&lt;<span class="hljs-keyword">uint64_t</span>, <span class="hljs-keyword">int</span>, custom_hash&gt; safe_map;<br></code></pre></td></tr></table></figure><blockquote><p>哈希集合 unordered_set 有时也会用到，方法类似，不再单独介绍。</p></blockquote><h3 id="multimap">multimap</h3><p>具有和 map 相同的诸多特性，最主要的区别在于，multimap容器中可以同时存储<strong>多个键相同的键值对</strong>。</p><p>和 map 相比，multimap 未提供 <code>at()</code> 成员方法，也没有重载<code>[]</code> 运算符。这意味着，map容器中通过指定键获取键值对的方式，将不再适用于 multimap容器。其实这很好理解，因为 multimap容器中指定的键可能对应多个键值对，而不再是 1 个。</p><p>值的一提的是，由于 multimap 可存储多个具有相同键的键值对，因此<code>lower_bound()</code>、<code>upper_bound()</code>、<code>equal_range()</code>以及 <code>count()</code> 方法会经常用到。</p><h2 id="集合-set">集合 | set</h2><p>set是一个存放同一类型元素的<strong>集合</strong>容器，满足数学定义上集合的<strong>互异性</strong>——即set中每个元素只能出现一次。其内部实现也为红黑树，因此能根据元素的值自动进行排序。</p><p>set 中对元素增删改查的时间复杂度都是 <spanclass="math inline">\(O(\log n)\)</span>，但使用迭代器遍历 set的复杂度是 <span class="math inline">\(O(n)\)</span>。</p><h3 id="基础操作-5">基础操作</h3><ul><li><p>头文件 <code>#include&lt;set&gt;</code></p></li><li><p>创建 set 对象：</p><ul><li>直接创建：<code>set&lt;int&gt; st;</code></li><li>创建并初始化：<code>set&lt;int&gt; st=&#123;1, 2, 3, 4&#125;;</code></li><li>创建并拷贝：<code>set&lt;int&gt; new_st(st);</code></li></ul></li><li><p>插入、删除元素：</p><ul><li>插入一个元素：<code>st.insert(3);</code></li><li>删除一个元素：<code>st.erase(3);</code></li><li>删除迭代器指向元素：<code>auto it=st.find(3); st.erase(it);</code></li><li>删除区间内的元素：<code>st.erase(++st.begin(), st.end());</code></li></ul></li><li><p>特性函数：</p><ul><li>元素个数：<code>st.size();</code></li><li>清空集合：<code>st.clear();</code></li><li>判断空：<code>st.empty();</code></li></ul></li><li><p>访问元素：</p><ul><li><p>查找元素：<code>auto it=st.find("hw");</code>返回指向元素的迭代器，如果没找到，则返回<code>st.end()</code>；此外，如果想<strong>判断一个元素是否存在</strong>，可以直接<code>st.count("hw") != 0</code>，该值在 map 和 set 中只能为 0 或1。</p></li><li><p>有序性：<code>st.lower_bound(2)</code>、<code>st.upper_bound(2)</code>，返回指向特定结构体的迭代器指针。</p></li><li><p>迭代访问：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 声明迭代器，此时的 it 类似指针</span><br><span class="hljs-comment">// 注意由于 set 不是键值对类型，因此不能像 map 一样用 it-</span><br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> it=st.<span class="hljs-built_in">begin</span>(); it!=st.<span class="hljs-built_in">end</span>(); it++)<br>    cout&lt;&lt;*it&lt;&lt;endl; <span class="hljs-comment">// 访问指针指向的元素，有 *</span><br><br><span class="hljs-comment">// C++11 新语法，此时的 item 为元素本身</span><br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> &amp;item: st)<br>    cout&lt;&lt;item&lt;&lt;endl; <span class="hljs-comment">// 不需要 *</span><br></code></pre></td></tr></table></figure></li></ul></li></ul><h3 id="特殊性质-4">特殊性质</h3><ul><li><p>map 和 set 本身都是以升序排列，但是对于 set而言有时候会改变其排序的方式，也可能引入结构体，因此需要用到自定义比较函数：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 元素不是结构体，重载 () 运算符</span><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">cmp</span>&#123;</span><br><span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">operator</span><span class="hljs-params">()</span><span class="hljs-params">(<span class="hljs-keyword">const</span> T &amp;a, <span class="hljs-keyword">const</span> T &amp;b)</span></span>&#123;<br><span class="hljs-keyword">return</span> a.data &gt; b.data;<br>    &#125;<br>&#125;<br>set&lt;T, cmp&gt; st;<br><br><span class="hljs-comment">// 元素是结构体，直接将比较函数写在结构体内</span><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">Info</span>&#123;</span><br>string name;<br><span class="hljs-keyword">float</span> score;<br><span class="hljs-comment">// 重载 &lt; 操作符，自定义排序规则</span><br>    <span class="hljs-keyword">bool</span> <span class="hljs-keyword">operator</span> &lt; (<span class="hljs-keyword">const</span> Info &amp;a) <span class="hljs-keyword">const</span>&#123;<br><span class="hljs-comment">// 按 score 从大到小排列</span><br>        <span class="hljs-keyword">return</span> a.score &lt; score;<br>    &#125;<br>&#125;<br>set&lt;Info&gt; s;<br></code></pre></td></tr></table></figure></li></ul><h3 id="multiset">multiset</h3><p>multiset 使用频率相对较低，其和 set 的区别在于，multiset容器中可以同时存储<strong>多个相同的元素</strong>，不再有互异性。</p><p>但是由于 set结构的有序性，当我们需要一个「<strong>时刻有序的数组</strong>」时，支持<span class="math inline">\(O(\log n)\)</span>地插入、删除、修改数组元素后<strong>依然保持有序</strong>，multiset就会派上用场。经常结合<code>lower_bound()</code>、<code>upper_bound()</code> 使用。</p><h2 id="栈-stack">栈 | stack</h2><p>从一段进栈，从同一端出栈，满足后进先出（LIFO）。</p><h3 id="基础操作-6">基础操作</h3><ul><li>头文件 <code>include&lt;stack&gt;</code></li><li>创建 stack 对象：<code>stack&lt;int&gt; s;</code></li><li>操作元素：<ul><li>栈顶压入元素：<code>s.push(x);</code></li><li>栈顶弹出元素：<code>s.pop();</code>，注意此时<strong>没有返回值</strong>；</li><li>返回栈顶元素：<code>s.top()</code>，返回<strong>元素的引用，因此可以直接修改</strong>。</li></ul></li><li>特性函数：<ul><li>栈长度：<code>s.size()</code></li><li>判断栈空：<code>s.empty()</code></li><li>注意队列和栈都没有 clear函数，想要清空只能重新初始化：<code>q = queue&lt;int&gt; ();</code></li></ul></li></ul><h3 id="特殊性质-5">特殊性质</h3><ul><li>当栈为空的时候，如果调用 <code>s.top()</code>则出现<strong>数组越界</strong>报错，解决办法是在使用该方法前加一个栈空判断：<code>!s.empty() &amp;&amp; s.top()</code>。该方法同样适用于队列、向量等容器。</li><li>C++11 新增 <code>s.emplace(x)</code>操作，用于在栈顶<strong>生成</strong>元素，参数为<strong>直接对象</strong>时相当于<code>s.push(x)</code>，区别在于当参数为<strong>构造函数对象</strong>时，例如<code>s.push(data(x,y))</code> 和 <code>s.emplace(data(x,y))</code>时，此时后者可以简化为 <code>s.emplace(x,y)</code>。</li></ul><h2 id="队列-queue">队列 | queue</h2><p>从一端入队，从另一端出队，满足先进先出（FIFO）的结构。普通的队列基于链表结构实现，而优先队列基于堆结构实现。</p><h3 id="基础操作-7">基础操作</h3><ul><li>头文件 <code>#include&lt;queue&gt;</code></li><li>创建 queue 对象：<code>queue&lt;int&gt; q;</code></li><li>操作元素：<ul><li>队尾插入元素：<code>q.push();</code></li><li>队首弹出元素：<code>q.pop();</code>，注意此时<strong>没有返回值</strong>；</li><li>返回队首元素：<code>q.front()</code>，返回<strong>元素的引用，因此可以直接修改</strong>；</li><li>返回队尾元素：<code>q.back()</code>，返回元素的引用，因此可以直接修改。</li></ul></li><li>特性函数：<ul><li>队列长度：<code>q.size();</code></li><li>判断队空：<code>q.empty();</code></li><li>注意队列和栈都没有 clear函数，想要清空只能重新初始化：<code>q = queue&lt;int&gt; ();</code></li></ul></li></ul><h3 id="优先队列-priority_queue">优先队列 | priority_queue</h3><p>利用自带的优先队列可以实现最大堆和最小堆，其<strong>头文件和队列相同</strong>，特性函数也相同。此时优先级最高的先出队，默认情况下优先级就是「整数的大小」。出入队的复杂度为<span class="math inline">\(O(\log n)\)</span>，<spanclass="math inline">\(n\)</span> 为队列的大小。下面介绍基础的用法：</p><ul><li>创建 priority_queue 对象：<ul><li>优先队列有<strong>三个参数</strong>，其声明形式为：<code>priority_queue&lt;类型, vector&lt;类型&gt;, less&lt;类型&gt;&gt;</code>，后两个参数可以省略，第一个参数不能省略，三个<code>类型</code> 保持一致。</li><li>构建最大堆（大顶堆）：<code>priority_queue&lt;int&gt; max_heap;</code>或<code>priority_queue&lt;int,vector&lt;int&gt;,less&lt;int&gt;&gt; max_heap;</code></li><li>构建最小堆（小顶堆）：<code>priority_queue&lt;int,vector&lt;int&gt;,greater&lt;int&gt;&gt; min_heap;</code></li></ul></li><li>操作元素：<ul><li>在完全二叉树的底部插入元素，并上浮到相应位置：<code>heap.push();</code></li><li>从堆顶弹出元素，并填充二叉树底部元素，然后下滤到相应位置：<code>heap.pop();</code></li><li>返回堆顶元素：<code>heap.top()</code>，注意和普通队列的区别！</li></ul></li></ul><p>如果想自定义其他优先级，则需要在<strong>自定义结构体</strong> cmp中重载括号运算符<code>()</code>，使其变成<strong>仿函数</strong>（Functor），并替换<code>less&lt;类型&gt;</code>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">cmp</span>&#123;</span><br><span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">operator</span><span class="hljs-params">()</span><span class="hljs-params">(<span class="hljs-keyword">const</span> T &amp;a, <span class="hljs-keyword">const</span> T &amp;b)</span></span>&#123;<br><span class="hljs-keyword">return</span> a.data &lt; b.data;<br>    &#125;<br>&#125;;<br>priority_queue&lt;T, vector&lt;T&gt;, cmp&gt; heap; <br></code></pre></td></tr></table></figure><p>或者在主函数外<strong>自定义比较函数</strong> cmp（类似 sort的写法），再用 <code>decltype</code>进行<strong>类型自动推断</strong>（转为仿函数）：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">bool</span> <span class="hljs-title">cmp</span><span class="hljs-params">(T &amp;a, T &amp;b)</span></span>&#123;<br><span class="hljs-keyword">return</span> a.data &gt; b.data;<br>&#125;<br>priority_queue&lt;T ,vector&lt;T&gt;, <span class="hljs-keyword">decltype</span>(&amp;cmp)&gt; <span class="hljs-built_in">heap</span>(cmp);<br></code></pre></td></tr></table></figure><h2 id="双端队列-deque">双端队列 | deque</h2><p>具备栈和队列的功能，但是操作要慢一点（常数级）。在实际使用中可以和vector 类比，vector的优势是对<strong>中间</strong>的操作速度快（例如索引遍历、迭代器遍历），deque优势是对<strong>首端</strong>的操作速度快（例如删除头部）。在对尾端操作上（例如尾端插入），二者速度相仿。</p><p>在实际应用中，常用于实现单调队列、滑动窗口等算法。</p><h3 id="基础操作-8">基础操作</h3><ul><li>头文件 <code>#include &lt;deque&gt;</code></li><li>创建 deque 对象：<code>deque&lt;int&gt; dq;</code></li><li>操作元素：<ul><li>返回队首元素：<code>dq.front();</code></li><li>返回队尾元素：<code>dq.back();</code></li><li>队首插入一个元素：<code>dq.push_front();</code></li><li>队尾插入一个元素：<code>dq.push_back();</code></li><li>队首弹出一个元素：<code>dq.pop_front();</code></li><li>队尾弹出一个元素：<code>dq.pop_back();</code></li></ul></li><li>特性函数：<ul><li>双端队列长度：<code>dq.size();</code></li><li>判断队空：<code>dq.empty();</code></li><li>清空队列：<code>dq.clear();</code></li><li>返回指向第一个元素和最后一个元素的指针（只能赋值给迭代器）：<code>v.begin()</code>、<code>v.end()</code></li></ul></li></ul><h2 id="双向链表-list">双向链表 | list</h2><p>非常少用的容器，底层采用双向链表的形式实现，元素可以分散存储在内存空间里，而不是必须存储在一整块连续的内存空间中。常配合<strong>哈希表存储链表节点指针</strong>，实现<span class="math inline">\(O(1)\)</span>的<strong>插入、删除中间节点</strong>。</p><h3 id="基础操作-9">基础操作</h3><ul><li>头文件 <code>#include &lt;list&gt;</code></li><li>创建 list 对象：<ul><li>直接创建：<code>list&lt;int&gt; ls;</code></li><li>创建含有 <code>n</code> 个元素且全 0的对象：<code>list&lt;int&gt; ls(n);</code></li><li>创建含有 <code>n</code> 个元素 <code>a</code>的对象：<code>list&lt;int&gt; ls(n,a);</code></li></ul></li><li>创建一个辅助哈希表：<ul><li><code>unordered_map&lt;int, list&lt;int&gt;::iterator&gt; table;</code></li></ul></li><li>插入、删除元素：<ul><li>尾部插入元素：<code>ls.push_back(a);</code> 或<code>ls.emplace_back(a);</code></li><li>尾部删除元素：<code>ls.pop_back();</code></li><li>首部插入元素：<code>ls.push_front(a);</code> 或<code>ls.emplace_front(a);</code></li><li>首部删除元素：<code>ls.pop_front();</code></li><li>在<strong>指定元素之前</strong>插入元素，插入后的元素就在该位置：<code>ls.insert((ls.begin()+i, a)</code></li><li>删除指定元素：<code>ls.erase(ls.begin()+2)</code></li></ul></li><li>迁移元素：<ul><li>从 <code>ls2</code> 中删除迭代器 <code>it</code>所指的元素，并插入到 <code>ls1</code>的首部：<code>ls1.splice(ls1.begin(), ls2, it);</code></li></ul></li><li>特性函数：<ul><li>链表长度：<code>ls.size();</code></li><li>判断空：<code>ls.empty();</code></li></ul></li><li>访问元素：<ul><li><code>ls.front()</code> 和 <code>ls.back()</code>返回第一个和最后一个元素的引用</li><li><code>ls.begin()</code> 和 <code>ls.end()</code>返回指向第一个和最后一个元素的迭代器</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>算法入门</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>算法入门笔记 #1 杂记</title>
    <link href="/Algo-Note-1.html"/>
    <url>/Algo-Note-1.html</url>
    
    <content type="html"><![CDATA[<p>该笔记是去年尝试入坑 ACM竞赛的遗留物，为作机试的准备，重新整理一遍以加深印象。以下部分内容出自<strong>刘汝佳</strong>的《算法计算入门经典》，也就是俗称的「<strong>紫书</strong>」，部分内容参考了网上流传甚广的博客。</p><h2 id="算法竞赛技巧">算法竞赛技巧</h2><ol type="1"><li><p>数组在 <code>main</code>外面定义可以开得更大：避免<strong>爆栈</strong>，在全局区申请分担压力；<code>main</code>函数返回 0，是为了告诉操作系统、IDE、调试器、OJ，程序正常结束。</p></li><li><p>对复杂的表达式化简不仅可以减少计算量，还能减少<strong>中间结果溢出</strong>的可能，尤其是带除法运算的浮点数，绝对会爆double，必须把表达式通分后除最大公约数。</p></li><li><p>使用<code>while(scanf("%d", &amp;x) == 1)</code>读入<strong>未知量</strong>数据：<code>scanf</code>会成功返回读入变量的个数，如果有多个变量，则用<code>== 2</code>。否则即使只成功一个，返回<code>1</code>，循环还是会执行。如果发生错误输入/中止符，中止前输入的数也会被保存。</p><ul><li><p>此时运行代码时如果用键盘输入，则最后一个 Enter无法中止程序，因为函数默认略过换行。</p></li><li><p>在 Windows 下，输入完毕后先按 Enter 再按 <code>Ctrl+Z</code> 再按Enter 可以结束输入，相当于人工输入一个 EOF，换成<code>while(scanf() != EOF)</code> 也行。</p></li><li><p>在 Linux 下，输入完毕后按 <code>Ctrl+D</code> 即可。</p></li></ul></li><li><p>使用 <code>while(cin&gt;&gt;x)</code>读入<strong>未知量</strong>数据：正常输入，直到遇到文件结束符 EOF或人工输入 <code>Ctrl+Z</code>，istream返回无效假条件；错误输入，如用字符输入整型变量，也会返回假条件，强制结束循环；如果有多个读入<code>&gt;&gt;x&gt;&gt;y</code>，只要发生上面任意一种，都会中止，但在中止前输入的数会保存；</p><ul><li>证明了 <code>&gt;&gt;</code> 运算符返回值是同样的 istream。</li></ul></li><li><p>字符串读入：常规方法 <code>cin&gt;&gt;s</code>可以满足大部分条件，但有时候需要<strong>整行读入</strong>的时候，直接<code>cin&gt;&gt;s</code> 遇到空格会停止。此时可以用<code>getline(cin, s)</code>，但要注意先清空输入缓冲区前缀的换行符<code>cin.ignore()</code>。</p></li><li><p>打表法：对于输入范围在 1e3 以内，暴力复杂度较高的情况，使用一个<code>freopen("ans.txt","w",stdout);</code>即可将输出重定向到文本，再<strong>造好数组格式的输出</strong>，手动复制到代码里提交即可；</p><ul><li>一维数组：造输出时用 <code>printf("%d,",num);</code>不要忘记输出逗号，复制到 <code>int ans[]=&#123;这里&#125;;</code> 。</li><li>二维数组：造输出时用<code>printf(f[%d][%d]=%d",i,j,d[n][m]);</code>，复制到一个<code>void get_ans()&#123;这里&#125;</code>。</li><li>如果有 <code>long long</code> 输出，必须带上 <code>LL</code>，用<code>printf("f[%d][%d]=%lldLL",i,j,d[n][m]);</code>。</li></ul></li><li><p>O3 优化：如果代码里有 STL 的话，请一定加上这句话<code>#pragma GCC optimize(3)</code>，一般的 OJ都是默认开启的，洛谷需要手动点一下。</p><ul><li>开启 O3 后编译器会自动 <code>inline/register</code>加速，节省时间（代价是汇编代码变多，可能编译失败）。</li><li>开启 O3 后编译器会自动 <code>const</code>所有不修改的常量，加速取模等复杂运算。</li><li>开启 O3 后编译器会自动将乘法变成移位运算。</li></ul></li><li><p><code>cin/cout</code> 解绑：原生的 <code>cin/cout</code>的速度非常慢，因为要和 <code>scanf/printf</code> 进行同步，所以需要加上<code>ios::sync_with_stdio(false); cin.tie(0);</code></p><ul><li>注意解绑后，不能使用 <code>scanf/printf</code>了，快读也是不可以的。解绑后的 <code>cin/cout</code> 的速度是比<code>scanf</code> 要快的，因为不用类型判断。</li><li>解绑后可以 <code>#define endl "\n"</code>，因为 <code>endl</code>在使用的时候不仅仅是换行，还会清空缓冲区。速度上可能比 <code>"\n"</code>换行慢了 10 倍。</li></ul></li></ol><h2 id="调试技巧">调试技巧</h2><ol type="1"><li>最常用的方法：<strong>输出中间结果</strong>，提交代码时记得注释掉；</li><li>使用 <code>exit(0);</code>中断程序，如果没有问题，则往下继续中断，也可以用来定位 bug；</li><li>编译时加入以下参数<code>-Wall -Wshadow</code>：增强警告信息，列举所有有遮盖关系的变量（覆盖）；</li><li>调试时不想反复输入数据，可用<strong>重定向输入输出</strong>，这样就会把文件作为流：<ul><li><code>freopen("input.in", "r", stdin);</code></li><li><code>freopen("output.out", "w", stdout);</code></li><li>如果懒得注释可以用 <code>#define LOCAL</code>，再在<code>#ifdef LOCAL</code> 和 <code>#endif</code>两句中重定向，提交代码时删除定义语句 <code>#define LOCAL</code>。</li></ul></li><li><strong>断言</strong>表达式<code>assert(判断语句)</code>：当判断语句为真时无变化，当表达式为假时中止程序，并给出错误提示。</li></ol><h2 id="实用代码">实用代码</h2><ol type="1"><li>如果题目要求文件提交，但禁止重定向：</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++">FILE *fin, *fout;<span class="hljs-comment">// c</span><br>fin=<span class="hljs-built_in">fopen</span>(<span class="hljs-string">&quot;data.in&quot;</span>, <span class="hljs-string">&quot;rb&quot;</span>);<br>fout=<span class="hljs-built_in">fopen</span>(<span class="hljs-string">&quot;data.out&quot;</span>, <span class="hljs-string">&quot;wb&quot;</span>);<br><span class="hljs-keyword">while</span>(<span class="hljs-built_in">fscanf</span>(fin, <span class="hljs-string">&quot;%d&quot;</span>, &amp;x) != EOF)<br><span class="hljs-built_in">fprintf</span>(fout, <span class="hljs-string">&quot;%d&quot;</span>, x);<br><span class="hljs-built_in">fclose</span>(fin);<br><span class="hljs-built_in">fclose</span>(fout);<br></code></pre></td></tr></table></figure><blockquote><p>在比赛前先了解是使用标准输入输出（即标准I/O，用键盘读写）还是文件输入输出。如果是文件输入输出，是否禁止用重定向的方式访问文件</p></blockquote><ol start="2" type="1"><li>如果允许重定向：</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">freopen</span>(<span class="hljs-string">&quot;in.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, stdin);<br><span class="hljs-built_in">freopen</span>(<span class="hljs-string">&quot;out.txt&quot;</span>, <span class="hljs-string">&quot;w&quot;</span>, stdout);<br><span class="hljs-keyword">while</span>(<span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%d&quot;</span>, &amp;x) != EOF) <span class="hljs-comment">// 这里也可以换成 cin &gt;&gt; x</span><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d&quot;</span>, x);<span class="hljs-comment">// 这里也可以换成 cout &lt;&lt; x</span><br><span class="hljs-built_in">fclose</span>(stdin);<br><span class="hljs-built_in">fclose</span>(stdout);<br></code></pre></td></tr></table></figure><ol start="3" type="1"><li><p>交换两数：<code>a^=b^=a^=b;</code></p></li><li><p>三整数排序：</p></li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-keyword">if</span>(a&gt;b) <span class="hljs-built_in">swap</span>(a,b); <span class="hljs-comment">// 执行完a≤b</span><br><span class="hljs-keyword">if</span>(a&gt;c) <span class="hljs-built_in">swap</span>(a,c); <span class="hljs-comment">// 执行完a≤c，且a≤b依然成立</span><br><span class="hljs-keyword">if</span>(b&gt;c) <span class="hljs-built_in">swap</span>(b,c); <span class="hljs-comment">// 执行完a≤b≤c</span><br></code></pre></td></tr></table></figure><ol start="4" type="1"><li><p>四舍五入：<code>floor(x+0.5)</code>，此时等于 1 的区间为<code>[0.5, 1.5)</code></p><ul><li><p>注意，小数部分为 0.5数可能受到浮点误差的影响，因为计算机中可能存了 0.49999。</p></li><li><p>并且 <code>floor</code> 函数返回值是 double 型，要强转或赋值给int 型。</p></li></ul></li><li><p>简单求无理数：<code>const double pi = acos(-1.0)</code>，注意三角函数使用弧度制。类似的还有<code>const double e = exp(1.0)</code>。</p></li><li><p>交错输出：<code>printf("%d", q?a:b); q=!q;</code></p></li><li><p>更新最值：<code>ans=max(ans,dfs(i,j));</code></p></li><li><p>初始化某个<strong>数组</strong>为正无穷：<code>memset(a,0x3f,sizeof(a));</code>，比直接用<code>0x7fffffff</code> 好，不容易加法溢出。同理初始化为负无穷用<code>memset(a,0xcf,sizeof(a));</code></p><ul><li>在 LeetCode 中如果用 vector 初始化，可以用<code>vector&lt;int&gt; a(n, INT_MAX)</code>，同理还有<code>INT_MIN</code> 常量，这是在 limits.h头文件中定义的最大最小值，分别是 2147483647 和-2147483648（<code>-INT_MAX-1</code>）。</li></ul></li><li><p>求数组实际长度：<code>sizeof(a)/sizeof(单位)</code>，前者仅仅是整个数组占内存大小，必须除以单位才能得到数组的实际长度。</p><ul><li>注意这里的 <code>a</code>是数组名，但如果用指针指向数组，<code>sizeof(指针)</code>得到的却是指针的长度，这是数组名和指针的一大区别。</li><li>在 32 位 (x86) 系统中，指针长度为 4，在 64 位 (x64)系统中，指针长度为 8。</li></ul></li><li><p>两个整数相除并<strong>向上取整</strong>：<code>(x + y - 1) / y</code>，比<code>ceil((double x / y))</code> 更快。</p></li></ol><h2 id="c-vs.-c">C vs. C++</h2><h3 id="版本比较">版本比较</h3><ul><li><p>在 C89 中不允许在 <code>for</code> 循环中<code>int i=0</code>，但 C99 后和 C++ 都可以。</p></li><li><p>在 C99 中，<strong>double 的输出</strong>必须用<code>%f</code>，输入要用 <code>%lf</code>，但 C89 和 C++ 中都可以全用<code>%lf</code>，所以尽量用 C++。</p></li><li><p>C99 中只规定了 int 至少是 16位，但没规定具体值，好在比赛平台几乎都是 32 位，即上限<code>2147483647</code>，这里有 10 位数，long long 最大值有 19位数。</p></li><li><p>C99 用 <strong>long long</strong>可以解决<strong>部分溢出</strong>，但是输入时要改成<code>%lld</code>。</p><ul><li>但是在 MinGW 的 gcc 中，要把 <code>%lld</code> 改成<code>%I64d</code>，在 VC2008 中又得改回 <code>%lld</code>。</li><li>因此如果涉及 long long 的输入输出，常用 C++的<strong>输入输出流</strong> <code>cin/cout</code>或<strong>自定义</strong>的输入输出方法。</li></ul></li><li><p>C 中的 <code>gets(s)</code> 存在缓冲区溢出漏洞，不推荐使用，在C11 标准中，已经被正式删除。</p><ul><li>使用 <code>fgets(buf, maxn, fin);</code> 替代，或者直接用 C++ 的<code>cin&gt;&gt;s;</code>。</li><li>但是 C++ 直接流读取 string类型会在<strong>遇到空格时停止</strong>，如果想要读取一整行包含空格的字符串，可以用<code>getline(cin,str);</code>。</li></ul></li><li><p>C++ 声明数组时，可以用 const 声明的<strong>常数数组</strong>，在C99 中是不允许的，推荐用 C++。</p></li></ul><h3 id="新增工具">新增工具</h3><ul><li><p>C++ 中特有的 bool 型变量，只有 0 或 1 两种取值，用 true 和 false表示真和假。</p></li><li><p>C 中的空指针 NULL 定义为<code>((void *)0)</code>，使用时可以强转赋值给任意类型指针，而 C++不能隐式强转，因此 NULL 被定义为 <code>0</code>，引入了新的关键字<code>nullptr</code> 来表示空指针。</p></li><li><p>C 中的 <code>qsort</code> 需要强转万能指针<code>void*</code>，并且让 cmp 函数在 <code>a&lt;b,a=b,a&gt;b</code>时分别返回负数、零、正数。注意 cmp 是被函数指针<code>int (*comparator)</code> 指向的函数。一般在算法竞赛中使用 C++ STL中的 sort 函数。</p><ul><li><p>sort 通常配合自定义 cmp 函数实现复杂排序<code>bool cmp(T &amp;a,T &amp;b)&#123;return a&lt;b;&#125;</code>，在力扣刷题时要在函数前加上<code>static</code>。</p></li><li><p>对于比较简短的比较函数，也可以用 <strong>Lambda表达式</strong>直接塞进 sort 里面：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// 完全闭包，无外部变量</span><br><span class="hljs-built_in">sort</span>(v.<span class="hljs-built_in">begin</span>(), v.<span class="hljs-built_in">end</span>(), [](<span class="hljs-keyword">auto</span> &amp;a, <span class="hljs-keyword">auto</span> &amp;b)&#123;<br>    <span class="hljs-keyword">return</span> a &lt; b;<br>&#125;);<br><span class="hljs-comment">// 捕捉闭包外的变量，&amp;hash 表示引用捕捉指定变量，也可以只用 &amp; 表示全部变量</span><br><span class="hljs-built_in">sort</span>(v.<span class="hljs-built_in">begin</span>(), v.<span class="hljs-built_in">end</span>(), [&amp;hash](<span class="hljs-keyword">auto</span> &amp;a, <span class="hljs-keyword">auto</span> &amp;b)&#123;<br>    <span class="hljs-keyword">if</span>(hash[a] == hash[b]) <span class="hljs-keyword">return</span> a &gt; b;<br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">return</span> hash[a] &lt; hash[b];<br>&#125;);<br></code></pre></td></tr></table></figure></li></ul></li><li><p>C++ 中可以用 C 的类型强转：<code>(ll)a</code> 或<code>ll(a)</code>，此外还新增了四种强转方法，用于类、结构体的强转。最基本的用法是<code>static_cast&lt;long long&gt;</code>，效果和 C 的类似。</p></li><li><p>C++ 中除了有 Lambda 表达式，还可以封装可调用的 Lambda<strong>仿函数</strong>（函数内嵌函数），可以<strong>避免用到全局变量、或传入冗长的参数</strong>，在LeetCode 中较为使用，具体用法如下：</p><p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-comment">// 使用 &amp; 表示捕获全部变量</span><br>function&lt;<span class="hljs-built_in"><span class="hljs-keyword">int</span></span>(<span class="hljs-keyword">int</span>, <span class="hljs-keyword">int</span>)&gt; dfs = [&amp;](<span class="hljs-keyword">int</span> x, <span class="hljs-keyword">int</span> y) &#123;<br>        <span class="hljs-keyword">return</span> x + y;<br>    &#125;; <span class="hljs-comment">// 注意这里必须要有分号</span><br><span class="hljs-keyword">return</span> <span class="hljs-built_in">dfs</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>);<br>&#125;<br></code></pre></td></tr></table></figure></p></li></ul><h3 id="全新特性">全新特性</h3><ul><li>C++ 中最重要的特点就是「<strong>函数传引用调用</strong>」，例如<code>void swap(int &amp;a, int &amp;b)</code>，不必再传指针调用，更方便快捷。对于字符串、长数组可以<strong>节省大量时间</strong>，且可以直接在函数内修改原参数。<ul><li>不过一旦使用 <code>&amp;</code>入参就<strong>不能为常量</strong>，如 <code>func(1)</code>。此时只能使用<code>const auto&amp; a</code>传参，这样<strong>速度更快</strong>，但是<strong>限制了函数内修改参数</strong>。</li></ul></li><li>C++ 中除了 struct 还有 class，工程中，一般用 struct定义纯数据的类型，用 class定义复杂行为的类型，二者最主要的区别是<strong>访问权限和继承方式</strong>不同。但算法竞赛中还是常用struct 来定义结构体，因为 C++的结构体还可以定义<strong>成员函数</strong>、<strong>构造函数</strong>等。<ul><li>拥有构造函数的结构体，可以通过 <code>T *a = new T(10);</code>调用，得到一个指向结构体的指针，在力扣刷题用。</li><li>此外也可以直接调用 <code>T a(10);</code> 或<code>T a = T(10);</code> 或<code>T b = a;</code>，在洛谷刷题比较常用。</li><li>下面给出一个经典的类模板：</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyClass</span> &#123;</span><br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-comment">// 成员变量</span><br>    <span class="hljs-keyword">static</span> <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> N = <span class="hljs-number">1e5</span> + <span class="hljs-number">6</span>;<br><span class="hljs-keyword">int</span> nums[N], param;<br><br>    <span class="hljs-comment">// 构造函数</span><br><span class="hljs-built_in">MyClass</span>(<span class="hljs-keyword">int</span> param_in) : <span class="hljs-built_in">param</span>(param_in)) &#123;<br>        <span class="hljs-built_in">memset</span>(nums, <span class="hljs-number">0</span>, <span class="hljs-built_in"><span class="hljs-keyword">sizeof</span></span>(nums));<br>&#125;<br><br>    <span class="hljs-comment">// 成员函数</span><br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">func1</span><span class="hljs-params">(<span class="hljs-keyword">int</span> num)</span> </span>&#123;<br><span class="hljs-comment">// TODO</span><br>&#125;<br>    <br>    <span class="hljs-function">vector&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">func2</span><span class="hljs-params">()</span> </span>&#123;<br>        <span class="hljs-comment">// TODO</span><br>        <span class="hljs-keyword">return</span> res_vec;<br>    &#125;<br>&#125;;<br><br><span class="hljs-comment">/** 使用方法</span><br><span class="hljs-comment"> * MyClass* obj = new MyClass(param_in);</span><br><span class="hljs-comment"> * obj-&gt;func1(num);</span><br><span class="hljs-comment"> * vector&lt;int&gt; res_vec = obj-&gt;func2();</span><br><span class="hljs-comment"> */</span><br></code></pre></td></tr></table></figure></li><li>C++ 中可以重载运算符，实现结构体之间的四则运算和输入输出。例如：<ul><li><code>T operator +(const T &amp;A) const&#123; return ...;&#125;</code>在结构体内重载加法运算；</li><li><code>T operator +(const T &amp;A, const T &amp;B)&#123; return ...;&#125;</code>在结构体外重载加法运算；</li><li><code>ostream&amp; operator &lt;&lt; (ostream &amp;out, const T&amp; p)&#123; out &lt;&lt; ...; return out;&#125;</code>在结构体外重载输出流；</li></ul></li><li>C++ 中具有<strong>泛型、模板</strong>的概念，即<code>template&lt;typename T&gt;</code>，可以实现<strong>函数端口参数的自适应</strong>，如各种类型数组的求和，甚至结构体数组的求和（需要重载加法）。</li></ul><h2 id="python">Python</h2><p>在算法竞赛中，Python有时会更有优势：高精度运算、字符串处理、记忆化搜索等，因此有必要掌握Python 的简单技巧。Python 的基础知识可以参考 <ahref="https://hwcoder.top/Python-Note-1">Python笔记 #1基础语法</a>，下面介绍算法竞赛中会用到的内容：</p><ul><li>类的模板</li><li>高精度运算</li><li>常用容器</li><li>常用函数</li><li>记忆化搜索（函数修饰符）</li></ul><h2 id="算法时间复杂度选择">算法时间复杂度选择</h2><p>一般 ACM 或者笔试题的时间限制是 1 秒或 2 秒。在这种情况下，C++代码中的操作次数控制在 <span class="math inline">\(10^7 \sim10^8\)</span> 为最佳。</p><p>下面给出在不同数据范围下，代码的时间复杂度和算法该如何选择：</p><table><thead><tr class="header"><th style="text-align: center;">数据范围</th><th style="text-align: center;">时间复杂度</th><th style="text-align: center;">算法</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><span class="math inline">\(n \leq30\)</span></td><td style="text-align: center;"><span class="math inline">\(O(n!) \;\;O\left(2^{n}\right)\)</span></td><td style="text-align: center;">DFS+剪枝、状态压缩 DP</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(n \leq100\)</span></td><td style="text-align: center;"><spanclass="math inline">\(O\left(n^{3}\right)\)</span></td><td style="text-align: center;">Floyd、三维 DP、区间 DP、高斯消元</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(n \leq1000\)</span></td><td style="text-align: center;"><spanclass="math inline">\(O\left(n^{2}\right) \;\; O\left(n^{2} \logn\right)\)</span></td><td style="text-align: center;">DP、二分、朴素版 Dijkstra、朴素版Prim、Bellman-Ford</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(n \leq10^{4}\)</span></td><td style="text-align: center;"><spanclass="math inline">\(O(n\sqrt{n})\)</span></td><td style="text-align: center;">块状链表、分块</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(n \leq10^{5}\)</span></td><td style="text-align: center;"><span class="math inline">\(O(n \logn)\)</span></td><td style="text-align: center;">各种sort、线段树、树状数组、set/map、heap、拓扑排序、Dijkstra+heap、Prim+heap、Kruskal、二分、树链剖分</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(n \leq10^{6}\)</span></td><td style="text-align: center;"><spanclass="math inline">\(O(n)\)</span>，常数较小的 <spanclass="math inline">\(O(n \log n)\)</span> 算法</td><tdstyle="text-align: center;">单调队列、hash、双指针、并查集、KMP<br>常数比较小的<span class="math inline">\(O(n \log n)\)</span>做法：sort、树状数组、heap、Dijkstra</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(n \leq10^{7}\)</span></td><td style="text-align: center;"><spanclass="math inline">\(O(n)\)</span></td><tdstyle="text-align: center;">双指针扫描、KMP、差分、前缀和、离散化</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(n \leq10^{9}\)</span></td><td style="text-align: center;"><spanclass="math inline">\(O(\sqrt{n})\)</span></td><td style="text-align: center;">判断质数</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(n \leq10^{18}\)</span></td><td style="text-align: center;"><span class="math inline">\(O(\logn)\)</span></td><td style="text-align: center;">最大公约数、快速幂</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(n \leq10^{1000}\)</span></td><td style="text-align: center;"><span class="math inline">\(O\left((\logn)^{2}\right)\)</span></td><td style="text-align: center;">高精度加减乘除</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(n \leq10^{100000}\)</span></td><td style="text-align: center;"><span class="math inline">\(O(\log k\times \log \log k)\)</span>，<span class="math inline">\(k\)</span>表示位数</td><td style="text-align: center;">高精度加减、FFT/NTT</td></tr></tbody></table><h2 id="ide-代码模板">IDE 代码模板</h2><p>ACM 模式：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;bits/stdc++.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> pb push_back</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> mp make_pair</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> X first</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> Y second</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> lb(x) ((x) &amp; (-x))</span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> mem(a,b) memset(a,b,sizeof(a))</span><br><br><span class="hljs-keyword">typedef</span> <span class="hljs-keyword">double</span> db;<br><span class="hljs-keyword">typedef</span> <span class="hljs-keyword">long</span> <span class="hljs-keyword">long</span> ll;<br><span class="hljs-keyword">typedef</span> vector&lt;<span class="hljs-keyword">int</span>&gt; vi;<br><span class="hljs-keyword">typedef</span> pair&lt;<span class="hljs-keyword">int</span>,<span class="hljs-keyword">int</span>&gt; pii;<br><br><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> inf = <span class="hljs-number">0x3f3f3f3f</span>;<br><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> mod = <span class="hljs-number">998244353</span>;<br><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> maxn = <span class="hljs-number">2e5</span> + <span class="hljs-number">5</span>;<br><br><span class="hljs-keyword">int</span> n,m,k;<br><span class="hljs-keyword">int</span> dp[maxn][maxn], a[maxn][maxn];<br><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>ios::<span class="hljs-built_in">sync_with_stdio</span>(<span class="hljs-literal">false</span>); cin.<span class="hljs-built_in">tie</span>(<span class="hljs-number">0</span>);<br><br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="常见错误">常见错误</h2><p>与<strong>取值范围</strong>有关：</p><ol type="1"><li>根据<strong>取值范围</strong>判断类型，注意 <spanclass="math inline">\(10^5\)</span> 个 <spanclass="math inline">\(10^5\)</span> 相加有 11 位数会爆 int，要用<code>long long</code>。</li><li>当题目明确说「对 <span class="math inline">\(1e9 + 7\)</span>取模」时，一定要用<code>long long</code>，可以确保一次相乘后还不会溢出，但是<strong>可能溢出的地方</strong>都要尽量取模。</li><li>当题目明确说「保证 XXX 在<strong>不取余</strong>的情况下可以用<strong>64 位有符号整数</strong>保存」时，此时不仅要开<code>long long</code>，还不能先取模，因为可能需要 <code>max</code>操作（<strong>不满足模不变性</strong>），必须在最后返回时才取模。</li><li>如果题目要求取余，则应当<strong>在所有运算之后都取余</strong>。避免最后<strong>减法导致负数</strong>，<strong>加法导致上溢</strong>等不易察觉的错误。</li><li>给 <code>long long</code>变量赋值（<code>+=</code>）时，<strong>等号右边</strong>也可能溢出，最好先进行转换<code>1LL *</code>。</li><li><code>nums.size() - 1</code> 不能乱用，因为 <code>.size()</code>返回 <code>unsigned long</code>，如果 0 - 1 答案不是 -1，而是越界。</li><li>使用 STL 中的 <code>accumulate</code> 时，可能会因为最后一个参数<code>0</code> 而爆 int，所有 STL算法都可能<strong>潜在溢出</strong>。</li></ol><p>与<strong>边界条件</strong>有关：</p><ol type="1"><li>使用 stack、queue等容器时，要注意<strong>容器可能为空</strong>，直接访问元素会报 heap溢出编译错误。</li><li>使用 <span class="math inline">\(i+1\)</span>、<spanclass="math inline">\(i-1\)</span> 等方式访问容器 vector的时候，要注意可能<strong>容器越界</strong>，会报 vector 编译错误。</li><li>DP 初始化 DP 数组时，考虑到初始状态<code>dp[0]</code>，通常会将数组<strong>扩大一些</strong>，防止越界报错。</li><li>DP<strong>访问历史值</strong>时，如果会跨很长距离，也要注意可能<strong>容器越界</strong>，会报vector 编译错误。</li><li>for <strong>循环的起始条件</strong>如果是 <spanclass="math inline">\(x-y\)</span>的形式，可能会是负数，导致<strong>容器越界</strong>，应该套上<code>max(0, x - y)</code>。</li><li>在链表操作中，如果代码本身能顺利执行，但是最后依旧报heap-use-after-free 错误，则说明链表最后出现环，而 LeetCode框架在执行完代码后释放链表空间时会重复访问已经被释放的节点。</li></ol><p>与<strong>题目的坑</strong>有关：</p><ol type="1"><li>有的坑题故意给排好序的样例，但实际上要<strong>先自己排序</strong>。</li><li>有的题目要统计满足某条件的 XX个数，还告诉你答案可能很大，但只要题目没说，一定要<strong>特判个数为</strong><span class="math inline">\(0\)</span> 的情况。</li><li>记得测试 <span class="math inline">\(n\)</span> 等于 <spanclass="math inline">\(0\)</span> 或 <spanclass="math inline">\(1\)</span>等边界情形时，答案是否需要<strong>特判</strong>。</li></ol><p>与<strong>复杂度</strong>有关：</p><ol type="1"><li>当题目出现树结构时（无论是题目给的或是自己用的），都要防止<strong>树退化成链表</strong>，卡复杂度<span class="math inline">\(O(n^2)\)</span>。</li><li>搜索题如果用到DFS，要根据参数看是否需要<strong>记忆化</strong>，如果会重复访问同一个状态则需要。</li><li>小技巧：如果力扣周赛平台的<strong>错误用例被隐藏</strong>，大概率是卡最坏复杂度、或整数溢出，数据过多而不显示。</li></ol><p>与<strong>代码实现</strong>有关：</p><ol type="1"><li><code>&amp;&amp;</code> 和 <code>||</code>运算符有<strong>短路机制</strong>，可能会漏执行后面的条件，而<code>&amp;</code> 和 <code>|</code> 位运算符则都会执行。</li><li><code>&gt;&gt;</code>等位运算的<strong>优先级最低</strong>，实现二分等操作时容易出错，建议全加上括号！</li><li>表达式的<strong>运算顺序</strong>可能会关系到是否溢出，如果有大数相乘、相加，尽量先处理缩小运算符。</li></ol>]]></content>
    
    
    <categories>
      
      <category>算法笔记</category>
      
      <category>算法入门</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Uplift Recommendation 论文汇总</title>
    <link href="/Uplift-Paper-Collection.html"/>
    <url>/Uplift-Paper-Collection.html</url>
    
    <content type="html"><![CDATA[<p>上一节介绍了 Uplift 在因果推断、推荐系统中的相关概念，展开介绍了几种Uplift 的基础模型和评估指标。本文继续聊聊 Uplift 近几年的发展。</p><p>本文综合考虑时间跨度、论文动机等因素，将相关论文分为若干类，包括Uplift Modeling 理论的发展、Uplift Recommendation的发展以及未来的趋势。</p><h2 id="探索增益模型理论的发展">探索：增益模型理论的发展</h2><p><strong>Using Control Groups to Target on Predicted Lift: Buildingand Assessing Uplift Models</strong>, 2007：</p><ul><li>提出 Two-Learner 的 Uplift Model 架构，同时提出了 Qini Curve指标。</li></ul><p><strong>Decision Trees for Uplift Modeling</strong>, ICDM 2010：</p><ul><li>基于 Decision Tree 的 Uplift Model 架构。</li></ul><p><strong>Real-World Uplift Modelling with Significance-Based UpliftTrees</strong>, 2011：</p><ul><li>基于 Decision Tree 的 Uplift Model 架构。</li></ul><p><strong>Decision Trees for Uplift Modeling with Single and MultipleTreatments</strong>, KAIS 2012：</p><ul><li>基于 Decision Tree 的 Uplift Model 架构。</li></ul><p><strong>Uplift Modeling for Clinical Trial Data</strong>, ICML2012：</p><ul><li>基于标签转换模型的 Uplift Model 架构。</li></ul><p><strong>A Literature Survey and Experimental Evaluation of theState-of-the-Art in Uplift Modeling: A Stepping Stone Toward theDevelopment of Prescriptive Analytics</strong>, Big Data 2018：</p><ul><li>综述，提出了 Qini Coefficient 基尼系数 AUUC。</li></ul><p><strong>Uplift Modeling with Multiple Treatments and General ResponseTypes</strong>, SIAM 2017：</p><ul><li>基于 Random Forest 的 Uplift Model 架构。</li></ul><h2 id="进入推荐领域从响应到增益">进入推荐领域：从响应到增益</h2><p><strong>Recommendation Systems with Purchase Data</strong>, JMR2008：</p><ul><li><p>过去的响应模型倾向于推荐给用户购买概率较高的那一个，而本文认为「营销行为的选择应该基于对行为的预期反应」，首次提出「推荐决定不仅应基于购买概率，而且还应基于<strong>购买概率对推荐行动的敏感性</strong>」。具备了Uplift Model 的雏形。</p></li><li><p>其他贡献：对一元数据进行建模，将购买行为看作是两个变量的叠加：知晓Awareness、满意Satisfaction。而不购买可能是因为不知晓，也可能是是因为知晓但不满意。使用两种数据集：自我发起的购买数据、推荐响应数据。</p></li></ul><p><strong>Estimating the Causal Impact of Recommendation Systems fromObservational Data</strong>, EC 2015：</p><ul><li>过去用随机实验来消除 Confonuder的方法代价过高（时间成本、用户反感），本文尝试直接从纯观测数据中获得因果效应。通过计数焦点产品和被推荐产品之间的「访问推荐驱动的流量」和「一段时间内对单个页面的总访问量」，将被推荐产品的点击拆分为「<strong>因果点击</strong>」和「<strong>便利点击</strong>」。所谓因果点击就是纯粹因为推荐造成的点击，而便利点击则是哪怕没有推荐用户也会去主动搜索的点击。</li></ul><p><strong>The Effect of Recommendations on Network Structure</strong>,UMAP 2016：</p><ul><li>介绍社交网络的两种常用推荐策略（基于内容的协同过滤、朋友之友算法）在图结构下的Rich Get Richer现象，即人们更有可能对受欢迎用户的推荐做出积极回应，进一步放大了知名人士的累积优势。文中简短地提及了「人们有可能早就对被推荐者感兴趣」的现象，这导致<strong>推荐效应的高估</strong>。</li></ul><p><strong>Modeling Individual Users' Responsiveness to MaximizeRecommendation Impact</strong>, WWW 2016：</p><ul><li>对推荐的反应可能取决于用户的类型，本文对用户属性（年龄、购物车大小、购买记录）和商品属性（展示时长、周销量）建模，提出了一个包含<strong>响应差异</strong>的购买预测模型。最后发现推荐效果和根据推荐日志建模的效果几乎相当，有助于克服推荐日志不足的冷启动（Cold-Start）问题。</li></ul><h2 id="完善增益模型的评估优化">完善：增益模型的评估、优化</h2><p><strong>Causal Embeddings for Recommendation</strong>, RecSys2018：</p><ul><li>领域自适应（DomainAdaptation）是另一种反事实的学习方法，适用于推荐系统。提出了<strong>CausE</strong> 模型。</li></ul><p><strong>Uplift-based evaluation and optimization ofrecommenders</strong>, RecSys 2019：</p><ul><li>首次对 Uplift模型的评估、优化进行定义。在评估中引入<strong>用户四象限、ATE、IPS、SNIPS</strong>，比传统的基于Precision的评估方法更能衡量模型；在优化中使用已有的购买和推荐日志作为<strong>现实参照</strong>，分别用<strong>逐点抽样和成对抽样</strong>方法得到两种Loss 训练的模型。提出了 <strong>ULRMF</strong> 和 <strong>ULBPR</strong>模型。</li></ul><p><strong>Action-Triggering Recommenders: Uplift Optimization andPersuasive Explanation</strong>, ICDMW 2019：</p><ul><li>这篇文章是前文的补充，描述的优化方法和上文一致，此外还补充了一个新任务：<strong>在推荐商品后解释推荐的原因</strong>。这是为了进一步帮助用户决策，唤起用户对商品的兴趣。过去对推荐的解释生成有四个维度：Neighbor（基于相似用户）、Influence（基于消费历史）、Demographic（基于用户画像）、Content（基于商品内容）。本文提出用「User-Item-Context」的三元匹配来对<strong>语境上下文</strong>进行解释。</li><li>有点 Multi-task Learning的感觉，原文只是优化了三元匹配，是否能当成<strong>自然语言生成</strong>任务来做呢？</li></ul><p><strong>Free Lunch! Retrospective Uplift Modeling for DynamicPromotions Recommendation within ROI Constraints</strong>, RecSys2020：</p><ul><li>从 ROI角度切入，只有当促销活动带来的增益能弥补促销的额外成本时，促销才能继续。而有的用户需要有促销才会购买，有的用户即使没有促销也会购买，对后者发放促销会带来不必要的开销，因此<strong>识别目标群体</strong>（可说服者、自愿购买者）。值得一提的是，本文提出用<strong>背包问题</strong>对ROI <strong>受限</strong> CATE 进行最大化，最终用贪心算法解决。</li></ul><p><strong>Online Evaluation Methods for the Causal Effect ofRecommendations</strong>, RecSys 2021：</p><ul><li>将过去用于信息检索的交织列表（InterleavingMethod）用于两种推荐模型的因果效应的比较，解决 A/B测试流量不足的问题。提出了等概率交织（EPI-RCT）、因果平衡交织（<strong>CBI-RCT</strong>）、结合逆倾向评分的因果平衡交织（CBI-IPS）模型。</li></ul><p><strong>Causality-Aware Neighborhood Methods for RecommenderSystems</strong>, 《Advances in Information Retrieval》2020：</p><ul><li>过去的 IPS方法容易受到高方差的影响（不同用户群体的真实占比不同），而匹配估计器（MatchingEstimator）是一种选取 A/B测试人群的方法，其通过<strong>计算协变量相似度</strong>（用户特征、购买记录）得出<strong>实验组和控制组人群</strong>，不依赖倾向性，不会有方差问题。本文结合了协同过滤中的近邻方法（User-based和Item-based）得到相似群体，利用其平滑单个样本的噪声，得到一种估计因果效应的方法。提出了<strong>CUBN</strong> 和 <strong>CIBN</strong> 模型。</li></ul><p><strong>Unbiased Learning for the Causal Effect ofRecommendation</strong>, RecSys 2020：</p><ul><li>提出另一种无偏学习框架 <strong>DLCE</strong>，还是针对使用 IPS的方差问题，首先构造无偏估计器对指标进行排序。然后对具有倾向性封顶的估计量进行经验风险最小化，减少了有限训练样本下的方差。</li></ul><h2 id="更进一步强化学习协同过滤">更进一步：强化学习、协同过滤</h2><p><strong>Partially observable environment estimation with upliftinference for reinforcement learning based recommendation</strong>, ML2021：</p><ul><li>将隐藏变量视为一种隐藏策略，提出了一种部分观测多智能体环境估计方法POMEE 来学习部分观测环境。提出一种强化学习框架 DUIN 用来学习每个动作的Uplift 效应。</li></ul><p><strong>CausCF: Causal Collaborative Filtering for RecommendationEffect Estimation</strong>, CIKM 2021：</p><ul><li>借鉴协同过滤技术，本文认为相似的用户不仅对物品有相似的品味，而且在推荐下也有<strong>相似的因果效应</strong>。以此提出了CasuCF 模型，将交互矩阵扩展为具有 User、Item 和 Treatment维度的三维张量，通过三者的成对内积来预测交互概率，User 和 Item之间的内积说明了用户对商品的偏好。</li></ul><h2 id="未来趋势">未来趋势</h2><ul><li>更复杂的干预：连续干预、多策略干预的建模（例如多种面额优惠券的组合），对应更广泛的业务场景，当然数据集样本也会更稀疏；</li><li>更丰富的任务：结合多任务学习来增强模型性能，例如前文提及的推荐语生成，可以和NLP 任务结合；</li><li>更有远见的模型：现实中的营销往往是常态化、持续化的，用户的心智可能会随着年龄、职业的变化而变化，因此有必要建立长期模型；</li><li>更优雅的测试方式：目前已有的 RCT测试存在的大量问题有待解决，交织列表的提出就是一个有效的尝试；</li><li>更多的使用场景：例如政治营销（针对摇摆者精准投送正面、负面新闻以改变总统大选的结果）、医疗效果评估等等。</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>推荐系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RecSys</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>浅谈 Uplift Modeling</title>
    <link href="/Uplift-Modeling.html"/>
    <url>/Uplift-Modeling.html</url>
    
    <content type="html"><![CDATA[<h2 id="概览">概览</h2><p>Uplift本意为「上升；抬起；提振」，在<strong>因果推断</strong>中引申为「<strong>某种干预对于个体状态或行为的因果效应</strong>」。在推荐系统领域中，<strong>增益模型</strong>（UpliftModel）用于预测「<strong>干预后的增量反馈价值</strong>」，可以表达为：<span class="math display">\[\mathrm{Lift}=P\left( \mathrm{State}\mid w\,\,\mathrm{Treatment} \right)-P\left( \mathrm{State}\mid w/o\,\,\mathrm{Treatment} \right)\]</span>增益模型的落地价值体现在「智能化营销」领域，用于衡量和预测营销干预带来的「增量提升」，把营销预算投入在<strong>说服型</strong>（Persuadables）用户上，不浪费在<strong>确认型</strong>（SureThings）和<strong>沉睡型</strong>（LostCauses）用户上，避免对<strong>勿扰型</strong>（SleepingDogs）用户产生反效果<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[《该把优惠券发送给哪些用户？一文读懂 Uplift 模型》赵小洛](http://www.woshipm.com/marketing/5117249.html)">[2]</span></a></sup>：</p><p><img src="/img/blog/Uplift-1-images/Four-Class-Users.png" alt="营销模型的四类用户" width=40% /></p><p>与之对应的是<strong>响应模型</strong>（ResponseModel），其目标是预测「<strong>干预后的个体状态或行为</strong>」，这是一个相关性分析问题（而不是因果推断问题），因此会导致我们无法区别自然转化人群。在营销领域，响应模型可以用于估计用户<strong>看过广告后的购买率</strong>，但增益模型是估计用户<strong>因为广告而购买</strong>的概率，帮助精准寻找营销敏感人群。响应模型可以表达为：<span class="math display">\[\mathrm{Outcome}=P\left( \mathrm{State}\mid w\,\,\mathrm{Treatment}\right)\]</span>用一个实际的营销案例来说明，假设此刻有两类等量的用户群体<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[【实践案例分享】阿里文娱智能营销增益模型 ( Uplift Model ) 技术实践，木东居士 - 腾讯云](https://cloud.tencent.com/developer/article/1620903)">[3]</span></a></sup>：</p><table><thead><tr class="header"><th style="text-align: center;">User</th><th style="text-align: center;">有广告 CVR</th><th style="text-align: center;">无广告 CVR</th><th style="text-align: center;">Uplift</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">用户群体 1</td><td style="text-align: center;"><spanclass="math inline">\(0.8\%\)</span></td><td style="text-align: center;"><spanclass="math inline">\(0.2\%\)</span></td><td style="text-align: center;"><spanclass="math inline">\(0.6\%\)</span></td></tr><tr class="even"><td style="text-align: center;">用户群体 2</td><td style="text-align: center;"><spanclass="math inline">\(2.0\%\)</span></td><td style="text-align: center;"><spanclass="math inline">\(1.7\%\)</span></td><td style="text-align: center;"><spanclass="math inline">\(0.3\%\)</span></td></tr></tbody></table><p>在传统响应模型中，我们可能会向第二类用户投放广告，因为其转化最高。但是在增益模型中，除了<strong>广告曝光转化率</strong>之外，我们还能知道这两类用户群体在没有广告触达情况下的<strong>自然转化率</strong>，从而推算出广告所带来的增量。按照这个逻辑，我们更应该向第一类用户投放广告。</p><p><img src="/img/blog/Uplift-1-images/Model-Diff.png" alt="两种模型推荐的关注点不同" width=50% /></p><h2 id="相关概念">相关概念</h2><h3 id="因果推断-causal-inference">因果推断 | Causal Inference</h3><blockquote><p>机器学习不过是在拟合数据和概率分布曲线，而变量的内在因果关系并未得到足够的重视。如果要真正解决科学问题，甚至开发真正意义上的智能机器，因果关系是必然要迈过的一道坎。<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[《因果关系之梯》望止洋 - 知乎专栏](https://www.zhihu.com/column/c_1217887302124773376)">[1]</span></a></sup></p></blockquote><ul><li><p>伪相关性（SpuriousCorrelation）：深度学习模型能拟合出数据的<strong>关联关系</strong>，但不能得到<strong>因果关系</strong>。产生统计关联性有<strong>四种情况</strong>，而其中两种则是伪相关的。伪相关会造成<strong>偏倚</strong>（Bias），例如「巧克力销量与诺贝尔奖」的故事，显然二者并没有因果关系。</p><p><img src="/img/blog/Uplift-1-images/Correlation.png" alt="产生统计关联性的四种情况" width=50% /></p></li><li><p>混杂（Confounding）：在上述例子中，<spanclass="math inline">\(X\)</span> 和 <spanclass="math inline">\(Y\)</span>的<strong>共因</strong>也被称为<strong>混杂因子</strong>（Confounder），而当<strong>伪相关性和真正的因果关系混合</strong>在一起时，就产生了「混杂、有偏」的情形：</p><p><img src="/img/blog/Uplift-1-images/Confounding.png" alt="混杂的基本情形" width=40% /></p></li><li><p>消除混杂：一种消除混杂的方法叫<strong>后门准则</strong>（BackdoorCriterion）。所谓「<strong>后门路径</strong>」就是第一种伪相关的情况，如果我们有足够的数据能够将所有<span class="math inline">\(X\)</span> 和 <spanclass="math inline">\(Y\)</span>之间的后门路径全部<strong>阻断</strong>（Separation），就可以识别其因果关系。所谓「阻断」，即给混杂因子<strong>赋予定值</strong>，以这个变量为条件（Conditioningon），缩小到样本的<strong>子集</strong>后分类讨论。</p></li></ul><p><img src="/img/blog/Uplift-1-images/Backdoor.png" alt="后门准则" width=40%  /></p><ul><li>干预（Treatment/Intervention）：意为对<strong>原始的数据分布</strong>加以改变，不同于以变量为条件选取子集，而是直接<strong>删除所有指向该变量的边</strong>，并直接赋予该变量新的值。在新的模型中检验相关性时，我们就可以发现该变量与目标变量是否真的存在因果关系了。</li></ul><p><img src="/img/blog/Uplift-1-images/Intervention.png" alt="干预" width=40% /></p><ul><li><p>反事实推断（Counterfactualinference）：基于数据对<strong>外生变量</strong> <spanclass="math inline">\(U\)</span>（未观测的变量）进行估算，利用干预改变模型<span class="math inline">\(M_x\)</span>（删除所有指向 <spanclass="math inline">\(X\)</span> 的箭头），赋予 <spanclass="math inline">\(X\)</span> 反事实假设，根据估算的外生变量 <spanclass="math inline">\(U\)</span> 和新模型 <spanclass="math inline">\(M_x\)</span> 来计算 <spanclass="math inline">\(Y\)</span>。</p><blockquote><p>例如：我们收集到一个事实（用户 A没收到广告时不会购买），要想知道「收到广告」和「购买」之间是否有因果关系，那我们就需要一个反事实（用户A收到广告时会怎么样），但这个数据是不存在的。若想对其进行反事实推理，就要删除所有指向「收到广告」的箭头，包括其他所有特征变量，再对「收到广告」这个变量进行干预。</p><p>此时有一个很重要的「<strong>独立假设</strong>」：若用 <spanclass="math inline">\(Y_1\)</span> 表示收到广告后的购买率，<spanclass="math inline">\(Y_0\)</span> 表示没收到广告的购买率，<spanclass="math inline">\(T\)</span> 表示我们的干预，则有 <spanclass="math inline">\((Y_1,Y_0) \botT\)</span>，即二者独立。这似乎很难理解，难道不是 <spanclass="math inline">\(Y_t\)</span> 不是跟 <spanclass="math inline">\(T=t\)</span>有关吗？然而，不管我们是否推送广告，结果 <spanclass="math inline">\(Y_1,Y_0\)</span> 永远是存在的，只是我们需要用<span class="math inline">\(T=t\)</span>来选择「观测」其中的一种，并不会影响其具体的值。</p></blockquote></li><li><p>整体因果效应（Averaged Treatment Effect,ATE）：<strong>一个个体不可能同时使用试验组和对照组的方法分别干预</strong>，因此我们只能构建两组非常相似的群组，在相似的群组里进行不同的干预，从而达到反事实推理的目的。在上述「独立假设」下，我们两组的观测结果之差就得到<span class="math inline">\(\mathrm{ATE}\)</span>：</p></li></ul><p><span class="math display">\[\begin{aligned}    \tau &amp;=\mathbb{E} \left[ Y_1-Y_0 \right] =\mathbb{E} \left[ Y_1\right] -\mathbb{E} \left[ Y_0 \right]\\    &amp;=P \left[ Y_1\mid T=1 \right] -P \left[ Y_0\mid T=0 \right]\\    &amp;=P \left[ Y\mid T=1 \right] -P \left[ Y\mid T=0 \right]\\\end{aligned}\]</span></p><ul><li><p>随机控制试验（Randomized Controlled Trials, RCT）：俗称线上 A/B测试。为了计算 <spanclass="math inline">\(\mathrm{ATE}\)</span>，需要将用户群体进行<strong>随机分流</strong>，实验组施加干预，测试组保持原先策略。由于<strong>完全随机</strong>，实验组和对照组的样本在<strong>分布上是一致的</strong>，因此满足前文提及的「独立假设」，可以直接计算<span class="math inline">\(\mathrm{ATE}\)</span>。</p></li><li><p>倾向性评分法（Propensity ScoreMethod，PSM）：在工业界中，做随机实验都会存在一定成本，甚至会引起客户流失，且<strong>大多数时候无法完全随机</strong>，因此也就没办法去准确估计<strong>无偏</strong>的<spanclass="math inline">\(\mathrm{ATE}\)</span>。此时实验组和对照组的样本分布存在差异，<spanclass="math inline">\(Y\)</span> 和 <spanclass="math inline">\(T\)</span> 不再独立使得 <spanclass="math inline">\(E\left[ Y_1 \right] \ne E\left[ Y_1\mid T=1\right]\)</span>，而倾向评分就是用来用来估计「在<strong>给定混杂因子</strong>的情况下，目标被施加干预」的概率，对概率大的施加一个小的权重，对概率小的施加一个大的权重，用来<strong>消除偏倚</strong>。</p><blockquote><p>例如：我们想知道「优惠券的领取」对「购买某商品」的因果效应，于是进行随机控制试验。理想情况下，我们会得到A 和 B 两组非常相似的群体（性别比例、年龄分布等），其中 A组「领取优惠券」而 B组「未领取优惠券」。然而干预因素（发放优惠券）的分配通常不受人为控制：假设女性用户群体可能更热衷于逛淘宝因此领到优惠券的概率更大，则两个组在「性别」这一特征上并不均衡可比，因此<strong>无法删除混杂因子指向的边</strong>。</p><p>此时我们就需要用到 PSM来尽可能消除「性别」带来的偏倚。两种常用的倾向性评分方法是<strong>近邻匹配</strong>（CollaborativeMatching, CM）和<strong>逆倾向评分加权</strong>（Inverse PropensityScore Weighting,IPSW）。<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[倾向性评分法（propensity score method，PSM） - 腾讯云](https://cloud.tencent.com/developer/article/1759064)">[4]</span></a></sup></p></blockquote></li></ul><h3 id="推荐系统与智能营销">推荐系统与智能营销</h3><ul><li>点击率（Click Through Rate,CTR）：反映了展品对客户的吸引，具体公式：<spanclass="math inline">\(\mathrm{CTR}=点击量/展现量\)</span>。</li><li>转化率（Conversion Rate,CVR）：用户点击展品到注册、付费的转化率，具体公式：<spanclass="math inline">\(\mathrm{CVR}=转化量/点击量\)</span>。</li><li>投资回报率（Return On Investment, ROI）：具体公式：<spanclass="math inline">\(\mathrm{ROI}=总利润/总投资\)</span>。</li><li>离散干预和连续干预（Treatment）：在营销领域，离散干预可以类比「发不发优惠券」，连续干预可以类比「优惠券的面额」或者「多种优惠券的组合」。现阶段连续干预是推荐系统领域的重大挑战。</li><li>协同过滤（CollaborativeFiltering）：在推荐系统中常用的一种技术，通过收集用户偏好信息，再去寻找<strong>相似的商品或者用户群体</strong>产生推荐（Item-based和User-based）。此外，还有诸如矩阵分解模型、神经网络模型等基于模型的方法。</li><li>交织列表（InterleavingList）：一种在线评估推荐系统的方法，在信息检索领域最先提出。由于 A/B测试存在的问题（用于测试的流量不足、重度用户在各组占比不同造成偏倚），使用Interleaving则无需分组，只需对所有用户投放一个交织列表，该列表包含<strong>两个要比较的模型</strong>推荐的结果，通过用户点击哪个多就可以看出哪个模型好。</li></ul><h2 id="基本模型">基本模型</h2><h3 id="two-learner">Two-Learner</h3><p>Two-Learner 是基于双模型的<strong>差分响应模型</strong>（DifferentialResponseModel），对实验组（有干预）和对照组（无干预）的购买行为进行分别建模，然后用训练所得两个模型分别对待测用户的购买行为进行预测，本质是<strong>响应模型</strong><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[《该把优惠券发送给哪些用户？一文读懂 Uplift 模型》赵小洛](http://www.woshipm.com/marketing/5117249.html)">[2]</span></a></sup>。</p><p><img src="/img/blog/Uplift-1-images/Two-Learner.png" alt="Two-Learner" width=30% /></p><p>此时一个样本用户可得出两种情况下各自的购买行为预测值。这两个预测值之差就是我们想要的<spanclass="math inline">\(\mathrm{ATE}\)</span>。这种建模方法较简单且易于理解，可以直接用回归、GBDT等模型实现。然而，Two-Learner 也存在一些局限性：</p><ul><li>对照组和实验组分别建模，两个模型完全隔离，两个模型容易有<strong>累积误差</strong>；</li><li>其次建模的目标是 Response 而不直接是 Uplift，因此模型对 Uplift的预测能力较有限；</li><li>干预策略只能是<strong>离散值</strong>（一种干预对应一个模型），更不能是连续变量。</li></ul><h3 id="single-learner">Single-Learner</h3><p>Single-Learner 在 Two-Learner的基础上，将对照组和实验组<strong>数据放在一起建模</strong>，将实验分组（<strong>干预与否</strong>）作为一个单独特征，和其他特征一起输入模型中对用户购买行为进行建模。模型的输出是一个条件值，使用其进行预测时，需要计算该样本<strong>分别进入实验组和对照组</strong>预测两次，将差异作为对<span class="math inline">\(\mathrm{ATE}\)</span> 的估计。</p><p><img src="/img/blog/Uplift-1-images/S-Learner.png" alt="Single-Learner" width=40% /></p><p>比起 Two-Learner 的优势在于：</p><ul><li>训练样本共用可以使单个模型学习更加充分，单模型也可以避免双模型打分<strong>累积误差</strong>较大的问题；</li><li>模型可以支持干预项为<strong>多策略及连续变量</strong>的建模，实用性较强。</li></ul><p>然而，Single-Learner 在本质上依然还是对 Response建模，只是分成了两次输出，对 Uplift的预测还是比较间接<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[《该把优惠券发送给哪些用户？一文读懂 Uplift 模型》赵小洛](http://www.woshipm.com/marketing/5117249.html)">[2]</span></a></sup>。</p><h3 id="标签转换模型">标签转换模型</h3><p>标签转换模型（Class TransformationMethod）既可以将实验组与对照组数据打通，同时又是直接对 <spanclass="math inline">\(\mathrm{ATE}\)</span>进行预测，其核心思想是将实验组和控制组样本<strong>直接混合</strong>并附加新的标签<span class="math inline">\(Z\)</span> 满足：</p><ul><li>当用户在实验组（发券）且用户最终购买时，<spanclass="math inline">\(Z=1\)</span></li><li>当用户在对照组（无干预）且用户最终未购买时，<spanclass="math inline">\(Z=1\)</span></li><li>当用户在实验组（发券）且用户最终未购买时，<spanclass="math inline">\(Z=0\)</span></li><li>当用户在对照组（无干预）且用户最终购买时，<spanclass="math inline">\(Z=0\)</span></li></ul><p>可以证明，<span class="math inline">\(P\left(Z=1\midT_i\right)\)</span> 和 <span class="math inline">\(\mathrm{ATE}\)</span>是线性正相关的，且当实验组与控制组样本比例为 <spanclass="math inline">\(1:1\)</span> 时： <span class="math display">\[\tau = 2 P\left(Z=1\mid T_i\right) - 1\]</span> 假设干预策略 <span class="math inline">\(T\)</span>与用户相互独立（满足前文的「<strong>独立假设</strong>」），则同理 <spanclass="math inline">\(T \bot \boldsymbol{X}\)</span>，<spanclass="math inline">\(\boldsymbol{X}\)</span> 为所有协变量之和，则有：<span class="math display">\[\begin{aligned}    P(Z=1\mid \boldsymbol{X})&amp;=P(Z=1\mid\boldsymbol{X},T=1)P(T=1\mid \boldsymbol{X})+P(Z=1\mid\boldsymbol{X},T=0)P(T=0\mid \boldsymbol{X})\\    &amp;=P(Y=1\mid \boldsymbol{X},T=1)P(T=1\mid\boldsymbol{X})+P(Y=0\mid \boldsymbol{X},T=0)P(T=0\mid \boldsymbol{X})\\    &amp;=P^T(Y_1\mid \boldsymbol{X})P(T=1)+P^C(Y_0\mid\boldsymbol{X})P(T=0)\\\end{aligned}\]</span> 在 RCT 实验中，当实验组与控制组样本比例为 <spanclass="math inline">\(1:1\)</span> 时，<spanclass="math inline">\(P(T=0)=P(T=1)=\frac{1}{2}\)</span>，即一个用户被分在两个组的概率是相等的，则有：<span class="math display">\[\begin{aligned}    2P(Z=1\mid \boldsymbol{X})&amp;=P^T(Y_1\mid\boldsymbol{X})+P^C(Y_0\mid \boldsymbol{X})\\    &amp;=P^T(Y_1\mid \boldsymbol{X})+1-P^C(Y_0\mid \boldsymbol{X})\\\end{aligned}\]</span></p><h3 id="决策树建模">决策树建模</h3><p>对于 Uplift直接建模的方式除了上述标签转换的方式外，还有一种就是通过修改已有的学习学习器结构直接对Uplift 进行建模，比如修改 LR、KNN、SVM等，比较流行的就是修改<strong>决策树模型</strong>的特征分裂方法<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[【实践案例分享】阿里文娱智能营销增益模型 ( Uplift Model ) 技术实践，木东居士 - 腾讯云](https://cloud.tencent.com/developer/article/1620903)">[3]</span></a></sup>。</p><p>传统决策树模型的分裂过程中，常用的指标是<strong>信息增益或信息增益比</strong>，其本质是希望通过特征分裂后下游正负样本的分布更加悬殊，即类别纯度变得更高。<span class="math display">\[\Delta_{\mathrm{gain}}=\mathrm{info}_{\mathrm{after-split}}(D)-\mathrm{info}_{\mathrm{before-split}}(D)\]</span> 同理这种思想也可以引入到 Uplift Model建模过程，我们希望通过特征分裂后能够把 Uplift更高和更低的两群人<strong>更好地区分开</strong>。 <spanclass="math display">\[\Delta _{\mathrm{gain}}=D_{\mathrm{after-split}}\left( P^T,P^C \right)-D_{\mathrm{before-split}}\left( P^T,P^C \right)\]</span> 其中 <span class="math inline">\(D\)</span> 的衡量可以基于 KL散度、欧式距离、卡方距离等。</p><h2 id="模型评估">模型评估</h2><h3 id="uplift-十分位柱状图">Uplift 十分位柱状图</h3><p>将测试集预测出的用户按照 Uplift Score 由高到低平均分为 10组，即依次选取 Top 10%、Top 20% 直到 Top 100%共<strong>十个区间</strong>内的用户，分别求每个区间内 Uplift Score的<strong>均值</strong>。绘制出如下的十分位柱状图：</p><p><img src="/img/blog/Uplift-1-images/Bar.png" alt="十分位柱状图" width=80% /></p><p>通过观察正负柱子的长短、占比、分布，可以直观地观察到该模型<strong>预测</strong>的营销增益。如果保留用户区间不变，将每个区间内的<strong>Groud Truth Uplift Score</strong>的均值拿来绘图，则得到的图不一定是有序的柱子。</p><h3 id="累计增益曲线-qini-curve">累计增益曲线 | Qini Curve</h3><p>计算十分位中<strong>累计用户</strong>的 Qini系数，将坐标点连接起来，得到一条<strong>累计增益曲线</strong>。Qini系数公式如下： <span class="math display">\[Q(\phi)=\frac{n_{t, y=1}(\phi)}{N_{t}}-\frac{n_{c, y=1}(\phi)}{N_{c}}\]</span> 其中：</p><ul><li><span class="math inline">\(\phi\)</span> 是按照 Uplift Score由高到低排序的用户数量占该组用户数量的比例，<spanclass="math inline">\(\phi=0.3\)</span> 代表验组或对照组中 Top 30%的所有用户；</li><li><span class="math inline">\(n_{t, y=1}(\phi)\)</span> 表示在 Top<span class="math inline">\(\phi\)</span>用户中，实验组中预测结果为购买的用户数量；同理 <spanclass="math inline">\(n_{c, y=1}(\phi)\)</span>表示对照组预测结果为购买的用户数量；</li><li><span class="math inline">\(N_t\)</span> 和 <spanclass="math inline">\(N_c\)</span>则分别代表实验组和对照组总用户样本数。</li></ul><p><img src="/img/blog/Uplift-1-images/Qini-Curve.png" alt="预测的累计增益曲线" width=70% /></p><p>上图中，橙色线是随机曲线（代表<strong>随机向人群发放优惠券</strong>的增益），Qini曲线则代表<strong>有针对性地向特定人群发放优惠券</strong>的增益。</p><p>将上图两条曲线作差，可以看到当横轴为 Top 40% 时，Qini曲线与随机曲线之间距离最大，对应的的纵轴大约是 0.037，这意味着对 Top 40%用户发放优惠券可以获得<strong>最大收益</strong>，这部分用户也就是我们需要对其进行营销干预的<strong>Persuadable</strong> 人群。相反，如果采用随机发放策略，需要发出80% 的量才能达到<strong>同等效果</strong>，大大节约成本。</p><blockquote><p>理论上 Qini曲线是不应该出现「先下降后上升」的，但是由于模型可能将用户<strong>分类到错误区间</strong>，因此用Groud Truth 绘制时可能会有所下降。利用 Groud Truth的真实区间绘制的图应当是一个光滑的「先上升后下降」曲线，例如下图。</p></blockquote><p><img src="/img/blog/Uplift-1-images/Qini-Curve-2.png" alt="真实的累计增益曲线" width=50% /></p><p>实际应用中，<span class="math inline">\(\phi\)</span>的取值越连续，刻画的曲线就越精确平滑，甚至能精确找出 <strong>SleepingDog</strong> 人群。</p><blockquote><p>当实验组和对照组用户数量不平衡，为避免指标失真，会采用另一种累积增益曲线;<span class="math display">\[G(\phi)=\left(\frac{n_{t, y=1}(\phi)}{n_{t}(\phi)}-\frac{n_{c,y=1}(\phi)}{n_{c}(\phi)}\right)\left(n_{t}(\phi)+n_{c}(\phi)\right)\]</span></p></blockquote><h3 id="auuc">AUUC</h3><p>计算 Qini 曲线和<strong>横坐标轴</strong>之间的面积，称为 Area Underthe Uplift Curve，也称为<strong>基尼系数</strong>（QiniCoefficient）。该面积越大越好，表示模型结果远超随机选择结果。</p><p><img src="/img/blog/Uplift-1-images/AUUC.png" alt="AUUC" width=50% /></p><p>需要注意的是，AUUC<strong>仅用于评价模型</strong>，对选取哪些用户发放优惠券并无指导意义。</p><h2 id="参考">参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><ahref="https://www.zhihu.com/column/c_1217887302124773376">《因果关系之梯》望止洋- 知乎专栏</a><a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><ahref="http://www.woshipm.com/marketing/5117249.html">《该把优惠券发送给哪些用户？一文读懂Uplift 模型》赵小洛</a><a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><ahref="https://cloud.tencent.com/developer/article/1620903">【实践案例分享】阿里文娱智能营销增益模型( Uplift Model ) 技术实践，木东居士 - 腾讯云</a><a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><ahref="https://cloud.tencent.com/developer/article/1759064">倾向性评分法（propensityscore method，PSM） - 腾讯云</a><a href="#fnref:4" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>推荐系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RecSys</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python笔记 #4 常用标准库和第三方库</title>
    <link href="/Python-Note-4.html"/>
    <url>/Python-Note-4.html</url>
    
    <content type="html"><![CDATA[<p>除了前面提及的的数据分析第三方库，Python中也自带了许多有用的标准库，提供了一系列基础接口。此外本文还将介绍一些项目中常用的第三方库，更多细节可以在<strong>交互式窗口</strong>中输入<code>dir(xx)</code> 和 <code>help(xx)</code> 检索。</p><h2 id="标准库">标准库</h2><p>标准库通常与 Python发行版捆绑安装，其提供的组件涉及范围十分广泛，此处仅介绍常用库。</p><p>导入标准库时建议使用 <code>import os</code> 风格而非<code>from os import *</code>，这样在调用的时候就可以用<code>os.open()</code>，避免覆盖内置的函数 <code>open()</code>。</p><h3 id="os-操作系统接口">os 操作系统接口</h3><p>Python中与操作系统交互的标准库，可以完成目录管理、进程管理、调度器接口等操作。</p><p>在深度学习中常用于<strong>创建目录、遍历目录、操作路径</strong>。使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># path 是字符串，可以是绝对路径，也可以是当前目录相对路径</span><br>os.mkdir(path)  <span class="hljs-comment"># 创建一级目录</span><br>os.makedirs(path)  <span class="hljs-comment"># 创建多级目录</span><br><br><span class="hljs-comment"># 先判断目标目录是否存在再新建</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(path):<br>os.makedirs(path)<br>    <br><span class="hljs-comment"># 如果 exist_ok 为 False（默认值），则在目标目录已存在的情况下触发 FileExistsError 异常</span><br>os.makedirs(path, exist_ok=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 路径相关用法 os.path</span><br>os.path.join(<span class="hljs-string">&#x27;path&#x27;</span>,<span class="hljs-string">&#x27;abc&#x27;</span>,<span class="hljs-string">&#x27;1.txt&#x27;</span>) <span class="hljs-comment"># 生成字符串 path/abc/1.txt</span><br>os.path.join(<span class="hljs-string">&#x27;./path&#x27;</span>,<span class="hljs-string">&#x27;abc&#x27;</span>,<span class="hljs-string">&#x27;1.txt&#x27;</span>) <span class="hljs-comment"># 生成字符串 ./path/abc/1.txt</span><br></code></pre></td></tr></table></figure><p>os 模块还能设置环境参数，在深度学习中常用于设置 GPU相关参数，但是必须放在 main 函数的入口处，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># 设置 GPU</span><br>    os.environ[<span class="hljs-string">&#x27;CUDA_LAUNCH_BLOCKING&#x27;</span>] = <span class="hljs-string">&#x27;1&#x27;</span><br>    os.environ[<span class="hljs-string">&quot;TOKENIZERS_PARALLELISM&quot;</span>] = <span class="hljs-string">&quot;false&quot;</span><br>    os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="hljs-string">&#x27;0&#x27;</span><br></code></pre></td></tr></table></figure><p>此外，os 模块还能在脚本中直接使用终端命令，只需要<code>os.command(" ")</code> 就能执行，常用于批量使用复杂命令。</p><h3 id="time-时间访问和转换">time 时间访问和转换</h3><p>Python中处理时间的标准库，提供系统级精确计时功能（可以用于程序性能分析）。其主要功能分为三块：时间获取、时间格式化、程序计时，这里介绍计时相关内容。</p><ul><li><code>sleep(s)</code>： <code>s</code>为休眠时间，单位秒，可以是浮点数。</li><li><code>perf_counter()</code>：返回一个 CPU级别的精确时间计数值，单位为秒。</li></ul><p>后者的使用由于<strong>计数器起点</strong>不确定，<strong>连续调用求差值</strong>才有意义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>arr = np.arange(<span class="hljs-number">100000000</span>)<br>start = time.perf_counter()<span class="hljs-comment"># 计数器起点</span><br>arr = arr * <span class="hljs-number">2</span><br>end = time.perf_counter()<span class="hljs-comment"># 计数器终点</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;numpy time cost: %.3f s&#x27;</span> % (end - start))<br></code></pre></td></tr></table></figure><p>当然，如果只是想测试单条指令的用时，可以在 IPython中调用如下命令，会<strong>多次执行计算平均值和方差</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>%%timeit<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.convolve(arr1, arr2)<br><span class="hljs-comment"># 4.87 s ± 3.06 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</span><br></code></pre></td></tr></table></figure><h3 id="datetime-日期与时间">datetime 日期与时间</h3><p>Python中用于处理日期和时间的标准库，包括三个主要的类：<code>date</code>、<code>time</code>和<code>datetime</code>。这些类用于表示<strong>日期、时间和日期时间</strong>。此外，还有一些函数和常量可用于日期和时间的处理。</p><ul><li><code>date</code>类用于表示日期，有三个属性：<code>year</code>、<code>month</code> 和<code>day</code>。</li><li><code>time</code>类用于表示时间，有三个属性：<code>hour</code>、<code>minute</code> 和<code>second</code>。</li><li><code>datetime</code> 类用于表示日期时间，是 <code>date</code> 类和<code>time</code> 类的组合。</li></ul><p>下面是具体命令，使用<code>strftime()</code>方法将<code>date</code>对象格式化为字符串：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> datetime<br><br><span class="hljs-comment"># 创建 date 对象</span><br>d = datetime.date(<span class="hljs-number">2023</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(d) <span class="hljs-comment"># 2023-04-04</span><br><span class="hljs-built_in">print</span>(d.strftime(<span class="hljs-string">&#x27;%Y/%m/%d&#x27;</span>)) <span class="hljs-comment"># 2023/04/04</span><br><br><span class="hljs-comment"># 创建 time 对象</span><br>t = datetime.time(<span class="hljs-number">12</span>, <span class="hljs-number">30</span>, <span class="hljs-number">45</span>)<br><span class="hljs-built_in">print</span>(t) <span class="hljs-comment"># 12:30:45</span><br><span class="hljs-built_in">print</span>(t.strftime(<span class="hljs-string">&#x27;%H:%M:%S&#x27;</span>)) <span class="hljs-comment"># 12:30:45</span><br><br><span class="hljs-comment"># 创建 datetime 对象</span><br>dt = datetime.datetime(<span class="hljs-number">2023</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">12</span>, <span class="hljs-number">30</span>, <span class="hljs-number">45</span>)<br><span class="hljs-built_in">print</span>(dt) <span class="hljs-comment"># 2023-04-04 12:30:45</span><br><span class="hljs-built_in">print</span>(dt.strftime(<span class="hljs-string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)) <span class="hljs-comment"># 2023-04-04 12:30:45</span><br></code></pre></td></tr></table></figure><p>以下是一些常用的命令，可用于日期和时间的处理：</p><ul><li><code>datetime.date.today()</code>：返回当前日期的<code>date</code>对象。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">today = datetime.date.today()<br><span class="hljs-built_in">print</span>(today) <span class="hljs-comment"># 2023-04-04</span><br></code></pre></td></tr></table></figure><ul><li><code>datetime.datetime.now()</code>：返回当前日期时间的<code>datetime</code>对象。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">now = datetime.datetime.now()<br><span class="hljs-built_in">print</span>(now) <span class="hljs-comment"># 2023-04-04 12:30:45.123</span><br></code></pre></td></tr></table></figure><ul><li><code>datetime.datetime.strptime()</code>：将字符串解析为<code>datetime</code>对象。需要指定字符串格式。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">dt_str = <span class="hljs-string">&#x27;2023-04-04 12:30:45&#x27;</span><br>dt = datetime.datetime.strptime(dt_str, <span class="hljs-string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)<br><span class="hljs-built_in">print</span>(dt) <span class="hljs-comment"># 2023-04-04 12:30:45</span><br></code></pre></td></tr></table></figure><ul><li><code>datetime.timedelta()</code>：用于计算时间差。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">delta = datetime.datetime.timedelta(days=<span class="hljs-number">3</span>)<br>new_date = today + delta<br><span class="hljs-built_in">print</span>(new_date) <span class="hljs-comment"># 2023-04-07</span><br></code></pre></td></tr></table></figure><ul><li><code>datetime.datetime.replace()</code>：用于更改<code>datetime</code>对象的某些属性。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">new_dt = dt.replace(hour=<span class="hljs-number">13</span>, minute=<span class="hljs-number">0</span>, second=<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(new_dt) <span class="hljs-comment"># 2023-04-04 13:00:00</span><br></code></pre></td></tr></table></figure><ul><li><code>datetime.datetime.fromtimestamp()</code>：用于从 UnixTimeStamp 得到 datetime。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">dt = datetime.datetime.fromtimestamp(ts) <span class="hljs-comment"># 1680582645</span><br><span class="hljs-built_in">print</span>(dt) <span class="hljs-comment"># 2023-04-04 12:30:45</span><br></code></pre></td></tr></table></figure><h3 id="logging-日志记录">logging 日志记录</h3><p>Python 中自带的日志模块，基本使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><br>logger = logging.getLogger(__name__)  <span class="hljs-comment"># 以当前文件名实例化 logger 记录器</span><br>logging.basicConfig(level=logging.INFO,<br>                    <span class="hljs-built_in">format</span>=<span class="hljs-string">&#x27;[%(asctime)s] [%(levelname)s] %(message)s&#x27;</span>,<br>                    datefmt=<span class="hljs-string">&#x27;%a, %d %b %Y %H:%M:%S&#x27;</span>,<br>                    filename=<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;Model_name&#125;</span>.log&#x27;</span>)  <span class="hljs-comment"># 简单配置日志级别与格式</span><br><br><span class="hljs-comment"># 具体用法，在代码中插入如下 message，常用 f-string</span><br>logging.debug(<span class="hljs-string">&#x27;This is a debug message&#x27;</span>)<br>logging.info(<span class="hljs-string">&#x27;This is a info message&#x27;</span>)<br>logging.warn(<span class="hljs-string">&#x27;This is a warn message&#x27;</span>)<br>logging.error(<span class="hljs-string">&#x27;This is a error message&#x27;</span>)<br>logging.critical(<span class="hljs-string">&#x27;This is a critical message&#x27;</span>) <br></code></pre></td></tr></table></figure><p>所谓日志级别，就是日志中只关注<strong>大于设定级别</strong>的内容：</p><table><thead><tr class="header"><th style="text-align: center;">级别</th><th style="text-align: center;">何时使用</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">DEBUG</td><td style="text-align: center;">详细信息，调试时对所有问题感兴趣</td></tr><tr class="even"><td style="text-align: center;">INFO</td><tdstyle="text-align: center;">事情按预期工作，训练模型<strong>最常用</strong></td></tr><tr class="odd"><td style="text-align: center;">WARNING</td><td style="text-align: center;">可能的意外，但仍能工作</td></tr><tr class="even"><td style="text-align: center;">ERROR</td><td style="text-align: center;">错误，软件已不能执行一些功能</td></tr><tr class="odd"><td style="text-align: center;">CRITICAL</td><td style="text-align: center;">严重错误，软件已不能继续运行</td></tr></tbody></table><blockquote><p>这里也可以换用暴力点的输出日志方法，直接用 Linux 的管道加<code>tee -a</code> 命令，将所有输出流复制到文件中，但是这种方法不好处理tqdm 产生的 bar。</p></blockquote><h3 id="glob-文件名模式匹配">glob 文件名模式匹配</h3><p>Python中用于查询符合特定规则的文件路径名的标准库，不用遍历整个目录判断每个文件是不是符合。使用<code>glob.glob('路径')</code> 调用，返回一个符合条件的路径字符串的 List结构，支持通配符<code>*</code>、<code>**</code>、<code>?</code>、<code>[]</code>，具体用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> glob<br><br><span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> glob.glob(<span class="hljs-string">&#x27;dir/*&#x27;</span>):<br>    <span class="hljs-keyword">pass</span>  <span class="hljs-comment"># * 遍历 dir 目录，但不会递归遍历子目录</span><br><br><span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> glob.glob(<span class="hljs-string">&#x27;dir/*/*&#x27;</span>):<br>    <span class="hljs-keyword">pass</span>  <span class="hljs-comment"># * 不会递归到子目录，因此只能手动指定</span><br><br><span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> glob.glob(<span class="hljs-string">&#x27;dir/**&#x27;</span>, recursive=<span class="hljs-literal">True</span>):<br>    <span class="hljs-keyword">pass</span>  <span class="hljs-comment"># ** 遍历 dir 下所有内容，包括子目录</span><br><br><span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> glob.glob(<span class="hljs-string">&#x27;dir/file?.txt&#x27;</span>):<br>    <span class="hljs-keyword">pass</span>  <span class="hljs-comment"># ? 匹配任何单个的字符</span><br><br><span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> glob.glob(<span class="hljs-string">&#x27;dir/*[0-9].txt&#x27;</span>):<br>    <span class="hljs-keyword">pass</span>  <span class="hljs-comment"># [] 匹配指定范围内的单个字符</span><br></code></pre></td></tr></table></figure><p>注意：<code>*</code> 通配符默认不匹配以 <code>.</code> 开始的文件，如<code>.config</code>，如果有这类文件，则需要显式标注点号。</p><h3 id="csv-文件读写">csv 文件读写</h3><p>Python 中用来读取 CSV 文件的标准库，一个 CSV文件经常包含若干行，每行包含若干列，列和列之前通常<strong>使用逗号分隔</strong>。其内部实现了一个reader 类和 writer 类，用于读写序列化的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> csv<br><br><span class="hljs-comment"># 读取文件，每一行是一个 List&lt;Str&gt;</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;some.csv&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, newline=<span class="hljs-string">&#x27;&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    reader = csv.reader(f)<br>    <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> reader:<br>        <span class="hljs-built_in">print</span>(row)<br><br><span class="hljs-comment"># 写入文件，每一行是 List&lt;Str&gt; 或 List&lt;List&lt;Str&gt;&gt;</span><br>data = [[<span class="hljs-string">&#x27;Name&#x27;</span>, <span class="hljs-string">&#x27;Age&#x27;</span>, <span class="hljs-string">&#x27;Country&#x27;</span>],<br>        [<span class="hljs-string">&#x27;John&#x27;</span>, <span class="hljs-string">&#x27;25&#x27;</span>, <span class="hljs-string">&#x27;USA&#x27;</span>],<br>        [<span class="hljs-string">&#x27;Alice&#x27;</span>, <span class="hljs-string">&#x27;30&#x27;</span>, <span class="hljs-string">&#x27;Canada&#x27;</span>],<br>        [<span class="hljs-string">&#x27;Bob&#x27;</span>, <span class="hljs-string">&#x27;35&#x27;</span>, <span class="hljs-string">&#x27;UK&#x27;</span>]]<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;some.csv&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, newline=<span class="hljs-string">&#x27;&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    writer = csv.writer(f)<br>    writer.writerows(data)<br><br><span class="hljs-comment"># 也可以逐行写入指定数据</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;some.csv&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, newline=<span class="hljs-string">&#x27;&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    writer = csv.writer(f)<br>    writer.writerow([<span class="hljs-string">&#x27;Name&#x27;</span>, <span class="hljs-string">&#x27;Age&#x27;</span>, <span class="hljs-string">&#x27;Country&#x27;</span>])<br>    writer.writerow([<span class="hljs-string">&#x27;John&#x27;</span>, <span class="hljs-string">&#x27;25&#x27;</span>, <span class="hljs-string">&#x27;USA&#x27;</span>])<br></code></pre></td></tr></table></figure><p>注意：如果没有指定<code>newline=''</code>，则嵌入引号中的换行符将无法正确解析，并且在写入时，使用<code>\r\n</code> 换行的平台会有多余的 <code>\r</code> 写入。由于 csv模块会执行自己的换行符处理，因此指定 <code>newline=''</code>应该<strong>总是安全的</strong>。</p><p>在定义 reader 或 writer 时，可以指定以下参数：</p><ul><li><code>delimiter=','</code>每一列的分隔符，默认为逗号，但在文本字符串中更常用 <code>\t</code>以避免错误分隔</li><li><code>quotechar='"'</code>为引号字符，默认为单引号，用于判断何时为字符串</li><li><code>escaoechar='\\'</code>为转义字符，默认为反斜杠，不需要修改</li></ul><p>除此之外，本库还针对<strong>每一行为字典的 JSONL文件</strong>定义了读写方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 每一行是一个 Dict&lt;&#x27;Str&#x27;:&#x27;Str&#x27;&gt;，即列名和值</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;some.csv&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, newline=<span class="hljs-string">&#x27;&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    reader = csv.DictReader(f)<br>    <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> reader:<br>        <span class="hljs-built_in">print</span>(row[<span class="hljs-string">&#x27;Name&#x27;</span>], row[<span class="hljs-string">&#x27;Age&#x27;</span>], row[<span class="hljs-string">&#x27;Country&#x27;</span>])<br>        <br>data = [&#123;<span class="hljs-string">&#x27;Name&#x27;</span>:<span class="hljs-string">&#x27;John&#x27;</span>, <span class="hljs-string">&#x27;Age&#x27;</span>:<span class="hljs-string">&#x27;25&#x27;</span>, <span class="hljs-string">&#x27;Country&#x27;</span>:<span class="hljs-string">&#x27;USA&#x27;</span>&#125;,<br>        &#123;<span class="hljs-string">&#x27;Name&#x27;</span>:<span class="hljs-string">&#x27;Alice&#x27;</span>, <span class="hljs-string">&#x27;Age&#x27;</span>:<span class="hljs-string">&#x27;30&#x27;</span>, <span class="hljs-string">&#x27;Country&#x27;</span>:<span class="hljs-string">&#x27;Canada&#x27;</span>&#125;,<br>        &#123;<span class="hljs-string">&#x27;Name&#x27;</span>:<span class="hljs-string">&#x27;Bob&#x27;</span>, <span class="hljs-string">&#x27;Age&#x27;</span>:<span class="hljs-string">&#x27;35&#x27;</span>, <span class="hljs-string">&#x27;Country&#x27;</span>:<span class="hljs-string">&#x27;UK&#x27;</span>&#125;]<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;some.csv&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, newline=<span class="hljs-string">&#x27;&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    fieldnames = [<span class="hljs-string">&#x27;Name&#x27;</span>, <span class="hljs-string">&#x27;Age&#x27;</span>, <span class="hljs-string">&#x27;Country&#x27;</span>]<br>    writer = csv.DictWriter(f, fieldnames=fieldnames)<br>    writer.writeheader()<br>    writer.writerows(data)<br></code></pre></td></tr></table></figure><blockquote><p>csv 库的功能大部分被 pandas所包含，如果结构化处理数据的需求，推荐直接用 pandas 的 csv读写函数。</p></blockquote><h3 id="argparse-命令行参数解析">argparse 命令行参数解析</h3><p>Python中用于解析命令行参数和选项的标准库。在复现较大型项目时，通常由工具脚本调用命令行参数，这些参数以链表形式存储于sys 模块的 argv变量，直接处理较为麻烦，因此有了该标准库。使用范例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> argparse<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_args</span>():</span><br>    <span class="hljs-comment"># 声明一个解析对象，以及当 -h 或参数出错时会打印的信息</span><br>    parser = argparse.ArgumentParser(description=<span class="hljs-string">&quot;You should add those parameter!&quot;</span>)  <br><br>    <span class="hljs-comment"># 罗列所需参数，下面给出常见例子</span><br>    <span class="hljs-comment"># 通常来讲，type 和 default 是必须的，其他视情况而定</span><br>    <span class="hljs-comment"># 其中 store_true 表示默认为 False，指定时切换为 True</span><br>    parser.add_argument(<span class="hljs-string">&quot;--path&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, required=<span class="hljs-literal">True</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;The path of file&quot;</span>)<br>    parser.add_argument(<span class="hljs-string">&quot;--input_num&quot;</span>, <span class="hljs-string">&quot;-n&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">50</span>)<br>    parser.add_argument(<span class="hljs-string">&quot;--mode&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, choices=[<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>])<br>    parser.add_argument(<span class="hljs-string">&#x27;--use_clip&#x27;</span>, action=<span class="hljs-string">&#x27;store_true&#x27;</span>)<br>    <br>    <br>    <span class="hljs-comment"># 解析参数并返回</span><br>    args = parser.parse_args()<br>    <span class="hljs-keyword">return</span> args<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    my_args = read_args()<br>    <span class="hljs-built_in">print</span>(my_args.path)<br></code></pre></td></tr></table></figure><h3 id="re-正则表达式">re 正则表达式</h3><p>Python 中正则表达式匹配的标准库。模式和被搜索的字符串既可以是 Unicode字符串（str） ，也可以是 8 位字节串（byte），但必须一致。</p><p>关于正则表达式的语法此处不过多介绍，仅给出一些示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> re<br><span class="hljs-meta">&gt;&gt;&gt; </span>re.findall(<span class="hljs-string">r&#x27;\bf[a-z]*&#x27;</span>, <span class="hljs-string">&#x27;which foot or hand fell fastest&#x27;</span>)<br>[<span class="hljs-string">&#x27;foot&#x27;</span>, <span class="hljs-string">&#x27;fell&#x27;</span>, <span class="hljs-string">&#x27;fastest&#x27;</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span>re.sub(<span class="hljs-string">r&#x27;(\b[a-z]+) \1&#x27;</span>, <span class="hljs-string">r&#x27;\1&#x27;</span>, <span class="hljs-string">&#x27;cat in the the hat&#x27;</span>)<br><span class="hljs-string">&#x27;cat in the hat&#x27;</span><br></code></pre></td></tr></table></figure><p>注意：若只需要简单的功能，应该首先考虑字符串方法，如<code>.replace()</code>、<code>.split()</code>等，更易于阅读和调试。</p><h3 id="zipfile-文件压缩">zipfile 文件压缩</h3><p>Python 标准库中的一个模块，用于创建、读取和操作 ZIP文件。主要作用包括：创建 ZIP 文件、解压 ZIP 文件、读取 ZIP文件（但不进行解压）、操作 ZIP 文件（增删改）。</p><p>其中，所有的方法被封装到 <code>zipfile.ZipFile</code>类，使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> zipfile<br><br><span class="hljs-comment"># 创建 zip 文件并将文件添加到其中</span><br><span class="hljs-keyword">with</span> zipfile.ZipFile(<span class="hljs-string">&#x27;archive.zip&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> zipf:<br>    zipf.write(<span class="hljs-string">&#x27;file1.txt&#x27;</span>)<br>    zipf.write(<span class="hljs-string">&#x27;file2.txt&#x27;</span>)<br><br><span class="hljs-comment"># 将 w 修改为 a，就可以向已有的文件中添加新文件</span><br><span class="hljs-keyword">with</span> zipfile.ZipFile(<span class="hljs-string">&#x27;existing.zip&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>) <span class="hljs-keyword">as</span> zipf:<br>    zipf.write(<span class="hljs-string">&#x27;new_file.txt&#x27;</span>)<br>    <br><span class="hljs-comment"># 解压缩 zip 文件，参数为目标目录</span><br><span class="hljs-keyword">with</span> zipfile.ZipFile(<span class="hljs-string">&#x27;archive.zip&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> zipf:<br>    zipf.extractall(<span class="hljs-string">&#x27;extracted_dir&#x27;</span>)<br></code></pre></td></tr></table></figure><p>对于一个已有的 ZIP 文件，可以进行的操作如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> zipfile<br><br><span class="hljs-comment"># 获取 zip 文件中的所有成员的名称列表</span><br><span class="hljs-keyword">with</span> zipfile.ZipFile(<span class="hljs-string">&#x27;archive.zip&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> zipf:<br>    file_list = zipf.namelist()<br>    <span class="hljs-built_in">print</span>(zipf.getinfo(<span class="hljs-string">&#x27;file1.txt&#x27;</span>))<br><br><span class="hljs-comment"># 想要修改其中的某个文件，需要先将其解压缩，并覆盖写入</span><br><span class="hljs-keyword">with</span> zipfile.ZipFile(<span class="hljs-string">&#x27;archive.zip&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> zipf:<br>    <span class="hljs-keyword">with</span> zipf.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;file1.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>        zipf.writestr(<span class="hljs-string">&#x27;file1.txt&#x27;</span>, file.read())<br></code></pre></td></tr></table></figure><p>zipfile库在实际应用中被广泛使用，特别是在需要<strong>处理大量文件或目录</strong>的情况下，使用ZIP 文件可以方便地进行文件的打包、传输、存储和管理。</p><h3 id="traceback-异常处理和调试">traceback 异常处理和调试</h3><p>Python中用于异常处理和调试的标准库之一。它允许查看程序执行时发生的<strong>异常信息</strong>，并追踪<strong>函数调用的堆栈</strong>。常用的函数有以下两种：</p><ul><li><code>traceback.print_tb(tb, limit=None, file=None)</code>：打印当前异常的堆栈跟踪信息。它接受以下参数：<ul><li><code>tb</code>：表示异常信息的 traceback 对象。</li><li><code>limit</code>：可选参数，用于指定打印堆栈跟踪信息的层数。如果不指定，将打印整个堆栈。</li><li><code>file</code>：可选参数，用于指定输出的文件对象。如果不指定，将打印到标准输出。</li></ul></li><li><code>traceback.print_exception(etype, value, tb, limit=None, file=None, chain=True)</code>：打印异常信息，包括异常类型、异常值和堆栈跟踪信息。它接受以下参数：<ul><li><code>etype</code>：异常类型。</li><li><code>value</code>：异常值。</li><li><code>tb</code>：表示异常信息的 traceback 对象。</li><li><code>limit</code>：可选参数，用于指定打印堆栈跟踪信息的层数。如果不指定，将打印整个堆栈。</li><li><code>file</code>：可选参数，用于指定输出的文件对象。如果不指定，将打印到标准输出。</li><li><code>chain</code>：可选参数，如果为<code>True</code>，将打印异常链中的所有异常信息。默认为<code>True</code>。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> traceback<br><br><span class="hljs-keyword">try</span>:<br>    result = <span class="hljs-number">10</span> / <span class="hljs-number">0</span><br><span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>    traceback.print_tb(e.__traceback__)<br>    traceback.print_exception(<span class="hljs-built_in">type</span>(e), e, e.__traceback__)<br></code></pre></td></tr></table></figure><h2 id="第三方库">第三方库</h2><p>第三方库通常需要<code>pip install</code>，建议安装在虚拟环境中，按需取用。</p><h3 id="pipreqs-依赖安装">pipreqs 依赖安装</h3><p>当我们写好一个 Python项目之后，如果要开源代码，为了让别人能快速运行项目，一般可以提供一个<code>requirements.txt</code> 的，用以配置环境。</p><p><code>pipreqs</code> 是用于自动生成上述文件的一个第三方库，使用<code>pip install pipreqs</code>安装，进入<strong>项目根目录</strong>，执行 <code>pipreqs ./</code>即可。如果出现编码错误，可以指定编码方式解决：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">pipreqs ./ -<span class="hljs-literal">-encoding</span> utf<span class="hljs-literal">-8</span><br></code></pre></td></tr></table></figure><p>其他人则可以使用以下命令配置环境：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">pip install <span class="hljs-literal">-r</span> requirements.txt<br></code></pre></td></tr></table></figure><h3 id="codecs-编码转换">codecs 编码转换</h3><p><code>codec</code> 意为字符的编解码器（Encoder&amp;Decoder），Python对多国语言的处理是支持的很好的，它可以处理现在任意编码的字符。在作字符转换时，Python会借助于内部的编码（Unicode），转换的过程是这样的：<code>原有编码 -&gt; 内部编码 -&gt; 目的编码</code>。而这个转换的过程就需要用到编解码器。</p><p>一种基本的用法是通过 <code>lookup</code> 获取特定的编码器，再用其与Unicode 进行转换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> codecs<br><span class="hljs-meta">&gt;&gt;&gt; </span>codec1 = codecs.lookup(<span class="hljs-string">&quot;utf-8&quot;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>a = <span class="hljs-string">&quot;你好&quot;</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>b = codec1.encode(a)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(b)<br>(<span class="hljs-string">b&#x27;\xe4\xbd\xa0\xe5\xa5\xbd&#x27;</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># 其中 2 为 len(a)</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>c = codec1.decode(b[<span class="hljs-number">0</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(c)<br>(<span class="hljs-string">&#x27;你好&#x27;</span>, <span class="hljs-number">6</span>) <span class="hljs-comment"># 其中 6 为 len(b[0])</span><br></code></pre></td></tr></table></figure><p>另一种更常用的用法是对<strong>文件</strong>字符编码的处理，常用于数据处理任务：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> codecs<br><span class="hljs-comment"># 使用 codecs 自带的 open 读取文件，指定编码方式</span><br>file = codecs.<span class="hljs-built_in">open</span>(path, encoding=<span class="hljs-string">&#x27;latin-1&#x27;</span>).readlines()<br></code></pre></td></tr></table></figure><h3 id="json-文件读写">json 文件读写</h3><p>JSON（JavaScript ObjectNotation）是一种轻量级的<strong>数据交换格式</strong>，采用完全独立于语言的文本格式，但格式类似C系语言。易于人阅读和编写，同时也易于机器解析和生成（一般用于提升网络传输速率）。使用<code>pip install json-parser</code> 安装。</p><p>Python 中的 List、Dict 最常用于 JSON文件的转换，在此前要先将数据类型转为字符串 Str。json模块提供了四个功能：</p><ul><li><code>dumps</code>：把数据类型转换成字符串</li><li><code>dump</code>：把数据类型转换成字符串并存储在文件中</li><li><code>loads</code>：把字符串转换成数据类型</li><li><code>load</code>：把文件打开从字符串转换成数据类型</li></ul><p>基本使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><br>test_dict = &#123;<span class="hljs-string">&#x27;bigberg&#x27;</span>: [<span class="hljs-number">7600</span>, <br>                         &#123;<span class="hljs-string">&#x27;details&#x27;</span>: [[<span class="hljs-string">&#x27;iPhone&#x27;</span>, <span class="hljs-number">6300</span>],<br>                                      [<span class="hljs-string">&#x27;Bike&#x27;</span>, <span class="hljs-number">800</span>],<br>                                      [<span class="hljs-string">&#x27;shirt&#x27;</span>, <span class="hljs-number">300</span>]]<br>                         &#125;]<br>            &#125;<br>json_str = json.dumps(test_dict)  <span class="hljs-comment"># dict -&gt; str</span><br>new_dict = json.loads(json_str)<span class="hljs-comment"># str -&gt; dict</span><br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;../config/record.json&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    json.dump(new_dict,f)<span class="hljs-comment"># dict -&gt; str -&gt; json</span><br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;../config/record.json&quot;</span>,<span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> load_f:<br>    load_dict = json.load(load_f)  <span class="hljs-comment"># json -&gt; str -&gt; dict</span><br></code></pre></td></tr></table></figure><p>注意：JSON文件通常是<strong>不可读的</strong>，因为其将整个列表或字典压缩到同一行字符串中，以减少文件体积。如果要增加其可读性，可以<strong>在转换时指定缩进</strong>，例如<code>dump(data, file, indent=4)</code>。</p><p>注意：JSON 保存输出时默认会<strong>将非 ASCII 字符进行 Unicode转移</strong>，生成 <code>\u...</code>，如果想要禁止这一过程，可以在dump 中使用 <code>ensure_ascii=False</code>。该设置不会影响 load方法。</p><p>此外，还有一种 JSONL文件，一行为一条记录或数据（可读性稍好），常用作<strong>日志文件</strong>或<strong>数据集</strong>，支持用json 操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">data = []<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;input.jsonl&quot;</span>,<span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    data_in = f.readlines()    <span class="hljs-comment"># jsonl -&gt; list[str]</span><br>    <span class="hljs-keyword">for</span> obj <span class="hljs-keyword">in</span> data_in:<br>        data.append(json.loads(obj))  <span class="hljs-comment"># list[str] -&gt; list[dict]</span><br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;output.jsonl&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-keyword">for</span> obj <span class="hljs-keyword">in</span> data:<br>    f.write(json.dumps(obj) + <span class="hljs-string">&#x27;\n&#x27;</span>)  <span class="hljs-comment"># list[dict] -&gt; list[str] -&gt; jsonl</span><br>        <span class="hljs-comment"># 这里的 \n 不可缺少，因为 write 不会自动换行</span><br></code></pre></td></tr></table></figure><p>常见错误：在使用 dump 的时候报错 TypeError: Object of type int64 isnot JSON serializable，这是<strong>由于 dump 不能处理 NumPy中的数据类型</strong>，此时需要手动给整数套上<code>int()</code>、浮点数套上 <code>float()</code>、多维数组加上<code>.tolist()</code>。也可以通过重加载 JSONEncoder 实现上述功能：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NpEncoder</span>(<span class="hljs-params">json.JSONEncoder</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">default</span>(<span class="hljs-params">self, obj</span>):</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(obj, np.integer):<br>            <span class="hljs-keyword">return</span> <span class="hljs-built_in">int</span>(obj)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(obj, np.floating):<br>            <span class="hljs-keyword">return</span> <span class="hljs-built_in">float</span>(obj)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(obj, np.ndarray):<br>            <span class="hljs-keyword">return</span> obj.tolist()<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>(NpEncoder, self).default(obj)<br><br>json.dumps(data, cls=NpEncoder)<br></code></pre></td></tr></table></figure><h3 id="jsonlines-文件读写">jsonlines 文件读写</h3><p>当然，上述操作也可以使用专门的 JSONL 处理模块替代：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> jsonlines<br><br>data = []<br><span class="hljs-keyword">with</span> jsonlines.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;input.jsonl&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> reader:<br>    <span class="hljs-keyword">for</span> obj <span class="hljs-keyword">in</span> reader:<span class="hljs-comment"># 隐含 jsonl -&gt; str -&gt; dict</span><br>        data.append(obj)   <span class="hljs-comment"># -&gt; list[dict]</span><br>        <br><span class="hljs-keyword">with</span> jsonlines.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;output.jsonl&#x27;</span>, mode=<span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> writer:<br>    <span class="hljs-keyword">for</span> obj <span class="hljs-keyword">in</span> data:<span class="hljs-comment"># -&gt; dict</span><br>    writer.write(obj)  <span class="hljs-comment"># 隐含 dict -&gt; str -&gt; jsonl，自动换行</span><br></code></pre></td></tr></table></figure><h3 id="tqdm-进度条">tqdm 进度条</h3><p>tqdm 是一个用于在 Python命令行中添加<strong>进度条</strong>的库。它提供了一种简单而有效的方式来监测长时间运行的任务的进度，只需用<code>tqdm(iterable)</code>装饰<strong>任何可迭代对象</strong>，就能直观地看到进度条：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm  <span class="hljs-comment"># 新手常常犯错的点：直接 import tqdm</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>)):<span class="hljs-comment"># tqdm 返回值本身也是一个可迭代对象</span><br>    time.sleep(<span class="hljs-number">0.1</span>) <span class="hljs-comment"># 模拟耗时操作</span><br></code></pre></td></tr></table></figure><p>此时命令行输出的样式如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-number">35</span>%|██████████                          | <span class="hljs-number">35</span>/<span class="hljs-number">100</span> [<span class="hljs-number">00</span>:<span class="hljs-number">02</span>&lt;<span class="hljs-number">00</span>:<span class="hljs-number">05</span>, <span class="hljs-number">18.78</span>it/s]<br></code></pre></td></tr></table></figure><p>在 <code>tqdm()</code> 中传入参数可以修改样式：</p><ul><li><code>bar_format="&#123;l_bar&#125;&#123;bar&#125;| &#123;n_fmt&#125;/&#123;total_fmt&#125;"</code>设置进度条样式</li><li><code>ncols=80</code> 设置进度条总长度，默认为填充整行命令行</li><li><code>desc="Processing"</code>设置前缀，默认为空，即只显示百分比</li><li><code>postfix=&#123;"loss": 0, "acc": 0&#125;</code> 设置额外的后缀项目</li><li><code>leave=True</code> 进度条在完成后是否保持可见，默认为 True</li></ul><p>实际上 <code>tqdm()</code> 除了可以直接传入可迭代对象，也可以使用<code>with</code> 语句来创建了一个 tqdm的<strong>上下文管理器</strong>。通过设置 <code>total</code>参数为迭代对象的长度来告诉 tqdm 进度条的总长度，并使用<code>update()</code> 来在每轮迭代中更新。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建上下文管理器，更优雅，不需要 pbar.close()</span><br><span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">len</span>(train_loader)) <span class="hljs-keyword">as</span> pbar:<br>    <span class="hljs-keyword">for</span> item, label <span class="hljs-keyword">in</span> train_loader:<br>        pbar.set_description(<span class="hljs-string">f&quot;Processing...&quot;</span>) <span class="hljs-comment"># 等同于 desc</span><br>        pbar.set_postfix(loss=loss, acc=acc)<span class="hljs-comment"># 等同于 postfix</span><br>        pbar.update()  <span class="hljs-comment"># 手动增加进度条的当前值，默认 +1</span><br></code></pre></td></tr></table></figure><p>那么如何优雅地在<strong>深度学习训练框架</strong>中应用 tqdm 呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    losses = []<br>    num_correct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> tqdm(total=<span class="hljs-built_in">len</span>(train_loader), ncols=<span class="hljs-number">20</span>) <span class="hljs-keyword">as</span> pbar:<br><span class="hljs-keyword">for</span> idx, (inputs, targets) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>            optimizer.zero_grad()<br>            inputs, targets = inputs.to(device), targets.to(device)<br>            preds = model(inputs)<br>            loss = criterion(preds, targets)<br><br>            losses.append(loss)<br>            loss.backward()<br>            optimizer.step()<br><br>        num_correct += (predictions == targets).<span class="hljs-built_in">sum</span>()<br>        acc = num_correct / (idx + <span class="hljs-number">1</span>) * batch_size<br>        <br>            pbar.set_description(<span class="hljs-string">f&#x27;Epoch [<span class="hljs-subst">&#123;epoch&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>]&#x27;</span>)<br>            pbar.set_postfix(loss=<span class="hljs-built_in">round</span>(loss.item(),<span class="hljs-number">4</span>), acc=<span class="hljs-built_in">round</span>(acc,<span class="hljs-number">4</span>))<br>            pbar.update()<br></code></pre></td></tr></table></figure><h3 id="pprint-美化输出">pprint 美化输出</h3><p>全称 Pretty Print，用于替代原来的 <code>print()</code>函数，主要用于打印复杂的数据结构对象，例如多层嵌套的列表、元组和字典等。常用于<strong>多层嵌套的内容</strong>（例如复杂的json 文件），或者有<strong>超多的元素</strong>（例如在列表中存了很多 URL链接）。功能有：</p><ul><li>设置合适的行宽度，作适当的换行；</li><li>设置打印的缩进、层级，进行格式化打印；</li><li>判断对象中是否有无限循环，并优化打印内容。</li></ul><p>基本使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pprint <span class="hljs-keyword">import</span> pprint<br><span class="hljs-comment"># 完整接口：pprint(object, stream=None, indent=1, width=80, depth=None, compact=False)</span><br>pprint(json.loads(sample_file[<span class="hljs-number">0</span>]))<br></code></pre></td></tr></table></figure><h3 id="pil-图像处理">PIL 图像处理</h3><p>全称 Python Imaging Library，有时也称 pillow，使用<code>pip install pillow</code>安装。支持众多图像格式，可用于执行裁剪、大小调整、旋转、滤镜效果等操作，引入时通常使用<code>from PIL import Image</code>。</p><p>基本使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-comment"># 打开、显示、保存，以及图像的三个属性：模式（RGB，L）、大小（像素）、格式</span><br>image = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;input.jpg&#x27;</span>)<br>image.show()<br>image.save(<span class="hljs-string">&#x27;output.jpg&#x27;</span>)<br><span class="hljs-built_in">print</span>(image.mode, image.size, image.<span class="hljs-built_in">format</span>)  <span class="hljs-comment"># RGB (481, 321) JPEG</span><br><br>grey_image = image.convert(<span class="hljs-string">&#x27;L&#x27;</span>)  <span class="hljs-comment"># 从 RGB 转为 L 灰度图像</span><br>grey_image.show()<br><br>r, g, b = image.split()  <span class="hljs-comment"># 彩色图像可以分离出 R、G、B 通道，每个通道都是一个图像</span><br>im = Image.merge(<span class="hljs-string">&#x27;RGB&#x27;</span>, (b, g, r))  <span class="hljs-comment"># 按照一定的顺序再合并成彩色图像</span><br></code></pre></td></tr></table></figure><p>支持对图像整体、局部进行像素级操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">box = (<span class="hljs-number">100</span>, <span class="hljs-number">100</span>, <span class="hljs-number">300</span>, <span class="hljs-number">300</span>)  <span class="hljs-comment"># 设置一个四元组裁剪框</span><br>region = image.crop(box)  <span class="hljs-comment"># 减除</span><br>region = region.transpose(Image.ROTATE_180)  <span class="hljs-comment"># 旋转 180 度</span><br>image.paste(region, box)  <span class="hljs-comment"># 粘贴</span><br><br>im = image.resize((<span class="hljs-number">300</span>, <span class="hljs-number">300</span>))  <span class="hljs-comment"># 使用插值算法缩放</span><br>im = image.rotate(<span class="hljs-number">45</span>)  <span class="hljs-comment"># 逆时针旋转 45 度</span><br>im = image.transpose(Image.FLIP_LEFT_RIGHT) <span class="hljs-comment"># 左右翻转</span><br>im = image.transpose(Image.FLIP_TOP_BOTTOM) <span class="hljs-comment"># 上下翻转</span><br><br>im = image.point(<span class="hljs-keyword">lambda</span> i: i * <span class="hljs-number">1.2</span>) <span class="hljs-comment"># 对每个像素值乘以 1.2</span><br>im = image.point(<span class="hljs-keyword">lambda</span> i: i &gt; <span class="hljs-number">128</span> <span class="hljs-keyword">and</span> <span class="hljs-number">255</span>) <span class="hljs-comment"># 二值化，利用 and 短路</span><br></code></pre></td></tr></table></figure><p>还支持与 Numpy 的交互，用于手动实现图像处理算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">array = np.array(image)<br><span class="hljs-built_in">print</span>(array.shape) <span class="hljs-comment">#(321, 481, 3)</span><br>i = Image.fromarray(array)<br></code></pre></td></tr></table></figure><p>注意，PIL 只能完成基础的图像处理，更高端的操作还得用到 OpenCV 的 cv2模块，例如：仿射变换、基本绘制、随机变化等。</p><h3 id="jupyter-notebook">Jupyter Notebook</h3><p>Jupyter Notebook 有两种键盘输入模式：</p><ol type="1"><li><strong>编辑模式</strong>，允许你往单元中键入代码或文本；这时的单元框线是绿色的。</li><li><strong>命令模式</strong>，键盘输入运行程序命令；这时的单元框线是灰色。</li></ol><p><strong>命令模式（按键 Esc 开启）快捷键</strong>：</p><ol type="1"><li><strong>Enter</strong>：转入编辑模式</li><li><strong>Ctrl-Enter</strong>：运行本单元</li><li><strong>Shift-Enter</strong>：运行本单元，选中下个单元</li><li><strong>Alt-Enter</strong>：运行本单元，在其下插入新单元</li><li><strong>Y</strong>：单元转入代码状态</li><li><strong>M</strong>：单元转入 Markdown 状态</li><li><strong>方向键</strong>：选中上方、下方单元</li><li><strong>A</strong>：在上方插入新单元</li><li><strong>B</strong>：在下方插入新单元</li><li><strong>X</strong>：剪切选中的单元</li><li><strong>C</strong>：复制选中的单元</li><li><strong>Shift-V</strong>：粘贴到上方单元</li><li><strong>V</strong>：粘贴到下方单元</li><li><strong>Z</strong>：恢复删除的最后一个单元</li><li><strong>Shift-M</strong>：合并选中的单元</li></ol><p><strong>编辑模式（Enter 键启动）下快捷键</strong>：</p><ol type="1"><li><strong>Tab</strong>：代码补全或缩进</li><li><strong>Shift-Tab</strong>：提示</li><li><strong>Esc</strong>：进入命令模式</li><li><strong>Ctrl-Enter</strong>：运行本单元</li><li><strong>Shift-Enter</strong>：运行本单元，选中下一单元</li><li><strong>Alt-Enter</strong>：运行本单元，在下面插入一单元</li></ol>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>科研常用网站合集</title>
    <link href="/Awesome-Sites.html"/>
    <url>/Awesome-Sites.html</url>
    
    <content type="html"><![CDATA[<p>记录科研常用网站，包括：论文检索、代码检索、学者信息、论文写作、科研论坛、截稿日期、电子书。本文持续更新。</p><h2 id="paper">Paper</h2><table><thead><tr class="header"><th style="text-align: center;">Site</th><th style="text-align: center;">Describe</th><th style="text-align: center;">推荐指数</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><ahref="http://www.arxivdaily.com/">ArXiv Daily</a></td><td style="text-align: center;"><strong>每日爬取 ArXiv</strong>各个领域论文，适合速刷</td><td style="text-align: center;">※※※※※</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://papers.labml.ai/papers/daily">Daliy Papers</a></td><tdstyle="text-align: center;">近期<strong>热点论文</strong>追踪，每天必刷！</td><td style="text-align: center;">※※※※※</td></tr><tr class="odd"><td style="text-align: center;"><ahref="https://www.semanticscholar.org/">Semantic Scholar</a></td><tdstyle="text-align: center;">查看一篇论文的<strong>被引</strong>，按次数排序，<strong>更新速度很快</strong></td><td style="text-align: center;">※※※※</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://www.connectedpapers.com/">Connected Papers</a></td><tdstyle="text-align: center;">用<strong>连通图</strong>展示同领域论文，大小论文都适用</td><td style="text-align: center;">※※※※</td></tr><tr class="odd"><td style="text-align: center;"><ahref="https://openreview.net/">OpenReview</a></td><tdstyle="text-align: center;">检索<strong>最新在投论文</strong>，追踪顶会动向，可以看到<strong>审稿意见</strong></td><td style="text-align: center;">※※※※</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://www.scholar-inbox.com/">Shcolar Inbox</a></td><tdstyle="text-align: center;">将最新论文按照<strong>研究兴趣匹配程度</strong>进行排序并推送</td><td style="text-align: center;">※※※※</td></tr><tr class="odd"><td style="text-align: center;"><ahref="https://arxiv.org/">ArXiv</a></td><td style="text-align: center;">预印版论文下载，适合占坑</td><td style="text-align: center;">※※※</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://github.com/">GitHub</a></td><td style="text-align: center;">偶尔会有好心人放出领域论文集</td><td style="text-align: center;">※※※</td></tr><tr class="odd"><td style="text-align: center;"><ahref="https://www.paperdigest.org/">Paper Digest</a></td><tdstyle="text-align: center;">快速搜索<strong>领域论文</strong>、最新会议论文索引+ <strong>highlight</strong></td><td style="text-align: center;">※※※</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://openaccess.thecvf.com/menu">CVF Open Access</a></td><td style="text-align: center;">CV 会议论文下载</td><td style="text-align: center;">※※</td></tr><tr class="odd"><td style="text-align: center;"><ahref="https://ai-paper-collector.vercel.app/">AI-Paper-Search</a></td><td style="text-align: center;">国人开发的插件，支持关键词匹配 AI顶会</td><td style="text-align: center;">※</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://ac.scmor.com/">思谋学术导航</a></td><td style="text-align: center;">谷歌学术镜像与 Sci-Hub 导航</td><td style="text-align: center;">※</td></tr><tr class="odd"><td style="text-align: center;"><ahref="https://xueshu.dailyheadlines.cc/">深度学术搜索</a></td><td style="text-align: center;">谷歌学术镜像</td><td style="text-align: center;">※</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://readpaper.com/">ReadPaper</a></td><tdstyle="text-align: center;">国内论文社区，可以看到别人对热点论文的<strong>笔记</strong></td><td style="text-align: center;">※</td></tr><tr class="odd"><td style="text-align: center;">其他途径</td><td style="text-align: center;">公众号、组会分享、学术主页、顶会 AcceptList</td><td style="text-align: center;">※※※※</td></tr></tbody></table><h2 id="code">Code</h2><table><thead><tr class="header"><th style="text-align: center;">Site</th><th style="text-align: center;">Describe</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><ahref="https://paperswithcode.com/">Papers With Code</a></td><tdstyle="text-align: center;">链接论文与仓库、数据集，部分有多个仓库</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://github.com/">GitHub</a></td><td style="text-align: center;">搜索论文标题、模型名称</td></tr><tr class="odd"><td style="text-align: center;"><ahref="https://www.catalyzex.com/">CatalyzeX</a></td><td style="text-align: center;">从下游任务查找论文和代码</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://zenodo.org/">Zenodo</a></td><td style="text-align: center;">很多论文上传模型和数据集的地方，容量50G</td></tr></tbody></table><h2 id="people">People</h2><table><thead><tr class="header"><th style="text-align: center;">Site</th><th style="text-align: center;">Describe</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><ahref="https://scholar.google.com/citations?view_op=search_authors&amp;mauthors=&amp;hl=zh-CN&amp;oi=drw">GoogleScholar</a></td><tdstyle="text-align: center;">个人学术档案，但有时会出错，且引用格式老旧</td></tr><tr class="even"><td style="text-align: center;"><ahref="http://csrankings.org/#/index?all&amp;cn">CSRanking</a></td><tdstyle="text-align: center;">按照单位、领域查找个人，且收录个人主页</td></tr><tr class="odd"><td style="text-align: center;"><ahref="https://dblp.org/">dblp</a></td><tdstyle="text-align: center;">仅存储文献的元数据，但收录速度快，比谷歌准，<strong>适合引用</strong></td></tr><tr class="even"><td style="text-align: center;"><ahref="https://www.aminer.cn/">Aminer</a></td><tdstyle="text-align: center;">用<strong>河流图</strong>展示研究兴趣演变，适合分析领域</td></tr></tbody></table><h2 id="writing">Writing</h2><table><thead><tr class="header"><th style="text-align: center;">Site</th><th style="text-align: center;">Describe</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><ahref="https://www.deepl.com/">DeepL</a></td><td style="text-align: center;">比 Google翻译更好，适合憋不出英文时用</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://hemingwayapp.com/">Hemingway App</a></td><tdstyle="text-align: center;"><strong>快速</strong>找出语病并修改，找出<strong>hard to read</strong> 段落</td></tr><tr class="odd"><td style="text-align: center;"><ahref="https://quillbot.com/">QuillBot</a></td><td style="text-align: center;"><strong>一键修改</strong> hard to read段落；<strong>一键检查语法</strong></td></tr><tr class="even"><td style="text-align: center;"><ahref="https://www.grammarly.com/">Grammarly</a></td><tdstyle="text-align: center;">检查<strong>基本错误</strong>并替换，适合插件使用</td></tr><tr class="odd"><td style="text-align: center;"><ahref="http://www.esoda.org/">ESODA</a></td><td style="text-align: center;">爬取领域论文，优化专用选词、搭配</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://linggle.com/">Linggle</a></td><td style="text-align: center;">查找搭配神器，支持 <code>n.</code><code>_</code> <code>?</code> <code>/</code> 语法</td></tr><tr class="odd"><td style="text-align: center;"><ahref="https://books.google.com/ngrams">Books Ngram Viewer</a></td><td style="text-align: center;">用 N-gram检查两个搭配谁的使用率更高</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://www.tablesgenerator.com/">Tables Generator</a></td><td style="text-align: center;">LaTeX<strong>表格</strong>生成器，所见即所得</td></tr></tbody></table><h2 id="forum">Forum</h2><table><thead><tr class="header"><th style="text-align: center;">Site</th><th style="text-align: center;">Describe</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><ahref="https://towardsdatascience.com/">Towards Data Science</a></td><td style="text-align: center;">数据科学领域的 CSDN，文章质量高</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://www.techbeat.net/">TechBeat</a></td><td style="text-align: center;">中文 AI 论坛，有大量 Talk 发布</td></tr></tbody></table><h2 id="deadline">Deadline</h2><table><thead><tr class="header"><th style="text-align: center;">Site</th><th style="text-align: center;">Describe</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><ahref="https://aideadlin.es/?sub=ML,CV,CG,NLP,RO,SP,DM">AI ConferenceDDL</a></td><td style="text-align: center;">人工智能领域会议截稿日期</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://sec-deadlines.github.io/">Security and PrivacyDDL</a></td><td style="text-align: center;">安全领域会议截稿日期</td></tr><tr class="odd"><td style="text-align: center;"><ahref="https://ddl.yanlin.info/?sub=">CS Conference DDL</a></td><td style="text-align: center;">比前两者多了一些领域</td></tr><tr class="even"><td style="text-align: center;"><a href="https://ccfddl.github.io/">CCFDDL</a></td><td style="text-align: center;">比前者多标注了 CCF 推荐</td></tr><tr class="odd"><td style="text-align: center;"><ahref="http://123.57.137.208/ccf/ccf-8.jsp">Call4Paper</a></td><td style="text-align: center;">CCF 推荐列表 +截稿日期（包括已结束）</br> 偶尔不准</td></tr></tbody></table><h2 id="book">Book</h2><table><thead><tr class="header"><th style="text-align: center;">Site</th><th style="text-align: center;">Describe</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><a href="http://libgen.rs/">LibraryGenesis</a></td><td style="text-align: center;">国外电子书、论文检索</td></tr><tr class="even"><td style="text-align: center;"><ahref="https://zh.z-lib.org/">Z-library</a></td><td style="text-align: center;">国内外电子书检索</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>科研笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #13 协同过滤推荐算法</title>
    <link href="/ML-Note-13.html"/>
    <url>/ML-Note-13.html</url>
    
    <content type="html"><![CDATA[<p>本文将介绍机器学习在<strong>推荐系统</strong>（RecommenderSystems）上的应用。推荐系统在互联网环境中的重要性不亚于搜索引擎，具有极高的落地价值。推荐系统的发展十分迅速，基于机器学习的方法在现在看来早已过时，但依然有学习的必要。本文将以电影推荐为例展开介绍。</p><h2 id="推荐系统概述">推荐系统概述</h2><p>传统推荐算法可以根据<strong>数据源的不同</strong>分为三类：</p><ul><li>基于人口统计学的推荐（Demographic-basedRecommendation）：依据用户间的相似性；</li><li>基于内容的推荐（Content-basedRecommendation）：依据内容间的相似性；</li><li>基于协同过滤的推荐（Collaborative Filtering-basedRecommendation）：结合前两者。</li></ul><p>假设我们是一个电影供应商，现在需要根据用户的观影历史以及评分，来预测每个人可能会给他们没看过的电影打多少分，并以此作为推荐的依据。首先定义如下符号：</p><ul><li><span class="math inline">\(n_u\)</span> 代表用户的数量；</li><li><span class="math inline">\(n_m\)</span> 代表电影的数量；</li><li><span class="math inline">\(r(i, j)\)</span> 代表用户 <spanclass="math inline">\(j\)</span> 是否给电影 <spanclass="math inline">\(i\)</span> 评过分，如果评过则 <spanclass="math inline">\(r(i,j)=1\)</span>；</li><li><span class="math inline">\(y^{(i, j)}\)</span> 代表用户 <spanclass="math inline">\(j\)</span> 给电影 <spanclass="math inline">\(i\)</span> 的评分，前提是 <spanclass="math inline">\(r(i,j)=1\)</span>；</li><li><span class="math inline">\(m^{(j)}\)</span> 代表用户 <spanclass="math inline">\(j\)</span> 评过分的电影的总数。</li></ul><h3 id="基于人口统计学的推荐算法">基于人口统计学的推荐算法</h3><p>假设我们经过人口统计学的方法，获得了每个用户的属性作为其特征向量<span class="math inline">\(u\)</span>，那么我们可以找出 <spanclass="math inline">\(u\)</span>相近的若干用户，则其中某个用户喜欢的电影，其他用户也可能喜欢。此时我们不需要知道关于电影的任何信息。</p><p>这种方法的本质是根据「用户间的相似度」，将相似用户喜爱的内容推荐给当前用户。因此，其最重要的数据源就是<strong>用户画像</strong>（UserProfile）：通过收集与分析用户的社会属性、生活习惯等主要信息的数据之后，将用户的信息抽象成标签。</p><p>原理上有点类似聚类算法，这里不展开介绍。但需要注意的是，这种方法存在<strong>物品冷启动</strong>问题（ItemColdStart），即当新物品出现时，由于未与任何用户有过数据交互历史，无法被主动推荐给与相关性较高的目标用户。</p><p>然而，获得用户的基本属性并非一件容易的事情（涉及隐私），包括地理位置、设备信息、社交关系、浏览器等等，因此，更好的办法是<strong>学习一个用户的偏好</strong><span class="math inline">\(\theta\)</span>。一旦获得了 <spanclass="math inline">\(\theta\)</span>，我们就可以根据不同电影的属性来<strong>预测</strong><span class="math inline">\(\theta\rightarrow y\)</span>，其中 <spanclass="math inline">\(y\)</span> 就是用户对某部电影的可能评分。</p><h3 id="基于内容的推荐算法">基于内容的推荐算法</h3><p>现在假设我们对每部电影构建了一个特征向量 <spanclass="math inline">\(x\)</span>，例如 <spanclass="math inline">\(x_1\)</span> 代表浪漫程度、$ x_2$代表动作程度等等。那么对于某个用户而言，我们将其对电影的评分设为 <spanclass="math inline">\(y\)</span>，目标就是做<strong>线性回归</strong>预测<span class="math inline">\(x\rightarrow y\)</span>。</p><p>具体地，我们记第 <span class="math inline">\(i\)</span>部电影的特征向量为 <span class="math inline">\(x^{(i)}\in\mathbbR^{n+1}\)</span>（包含偏置项），那么对于第 <spanclass="math inline">\(j\)</span> 个用户，线性回归的目标就是学习一个参数<spanclass="math inline">\(\theta^{(j)}\)</span>（代表<strong>用户对电影特征的偏好</strong>），使得：<span class="math display">\[\min_{\theta ^{\left( j \right)}} \frac{1}{2}\sum_{i:r\left( i,j \right)=1}{\left( \left( \theta ^{\left( j \right)} \right) ^Tx^{\left( i\right)}-y^{\left( i,j \right)} \right)^2}+\frac{\lambda}{2}\sum_{k=1}^n{\left( \theta _{k}^{\left( j \right)}\right) ^2}\]</span> 注意此处去掉了分母中的 <spanclass="math inline">\(m^{(j)}\)</span>，因为这个常数不会影响结果。上面的代价函数只是针对一个用户的，由于每个用户的线性回归都是独立的，我们可以将所有用户的代价函数求和：<span class="math display">\[\min_{\theta} J\left( \theta ^{\left( 1 \right)},\cdots ,\theta ^{\left(n_u \right)} \right):=\frac{1}{2}\sum_{j=1}^{n_u}{\sum_{i:r(i,j)=1}{\left( \left( \theta^{\left( j \right)} \right) ^Tx^{\left( i \right)}-y^{\left( i,j\right)} \right)^2}}+\frac{\lambda}{2}\sum_{j=1}^{n_u}{\sum_{k=1}^n{\left( \theta_{k}^{\left( j \right)} \right) ^2}}\]</span> 使用梯度下降法来求解最优解，计算导函数： <spanclass="math display">\[\frac{\partial J}{\partial \theta _{k}^{\left( j\right)}}=\sum_{i:r(i,j)=1}{\left( \left( \theta ^{\left( j \right)}\right) ^Tx^{\left( i \right)}-y^{\left( i,j \right)} \right) \theta_{k}^{\left( j \right)}}+\lambda \theta _{k}^{\left( j \right)}\;\;[k\ne0]\]</span> 那么对于一个特征向量为 <span class="math inline">\(x\)</span>的电影，用户 <span class="math inline">\(j\)</span>对它的评分的预测值就是：<span class="math inline">\(\left( \theta^{\left( j \right)} \right) ^Tx\)</span>。</p><h2 id="协同过滤-collaborative-filtering">协同过滤 | CollaborativeFiltering</h2><p>基于内容的推荐算法的缺点在于，我们需要知道描述每部电影的特征向量，然而这一点通常很难做到。所以我们需要不是基于内容的推荐算法——<strong>协同过滤</strong>。协同过滤的本质是一种「<strong>特征学习</strong>」算法，即自行学习模型所需要的特征，这在<strong>深度学习</strong>领域是非常经典的思想（尤其是对于复杂特征的建立）。</p><h3 id="初始版本">初始版本</h3><p>现在我们不知道每部电影的特征向量，但是我们可以<strong>询问用户</strong>以得到用户的参数<spanclass="math inline">\(\theta^{(j)}\)</span>（譬如让用户选择对不同类型电影的偏好），然后反过来，用<span class="math inline">\(\theta^{(j)}\)</span> 去训练出 <spanclass="math inline">\(x^{(i)}\)</span>，得到每部电影的特征。具体地，对于第<span class="math inline">\(i\)</span> 部电影，我们可以学习它的特征<span class="math inline">\(x^{(i)}\)</span>，使得： <spanclass="math display">\[\min_{x^{\left( i \right)}} \frac{1}{2}\sum_{j:r\left( i,j \right)=1}{\left( \left( \theta ^{\left( j \right)} \right) ^Tx^{\left( i\right)}-y^{\left( i,j \right)} \right)^2}+\frac{\lambda}{2}\sum_{k=1}^n{\left( x_{k}^{\left( i \right)}\right) ^2}\]</span> 由于每部电影的线性回归是独立的，所以我们可以放在一起训练：<span class="math display">\[\min_x J(x^{\left( 1 \right)},\cdots ,x^{\left( n_m\right)}):=\frac{1}{2}\sum_{i=1}^{n_m}{\sum_{j:r\left( i,j \right)=1}{\left( \left( \theta ^{\left( j \right)} \right) ^Tx^{\left( i\right)}-y^{\left( i,j \right)} \right)^2}}+\frac{\lambda}{2}\sum_{i=1}^{n_m}{\sum_{k=1}^n{\left( x_{k}^{\left(i \right)} \right) ^2}}\]</span> 训练过程可能用到导函数： <span class="math display">\[\frac{\partial J}{\partial x_{k}^{\left( i \right)}}=\sum_{j:r\left( i,j\right) =1}{\left( \left( \theta ^{\left( j \right)} \right) ^Tx^{\left(i \right)}-y^{\left( i,j \right)} \right)}x_{k}^{\left( i\right)}+\lambda x_{k}^{\left( i \right)}\,\,[k\ne 0]\]</span>看到这里我们已经发现，这个就是<strong>基于人口统计学的推荐算法</strong>啊！也就是说「用户」和「内容」是相互的，已知<span class="math inline">\(\theta^{(j)}\)</span>，我们可以学习 <spanclass="math inline">\(x^{(i)}\)</span>；已知 <spanclass="math inline">\(x^{(i)}\)</span>，我们可以学习 <spanclass="math inline">\(\theta^{(j)}\)</span>。</p><p>因此，我们可以采用迭代的想法（<strong>期望最大化算法</strong>的思想），随机初始化一个<span class="math inline">\(\theta^{(j)}\)</span>，学习出 <spanclass="math inline">\(x^{(i)}\)</span>，再用学习出的 <spanclass="math inline">\(x^{(i)}\)</span> 去学习 <spanclass="math inline">\(\theta^{(j)}\)</span>，再用新的 <spanclass="math inline">\(\theta^{(j)}\)</span> 去学习 <spanclass="math inline">\(x^{(i)}\)</span>。如此<strong>反复迭代</strong>，最终得到稳定的电影特征和用户参数。这就是最初始版本的协同过滤算法。</p><h3 id="改进版本">改进版本</h3><p>事实上，我们没有反复迭代的必要。观察用 <spanclass="math inline">\(\theta^{(j)}\)</span> 训练 <spanclass="math inline">\(x^{(i)}\)</span> 的优化目标和用 <spanclass="math inline">\(x^{(i)}\)</span> 训练 <spanclass="math inline">\(\theta^{(j)}\)</span>的优化目标，我们可以发现，它们<strong>除了正则化项以外</strong>其实是相同的，都是预测值与实际值的平方误差损失，所以，我们将两个目标综合起来，优化以下函数即可：<span class="math display">\[\begin{aligned}    &amp;\min_{x,\theta} J(x^{\left( 1 \right)},\cdots ,x^{\left( n_m\right)},\theta ^{\left( 1 \right)},\cdots ,\theta ^{\left( n_u\right)})\\    &amp;=\frac{1}{2}\sum_{\left( i,j \right) :r\left( i,j \right)=1}{\left( \left( \theta ^{\left( j \right)} \right) ^Tx^{\left( i\right)}-y^{\left( i,j \right)} \right)^2}+\frac{\lambda}{2}\sum_{i=1}^{n_m}{\sum_{k=1}^n{\left( x_{k}^{\left(i \right)} \right)^2}}+\frac{\lambda}{2}\sum_{j=1}^{n_u}{\sum_{k=1}^n{\left( \theta_{k}^{\left( j \right)} \right) ^2}}\\\end{aligned}\]</span> 值得注意的是，在综合起来之前，<spanclass="math inline">\(n\)</span>是我们人为选定的特征维度数，是一个定值；而现在，<spanclass="math inline">\(n\)</span>变成了一个<strong>超参数</strong>，因此我们也<strong>没有必要加上偏置项</strong>，所以这里<span class="math inline">\(x^{(i)},\theta ^{(j)}\in \mathbb{R}^n\)</span>.</p><p>上式的导函数为： <span class="math display">\[\begin{aligned}    &amp;\frac{\partial J}{\partial \theta _{k}^{\left( j\right)}}=\sum_{i:r\left( i,j \right) =1}{\left( \left( \theta ^{\left(j \right)} \right) ^Tx^{\left( i \right)}-y^{\left( i,j \right)} \right)\theta _{k}^{\left( j \right)}+\lambda \theta _{k}^{\left( j \right)}}\\    &amp;\frac{\partial J}{\partial x_{k}^{\left( i\right)}}=\sum_{j:r\left( i,j \right) =1}{\left( \left( \theta ^{\left(j \right)} \right) ^Tx^{\left( i \right)}-y^{\left( i,j \right)} \right)x_{k}^{\left( i \right)}+\lambda x_{k}^{\left( i \right)}}\\\end{aligned}\]</span> 接下来就可以梯度下降算法完成优化了。最终收敛得到的向量 <spanclass="math inline">\(x^{(i)},\theta^{(j)}\)</span>，可以用于相似电影的推荐，也可以用于相似用户的发掘。</p><h3 id="向量化版本">向量化版本</h3><p>为了代码的运行效率，我们先将该算法向量化。</p><ul><li>矩阵 <span class="math inline">\(Y=\left[ y^{\left( i,j \right)}\right] \in \mathbb{R} ^{n_m\times n_u}\)</span>，即第 <spanclass="math inline">\(i\)</span> 行第 <spanclass="math inline">\(j\)</span> 列表示用户 <spanclass="math inline">\(j\)</span> 对电影 <spanclass="math inline">\(i\)</span> 的评分；</li><li>矩阵 <span class="math inline">\(X=\left[ \begin{array}{c}\left(x^{\left( 1 \right)} \right) ^T\\\vdots\\\left( x^{\left( n_m \right)}\right) ^T\\\end{array} \right] \in \mathbb{R} ^{n_m\timesn}\)</span>，即第 <span class="math inline">\(i\)</span> 行表示电影<span class="math inline">\(i\)</span> 的特征向量；</li><li>矩阵 <span class="math inline">\(\Theta =\left[\begin{array}{c}\left( \theta ^{\left( 1 \right)} \right)^T\\\vdots\\\left( \theta ^{\left( n_u \right)} \right) ^T\\\end{array}\right] \in \mathbb{R} ^{n_u\times n}\)</span>，即第 <spanclass="math inline">\(j\)</span> 行表示用户 <spanclass="math inline">\(j\)</span> 的偏好向量。</li></ul><p>如此，线性回归的预测值可以构成矩阵： <span class="math display">\[Y=X\Theta^T\in\mathbb R^{n_m\times n_u}\]</span> 上式可以视作对矩阵 <span class="math inline">\(Y\)</span>的一个<strong>低秩分解</strong>（Low Rank Factorization）。</p><h3 id="均值归一化与冷启动">均值归一化与冷启动</h3><p>当一个新用户下载软件、新电影上市时，我们还是会遇到<strong>冷启动</strong>问题，即没有电影或用户与之产生过交互！此时我们的矩阵<span class="math inline">\(Y\)</span> 会新增<strong>一行或一列</strong>N/A值（没有评分），如果我们还是将其代入到目标函数中，则对其产生影响的只有正则化项：<span class="math display">\[\frac{\lambda}{2}\sum_{i=1}^{n_m}{\sum_{k=1}^n{\left( x_{k}^{\left( i\right)} \right)^2}}+\frac{\lambda}{2}\sum_{j=1}^{n_u}{\sum_{k=1}^n{\left( \theta_{k}^{\left( j \right)} \right) ^2}}\]</span> 而显然当 <span class="math inline">\(\theta _{k}^{\left( j\right)}\)</span> 或 <span class="math inline">\(x_{k}^{\left( i\right)}\)</span> 取全 <span class="math inline">\(0\)</span>时，正则化项取值最小。代入 <span class="math inline">\(y^{\left( i,j\right)}=\left( \theta ^{\left( j \right)} \right) ^Tx^{\left( i\right)}\)</span> 进行预测，得到的所有值也都是 <spanclass="math inline">\(0\)</span>。但实际上新用户不会给所有电影都打 <spanclass="math inline">\(0\)</span> 分，新电影也不会收到所有人的 <spanclass="math inline">\(0\)</span> 分差评。</p><p>为了能真正进行推荐，我们选择将 <span class="math inline">\(Y\)</span>矩阵<strong>按行</strong>进行<strong>均值归一化</strong>（MeanNormalization），即减去每部电影得分的平均值。此时我们再将 <spanclass="math inline">\(Y&#39;\)</span>代入到目标函数中计算，得到的结果虽然 <span class="math inline">\(\theta_{k}^{\left( j \right)}\)</span> 或 <spanclass="math inline">\(x_{k}^{\left( i \right)}\)</span> 还是取全 <spanclass="math inline">\(0\)</span>，且 <spanclass="math inline">\(Y&#39;\)</span> 中对应的行或列也是全 <spanclass="math inline">\(0\)</span>，但当我们恢复均值时，原始的 <spanclass="math inline">\(Y\)</span> 就有取值了。</p><p>不难发现，这里 <span class="math inline">\(Y\)</span>的取值就是该行或该列的其他值的平均值，也就是说我们朴素地假设新用户给电影评分就是电影现有平均分，新电影得到的评分也是用户的历史打分平均分。这也是我们平时启动软件看到的「热门电影」所做的事情。</p><h2 id="代码实现">代码实现</h2><p>下面以 <a href="https://www.coursera.org/">Coursera</a> 上的数据集<code>ex8_movies.mat</code> 为例，这是一个电影评分数据集 <ahref="https://grouplens.org/datasets/movielens/">MovieLens</a>的子集。其中包含 <span class="math inline">\(Y\)</span>矩阵表示评分结果，<span class="math inline">\(R\)</span>矩阵表示用户是否给电影评过分。现在我们要在上面实现一个协同过滤算法，预测每个用户对所有未评分电影的打分。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">import</span> scipy.optimize <span class="hljs-keyword">as</span> opt<br><br><span class="hljs-comment"># load data</span><br>data = loadmat(<span class="hljs-string">&#x27;ex8_movies.mat&#x27;</span>)<br>Y = data[<span class="hljs-string">&#x27;Y&#x27;</span>] <span class="hljs-comment"># (1682, 943)</span><br>R = data[<span class="hljs-string">&#x27;R&#x27;</span>] <span class="hljs-comment"># (1682, 943)</span><br>n_movie, n_user = Y.shape <span class="hljs-comment"># (1682, 943)</span><br>Ymean = Y.mean(axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># parameter</span><br>n_features = <span class="hljs-number">50</span> <span class="hljs-comment"># numbers of feature</span><br>X = np.random.standard_normal((n_movie, n_features))<br>Theta = np.random.standard_normal((n_user, n_features))<br>lamb = <span class="hljs-number">1</span> <span class="hljs-comment"># regularization</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">serialize</span>(<span class="hljs-params">X, Theta</span>):</span><br>    <span class="hljs-keyword">return</span> np.concatenate((X.ravel(), Theta.ravel()))<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">deserialize</span>(<span class="hljs-params">param</span>):</span><br>    <span class="hljs-keyword">return</span> param[:n_movie * n_features].reshape(n_movie, n_features), \<br>           param[n_movie * n_features:].reshape(n_user, n_features)<br><br><span class="hljs-comment"># linear regression</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">J</span>(<span class="hljs-params">param, Y, R, lamb</span>):</span><br><span class="hljs-comment"># deserialize</span><br>X, Theta = deserialize(param)<br><span class="hljs-comment"># calculate</span><br>inner = np.multiply(X @ Theta.T - Y, R)<br>inner_term = np.power(inner, <span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>() / <span class="hljs-number">2</span><br>reg_term = lamb / <span class="hljs-number">2</span> * np.power(param, <span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>()<br><span class="hljs-keyword">return</span> inner_term + reg_term<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">partJ</span>(<span class="hljs-params">param, Y, R, lamb</span>):</span><br><span class="hljs-comment"># deserialize</span><br>X, Theta = deserialize(param)<br><span class="hljs-comment"># calculate</span><br>inner = np.multiply(X @ Theta.T - Y, R)<br>X_grad = inner @ Theta + lamb * X<br>Theta_grad = inner.T @ X + lamb * Theta<br><span class="hljs-keyword">return</span> serialize(X_grad, Theta_grad)<br><br><span class="hljs-comment"># train, about 10 mins</span><br>res = opt.minimize(fun=J, <span class="hljs-comment"># cost function</span><br>                   x0=serialize(X, Theta), <span class="hljs-comment"># serialize input param</span><br>                   args=(Y - Ymean, R, lamb), <span class="hljs-comment"># input data</span><br>                   method=<span class="hljs-string">&#x27;TNC&#x27;</span>,<br>                   jac=partJ) <span class="hljs-comment"># serialize gradient</span><br><span class="hljs-built_in">print</span>(res)<br><br><span class="hljs-comment"># predict</span><br>X_trained, Theta_trained = deserialize(res.x)<br>Y_pred = X_trained @ Theta_trained.T + Ymean<br><br><span class="hljs-comment"># parse movie_id.txt</span><br>movie_list = []<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;movie_ids.txt&#x27;</span>, encoding=<span class="hljs-string">&#x27;latin-1&#x27;</span>) <span class="hljs-keyword">as</span> file:<br><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> file:<br>tokens = line.strip().split(<span class="hljs-string">&#x27; &#x27;</span>)<br>movie_list.append(<span class="hljs-string">&#x27; &#x27;</span>.join(tokens[<span class="hljs-number">1</span>:]))<br>movie_list = np.array(movie_list)<br><br><span class="hljs-comment"># sort rating and show result of first user</span><br>idx = np.argsort(Y_pred[:, <span class="hljs-number">0</span>])[::-<span class="hljs-number">1</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Top 10 movies for first user:&#x27;</span>)<br><span class="hljs-keyword">for</span> movie <span class="hljs-keyword">in</span> movie_list[idx][:<span class="hljs-number">10</span>]:<br><span class="hljs-built_in">print</span>(movie)<br></code></pre></td></tr></table></figure><p>使用 Scipy 第三方库优化结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">    fun: <span class="hljs-number">11099.432068949513</span><br>    jac: array([-<span class="hljs-number">5.25906371e-06</span>, -<span class="hljs-number">3.12407994e-07</span>, -<span class="hljs-number">1.53592980e-06</span>, ...,<br>      -<span class="hljs-number">5.59603399e-07</span>, -<span class="hljs-number">3.29349047e-07</span>, -<span class="hljs-number">3.10467495e-07</span>])<br>message: <span class="hljs-string">&#x27;Converged (|f_n-f_(n-1)| ~= 0)&#x27;</span><br>   nfev: <span class="hljs-number">7740</span><br>    nit: <span class="hljs-number">235</span><br> status: <span class="hljs-number">1</span><br>success: <span class="hljs-literal">True</span><br>      x: array([-<span class="hljs-number">0.50327981</span>,  <span class="hljs-number">0.29839561</span>, -<span class="hljs-number">0.27672091</span>, ...,  <span class="hljs-number">0.34445801</span>,<br>       <span class="hljs-number">1.09017551</span>, -<span class="hljs-number">0.34508789</span>])<br></code></pre></td></tr></table></figure><p>用该参数找到第一个用户预测评分最高的 <spanclass="math inline">\(10\)</span> 部电影：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">Top <span class="hljs-number">10</span> movies <span class="hljs-keyword">for</span> first user:<br>Titanic (<span class="hljs-number">1997</span>)<br>Deer Hunter, The (<span class="hljs-number">1978</span>)<br>Lawrence of Arabia (<span class="hljs-number">1962</span>)<br>In the Name of the Father (<span class="hljs-number">1993</span>)<br>Being There (<span class="hljs-number">1979</span>)<br>Ran (<span class="hljs-number">1985</span>)<br>Philadelphia (<span class="hljs-number">1993</span>)<br>Tombstone (<span class="hljs-number">1993</span>)<br>Great Escape, The (<span class="hljs-number">1963</span>)<br>Manchurian Candidate, The (<span class="hljs-number">1962</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #12 异常检测</title>
    <link href="/ML-Note-12.html"/>
    <url>/ML-Note-12.html</url>
    
    <content type="html"><![CDATA[<p>本文介绍机器学习的一种应用——<strong>异常检测</strong>（AnomalyDetection）。异常检测是一类经典的<strong>生成式</strong>（Generative）任务，和朴素贝叶斯分类器有点类似，都需要先建模已有样本的分布。但异常检测的特点在于其假设样本特征服从<strong>高斯分布</strong>（GaussianDistribution），而异常样本就是<strong>出现概率极小</strong>的 <spanclass="math inline">\(3\sigma\)</span> 外的点，因此可以无监督学习。</p><h2 id="异常检测-anomaly-detection">异常检测 | Anomaly Detection</h2><p>异常检测的应用情景十分广泛：从工业产品的质量检测、银行用户的欺诈检测、集群的设备异常检测，都可以应用。其特点是：<strong>异常点通常偏离正常数据，且可能性较低</strong>。</p><p>传统的异常检测通过<strong>手工选择特征</strong>，并假设这些特征都服从高斯分布（正态分布）<spanclass="math inline">\(x \sim N(\mu,\sigma^2)\)</span>，则其<strong>概率密度函数</strong>： <spanclass="math display">\[P(x;\mu ,\sigma ^2)=\frac{1}{\sqrt{2\pi}\sigma}\exp \left( -\frac{(x-\mu)^2}{2\sigma ^2} \right)\]</span> 其中：</p><ul><li><span class="math inline">\(\mu\)</span>是样本均值，代表分布的中心；</li><li><span class="math inline">\(\sigma^2\)</span>是样本方差，代表样本偏离中心的程度，<spanclass="math inline">\(\sigma\)</span> 越小，概率密度曲线就越瘦高；</li><li>概率密度函数满足：<spanclass="math inline">\(\int_{-\infty}^{+\infty}{P(x;\mu ,\sigma^2)\mathrm{d}x}=1\)</span>。</li></ul><h3 id="多元高斯分布">多元高斯分布</h3><p>一个样本往往含有多个特征，多个特征组成的分布就是<strong>多元高斯分布</strong>（MultivariateGaussian Distribution）。通常，我们朴素地假设 <spanclass="math inline">\(x\)</span>的各个特征维度<strong>互不相关</strong>，则联合概率密度函数等于各分量的概率密度函数之积，即：<span class="math display">\[\begin{aligned}    P\left( x;\mu ,\Sigma \right) &amp;=\prod_{i=1}^n{P\left( x_i;\mu_i,\sigma _{i}^{2} \right)}\\    &amp;=\frac{1}{\left( 2\pi \right) ^{n/2}\prod_{i=1}^n{\sigma_i}}\exp \left( -\frac{1}{2}\sum_{i=1}^n{\frac{\left( x_i-\mu _i \right)^2}{\sigma _i^2}} \right)\\\end{aligned}\]</span> 而对于其中的指数部分，可以表示为矩阵乘法的形式： <spanclass="math display">\[\begin{aligned}    \xi ^2(x,\mu ,\sigma )&amp;=\sum_{i=1}^n{\left( \frac{x_i-\mu_i}{\sigma _i} \right) ^2}\\    &amp;=\sum_{j=i}^n{\left( x_i-\mu _i \right)}\left( x_i-\mu _i\right) \left( \frac{1}{\sigma _i} \right) ^2\\    &amp;=\left[ x_1-\mu _1,x_2-\mu _2,\cdots ,x_n-\mu _n \right] \left[\begin{matrix}    \frac{1}{\sigma ^2}&amp;        0&amp;      \cdots&amp;     0\\    0&amp;      \frac{1}{\sigma ^2}&amp;        \cdots&amp;     0\\    \vdots&amp;     \vdots&amp;     \ddots&amp;     \vdots\\    0&amp;      0&amp;      \cdots&amp;     \frac{1}{\sigma ^2}\\\end{matrix} \right] \left[ \begin{array}{c}    x_1-\mu _1\\    x_2-\mu _2\\    \vdots\\    x_n-\mu _n\\\end{array} \right]\\    &amp;=(x-\mu )^T\Sigma ^{-1}(x-\mu )\\\end{aligned}\]</span> 其中根据 <span class="math inline">\(x_i\)</span> 相互独立知：<span class="math display">\[\begin{aligned}    \Sigma &amp;=\mathbb{E} \left\{ (x-\mathbb{E} x)(x-\mathbb{E} x)^T\right\}\\    &amp;=\left[ \begin{matrix}    \mathrm{var}\left( x_1 \right)&amp;     \mathrm{cov}\left( x_1,x_2\right)&amp;     \cdots&amp;     \mathrm{cov}\left( x_1,x_n \right)\\    \mathrm{cov}\left( x_2,x_1 \right)&amp;     \mathrm{var}\left( x_2\right)&amp;     \cdots&amp;     \mathrm{cov}\left( x_2,x_n \right)\\    \vdots&amp;     \vdots&amp;     \ddots&amp;     \vdots\\    \mathrm{cov}\left( x_n,x_1 \right)&amp;     \mathrm{cov}\left(x_n,x_2 \right)&amp;     \cdots&amp;     \mathrm{var}\left( x_n\right)\\\end{matrix} \right]\\    &amp;=\left[ \begin{matrix}    \sigma ^2&amp;      0&amp;      \cdots&amp;     0\\    0&amp;      \sigma ^2&amp;      \cdots&amp;     0\\    \vdots&amp;     \vdots&amp;     \ddots&amp;     \vdots\\    0&amp;      0&amp;      \cdots&amp;     \sigma ^2\\\end{matrix} \right]\\\end{aligned}\]</span> 从而得到多元正态分布的概率密度函数： <spanclass="math display">\[P(x;\mu ,\Sigma )=\frac{1}{(2\pi )^{n/2}|\Sigma |^{1/2}}\exp \left(-\frac{1}{2}(x-\mu )^T\Sigma ^{-1}(x-\mu ) \right)\]</span> 其中，<span class="math inline">\(x\in\mathbb R^n\)</span> 是<span class="math inline">\(n\)</span> 维随机变量，<spanclass="math inline">\(\mu\in\mathbb R^n\)</span> 是 <spanclass="math inline">\(x\)</span> 的均值，<spanclass="math inline">\(\Sigma\in\mathbb R^{n\times n}\)</span>是<strong>协方差矩阵</strong>。</p><blockquote><p>该式子在各变量不独立时也成立，因此这里的协方差矩阵，不一定是上述的对角矩阵形式！此处只是乱证。</p></blockquote><p>下面我们用一组图来展示协方差矩阵对多元高斯分布的影响：</p><p><img src="/img/blog/ML-Note-12-images/gaussian1.png" alt="改变两个特征的方差" width=90% /></p><p><img src="/img/blog/ML-Note-12-images/gaussian2.png" alt="在不改变原有方差的基础上，增加两者之间的相关性" width=90% /></p><h3 id="问题描述">问题描述</h3><p>异常检测的原理非常简单：假设有正常的数据集 <spanclass="math inline">\(\{x^{(1)},x^{(2)},\ldots,x^{(m)}\}\)</span>，我们构建一个多元正态分布，其均值为样本均值，协方差矩阵为样本的协方差矩阵，即：<span class="math display">\[\begin{aligned}    \mu &amp;=\frac{1}{m}\sum_{i=1}^m{x^{(i)}}\\    \Sigma &amp;=\frac{1}{m}\sum_{i=1}^m{(}x^{(i)}-\mu )(x^{(i)}-\mu)^T\\    P(x;\mu ,\Sigma )&amp;=\frac{1}{(2\pi )^{n/2}|\Sigma |^{1/2}}\exp\left( -\frac{1}{2}(x-\mu )^T\Sigma ^{-1}(x-\mu ) \right)\\\end{aligned}\]</span> 或者直接认定 <span class="math inline">\(x\)</span>各维度不相关，取： <span class="math display">\[P(x;\mu ,\Sigma )=\prod_{i=1}^n{P}(x_i;\mu _i,\sigma _{i}^{2})\]</span></p><blockquote><p>前者能够自动捕捉<strong>特征之间的相关性</strong>，但计算代价较高，且通常需要<span class="math inline">\(m&gt;10n\)</span>，否则协方差矩阵 <spanclass="math inline">\(\Sigma\)</span> 不可逆。</p><p>后者不能捕捉特征之间的相关性，但在计算上更快，且允许 <spanclass="math inline">\(m\leqslant n\)</span> 的情况。</p></blockquote><p>对于要检测的数据 <span class="math inline">\(x\)</span>，如果 <spanclass="math inline">\(P(x;\mu,\Sigma)\)</span><strong>小于某个阈值</strong> <spanclass="math inline">\(\varepsilon\)</span>，那么就认为该数据是<strong>异常数据</strong>，否则正常。</p><h2 id="开发异常检测系统">开发异常检测系统</h2><h3 id="特征选择">特征选择</h3><p>异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，我们通常先将数据转换成高斯分布。通常需要先画出特征分布的直方图，如果数据呈现<strong>偏态分布</strong>（SkewedDistribution）:</p><p><img src="/img/blog/ML-Note-12-images/Skewed.jpeg" alt="两种偏态分布" width=60% /></p><p>常用的转换方式有：对数变换、指数（平方根、倒数）变换、正反旋变换等。</p><p><img src="/img/blog/ML-Note-12-images/log.png" alt="对数变换" width=60% /></p><h3 id="训练和验证">训练和验证</h3><p>如果我们有标注过的数据（标注是否异常），则可以将数据划分为训练集、验证集和测试集。</p><ol type="1"><li>训练集包含大部分<strong>正常数据</strong>，并据此构建出正态分布模型；</li><li>验证集和测试集包含<strong>正常和异常数据</strong>；</li><li>根据验证集的结果<strong>调整参数</strong> <spanclass="math inline">\(\varepsilon\)</span>，最后在测试集上进行测试。</li></ol><p>此外，由于异常检测数据集通常<strong>分布不均</strong>（正常数据远远多于异常数据），所以实验指标应该取<spanclass="math inline">\(\text{Precision},\text{Recall},\text{F1}\)</span>等值。</p><h3 id="误差分析">误差分析</h3><p>在通过上述流程后，如果模型效果仍然不好，则需要进行误差分析。</p><p>一个常见的问题是：一些异常的数据可能也会有较高的 <spanclass="math inline">\(P(x)\)</span>值，从而被算法认为是正常的。这种情况下我们应该去分析那些被算法错误预测的数据，观察其特征的选择是否存在问题。考虑更换特征、或将已有的特征进行组合以获取更好的特征。</p><h2 id="代码实现">代码实现</h2><p>下面以 <a href="https://www.coursera.org/">Coursera</a> 上的数据集<code>ex8data1.mat</code>为例，这是一个二维平面上的点集，包含无标注的训练集和有标注的的测试集。这里采用Scipy科学计算库中的多元正态分布方法，将测试集再划分出验证集，使用如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> multivariate_normal<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> f1_score<br><br><span class="hljs-comment"># load data and split</span><br>data = loadmat(<span class="hljs-string">&#x27;ex8data1.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>]  <span class="hljs-comment"># (307, 2)</span><br>Xval = data[<span class="hljs-string">&#x27;Xval&#x27;</span>]  <span class="hljs-comment"># (307, 2)</span><br>yval = data[<span class="hljs-string">&#x27;yval&#x27;</span>].flatten()  <span class="hljs-comment">#(307, )</span><br>Xval, Xtest, yval, ytest = train_test_split(Xval, yval, train_size=<span class="hljs-number">0.5</span>)<br><br><span class="hljs-comment"># train</span><br>Xmeans = X.mean(axis=<span class="hljs-number">0</span>)<br>Xcov = ((X - Xmeans).T @ (X - Xmeans)) / X.shape[<span class="hljs-number">0</span>]<br>normDist = multivariate_normal(mean=Xmeans, cov=Xcov)<br><br><span class="hljs-comment"># valid</span><br>besteps, bestf1 = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>pdf_X = normDist.pdf(X)<br>pdf_Xval = normDist.pdf(Xval)<br>pdf_Xtest = normDist.pdf(Xtest)<br><br><span class="hljs-keyword">for</span> eps <span class="hljs-keyword">in</span> np.linspace(pdf_Xval.<span class="hljs-built_in">min</span>(), pdf_Xval.<span class="hljs-built_in">max</span>(), <span class="hljs-number">10000</span>):<br>f1 = f1_score(yval, pdf_Xval &lt; eps, average=<span class="hljs-string">&#x27;micro&#x27;</span>)<br><span class="hljs-keyword">if</span> f1 &gt; bestf1:<br>bestf1, besteps = f1, eps<br><br><span class="hljs-comment"># test</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;besteps is:&#x27;</span>, besteps)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;f1 on test set:&#x27;</span>, f1_score(ytest, pdf_Xtest &lt; besteps, average=<span class="hljs-string">&#x27;micro&#x27;</span>))<br><br>x1, x2 = np.meshgrid(np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">30</span>, <span class="hljs-number">100</span>), np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">30</span>, <span class="hljs-number">100</span>))<br>plt.xlabel(<span class="hljs-string">&#x27;Latency (ms)&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Throughput (mb/s)&#x27;</span>)<br>plt.plot(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;x&#x27;</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, alpha=<span class="hljs-number">0.5</span>)<br>plt.plot(X[pdf_X&lt;besteps][:, <span class="hljs-number">0</span>], X[pdf_X&lt;besteps][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>, ms=<span class="hljs-number">10</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>根据验证集找到的最佳 <span class="math inline">\(\varepsilon\)</span>约为：<span class="math inline">\(2.72\times10^{-5}\)</span>；此时测试集上 <spanclass="math inline">\(\text{F1}\)</span> 值约为：<spanclass="math inline">\(0.98\)</span>.</p><p><img src="/img/blog/ML-Note-12-images/ex8data1.png" alt="ex8data1" width=60%  /></p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux学习笔记 #4 服务器环境配置</title>
    <link href="/Linux-Note-4.html"/>
    <url>/Linux-Note-4.html</url>
    
    <content type="html"><![CDATA[<p>上一节 <a href="https://hwcoder.top/Linux-Note-3">Linux学习笔记 #3连接远程服务器</a> 中，介绍了远程控制 Linux服务器的情景。假设我们已经完成了初次 SSH鉴权，以下内容为<strong>后续</strong>的操作。</p><p>当新用户登录服务器时，会默认在 <code>/home/user</code>目录下，此时这里只有配置好的 <code>.ssh</code>目录。用户目录下的所有配置<strong>仅会与用户个人绑定</strong>，包括Python、Conda、Git、Bash 等环境，所以此时我们需要一一配置。</p><blockquote><p><code>/home/user</code>目录通常空间有限，因此对于<strong>开放权限较高</strong>的私人服务器，用户目录只用于存放个人使用的配置文件。而工程以及大文件则会存放在另外的<code>/data/user</code> 目录下，以节省服务器的存储空间。下面我们称<code>/home/user</code> 目录为 <code>~</code>。</p></blockquote><h2 id="anaconda-环境">Anaconda 环境</h2><p>首先需要安装 Anaconda 环境，使用 <code>wget</code>命令从清华镜像源下载（该命令由 Unix自带），不需要下载最新版（这里选用的版本仅支持到 Py3.8）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2020.02-Linux-x86_64.sh</span><br></code></pre></td></tr></table></figure><p>下载完成后，当前目录下就会出现 <code>.sh</code>脚本形式的安装包，输入以下命令运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> bash ./Anaconda3-2020.02-Linux-x86_64.sh</span><br></code></pre></td></tr></table></figure><p>一路 Enter 确定，最后输入 yes，安装后可用 <code>conda -V</code>查看版本。如果这里提示无法识别命令，参考接下来的做法即可。</p><h3 id="虚拟环境">虚拟环境</h3><p>在 <code>~</code> 目录下，还有一个 <code>.bashrc</code>文件，对于大部分 Unix 系统，Bash 是作为默认终端被安装的。你对 bashrc所做的任何修改将在<strong>下一次启动终端</strong>时生效。如果你想立刻生效的话，运行下面的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">source</span> ~/.bashrc</span><br></code></pre></td></tr></table></figure><p>此时会发现用户名前面就会有一个<code>(base)</code>，表示现在处于<strong>基础环境</strong>中。Conda集成了 Python 发行版，同时可以对 Python虚拟环境进行管理，常用的命令有：</p><ul><li><code>conda create -n &lt;env_name&gt; python=&lt;ver&gt;</code>：创建conda 环境。</li><li><code>conda create -n &lt;env_name&gt; --file &lt;this file&gt;</code>：创建conda 环境并指定 requirement.txt 路径配置。</li><li><code>conda create -n &lt;new_env&gt; --clone &lt;old_env&gt;</code>：复制原有conda 环境。</li><li><code>conda env list</code> 或<code>conda info -e</code>：查看现有的 conda 环境。</li><li><code>conda activate &lt;env_name&gt;</code>：切换 conda 环境。</li><li><code>conda deactivate</code>：关闭虚拟环境。</li><li><code>conda remove -n &lt;env_name&gt; --all</code>：删除环境。</li><li><code>conda list --export &gt; environment.yml</code>：导出环境配置列表。</li><li><code>conda env create -f environment.yml</code>：根据环境配置列表创建新环境。</li></ul><p>良好的环境管理习惯有益于在同一机器上运行多个深度学习框架而相互之间不会影响，用户创建的环境会被保存在<code>~/anaconda3/env/xxx</code>，需要时可以直接拷贝。</p><h3 id="cuda-环境与-pytorch">CUDA 环境与 PyTorch</h3><p>安装完 Anaconda 后，我们就能使用 <code>pip</code> 和<code>conda</code> 两种包管理器了（此前只能用 Linux 自带的<code>apt</code>）。</p><blockquote><p>在服务器里，我们只推荐用 <code>conda</code> 安装<code>cudatoolkit</code>、<code>cudnn</code> 以及 <code>jupyter</code>相关的包，其他包建议用 <code>pip</code> 进行安装。</p></blockquote><p>安装 CUDA 前，可以先查看显卡驱动的版本：通常是你的环境需要什么版本的CUDA，你就需要对应版本的驱动。对应关系可以查看：<ahref="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">CUDARelease Notes (nvidia.com)</a>。</p><p>初次登录可以先测试 <code>nvidia-smi</code>，其中会显示 DriverVersion（当前显卡驱动）以及 CUDA Version（当前驱动支持的<strong>最高CUDA 版本</strong>）。</p><blockquote><p>如果没有反应，可能需要手动安装 N 卡驱动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> sudo add-apt-repository ppa:graphics-drivers/ppa  <span class="hljs-comment"># 把显卡驱动加入PPA</span></span><br><span class="hljs-meta">$</span><span class="bash"> ubuntu-drivers devices  <span class="hljs-comment"># 查找当前设备适合的驱动版本</span></span><br><span class="hljs-meta">$</span><span class="bash"> sudo apt-get install nvidia-driver-418  <span class="hljs-comment"># 安装对应版本</span></span><br></code></pre></td></tr></table></figure><p>如果要更换驱动版本，则需要卸载后重装，中间需要重启机器一次：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> sudo apt-get remove --purge nvidia*  <span class="hljs-comment"># 直接卸载</span></span><br><span class="hljs-meta">$</span><span class="bash"> sudo apt autoremove</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">$</span><span class="bash"> ./nvidia-uninstall  <span class="hljs-comment"># 也可以用二进制文件进行卸载，一般在 /usr/bin 下</span></span><br><span class="hljs-meta"></span><br><span class="hljs-meta">$</span><span class="bash"> sudo apt install nvidia-driver-530 <span class="hljs-comment"># 530 是版本号</span></span><br></code></pre></td></tr></table></figure></blockquote><p>之后就可以安装 CUDA 了，官网的链接有 <ahref="https://developer.nvidia.com/cuda-downloads">CUDAToolkit</a> 和 <ahref="https://developer.nvidia.com/cudnn">cuDNN</a>。不过其实现在安装PyTorch 时可以帮你带上 cudatoolkit 和 cudnn等东西的安装，所以也可以跳过。如果要安装，建议使用 <code>conda</code> 在<code>conda-forge</code> 源下进行安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> conda install -c conda-forge cudatoolkit=12.1 cudnn</span> <br></code></pre></td></tr></table></figure><p>安装在<strong>基本环境</strong>后，可以使用 <code>nvcc -V</code> 查看CUDA 版本。</p><h3 id="第三方库">第三方库</h3><p>使用 <code>pip</code>进行安装时，建议安装在<strong>虚拟环境</strong>中，特别是一些<strong>不常用</strong>的包。如果下载过慢或失败，可以使用清华源进行下载，使用以下命令在<strong>个人配置</strong>中更新及换源：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> pip install -U pip -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="hljs-meta">$</span><span class="bash"> pip config <span class="hljs-built_in">set</span> global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br></code></pre></td></tr></table></figure><p>如果不方便换源，临时使用的方法为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> pip install &lt;package&gt; -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></code></pre></td></tr></table></figure><p>这里以 PyTorch为例，我们将其装在<strong>虚拟环境</strong>中，使用以下命令，不同库之间用<strong>空格隔开</strong>可一并安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> pip install torch torchvision torchaudio</span> <br></code></pre></td></tr></table></figure><blockquote><p>为什么 PyTorch <strong>不要装在基本环境</strong>？因为安装高版本的PyTorch 时会自动替换 Numpy等依赖库至相应版本。而这些自动替换的基础库很有可能与其他高级库产生不匹配的冲突，导致原先功能无法正常使用，也就是原先的运行环境会被污染。此外这样也便于虚拟环境的拷贝。</p><p>注意：通常安装 PyTorch 会在 <ahref="https://pytorch.org/get-started/locally/">官网</a>选择合适的版本，如果待安装的 PyTorch 版本较旧，可以指定安装时的 index地址。官方提供的 whl 目录在这：<ahref="https://download.pytorch.org/whl/torch/">download.pytorch.org/whl/torch/</a>。</p></blockquote><p>成功安装后可以在命令行里打开 Python 测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.__version__<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.cuda.is_available()<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.cuda.device_count()<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>]).cuda()<br><span class="hljs-comment"># 1.9.1+cu11.1</span><br><span class="hljs-comment"># True</span><br><span class="hljs-comment"># 4</span><br><span class="hljs-comment"># tensor([1., 2.], device=&#x27;cuda:0&#x27;)</span><br></code></pre></td></tr></table></figure><p>下面列举需要安装在<strong>基本环境</strong>中的常用库，安装方法同上：</p><ul><li><code>gpustat</code>：用于查看动态查看 GPU 状态（需要 Nvidia驱动），<code>watch -n1 -c gpustat --color</code>。</li><li><code>ipdb</code>：用于简易的断点调试，可以看做 <code>pdb</code>的升级版，<code>python -m ipdb main.py</code>。</li></ul><h3 id="notebook">Notebook</h3><p>有时候需要在远程服务器中使用 JupyterNotebook，就需要先在远程安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ conda install -c conda-forge jupyter notebook<br></code></pre></td></tr></table></figure><p>之后就可以在远程启动 Jupyter 服务，内核放在远程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ jupyter notebook --port=8889<br></code></pre></td></tr></table></figure><p>启动后会获得服务的 URL地址，这个地址可以在本地用浏览器直接打开，也可以在 VSCode 中安装 Jupyter插件后打开。</p><h2 id="个人配置">个人配置</h2><p>如果这个服务器需要<strong>长期使用</strong>，那么进行下面的配置就很有必要。以下大部分内容需要用<code>apt</code> 包管理器安装，因此需要获取 <code>sudo</code>临时<strong>管理员权限</strong>，如果没有可以向管理员申请。</p><h3 id="git-配置">Git 配置</h3><p>安装 Git 后，在 <code>~</code> 下就会存在一个 <code>.gitconfig</code>或 <code>.config/git/config</code> 的文件，如果没有则需自行安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> sudo apt-get install git -y  <span class="hljs-comment"># -y 表示默认 yes</span></span><br></code></pre></td></tr></table></figure><p>在服务器里配置 Git 有三层，每层的配置（系统 <code>--system</code>、全局 <code>--global</code>、仓库<code>--local</code>）都会<strong>覆盖掉上一层次</strong>的配置，这里要修改<code>~</code> 下的<strong>个人配置</strong>，则需要用到<code>--global</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 替换成个人信息！</span><br>$ git config --global user.name <span class="hljs-string">&quot;hewei2001&quot;</span><br>$ git config --global user.email <span class="hljs-string">&quot;631670924@qq.com&quot;</span><br><span class="hljs-comment"># 解除 GitHub x</span><br>$ git config --global http.postBuffer 524288000<br><span class="hljs-comment"># 常用的指令可以取 alias 别名</span><br>$ git config --global alias.showlog <span class="hljs-string">&quot;log --graph --pretty=format:&#x27;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#x27; --abbrev-commit --date=relative&quot;</span><br><span class="hljs-comment"># 如果要修改指定仓库下的配置，则可以替换成 --local</span><br></code></pre></td></tr></table></figure><p>如果需要建立远程 SSH鉴权，使用以下命令打印出<strong>公钥</strong>，手动复制到 GitHub上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> ssh-keygen -t rsa -C <span class="hljs-string">&quot;631670924@qq.com&quot;</span>  <span class="hljs-comment"># 生成</span></span><br><span class="hljs-meta">$</span><span class="bash"> cat ~/.ssh/id_rsa.pub  <span class="hljs-comment"># 打印</span></span><br><span class="hljs-meta">$</span><span class="bash"> ssh -T git@github.com  <span class="hljs-comment"># 测试</span></span><br></code></pre></td></tr></table></figure><h3 id="oh-my-zsh">oh-my-zsh</h3><p>首先检查<strong>服务器可用终端</strong>是否包含<code>zsh</code>，如果有直接切换，并安装<strong>插件管理器</strong><code>oh-my-zsh</code>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> cat /etc/shells  <span class="hljs-comment"># 打印可用终端，看有没有 /bin/zsh</span></span> <br><span class="hljs-meta">$</span><span class="bash"> chsh -s /bin/zsh</span> <br><span class="hljs-meta">$</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://gitee.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh</span><br><span class="hljs-meta">$</span><span class="bash"> cp ~/.zshrc ~/.zshrc.bak  <span class="hljs-comment"># 备份原配置</span></span><br><span class="hljs-meta">$</span><span class="bash"> cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc</span><br><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">source</span> ~/.zshrc</span><br></code></pre></td></tr></table></figure><p>如果没有自带<code>zsh</code>，完成以下几步后，就可以成功安装二者：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> sudo apt-get install zsh -y</span><br><span class="hljs-meta">$</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://g.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh</span><br><span class="hljs-meta">$</span><span class="bash"> cp ~/.zshrc ~/.zshrc.bak  <span class="hljs-comment"># 备份原配置</span></span><br><span class="hljs-meta">$</span><span class="bash"> cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc</span><br><span class="hljs-meta">$</span><span class="bash"> chsh -s /bin/zsh  <span class="hljs-comment"># 切换终端</span></span><br></code></pre></td></tr></table></figure><blockquote><p>如果执行第一步时无 <code>sudo</code>权限，则需要自行安装资源包及依赖，参考：<ahref="https://www.cnblogs.com/XiiX/p/14618799.html">Linux 以非 root用户安装 zsh</a>。</p></blockquote><p>重启系统，此时已经进入新的 <code>zsh</code>终端，下载将要用到的主题和插件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 主题 powerlevel10k</span><br>git <span class="hljs-built_in">clone</span> --depth=1 https://gitee.com/romkatv/powerlevel10k.git <span class="hljs-variable">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/themes/powerlevel10k<br><span class="hljs-comment"># 两个插件</span><br>git <span class="hljs-built_in">clone</span> --depth=1 https://gitee.com/zsh-users/zsh-autosuggestions <span class="hljs-variable">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-autosuggestions<br>git <span class="hljs-built_in">clone</span> --depth=1 https://gitee.com/zsh-users/zsh-syntax-highlighting.git <span class="hljs-variable">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-syntax-highlighting<br></code></pre></td></tr></table></figure><p>打开 <code>.zshrc</code>，<strong>手动修改</strong>以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 主题，如果要用自带的主题推荐 bira</span><br>ZSH_THEME=<span class="hljs-string">&quot;powerlevel10k/powerlevel10k&quot;</span><br><span class="hljs-comment"># 插件</span><br>plugins=(git <br> tmux <br> z <br> extract <br> zsh-autosuggestions<br> zsh-syntax-highlighting)<br></code></pre></td></tr></table></figure><p><code>oh-my-zsh</code> 自带的插件定义了常用的别名，可以用<code>alias</code> 命令查看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># git 相关</span><br>gaa = git add --all <br>gcmsg = git commit -m <br>ga = git add <br>gst = git status <br>gp = git push<br><span class="hljs-comment"># tmux 相关</span><br>tl = tmux list-sessions<br>tkss = tmux kill-session -t<br>ta = tmux attach -t<br>ts = tmux new-session -s<br></code></pre></td></tr></table></figure><p>下面是部分自带的插件、需要手动安装的插件介绍：</p><ul><li><code>z</code>：在常用目录之间<strong>直接跳转</strong>，使用<code>z &lt;dir_name&gt;</code>。</li><li><code>extract</code>：<strong>一键解压</strong>各种形式压缩包，使用<code>x &lt;file_name&gt;</code>。</li><li><code>gitignore</code>：一键生成 gitignore 模板，使用<code>gi python &gt; .gitignore</code>。</li><li><code>cpv</code>：带进度条复制文件，使用<code>cpv &lt;a&gt; &lt;b&gt;</code>。</li><li><code>colored-man-pages</code>：带颜色的 <code>man</code>菜单。</li><li><code>sudo</code>：上一条命令忘记加 <code>sudo</code>，双击<code>ESC</code> 就会自动加上。</li><li><code>zsh-autosuggestions</code>：<strong>命令补全</strong>插件，需要手动安装。</li><li><code>zsh-syntax-highlighting</code>：<strong>命令高亮</strong>插件，需要手动安装。</li><li><code>tldr</code>：简化 <code>man</code> 菜单，使用<code>tldr &lt;command&gt;</code>。需要手动安装并更新数据库。</li></ul><blockquote><p>之前在 <code>.bashrc</code> 里的一些配置也要搬过来，主要有<strong>Conda 环境的路径配置</strong>和一些 alias，配置完成后使用<code>source ~/.zshrc</code>命令<strong>重启终端</strong>即可。之后会进入主题的引导设置，如果想重新配置可以输入<code>p10k configure</code> 命令。</p><p>有时候配置完 zsh 后会使用不了 pip 等命令，出现 zsh: command notfound: xxx 报错，这时候需要在 <code>~/.zshrc</code> 中找到 Userconfiguration 位置，将 <code>~/.bash_profile</code> 或者<code>~/.profile</code>中的跟<strong>系统路径</strong>有关的命令复制进去。（<code>export $(cat /proc/1/environ |tr 0 n | xargs --null)</code>）</p></blockquote><h3 id="反向代理">反向代理</h3><p>加载预训练模型时，总是不可避免地要去访问国外下载源，而部署在国内网络的服务器则无能为力。此时就需要在主机使用VPN，通过「<strong>反向代理</strong>」的方法让服务器也能使用到主机的VPN。</p><p>在<strong>本机</strong>终端输入以下命令，并保持终端开启：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ssh -NR 12306:localhost:7890 hewei@10.249.xx.xxx<br><span class="hljs-comment"># 其中 12306 可以是任意端口，7890 则必须改成 VPN 的代理端口！</span><br></code></pre></td></tr></table></figure><p>在<strong>服务器</strong>终端输入以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">export</span> http_proxy=http://127.0.0.1:12306/<br>$ <span class="hljs-built_in">export</span> https_proxy=http://127.0.0.1:12306/<br><span class="hljs-comment"># 其中 12306 必须和 前述端口 一致！</span><br></code></pre></td></tr></table></figure><p>完成后，可以使用 <code>wget google.com</code>测试是否成功。为了方便在服务器开启服务，可以在 <code>~./zshrc</code>中加上别名:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">alias</span> proxyon=<span class="hljs-string">&#x27;export http_proxy=http://127.0.0.1:12306 https_proxy=http://127.0.0.1:12306 &amp;&amp; echo Proxy On!&#x27;</span></span><br><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">alias</span> proxyoff=<span class="hljs-string">&#x27;unset http_proxy https_proxy &amp;&amp; echo Proxy Off!&#x27;</span></span><br></code></pre></td></tr></table></figure><blockquote><p>如果上述方法行不通，除了用本机下载预训练模型后 <code>scp</code>到服务器，还可尝试此方案：<ahref="https://zhuanlan.zhihu.com/p/475260268">如何优雅的下载huggingface-transformers 模型</a>。</p><p>最新的总结，包含镜像站：<ahref="https://zhuanlan.zhihu.com/p/663712983">如何快速下载huggingface模型——全方法总结- 知乎 (zhihu.com)</a></p></blockquote><h3 id="服务器安装-clash">服务器安装 Clash</h3><p>这里展示在 Linux 系统中使用 Clash 挂载 VPN 的方法：</p><ol type="1"><li><p>下载 <ahref="https://github.com/Dreamacro/clash/releases/tag/premium">ClashPremium</a>：参考 clash-linux-amd64-vX.Y.Z.gz 或clash-linux-amd64-YYYY.MM.DD.gz</p></li><li><p>进入服务器，在任意目录下新建文件夹 /clash，解压上述文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 解压上述文件</span><br>gzip -d clash-linux-amd64-vX.Y.Z.gz<br><span class="hljs-comment"># 将解压后的文件重命名为 `clash`，再赋权</span><br>chmod 777 clash<br></code></pre></td></tr></table></figure></li><li><p>在该目录下运行解压后的文件：<code>./clash</code></p><ul><li>这个时候会<strong>报错</strong>，然后在 ~/.config 下会自动创建一个clash 目录，并自动下载 MMDB 文件</li><li>如果没有自动下载，则到 <ahref="https://github.com/Dreamacro/maxmind-geoip/releases/latest/download/Country.mmdb">仓库</a>中手动下载然后拖到 ~/.config/clash 下</li><li><strong>此时这个目录下有三个文件</strong>：cache.db + config.yaml +Country.mmdb</li></ul></li><li><p>在本地找到 VPN 的 Clash 配置文件 config.yaml，复制到~/.config/clash/config.yaml 中：</p><ul><li><p>直接配置的服务器可能时断时续，可以<strong>打开 Tun模式</strong>使用（只有 Premium 版才能开启），在 YAML配置文件末尾加入以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">tun:</span><br>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">stack:</span> <span class="hljs-string">system</span> <span class="hljs-comment"># gvisor</span><br>    <span class="hljs-attr">dns-hijack:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">any:53</span><br>    <span class="hljs-attr">auto-route:</span> <span class="hljs-literal">true</span> <span class="hljs-comment"># auto set global route</span><br>    <span class="hljs-attr">auto-redir:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">auto-detect-interface:</span> <span class="hljs-literal">true</span> <span class="hljs-comment"># conflict with interface-name</span><br></code></pre></td></tr></table></figure></li><li><p>复制完再到 /clash 目录下运行<code>./clash</code>，此时就能跑通</p></li><li><p>如果报错显示 7890 或 9090端口被占用，一般是被僵尸进程占用了，使用<code>ps aux | grep clash</code> 找到进程号，再 <code>kill</code>即可</p></li></ul></li><li><p>成功运行后，在终端中配置端口转发：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> http_proxy=127.0.0.1:7890<br><span class="hljs-built_in">export</span> https_proxy=127.0.0.1:7890<br></code></pre></td></tr></table></figure></li></ol><p>大功告成！可以测试连接：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl google.com<br></code></pre></td></tr></table></figure><h2 id="一键配置">一键配置</h2><p>下面对于<strong>临时使用的命令</strong>进行汇总，由 Shell脚本的形式一键执行。只需在 <code>/home/user</code> 目录下新建一个<code>run.sh</code> 文件，输入以下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><br><span class="hljs-comment"># conda 相关</span><br>wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2020.02-Linux-x86_64.sh<br>bash Anaconda3-2020.02-Linux-x86_64.sh<br><span class="hljs-built_in">source</span> ~/.bashrc<br>conda install -c conda-forge cudatoolkit=11.1 cudnn<br><br><span class="hljs-comment"># pip 相关</span><br>pip install -U pip -i https://pypi.tuna.tsinghua.edu.cn/simple<br>pip config <span class="hljs-built_in">set</span> global.index-url https://pypi.tuna.tsinghua.edu.cn/simple<br>pip install gpustat<br>pip install nvitop<br><br><span class="hljs-comment"># apt 相关</span><br>apt update<br>apt install zip -y<br>apt install unzip -y<br>apt install tmux -y<br>apt install inetutils-ping -y<br><br><span class="hljs-comment"># 别名配置</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias gpus=&#x27;watch -n1 -c gpustat --color&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias nvi=&#x27;nvitop&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias ca=&#x27;conda activate&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias cda=&#x27;conda deactivate&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias proxyon=&#x27;export http_proxy=http://127.0.0.1:12306 https_proxy=http://127.0.0.1:12306 &amp;&amp; echo Proxy On!&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias proxyoff=&#x27;unset http_proxy https_proxy &amp;&amp; echo Proxy Off!&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias tl=&#x27;tmux list-sessions&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias tk=&#x27;tmux kill-session -t&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias ta=&#x27;tmux attach -t&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias tn=&#x27;tmux new-session -s&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias ll=&#x27;ls -alF&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias la=&#x27;ls -A&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias l=&#x27;ls -CF&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias ls=&#x27;ls --color=tty&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias md=&#x27;mkdir -p&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;alias rd=&#x27;rmdir&#x27;&quot;</span> &gt;&gt; ~/.bashrc<br><br><span class="hljs-built_in">source</span> ~/.bashrc<br><br><span class="hljs-comment"># git 配置，替换成自己的</span><br>git config --global user.name <span class="hljs-string">&quot;hewei2001&quot;</span><br>git config --global user.email <span class="hljs-string">&quot;631670924@qq.com&quot;</span><br></code></pre></td></tr></table></figure><p>在命令行中执行 <code>sh run.sh</code>。</p>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #11 主成分分析</title>
    <link href="/ML-Note-11.html"/>
    <url>/ML-Note-11.html</url>
    
    <content type="html"><![CDATA[<p>我们收集的数据集特征常常包含众多维度，但其中有些维度其实没有存在的必要。最极端的情况就是某一维度是其他若干维度的<strong>线性组合</strong>，那么这一维度就完全可以丢掉；但现实不会这么精准，如果某一维度是其他若干维度的线性组合<strong>加上微小的扰动</strong>，其实也可以将其丢掉。</p><p>这就是无监督学习中的另一类问题：<strong>数据降维</strong>（DimensionalityReduction）。降维的好处有很多，在介绍 <ahref="https://hwcoder.top/ML-Note-3">正规方程</a>时曾提过，冗余特征可能会导致 <span class="math inline">\(X^TX\)</span>矩阵不可逆。此外，高维特征还可能会造成<strong>特征空间稀疏</strong>，阻碍模型学习数据，也可能会导致<strong>计算量过大</strong>的问题。同时，低维特征（<spanclass="math inline">\(2/3\)</span>维）也方便进行<strong>可视化</strong>分析。</p><p><img src="/img/blog/ML-Note-11-images/data compression.png" alt="数据降维" width=90%  /></p><p>数据降维有众多算法可以完成，<strong>主成分分析</strong>（PrincipalComponent Analysis，PCA）便是其中之一。</p><h2 id="主成分分析-pca">主成分分析 | PCA</h2><p>主成分分析的基本思想是：假设原始数据的特征有 <spanclass="math inline">\(n\)</span> 维，我们想将其缩减到 <spanclass="math inline">\(k\)</span> 维，那么我们只需要在原来的 <spanclass="math inline">\(n\)</span> 维空间中找到一个 <spanclass="math inline">\(k\)</span>维的子空间，使得<strong>所有数据到这个子空间的距离平方和最小</strong>；此时，原数据在这个子空间上的投影就是我们新的<span class="math inline">\(k\)</span> 维的数据。</p><p>以 <span class="math inline">\(2\)</span> 维数据投影到 <spanclass="math inline">\(1\)</span>维为例，我们要做的就是找到一个方向向量，使得将所有数据投影到该向量上时，<strong>投影距离平方和</strong>最小（<strong>最小降维损失理论</strong>）。如下图所示，红色直线作为方向向量时，投影距离（<strong>垂线段</strong>）明显比绿色直线更小，因此我们更倾向于选取红色直线。</p><p><img src="/img/blog/ML-Note-11-images/2d-1d.png" alt="寻找最优的方向向量使得投影误差最小" width=50% /></p><p>我们可以直观地理解为：红色直线作为子空间时，<strong>极大的保留了原始空间中无标注样本间的特征</strong>，原本距离远的点，投影后距离也远（<strong>最大方差理论</strong>）；而绿色直线则会<strong>模糊了原有的特征</strong>。下面从数学的角度进行推导。</p><blockquote><p>注意，以上的「寻找方向向量」过程并非线性回归中「寻找拟合直线」，下面一张图可以展现二者的区别：</p><p><img src="/img/blog/ML-Note-11-images/linear.png" alt="寻找最优的直线拟合使得误差平方和最小" width=40% /></p></blockquote><h3 id="数学推导">数学推导</h3><p>为方便，我们首先将数据<strong>均值归一化</strong>（MeanNormalization），即使得数据的平均值在原点处。一个获得所需要的 <spanclass="math inline">\(k\)</span> 维子空间的简单方式是：找到一个合适的<span class="math inline">\(n\)</span>维空间，直接选取<strong>最重要的</strong>前 <spanclass="math inline">\(k\)</span> 维作为子空间。</p><p>具体而言，对于一个数据点 <spanclass="math inline">\(\mathbf{x}=\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix}\in\mathbbR^n\)</span>，设我们要找的 <span class="math inline">\(n\)</span>维空间的<strong>规范正交基</strong>为 <spanclass="math inline">\(\mathbf{U}^T=[\mathbf{u}_1,\mathbf{u}_2,\cdots,\mathbf{u}_n]\)</span>，则 <span class="math inline">\(x\)</span>在其中的新坐标为 <spanclass="math inline">\(\mathbf{y}=\mathbf{Ux}\)</span>。其中： <spanclass="math display">\[y_i=\mathbf{u}_i\cdot \mathbf{x}=u_{i1}x_1+u_{i2}x_2+\cdots +u_{in}x_n\]</span> 它到前 <span class="math inline">\(k\)</span>维形成的子空间（即以 <spanclass="math inline">\(\mathbf{u}_1,\mathbf{u}_2,\cdots,\mathbf{u}_k\)</span> 为基底的子空间）的距离之平方为： <spanclass="math display">\[{y_{k+1}}^2+{y_{k+2}}^2+\cdots+{y_n}^2\]</span> 假设我们有 <span class="math inline">\(m\)</span> 个数据 <spanclass="math inline">\(\mathbf{x}^{(1)},\mathbf{x}^{(2)},\cdots,\mathbf{x}^{(m)}\)</span>，于是我们的优化目标为： <spanclass="math display">\[\min \sum_{i=1}^m{y^{(i)}_{k+1}}^2+{y^{(i)}_{k+2}}^2+\cdots+{y^{(i)}_{n}}^2\]</span> 又由于在不同的基下 <spanclass="math inline">\({||\mathbf{x}||}^2\)</span>都是一个定值，于是<strong>最小化上述距离等价于最大化下式</strong>：<span class="math display">\[\max \sum_{i=1}^m{y_{1}^{\left( i \right)}}^2+{y_{2}^{\left( i\right)}}^2+\cdots +{y_{k}^{\left( i \right)}}^2\]</span> 其充分条件为： <span class="math display">\[\max \sum_{i=1}^m{y^{(i)}_{r}}^{2},\quad r=1,2,...,k\]</span> 这就是我们的优化目标。由于：</p><p><span class="math display">\[\begin{aligned}    \sum_{i=1}^m{y^{(i)}_r}^2&amp;=\sum_{i=1}^m{\left( \mathbf{u}_r\cdot\mathbf{x}^{\left( i \right)} \right) ^2}\\    &amp;=\sum_{i=1}^m{\left( \mathbf{u}_{r}^{T}\mathbf{x}^{\left( i\right)} \right) \left( {\mathbf{x}^{\left( i \right)}}^T\mathbf{u}_r\right)}\\    &amp;=\mathbf{u}_{r}^{T}\left( \sum_{i=1}^m{\mathbf{x}^{\left( i\right)}{\mathbf{x}^{\left( i \right)}}^T} \right) \mathbf{u}_r\\\end{aligned}\]</span> 是一个<strong>正定二次型</strong>，<spanclass="math inline">\(\sum\limits_{i=1}^m {\mathbf{x}^{\left( i\right)}{\mathbf{x}^{\left( i \right)}}^T}\)</span>是一个<strong>正定矩阵</strong>，可以进行<strong>奇异值分解</strong>（SVD，在<a href="https://hwcoder.top/IR-Note-3">IR学习笔记</a> 中曾介绍过）：<span class="math display">\[\sum_{i=1}^m{\mathbf{x}^{\left( i \right)}{\mathbf{x}^{\left( i\right)}}^T}=\mathbf{U\Sigma U}^T\]</span> 其中，<span class="math inline">\(\mathbf{U}\)</span>是<strong>正交矩阵</strong>，<spanclass="math inline">\(\mathbf{\Sigma}\)</span>是<strong>对角矩阵</strong> <spanclass="math inline">\(\begin{bmatrix}\sigma_1&amp;\cdots&amp;0\\\vdots&amp;\ddots&amp;\vdots\\0&amp;\cdots&amp;\sigma_n\end{bmatrix}\)</span>，<spanclass="math inline">\(\sigma_1,\cdots,\sigma_n\)</span>是<strong>奇异值</strong>，<spanclass="math inline">\(\sigma_1&gt;\cdots&gt;\sigma_n\)</span>。</p><blockquote><p>奇异值（Singular Value）不同于特征值（Eigen Value），以下摘录自 <ahref="https://zhuanlan.zhihu.com/p/353637184">奇异值与特征值辨析</a>：</p><ul><li>矩阵作用于特征向量时（<spanclass="math inline">\(Ax\)</span>），只是将 <spanclass="math inline">\(x\)</span> 乘上一个标量 <spanclass="math inline">\(\lambda\)</span>，相当于将长度缩放了 <spanclass="math inline">\(\lambda\)</span> 倍。<spanclass="math inline">\(\lambda\)</span> 可正可负，代表两个方向。</li><li>奇异值 $$是非负实数，通常从大到小顺序排列。矩阵作用于奇异向量时（<spanclass="math inline">\(Av\)</span>），得到的向量长度就是 $$，最大的 $_1 $能使作用后向量最长（对应圆的半径投影到椭圆的长轴），但不能保证方向一致。</li></ul><p>特征值一般是对方阵而言的，对于非方阵 <spanclass="math inline">\(\mathbf{X}\)</span>，通常将其乘上自身的转置得到方阵<span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span>，此时 <spanclass="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> 的特征值的开方等于<span class="math inline">\(\mathbf{X}\)</span> 的奇异值。</p></blockquote><p>令 <spanclass="math inline">\(\mathbf{v}_r=\mathbf{U}^T\mathbf{u}_r\)</span>，由于<span class="math inline">\(\mathbf{U}\)</span> 正交，所以 <spanclass="math inline">\(\mathbf{v}_r\)</span>也是<strong>单位向量</strong>（模为 <spanclass="math inline">\(1\)</span>），代回得到： <spanclass="math display">\[\begin{aligned}\sum_{i=1}^m{y^{(i)}_r}^2&amp;=\mathbf{u}_{r}^{T}\left ( \mathbf{U\SigmaU}^T \right ) \mathbf{u}_r\\&amp;=\left( \mathbf{U}^T\mathbf{u}_r \right) ^T\mathbf{\Sigma }\left(\mathbf{U}^T\mathbf{u}_r \right)\\&amp;=\mathbf{v}_{r}^{T}\mathbf{\Sigma v}_r\\&amp;=\sigma _1v_{r1}^{2}+\sigma _2v_{r2}^{2}+\cdots +\sigma_nv_{rn}^{2}\\\end{aligned}\]</span> 所以我们的优化目标变成了： <span class="math display">\[\begin{align}&amp;\max\sum_{i=1}^n\sigma_iv_{ri}^2\\&amp;\text{s.t.}\begin{cases}\sum\limits_{i=1}^nv_{ri}^2=1\\\sigma_1&gt;\cdots&gt;\sigma_n\end{cases}\end{align}\]</span> 很显然，它的解是：<spanclass="math inline">\(v_{r1}=1\)</span> 且 <spanclass="math inline">\(v_{r2}=\cdots=v_{rn}=0\)</span>，即 <spanclass="math inline">\(\mathbf{v}_r=\begin{bmatrix}1\\0\\\vdots\\0\end{bmatrix}\)</span>。但是由于<span class="math inline">\(\mathbf{u}_r\)</span> 正交，所以 <spanclass="math inline">\(\mathbf{v}_r\)</span> 只能在 <spanclass="math inline">\(v_{rr}\)</span> 处取 <spanclass="math inline">\(1\)</span>。</p><p>又由于 <spanclass="math inline">\(\mathbf{u}_r=\mathbf{Uv}_r\)</span>，所以我们要找的<span class="math inline">\(n\)</span>维空间的各个<strong>基向量</strong>就是矩阵 <spanclass="math inline">\(\sum\limits_{i=1}^m {\mathbf{x}^{\left( i\right)}{\mathbf{x}^{\left( i \right)}}^T}\)</span>的各个奇异值对应的<strong>奇异向量</strong>，我们要降维到的 <spanclass="math inline">\(k\)</span> 维子空间的各个基向量就是前 <spanclass="math inline">\(k\)</span>个奇异向量，对原来的数据进行<strong>基变换</strong>，就得到了降维后的数据。</p><h3 id="算法步骤">算法步骤</h3><p>总结一下，主成分分析的推导过程稍显复杂，但是它的实现很简单，主要是以下步骤：</p><ol type="1"><li><p>计算矩阵 <span class="math inline">\(\sum\limits_{i=1}^m{\mathbf{x}^{\left( i \right)}{\mathbf{x}^{\left( i\right)}}^T}\)</span>，更简单的表达是：设矩阵 <spanclass="math inline">\(\mathbf{X}_{m\timesn}=\begin{bmatrix}{\mathbf{x}^{(1)}}^T\\{\mathbf{x}^{(2)}}^T\\\vdots\\{\mathbf{x}^{(m)}}^T\end{bmatrix}\)</span>为数据集，那么计算样本<strong>内积</strong>矩阵 <spanclass="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> 即可；</p></li><li><p>对 <span class="math inline">\(n\times n\)</span> 的内积矩阵<span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span>进行奇异值分解，得到奇异向量；</p></li><li><p>选取前 <span class="math inline">\(k\)</span>个奇异向量作为降维后的空间的基向量，构成<strong>基变换</strong>矩阵<span class="math inline">\(\mathbf{C}_{n\times k}\)</span>；</p></li><li><p>对于原数据 <span class="math inline">\(\mathbf{x}\)</span>，取<span class="math inline">\(\mathbf{z}=\mathbf{C}^T\mathbf{x}\)</span>为其降维后的数据。更简单的表达是：取 <spanclass="math inline">\(\mathbf{Z}_{m\times k}=\mathbf{X}_{m\timesn}\mathbf{C}_{n\times k}\)</span>，则 <spanclass="math inline">\(\mathbf{Z}_{m\times k}\)</span>是降维后的数据集。</p></li></ol><blockquote><p>PCA的算法过程中完全无超参数参与，不需要人为进行干预，最后的结果只与数据有关。这既是优点也是缺点，缺点在于难以利用已有先验进行额外的干预。</p></blockquote><h3 id="主成分数量的选择">主成分数量的选择</h3><p>那么在实践中，我们到底选择多大的 <spanclass="math inline">\(k\)</span>值比较好呢？对此，我们定义一个<strong>平均误差</strong>为： <spanclass="math display">\[\frac{1}{m}\sum_{i=1}^m{\left\| \mathbf{x}^{\left( i\right)}-\mathbf{x}_{\mathrm{approx}}^{\left( i \right)} \right\| ^2}\]</span> 其中，<spanclass="math inline">\(\mathbf{x}_\mathrm{approx}^{(i)}\)</span> 表示数据<span class="math inline">\(\mathbf{x}^{(i)}\)</span> 在我们找到的 <spanclass="math inline">\(k\)</span>维子空间上的投影。再定义一个<strong>总方差</strong>为： <spanclass="math display">\[\frac{1}{m}\sum_{i=1}^m{\left\| \mathbf{x}^{\left( i \right)} \right\|^2}\]</span> 则一般的，我们会选择最小的 <spanclass="math inline">\(k\)</span> 使得： <span class="math display">\[\frac{\frac{1}{m}\sum_{i=1}^m{\left\| \mathbf{x}^{\left( i\right)}-\mathbf{x}_{\mathrm{approx}}^{\left( i \right)} \right\|^2}}{\frac{1}{m}\sum_{i=1}^m{\left\| \mathbf{x}^{\left( i \right)}\right\| ^2}}\leqslant 0.01\]</span> 并称之为「<span class="math inline">\(99\%\)</span>的<strong>方差得以保留</strong>」，这样能使得丢失的信息最小化。</p><blockquote><p>丢失信息越少就越好吗？有时候丢失一部分信息，识别的准确率反而更高了，说明数据集中可能存在噪音特征，而我们将那些噪音丢弃了。但是主成分分析舍弃的特征通常是与标签无关的，在更多时候会不如有监督的LDA 降维。</p></blockquote><p>此外，上述式子看起来并不好计算，但可以证明，对于给定的 <spanclass="math inline">\(k\)</span>，我们可以借助<strong>奇异值</strong>（来自SVD 分解中的 <span class="math inline">\(\mathbf{\Sigma}\)</span>矩阵）进行计算： <span class="math display">\[\frac{\frac{1}{m}\sum_{i=1}^m{\left\| \mathbf{x}^{\left( i\right)}-\mathbf{x}_{\mathrm{approx}}^{\left( i \right)} \right\|^2}}{\frac{1}{m}\sum_{i=1}^m{\left\| \mathbf{x}^{\left( i \right)}\right\| ^2}}=1-\frac{\sum_{i=1}^k{\sigma _k}}{\sum_{i=1}^n{\sigma _i}}\]</span> 由于奇异值衰减非常快，<span class="math inline">\(k\)</span>不需要很大时上式的值就会低于 <spanclass="math inline">\(0.01\)</span>。</p><h3 id="重建压缩数据">重建压缩数据</h3><p>作为一种压缩算法，主成分分析也有解压缩的过程，将低维数据还原到高维数据的<strong>近似值</strong>，也就是前面的<span class="math inline">\(\mathbf{x}_{\mathrm{approx}}\in\mathbbR^n\)</span>。假设我们有压缩后的数据 <spanclass="math inline">\(\mathbf{z}\in\mathbbR^k\)</span>，则只需用基变换矩阵 <spanclass="math inline">\(\mathbf{C}_{n\times k}\)</span> 倒过来算： <spanclass="math display">\[\mathbf{x}\approx \mathbf{x}_{\mathrm{approx}}=\mathbf{C}_{n\timesk}\mathbf{z}\]</span><img src="/img/blog/ML-Note-11-images/reconstruction.png" alt="重建压缩数据" width=70%/></p><h2 id="练习">练习</h2><h3 id="二维数据压缩到一维">二维数据压缩到一维</h3><p>下面以 <a href="https://www.coursera.org/">Coursera</a> 上的数据集<code>ex7data1.mat</code> 为例，这是一个二维平面上的点集：</p><p><img src="/img/blog/ML-Note-11-images/ex7data1.png" alt="二维平面上的点集" width=40% /></p><p>下面我们借助 Numpy 中自带的 <code>np.linalg.svd(arr)</code>命令实现主成分分析代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">PCA</span>(<span class="hljs-params">X, dim = -<span class="hljs-number">1</span></span>):</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">X is the input data: (m, n)</span><br><span class="hljs-string"></span><br><span class="hljs-string">dim is the dimension after reduction</span><br><span class="hljs-string">if dim=-1, then the program select the smallest dim</span><br><span class="hljs-string">such that 99% of variance is retained</span><br><span class="hljs-string"></span><br><span class="hljs-string">return the data after reduction: (m, dim)</span><br><span class="hljs-string">and the data recovered from reduced data: (m, n)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>Xmean = np.mean(X, axis=<span class="hljs-number">0</span>)<br>Xstd = np.std(X, axis=<span class="hljs-number">0</span>, ddof=<span class="hljs-number">1</span>)<br>Xnorm = (X - Xmean) / Xstd   <span class="hljs-comment"># (m, n)</span><br><br>U, S, V = np.linalg.svd(Xnorm.T @ Xnorm)<br><span class="hljs-keyword">if</span> dim == -<span class="hljs-number">1</span>:<br>dim = <span class="hljs-number">1</span><br><span class="hljs-keyword">while</span> S[:dim].<span class="hljs-built_in">sum</span>() / S.<span class="hljs-built_in">sum</span>() &lt; <span class="hljs-number">0.99</span>:<br>dim += <span class="hljs-number">1</span><br>Z = Xnorm @ U[:, :dim]   <span class="hljs-comment"># (m, dim)</span><br>Xapprox = (Z @ U[:, :dim].T) * Xstd + Xmean  <span class="hljs-comment"># (m, n)</span><br><br><span class="hljs-keyword">return</span> Z, Xapprox<br><br><span class="hljs-comment"># X.shape = (50, 2)</span><br>X = loadmat(<span class="hljs-string">&#x27;ex7data1.mat&#x27;</span>)[<span class="hljs-string">&#x27;X&#x27;</span>]<br>redX, recX = PCA(X, dim=<span class="hljs-number">1</span>)<br><br>plt.xlabel(<span class="hljs-string">&#x27;x1&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;x2&#x27;</span>)<br>plt.plot(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;black&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>)<br>plt.plot(recX[:, <span class="hljs-number">0</span>], recX[:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>)<br>plt.axis(<span class="hljs-string">&#x27;square&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>得到投影的结果如下：</p><p><img src="/img/blog/ML-Note-11-images/ex7data1_result.png" alt="二维数据压缩到一维的结果" width=40% /></p><h3 id="人脸特征压缩">人脸特征压缩</h3><p>数据集 <code>ex7faces.mat</code> 给出了 <spanclass="math inline">\(5000\)</span> 张人脸照片，每张照片含有 <spanclass="math inline">\(32\times32\)</span> 的灰度像素，形成维度为 <spanclass="math inline">\(1024\)</span> 的向量作为其特征。前 <spanclass="math inline">\(64\)</span> 张照片如图所示：</p><p><img src="/img/blog/ML-Note-11-images/ex7faces.png" alt="人脸照片示例" width=60%/></p><p>现在将其压缩为 <spanclass="math inline">\(\text{dim}={36,100}\)</span> 维的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib<br><br>X = loadmat(<span class="hljs-string">&#x27;ex7faces.mat&#x27;</span>)[<span class="hljs-string">&#x27;X&#x27;</span>] <span class="hljs-comment"># (5000, 1024)</span><br>X = np.transpose(X.reshape((<span class="hljs-number">5000</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)), [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]).reshape(<span class="hljs-number">5000</span>, <span class="hljs-number">1024</span>) <span class="hljs-comment"># 转置处理</span><br>X = -X <span class="hljs-comment"># 反色</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_n_image</span>(<span class="hljs-params">X, n</span>):</span><br><span class="hljs-string">&quot;&quot;&quot; plot first n images</span><br><span class="hljs-string">n has to be a square number</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>pic_size = <span class="hljs-built_in">int</span>(np.sqrt(X.shape[<span class="hljs-number">1</span>]))<br>grid_size = <span class="hljs-built_in">int</span>(np.sqrt(n))<br><br>first_n_images = X[:n, :]<br><br>fig, ax_array = plt.subplots(nrows=grid_size, ncols=grid_size,<br>sharey=<span class="hljs-literal">True</span>, sharex=<span class="hljs-literal">True</span>, figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br><br><span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(grid_size):<br><span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(grid_size):<br>ax_array[r, c].matshow(first_n_images[grid_size * r + c].reshape((pic_size, pic_size)), <br>  cmap=matplotlib.cm.binary)<br>plt.xticks(np.array([]))<br>plt.yticks(np.array([]))<br><br>redX, recX = PCA(X, dim=<span class="hljs-number">36</span>) <span class="hljs-comment"># 调用之前写好的 PCA 函数</span><br>plot_n_image(recX, n=<span class="hljs-number">64</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>恢复后结果如下：</p><p><img src="/img/blog/ML-Note-11-images/ex7faces_dim=36.png" alt="压缩为 dim=36 的恢复结果较为模糊" width=60% /></p><p><img src="/img/blog/ML-Note-11-images/ex7faces_dim=100.png" alt="压缩为 dim=100 的恢复结果有更多细节" width=60% /></p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #10 K-Means 聚类</title>
    <link href="/ML-Note-10.html"/>
    <url>/ML-Note-10.html</url>
    
    <content type="html"><![CDATA[<p>本文将开始介绍<strong>无监督学习</strong>。与之前的内容不同，无监督学习的数据不再包含标注的标签，即采用<strong>完全无标注的数据集</strong>。其中聚类聚类问题属于无监督学习的范畴，其目的是在无标注的情况下将样本集划分为若干类。</p><p>本节要介绍的 K-Means 算法就是聚类算法的典型，其中的 <spanclass="math inline">\(K\)</span> 指将样本集划分为 <spanclass="math inline">\(K\)</span>个<strong>簇</strong>（Cluster）。而我们之前介绍过的 <ahref="https://hwcoder.top/PR-Note-1">KNN算法</a>，则是一种有监督分类器，其中的 <spanclass="math inline">\(K\)</span>是设定的近邻数量，初学者容易混淆二者。</p><h2 id="聚类问题-clustering">聚类问题 | Clustering</h2><p>首先简单介绍一下聚类问题，其本质是「根据<strong>样本之间的相似度</strong>，将数据进行归类」，由于没有显式的标签，唯一的依据就是样本之间的相似度。而相似度的度量方法，可以大致分为：</p><ul><li>距离相似性度量：以欧式距离为代表的各种距离、余弦相似度等。</li><li>密度相似性度量：以 KL 散度为代表的各种熵。</li><li>连通相似性度量：以杰卡德指数为代表的各种统计量，在集合与图背景下更为常用。</li></ul><p>由以上度量方法引申出的聚类算法也有很多：</p><ol type="1"><li>基于<strong>划分</strong>的聚类：K-Means、K-Means++、BisectingK-Means 等。</li><li>基于<strong>密度</strong>的聚类：DBSCAN、Mean Shift、OPTICS等。</li><li><strong>层次</strong>聚类：DIANA、AGNES、HDBSCAN、Agglomerative、Divisive等。</li><li>基于<strong>图</strong>的聚类：Chinese Whisper、CDP 等。</li></ol><h3 id="聚类评价指标">聚类评价指标</h3><p>如何评价一个无监督的算法？和有监督类似，仍然需要一个测试集，但无监督算法的测试集则有无标签都可以。针对数据<strong>有类别标签</strong>的情况，最常用的评价指标有两种：</p><ul><li><strong>均一性：</strong>每个聚簇中正确分类的样本数占该聚簇总样本数的比例和，类似于<strong>精确率</strong>，如果一个簇中只包含一个类别的样本，则称其满足均一性。</li><li><strong>完整性</strong>：每个聚簇中正确分类的样本数占该类型的总样本数比例的和，类似于<strong>召回率</strong>，同类别样本被归类到相同簇中，则称其满足完整性。</li></ul><p>同样，将上述二者加权平均，就能得到类似 <spanclass="math inline">\(\mathrm{F} \text {-score}\)</span> 的 <spanclass="math inline">\(\mathrm{V} \text{-score}\)</span>。此外，还有：</p><ul><li><strong>调节兰德系数</strong>（Adjusted Randindex，ARI）：计算聚类结果与实际划分的<strong>重叠程度</strong>，重叠程度越高表示聚类效果越好。</li><li><strong>归一化互信息</strong>（Normalized MutualInformation，NMI）：计算聚类结果的<strong>簇内互信息</strong>，值越大表示聚类结果越相近，效果越好。</li></ul><p>对于无类别标签的情况，最常用的指标是<strong>轮廓系数</strong>（SilhouetteCoefficient），结合了<strong>内聚度</strong>（Compactness）和<strong>分离度</strong>（Separation）两种因素。简单来说，就是希望簇内样本尽量相近，簇间样本尽量相远，下文将详细介绍。</p><h2 id="k-means-算法">K-Means 算法</h2><p><strong>K-Means</strong> 算法目的在于寻找最优的 <spanclass="math inline">\(K\)</span>个<strong>聚类中心</strong>，并将每个样本点归到距离最近的中心点，而聚类中心的优劣显然就取决于能否<strong>使距离之和最小化</strong>。</p><p>如果已知每个样本点的所属类，那很容易就能算出中心（即<strong>质心</strong>）。如果已知每个类的中心，那么也很容易进行分类（按<strong>距离</strong>归类）。因此这是一个「先有鸡还是先有蛋」的问题，通过交替迭代计算，直到收敛就能得到两个问题的答案。计算步骤如下：</p><ol type="1"><li>首先随机初始化 <span class="math inline">\(K\)</span>个聚类中心；</li><li>计算所有点到 <span class="math inline">\(K\)</span>个中心的距离，将每个点归到<strong>距离最近的中心</strong>，得到 <spanclass="math inline">\(K\)</span> 类；</li><li>计算 <span class="math inline">\(K\)</span>个类的最优聚类中心，即用该类别样本点的<strong>平均位置</strong>来更新<span class="math inline">\(K\)</span> 个中心；</li><li>重复 2-3 步，直到聚类中心不再改变，算法结束。</li></ol><h3 id="问题定义">问题定义</h3><p>下面将更严谨地给出 K-Means 算法的数学定义，首先我们有数据集： <spanclass="math display">\[\begin{array}{c}    \left\{ x^{(i)}\in \mathbb{R} ^n,i=1,2,\cdots ,m \right\}\\    \left\{ \mu _k\in \mathbb{R} ^n,k=1,2,\cdots ,k \right\}\\    x^{(i)}\rightarrow \mu _{c^{\left( i \right)}}\\\end{array}\]</span></p><ul><li><span class="math inline">\(m\)</span> 代表数据集中样本的数量；</li><li><span class="math inline">\(x\)</span> 代表样本特征，是一个 <spanclass="math inline">\(n\)</span> 维向量，不需要设定偏置项 <spanclass="math inline">\(x_0=1\)</span>；</li><li><span class="math inline">\(K\)</span>代表设定的目标类别数，为超参数；</li><li><span class="math inline">\(\mu\)</span> 代表聚类中心，共有 <spanclass="math inline">\(K\)</span> 个，下标取值为类别编号；</li><li><span class="math inline">\(c^{(i)}\)</span> 代表样本 <spanclass="math inline">\(x^{(i)}\)</span> 所属的类别，取值范围也是 <spanclass="math inline">\([1,K]\)</span>；</li><li><span class="math inline">\(\mu _{c^{\left( i \right)}}\)</span>代表样本 <span class="math inline">\(x^{(i)}\)</span>所属的聚类中心。</li></ul><p>此时我们可以将前文的第 2 步视为： <span class="math display">\[c^{\left( i \right)}:=\underset{k}{\min}\left\| x^{\left( i \right)}-\mu_k \right\| _2^2\]</span> 这里的距离定义为<strong>欧氏距离的平方</strong>。第 3 步视为：<span class="math display">\[\mu _k:=\frac{1}{\mathrm{Count}\left( c^{\left( i \right)}=k\right)}\sum_{c^{\left( i \right)}=k}{x^{\left( i \right)}}\]</span> 注意，如果此时对于某个类别 <spanclass="math inline">\(k\)</span>，没有数据属于某一聚类中心，即 <spanclass="math inline">\(\mathrm{Count}\left( c^{\left( i \right)}=k\right)=0\)</span>，则可以将该聚类中心<strong>删去</strong>（这样分类数会减少）或者<strong>置于随机位置</strong>上（保持分类数不变）。</p><h3 id="优化目标">优化目标</h3><p>经过上文的定义，我们可以引入一个代价函数——<strong>残差平方和</strong>（Sumof SquaredError，SSE），即各数据点到它所属于的聚类中心的距离之平方和，取<strong>均值</strong>（Mean）：<span class="math display">\[J\left( c^{(1)},\cdots ,c^{(m)},\mu _1,\cdots ,\mu _K \right)=\frac{1}{m}\sum_{i=1}^m{\left\| x^{\left( i \right)}-\mu _{c^{\left( i\right)}} \right\| _{2}^{2}}\]</span> 很容易证明，2、3 两个步骤都是在减小这个代价：第 2 步减小 <spanclass="math inline">\(c^{(i)}\)</span> 引起的代价，第 3 步减小 <spanclass="math inline">\(\mu _k\)</span> 引起的代价。所以正确实现的 K-Means算法的<strong>代价应随着迭代次数增加而减小</strong>，虽然不是凸函数，但是也可以收敛到<strong>局部最优解</strong>，不会陷入一直选择质心的循环停不下来。</p><h3 id="选取簇数-k">选取簇数 <span class="math inline">\(K\)</span></h3><p>在实践中，我们可以任取 <span class="math inline">\(K&lt;m\)</span>个数据点作为聚类中心。随着聚类簇数 <spanclass="math inline">\(K\)</span>不断增大，样本数据划分会逐渐变得更加精细；因此，随着 <spanclass="math inline">\(K\)</span>不断增大，每个簇的聚合程度会逐渐提高，代价函数 SSE会逐渐变小。极端情况下，当 <span class="math inline">\(K=m\)</span>时，每个样本自成一簇，此时代价函数为 <spanclass="math inline">\(0\)</span>。</p><p><span class="math inline">\(K\)</span> 增大则 <spanclass="math inline">\(J\)</span>减小，绘制出代价函数关于簇数的变化曲线：</p><p><img src="/img/blog/ML-Note-10-images/elbow.png" alt="代价函数关于簇数的变化曲线" width=60% /></p><p>观察上图，我们发现当 <span class="math inline">\(K\)</span>小于「合适的簇数」时，代价函数下降的幅度极大；而当 <spanclass="math inline">\(K\)</span>大于「合适的簇数」时，下降的幅度又逐渐变缓。曲线呈现出「手肘」形状，因此人们通常以<strong>手肘法则</strong>（ElbowMethod）为依据来指导选择这个超参数。手肘法的缺点在于需要人工观察、不够自动化，所以又有了<strong>Gap Statistic</strong> 方法，此处不展开介绍。</p><p>另一种思路则是从特征空间的角度出发，绘制样本的<strong>轮廓图</strong>（SilhouettePlot）来选择簇数。轮廓图是针对单个样本 <spanclass="math inline">\(x^{(i)}\)</span> 而言的，样本 <spanclass="math inline">\(x^{(i)}\)</span>的<strong>轮廓系数</strong>（SilhouetteCoefficient）可以通过以下方式计算： <span class="math display">\[s^{\left( i \right)}=\frac{b^{\left( i \right)}-a^{\left( i\right)}}{\max \left\{ a^{\left( i \right)},b^{\left( i \right)}\right\}}\]</span><img src="/img/blog/ML-Note-10-images/Silhouette.png" alt="第 i 个样本的轮廓图" width=60%/></p><p>其中，左图为簇内不相似度 <span class="math inline">\(a^{\left( i\right)}\)</span>，代表样本 <spanclass="math inline">\(x^{(i)}\)</span>，到同簇的其他样本 <spanclass="math inline">\(x^{(j)}\)</span> 的距离平均值： <spanclass="math display">\[a^{\left( i \right)}=\frac{1}{\mathrm{Count}\left( c^{\left( i\right)}=k \right) -1}\sum_{c^{\left( j \right)}=k,i\ne j}{\left\|x^{\left( i \right)}-x^{\left( j \right)} \right\| _{2}^{2}}\]</span> 右图为簇间不相似度 <span class="math inline">\(b^{\left( i\right)}\)</span>，代表样本 <span class="math inline">\(x^{(i)}\)</span>到其他簇样本 <span class="math inline">\(x^{(j)}\)</span>的距离平均值的最小值： <span class="math display">\[b^{\left( i \right)}=\underset{k&#39;\nek}{\min}\frac{1}{\mathrm{Count}\left( c^{\left( j \right)}=k&#39;\right)}\sum_{c^{\left( j \right)}=k&#39;}{\left\| x^{\left( i\right)}-x^{\left( j \right)} \right\| _{2}^{2}}\]</span> 显然，轮廓系数的取值在 <spanclass="math inline">\([-1,1]\)</span> 之间，<spanclass="math inline">\(s^{\left( i \right)}\)</span> 越趋向于 <spanclass="math inline">\(1\)</span>，说明样本 <spanclass="math inline">\(x^{(i)}\)</span>分类<strong>越正确</strong>；<span class="math inline">\(s^{\left( i\right)}\)</span> 越趋向于 <spanclass="math inline">\(-1\)</span>，说明样本 <spanclass="math inline">\(x^{(i)}\)</span> 分类<strong>越错误</strong>；当<span class="math inline">\(s^{\left( i \right)}\)</span> 在 <spanclass="math inline">\(0\)</span> 附近时，说明样本 <spanclass="math inline">\(x^{(i)}\)</span>靠近<strong>聚类边界</strong>。</p><h2 id="k-means-的衍生算法">K-Means 的衍生算法</h2><p>K-Means简单易用，方便部署，但是也存在着许多缺点。除了上文提到的<strong>簇数</strong><span class="math inline">\(K\)</span> 的选择问题，它还有以下缺点：</p><ul><li>容易陷入<strong>局部最优解</strong>；</li><li>对<strong>噪音</strong>、<strong>异常点</strong>、<strong>离群点</strong>（Outlier）过于敏感；</li><li>单次迭代<strong>复杂度</strong>高达 <spanclass="math inline">\(O(Knm)\)</span>；</li><li>对于<strong>非凸</strong>的数据集难以有效。</li></ul><p>针对这些缺点也有许多的衍生算法，下面将简要介绍。</p><h3 id="k-means">K-Means++</h3><p>在运行 K-Means算法时，首先需要随机初始化所有聚类中心点，然而不同的选取方式可能导致不同的收敛结果——停留于<strong>局部最优解</strong>。例如下图：</p><p><img src="/img/blog/ML-Note-10-images/local.png" alt="随机初始化导致的局部最优解" width=90% /></p><p>比较朴素的做法是多次运行 K-Means算法，每一次都重新随机初始化，最后选取代价函数最小的结果。这种方法在<span class="math inline">\(K\)</span><strong>较小</strong>的时候还是可行的，但是对于 <spanclass="math inline">\(K\)</span> 较大的情况需要重复的次数则非常多。</p><p>K-Means++是最经典的改进算法，其核心思想是：<strong>逐个选取</strong>聚类中心，且其他<strong>距离现有中心越远的样本点越有可能</strong>被选为下一个聚类中心。具体步骤如下：</p><ol type="1"><li>首先随机选取第一个初始聚类中心 <spanclass="math inline">\(\mu_1\)</span>；</li><li>计算每个样本与<strong>当前已有</strong>聚类中心之间的<strong>最短距离</strong>，用<span class="math inline">\(D(x^{(i)})\)</span>表示，则该样本被选取为下一个聚类中心的<strong>概率</strong>为 <spanclass="math inline">\(P(x^{(i)})\)</span>；</li><li>重复第 2 步，直到选出 <span class="math inline">\(K\)</span>个初始聚类中心。</li></ol><p>其中概率 <span class="math inline">\(P(x^{(i)})\)</span>的计算方法为： <span class="math display">\[P(x^{\left( i \right)})=\frac{D(x^{\left( i\right)})}{\sum_{j=1}^m{D(x^{\left( j \right)})}}\]</span></p><blockquote><p>注意，K-Means++选点是基于概率，而非直接选择距离最远的点，因为这样很容易陷入离群点，导致一个离群点被单独聚为一类。</p></blockquote><h3 id="bisecting-k-means">Bisecting K-Means</h3><p>二分 K-Means算法也是为了改进初始中心选取的问题，它巧妙地规避了「一次选出 <spanclass="math inline">\(K\)</span>个」的问题。其核心思想是：自上而下地逐渐增加簇数，每次选择<strong>能最大程度降低代价函数的簇</strong>将其划分为两个簇。具体步骤如下：</p><ol type="1"><li>首先将所有数据点看作一个簇类；</li><li>对于每一个簇计算<strong>簇内的SSE</strong>，同时在每一个簇上单独进行 <spanclass="math inline">\(K=2\)</span> 的 K-Means聚类，计算将该簇<strong>一分为二后的总 SSE</strong>；</li><li>求出每个簇的两个 SSE之差，将差值最大者一分为二，并重新更新每个点所属的中心；</li><li>重复第 2-3 步，直到选出 <span class="math inline">\(K\)</span>个初始聚类中心。</li></ol><blockquote><p>该方法与<strong>层次聚类</strong>（AgglomerativeClustering）有点类似但却不同。层次聚类每次选取空间中距离最近的两个点，将其聚为新的「<strong>广义点</strong>」。最终可以将所有点<strong>自下而上</strong>聚集为一类，进而<strong>自上而下</strong>划分为若干类。</p></blockquote><h3 id="k-mediods">K-Mediods</h3><p>离群点问题带来的影响有很多，直观地就是：如果选中了离群点作为初始聚类中心，大概率会单独聚为一类；即使没有选中，离群点也会给SSE 带来较大影响，导致预测的<strong>聚类中心偏移</strong>。</p><p>一种可行的方法是通过<strong>局部离群因子</strong>（Local OutlierFactor，LOF）算法找出离群点，这是一种基于密度的检测方法。此外，还有基于<strong>中位数计算</strong>的K-Mediods聚类。该算法不选用均值，转而采用<strong>簇中位置最中心的样本点</strong>（Mediods）作为参考点，必须是实际存在的点。选取的准则是所有点到该中心的SSE。</p><p>由于选取的是实际存在的点，那么稀少的离群点肯定不会被选中，也不会导致聚类中心偏移到太远的位置。该方法对于小规模数据是非常有效的，但是注意到选取Mediods 时需要遍历簇内每一个点，并计算所有距离，所有复杂度是 <spanclass="math inline">\(O(n^2)\)</span> 的，比起 K-Means要<strong>慢许多</strong>。</p><h3 id="mini-batch-k-means">Mini Batch K-Means</h3><p>对于超大规模数据量 <span class="math inline">\(m\)</span> 的K-Means，每一轮迭代需要 <span class="math inline">\(O(Knm)\)</span>完成，且初值选取不好时需要<strong>很多轮次才能收敛</strong>，总耗时太久。优化的思路有两个：减少单次迭代的复杂度、减少迭代次数。</p><p>其中前者常用的方法有 Elkan K-Means优化，其在计算样本与聚类中心的距离时，利用<strong>三角形关系</strong>中的两边之和大于第三边，减少距离计算的次数。该方法可以实现常数级的优化，但依然不够。</p><p>后者采用的方法是 MiniBatch（<strong>分批处理</strong>），通过随机采样得到<strong>小规模的数据子集</strong>，由于总量很大，采样时能确保数据分布几乎一致。在子集上运行K-Means算法能<strong>大幅减少收敛时间</strong>，并且将<strong>聚类中心初始化到一个相对好的位置</strong>。具体步骤如下：</p><ol type="1"><li>随机采样出部分数据子集，并在子集上使用 K-Means算法<strong>收敛</strong>得到 <span class="math inline">\(K\)</span>个聚类中心；</li><li><strong>继续采样</strong>出部分数据，<strong>加到</strong>上述的子集中，并将其归到最近的聚类中心；</li><li>更新 <span class="math inline">\(K\)</span> 个聚类中心；</li><li>重复第 2-3 步，直到所有数据都加入子集。</li></ol><h3 id="kernel-k-means">Kernel K-Means</h3><p>对于非凸的数据样本，K-Means 往往无法正确聚类：</p><p><img src="/img/blog/ML-Note-10-images/kernel-kmeans.jpg" alt="非凸样本的聚类结果" width=80% /></p><p>在 <a href="https://hwcoder.top/ML-Note-9">ML学习笔记 #9支持向量机</a> 中我们曾介绍过 Kernel 方法，在 K-Means中同样也可以使用。此外，基于<strong>密度</strong>的聚类算法则更加适合于非凸的样本，这里不展开介绍。</p><h2 id="练习">练习</h2><h3 id="平面点集聚类">平面点集聚类</h3><p>下面以 <a href="https://www.coursera.org/">Coursera</a> 上的数据集<code>ex7data2.mat</code>为例，这是一个三类的平面点集，首先看看数据集的样子：</p><p><img src="/img/blog/ML-Note-10-images/raw_data.png" alt="平面点集聚类的原始数据" width=60%  /></p><p>使用如下代码聚类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data</span><br>X = loadmat(<span class="hljs-string">&#x27;ex7data2.mat&#x27;</span>)[<span class="hljs-string">&#x27;X&#x27;</span>] <span class="hljs-comment"># (300, 2)</span><br><br><span class="hljs-comment"># cost function, c := (300,) mu := (K, 2)</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">J</span>(<span class="hljs-params">X, c, mu</span>):</span><br>m = X.shape[<span class="hljs-number">0</span>]<br>expand_c = mu[c] <span class="hljs-comment"># (300, 2)</span><br><span class="hljs-keyword">return</span> np.linalg.norm(X - expand_c, axis=<span class="hljs-number">1</span>, <span class="hljs-built_in">ord</span>=<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>() / m<br><br><span class="hljs-comment"># K-Means</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">K_means</span>(<span class="hljs-params">K, X, times=<span class="hljs-number">100</span></span>):</span><br>(m, n) = X.shape<br>best_c, best_mu, best_J = np.empty(m), np.empty((K, n)), np.inf<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(times):<br>mu = X[np.random.randint(<span class="hljs-number">0</span>, m, K)]<br>c = np.empty(m, dtype=<span class="hljs-string">&#x27;int&#x27;</span>)<br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>next_mu = np.zeros((K, n))<br>cnt = np.zeros(K)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>c[i] = np.argmin(np.<span class="hljs-built_in">sum</span>(np.square(X[i]-mu), axis=<span class="hljs-number">1</span>)) <span class="hljs-comment"># (X[i]-mu) := (K, 2)</span><br>next_mu[c[i]] += X[i]<br>cnt[c[i]] += <span class="hljs-number">1</span><br>next_mu[cnt!=<span class="hljs-number">0</span>] /= cnt[cnt!=<span class="hljs-number">0</span>][:, np.newaxis]<br>next_mu[cnt==<span class="hljs-number">0</span>] = X[np.random.randint(<span class="hljs-number">0</span>, m, <span class="hljs-built_in">len</span>(cnt[cnt==<span class="hljs-number">0</span>]))]<br><span class="hljs-keyword">if</span> (mu == next_mu).<span class="hljs-built_in">all</span>():<br><span class="hljs-keyword">break</span><br>mu = next_mu.copy()<br>cost = J(X, c, mu)<br><span class="hljs-keyword">if</span> cost &lt; best_J:<br>best_c, best_mu, best_J = c.copy(), mu.copy(), cost.copy()<br><span class="hljs-keyword">return</span> best_c, best_mu<br><br><span class="hljs-comment"># c.shape = (m,) mu.shape = (K, n)</span><br>c, mu = K_means(<span class="hljs-number">3</span>, X, times=<span class="hljs-number">100</span>)<br><br>plt.xlabel(<span class="hljs-string">&#x27;x1&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;x2&#x27;</span>)<br>plt.plot(X[c==<span class="hljs-number">0</span>][:, <span class="hljs-number">0</span>], X[c==<span class="hljs-number">0</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>, alpha=<span class="hljs-number">0.4</span>)<br>plt.plot(X[c==<span class="hljs-number">1</span>][:, <span class="hljs-number">0</span>], X[c==<span class="hljs-number">1</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;green&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>, alpha=<span class="hljs-number">0.4</span>)<br>plt.plot(X[c==<span class="hljs-number">2</span>][:, <span class="hljs-number">0</span>], X[c==<span class="hljs-number">2</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>, alpha=<span class="hljs-number">0.4</span>)<br>plt.plot(mu[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], mu[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;*&#x27;</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, ms=<span class="hljs-number">10</span>)<br>plt.plot(mu[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], mu[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;*&#x27;</span>, color=<span class="hljs-string">&#x27;green&#x27;</span>, ms=<span class="hljs-number">10</span>)<br>plt.plot(mu[<span class="hljs-number">2</span>, <span class="hljs-number">0</span>], mu[<span class="hljs-number">2</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;*&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>, ms=<span class="hljs-number">10</span>)<br>plt.plot([], [], <span class="hljs-string">&#x27;*&#x27;</span>, color=<span class="hljs-string">&#x27;black&#x27;</span>, ms=<span class="hljs-number">10</span>, label=<span class="hljs-string">&#x27;cluster centroid&#x27;</span>)<br>plt.legend()<br>plt.plot()<br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="/img/blog/ML-Note-10-images/result1.png" alt="平面点集聚类的结果" width=60%  /></p><h3 id="图像压缩">图像压缩</h3><p>图像是有若干像素组成的，每个像素存放 <spanclass="math inline">\(3\)</span> 个字节的信息代表其 <spanclass="math inline">\(\text{RGB}\)</span>三通道，三通道可以组成成百上千种颜色，如果可以将其压缩到 <spanclass="math inline">\(16\)</span>种颜色，那么只需要在对应像素位置存放一个 <spanclass="math inline">\(4\)</span>位二进制数即可，这样就把图像压缩到了原来的 <spanclass="math inline">\(\frac{1}{6}\)</span> 大小。</p><p>现在我们用 K-Means 算法去得到这 <spanclass="math inline">\(16\)</span> 种颜色：把原来的所有像素点映射到 <spanclass="math inline">\(\text{RGB}\)</span> 空间，再将其聚为 <spanclass="math inline">\(16\)</span> 类，得到 <spanclass="math inline">\(16\)</span> 个聚类中心用来表示新的压缩像素点。</p><p>我们使用的图像含有 <span class="math inline">\(128\times128\)</span>个像素，可以处理为 <span class="math inline">\((128\times128,3)\)</span>的二维数组，每一行就是一个像素，包含 <spanclass="math inline">\(3\)</span> 个值，即 <spanclass="math inline">\(\text{RGB}\)</span>三通道。这就是我们的输入数据。原图如下：</p><figure><img src="/img/blog/ML-Note-10-images/bird_small.png"alt="bird_small" /><figcaption aria-hidden="true">bird_small</figcaption></figure><p>使用如下代码聚类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-comment"># load data</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">readin</span>():</span><br>data = np.array(Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;bird_small.png&#x27;</span>))<br>data = data.reshape((<span class="hljs-number">128</span>*<span class="hljs-number">128</span>, <span class="hljs-number">3</span>))<br><span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># cost function</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">J</span>(<span class="hljs-params">X, c, mu</span>):</span><br>m = X.shape[<span class="hljs-number">0</span>]<br>expand_c = mu[c] <span class="hljs-comment"># (300, 2)</span><br><span class="hljs-keyword">return</span> np.linalg.norm(X - expand_c, axis=<span class="hljs-number">1</span>, <span class="hljs-built_in">ord</span>=<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>() / m<br><br><span class="hljs-comment"># K-Means</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">K_means</span>(<span class="hljs-params">K, X, times=<span class="hljs-number">100</span></span>):</span><br>(m, n) = X.shape<br>best_c, best_mu, best_J = np.empty(m), np.empty((K, n)), np.inf<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(times):<br>mu = X[np.random.randint(<span class="hljs-number">0</span>, m, K)]<br>c = np.empty(m, dtype=<span class="hljs-string">&#x27;int&#x27;</span>)<br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>next_mu = np.zeros((K, n))<br>cnt = np.zeros(K)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>c[i] = np.argmin(np.<span class="hljs-built_in">sum</span>(np.square(X[i]-mu), axis=<span class="hljs-number">1</span>))<br>next_mu[c[i]] += X[i]<br>cnt[c[i]] += <span class="hljs-number">1</span><br>next_mu[cnt!=<span class="hljs-number">0</span>] /= cnt[cnt!=<span class="hljs-number">0</span>][:, np.newaxis]<br>next_mu[cnt==<span class="hljs-number">0</span>] = X[np.random.randint(<span class="hljs-number">0</span>, m, <span class="hljs-built_in">len</span>(cnt[cnt==<span class="hljs-number">0</span>]))]<br><span class="hljs-keyword">if</span> (mu == next_mu).<span class="hljs-built_in">all</span>():<br><span class="hljs-keyword">break</span><br>mu = next_mu.copy()<br>cost = J(X, c, mu)<br><span class="hljs-keyword">if</span> cost &lt; best_J:<br>best_c, best_mu, best_J = c.copy(), mu.copy(), cost.copy()<br><span class="hljs-keyword">return</span> best_c, best_mu<br><br>X = readin()<br>c, mu = K_means(<span class="hljs-number">16</span>, X, times=<span class="hljs-number">20</span>)<br><br>comImg = np.empty((<span class="hljs-number">128</span>*<span class="hljs-number">128</span>, <span class="hljs-number">3</span>), dtype=<span class="hljs-string">&#x27;uint8&#x27;</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>*<span class="hljs-number">128</span>):<br>comImg[i] = np.floor(mu[c[i]])<br>comImg = comImg.reshape((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>))<br>im = Image.fromarray(comImg)<br>im.save(<span class="hljs-string">&#x27;bird_compression.png&#x27;</span>)<br></code></pre></td></tr></table></figure><p>压缩后的图片如下：</p><figure><img src="/img/blog/ML-Note-10-images/bird_compression.png"alt="bird_compression" /><figcaption aria-hidden="true">bird_compression</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #09 支持向量机</title>
    <link href="/ML-Note-9.html"/>
    <url>/ML-Note-9.html</url>
    
    <content type="html"><![CDATA[<p>本节将继续介绍监督学习中的一个强力的分类器——支持向量机（SupportVectorMachine，SVM），其通过「<strong>间隔最大化</strong>」的思想来学习最优的<strong>决策边界</strong>，比逻辑回归算法更为鲁棒。</p><h2 id="支持向量机-svm">支持向量机 | SVM</h2><p>首先来直观地理解一下决策边界，考虑一个<strong>线性可分</strong>的二分类问题：</p><p><img src="/img/blog/ML-Note-9-images/boundarys.jpg" alt="不同的决策边界" width = 35% /></p><p>图中有三条直线分别代表三个分类器的决策边界，可以直观地感受到 <spanclass="math inline">\(H_3\)</span> 应该会好于 <spanclass="math inline">\(H_1\)</span> 和 <spanclass="math inline">\(H_2\)</span>。显然，<spanclass="math inline">\(H_1\)</span>甚至不能把类别正确分开，肯定不能考虑；<spanclass="math inline">\(H_2\)</span>虽然可以，但是边界距离最近的数据点只有<strong>很小的间隔</strong>，如果测试数据有一些噪声的话，很可能会被<span class="math inline">\(H_2\)</span>错误地分到另一类——即<strong>对噪声敏感、泛化能力弱</strong>。</p><p>而 <span class="math inline">\(H_3\)</span>似乎<strong>距离两侧都有较大的间隔</strong>，可以容忍一些微小的噪声。如何能学习到这样一个分类器呢？</p><h3 id="优化目标">优化目标</h3><p>回忆逻辑回归的优化目标： <span class="math display">\[\min_{\theta} \frac{1}{m}\left[ \sum_{i=1}^m{y^{\left( i \right)}}\left(-\ln h_{\theta}\left( x^{\left( i \right)} \right) \right) +\left(1-y^{\left( i \right)} \right) \left( -\ln \left( 1-h_{\theta}\left(x^{\left( i \right)} \right) \right) \right) \right]+\frac{\lambda}{2m}\sum_{j=1}^n{\theta _{j}^{2}}\]</span> 其中的 <span class="math inline">\(h_{\theta}\left( x^{\left(i \right)} \right)\)</span> 使用 <spanclass="math inline">\(\mathrm{sigmoid}\)</span> 函数，因此对于一个 <spanclass="math inline">\(y=1\)</span> 的样本，我们希望 <spanclass="math inline">\(h_{\theta}\left( x\right)\)</span> 也趋近于 <spanclass="math inline">\(1\)</span>，也就是 <spanclass="math inline">\(\theta^Tx\)</span> 远大于 <spanclass="math inline">\(0\)</span>，反之同理。进一步地，我们只考虑优化目标中的代价函数：<span class="math display">\[\mathrm{Cost}\left( h_{\theta}(x),y \right) =\begin{cases}    -\ln \left( h_{\theta}\left( x \right) \right)&amp;     y=1\\    -\ln \left( 1-h_{\theta}\left( x \right) \right)&amp;       y=0\\\end{cases}\]</span> 画出两种情况下的代价函数关于 <spanclass="math inline">\(\theta^Tx\)</span> 的曲线：</p><p><img src="/img/blog/ML-Note-9-images/sigmoid-cost.png" alt="Sigmoid 代价函数" width = 70%  /></p><p>记上图中左图为 <span class="math inline">\(\text{cost}_1(\theta^Tx)\)</span>，即分类为 <span class="math inline">\(1\)</span>时采用的代价；右图为 <span class="math inline">\(\text{cost}_0(\theta^Tx)\)</span>，即分类为 <span class="math inline">\(0\)</span>时采用的代价。在支持向量机中，我们不再采用 <spanclass="math inline">\(\mathrm{sigmoid}\)</span>函数，而是将其修改为一条<strong>非常接近的折线</strong>：</p><p><img src="/img/blog/ML-Note-9-images/svm-cost.png" alt="支持向量机的代价函数" width = 70%  /></p><p>其中，折线的拐点位于 <span class="math inline">\(z=\pm 1\)</span>的位置，先不考虑斜率的大小，则代价函数可以表示为： <spanclass="math display">\[\text{cost}_1(z)=\begin{cases}0&amp;z\geqslant1\\k_1(z-1)&amp;z&lt;1\end{cases}\quad\quad\text{cost}_0(z)=\begin{cases}0&amp;z\leqslant-1\\k_0(z+1)&amp;z&gt;-1\end{cases}\]</span> &gt;此代价函数也被称为<strong>折页损失</strong>（<strong>HingeLoss</strong>），非常形象，常用于凸优化任务中，也被写作： &gt; <spanclass="math display">\[&gt; L(y)=\max (0, 1-\hat{y}\cdot y)&gt; \]</span> &gt; 其中 <span class="math inline">\(y=\pm 1\)</span>为正确的标签，<span class="math inline">\(\hat{y}\)</span>为预测输出，通常是软结果，即取 <spanclass="math inline">\([-1,1]\)</span> 区间中的值。</p><p>另外，SVM 中习惯不除以样本大小 <spanclass="math inline">\(m\)</span>，这不会影响最终的优化目标。并将<strong>正则化参数</strong>放在第一项而非第二项，即支持向量机的优化目标为：<span class="math display">\[\min_{\theta} C\sum_{i=1}^m{\left[ y^{\left( i\right)}\mathrm{cost}_1\left( \theta ^Tx^{\left( i \right)} \right)+\left( 1-y^{\left( i \right)} \right) \mathrm{cost}_0\left( \theta^Tx^{\left( i \right)} \right) \right]}+\frac{1}{2}\sum_{j=1}^n{\theta_{j}^{2}}\]</span> 其中，<span class="math inline">\(C\)</span>就是正则化参数，类比逻辑回归中 <spanclass="math inline">\(\lambda\)</span> 的作用。并且当 <spanclass="math inline">\(C\)</span> 取到 <spanclass="math inline">\(\frac{1}{\lambda}\)</span>时两者的优化目标一致。</p><h3 id="大间隔思想-large-margin">大间隔思想 | Large Margin</h3><p>显然，新的代价函数会在计算速度上有一定的优势，但它是如何<strong>最大化间隔</strong>（Margin）的呢？我们首先观察第一项代价$y_1( ^Tx ) +( 1-y ) _0( ^Tx )$，欲使之最小，最理想的情况就是对于每个样本，都有 <spanclass="math inline">\(\text{cost}_y(\theta^T x)=0\)</span>，对应着：<span class="math display">\[\begin{cases}\theta^Tx\geqslant 1&amp;\text{if }y=1\\\theta^Tx\leqslant -1&amp;\text{if }y=0\end{cases}\]</span>而在逻辑回归中，我们<strong>只需要使误分类的样本最少</strong>，也就是：<span class="math display">\[\begin{cases}\theta^Tx\geqslant 0&amp;\text{if }y=1\\\theta^Tx\leqslant0&amp;\text{if }y=0\end{cases}\]</span> 因此 SVM相当于将<strong>判别条件变得更苛刻</strong>，以文章开篇提到的三条决策边界为例，直线<span class="math inline">\(H_2\)</span> 和 <spanclass="math inline">\(H_3\)</span>以及它俩的任意线性组合，都能完美分开两类，也就是能有无穷多个解满足逻辑回归的优化目标。但对于支持向量机而言，可选的解会相对少很多。而这也使SVM能相对更具有鲁棒性，因为我们留出了一个「<strong>安全边界</strong>」来应对噪声样本。</p><p>接下来假设一种极端的情况，我们将正则化参数 <spanclass="math inline">\(C\)</span>取一个<strong>非常大的值</strong>，此时模型会倾向于将第一项收敛为 <spanclass="math inline">\(0\)</span>。那么模型的优化目标可以改写为： <spanclass="math display">\[\min_{\theta} \frac{1}{2}\sum_{j=1}^n{\theta _{j}^{2}}\,\,\\\mathrm{s}.\mathrm{t} \begin{cases}    \theta^Tx^{(i)}\geqslant 1 &amp;\text{if } y^{(i)}=1\\    \theta^Tx^{(i)}\leqslant -1 &amp;\text{if } y^{(i)}=0\\\end{cases}\]</span> 上式也被称为<strong>硬间隔</strong>（HardMargin）优化目标，用于在线性可分的数据集中找到<strong>唯一的</strong>最优边界。但在在现实情况中，数据样本通常更加复杂，即使真的线性可分，也可能会存在部分<strong>离群点</strong>，而模型为了得到完美的边界，会学习到偏差很大的边界。因此在实际中通常不将<span class="math inline">\(C\)</span>设置得过大，采用<strong>软间隔</strong>（Soft Margin）。</p><p><img src="/img/blog/ML-Note-9-images/hard-margin.png" alt="存在离群点时的硬间隔和软间隔" width= 40% /></p><p>回顾我们前面说到的，可以将 <span class="math inline">\(C\)</span>类比为 <spanclass="math inline">\(\frac{1}{\lambda}\)</span>，因此：</p><ul><li><span class="math inline">\(C\)</span> 较大时，相当于 <spanclass="math inline">\(\lambda\)</span>较小，可能会导致过拟合、高方差，出现如上图红线那样的情形。</li><li><span class="math inline">\(C\)</span> 较小时，相当于 <spanclass="math inline">\(\lambda\)</span>较大，可能会导致欠拟合、高偏差，甚至无法分类。</li></ul><h3 id="几何角度理解大间隔">几何角度理解大间隔</h3><p>在前文改写的优化目标的基础上，我们注意到 <spanclass="math inline">\(\theta^Tx\)</span> 其实是参数 <spanclass="math inline">\(\theta\)</span> 和 <spanclass="math inline">\(x\)</span>两个向量的<strong>点积（内积）</strong>，可以视作 <spanclass="math inline">\(x\)</span> 向 <spanclass="math inline">\(\theta\)</span> 的投影长度乘上 <spanclass="math inline">\(\theta\)</span> 的长度 <spanclass="math inline">\(||\theta||\)</span>。</p><p>为了更好理解，我们忽略掉截距，令 <spanclass="math inline">\(\theta_0=0\)</span>，将特征数 <spanclass="math inline">\(n\)</span> 置为 <spanclass="math inline">\(2\)</span>，于是仅有两个特征 <spanclass="math inline">\(\{x_1,x_2\}\)</span> 和两个参数 <spanclass="math inline">\(\{\theta_1,\theta_2\}\)</span>。则此时有： <spanclass="math display">\[\frac{1}{2}\sum_{j=1}^n{\theta _{j}^{2}}=\frac{1}{2}\left( \theta_{1}^{2}+\theta _{2}^{2} \right) =\frac{1}{2}\left( \sqrt{\theta_{1}^{2}+\theta _{2}^{2}} \right) ^2=\frac{1}{2}\left\| \theta \right\|^2\]</span> 从几何角度理解，参数 <spanclass="math inline">\(\theta\)</span>其实是<strong>决策边界的法向量</strong>，因此会和边界本身呈现垂直关系。而特征<span class="math inline">\(x\)</span>就是样本点在（特征空间）坐标系上的向量，起点为原点。绘制出如下样例：</p><p><img src="/img/blog/ML-Note-9-images/project.png" alt="样本点在法向量上的投影" width=40% /></p><p>其中 <span class="math inline">\(p^{\left( i \right)}\)</span>为投影轴的长度（可正可负），于是可以得到： <span class="math display">\[\theta ^Tx^{\left( i \right)}=\theta _1x_{1}^{\left( i \right)}+\theta_2x_{2}^{\left( i \right)}=p^{\left( i \right)}\left\| \theta \right\|\]</span> 因此优化目标可以从<strong>几何角度</strong>再次改写为： <spanclass="math display">\[\min_{\theta} \frac{1}{2}\left\| \theta \right\| ^2\,\,\\\mathrm{s}.\mathrm{t}\begin{cases}    p^{\left( i \right)}\left\| \theta \right\| \geqslant1&amp;    \text{if }  y^{(i)}=1\\    p^{\left( i \right)}\left\| \theta \right\| \leqslant-1&amp;   \text{if }  y^{(i)}=0\\\end{cases}\]</span> 显然，为了让法向量的范数 <span class="math inline">\(\left\|\theta \right\| ^2\)</span> 最小化，我们必须增大样本在法向量上的投影<span class="math inline">\(p^{\left( i \right)}\)</span>长度，如下图所示：</p><p><img src="/img/blog/ML-Note-9-images/margin.png" alt="不同方向的法向量具有不同的投影长度" width=90% /></p><p>以上就是为什么支持向量机最终会找到最大间隔的原因。当然，在推导的过程中我们使用了简化的假设，例如<spanclass="math inline">\(θ_0=0\)</span>，这是为了让决策平面通过原点，下面我们将移除这个假设进行数学推导。</p><h2 id="线性-svm-的通解">线性 SVM 的通解</h2><p>前面我们介绍了 SVM优化目标的设定，显然，对于线性可分的数据集，<strong>有且仅有一个</strong>最优超平面可以达到间隔最大化。接下来我们将以硬间隔为例，推导线性可分数据集的最优决策平面的求解过程。本节将继续沿用前文的参数定义。</p><p><img src="/img/blog/ML-Note-9-images/support-vector.png" alt="最优超平面与支持向量" width=60%  /></p><h3 id="支持向量-support-vector">支持向量 | Support Vector</h3><p>一个超平面由法向量 <spanclass="math inline">\(\theta\)</span>（不包含 <spanclass="math inline">\(\theta_0\)</span>）和截距 <spanclass="math inline">\(\theta_0\)</span> 决定，其方程为 <spanclass="math inline">\(\theta^Tx+\theta_0=0\)</span>，这里规定法向量指向的一侧为正类，另一侧为负类。上图中最优超平面的法方向取右上方向。</p><p>在线性可分的情况下，训练数据集的<strong>样本点中与分离超平面距离最近的数据点</strong>称为支持向量（SupportVector），支持向量是使得优化目标中的约束条件取等的点，即满足： <spanclass="math display">\[y^{\left( i \right)}\left( \theta ^Tx^{\left( i \right)}+\theta _0\right) =0\]</span><strong>在决定最佳超平面时只有支持向量起作用，而其他数据点并不起作用</strong>。如果移动非支持向量，甚至删除非支持向量都不会对最优超平面产生任何影响。也即支持向量对模型起着决定性的作用，这也是「支持向量机」名称的由来。</p><p>容易发现，两条支持向量其实是平行的直线，通过两条平行直线的距离公式可得：<span class="math display">\[\rho =\frac{2}{\left\| \theta \right\|}\]</span> 要使得间隔 $$（Margin）尽量大，存在以下等价关系： <spanclass="math display">\[\max_{\theta ,\theta _0} \rho \Longleftrightarrow \max_{\theta ,\theta_0} \rho ^2\Longleftrightarrow \min_{\theta ,\theta _0}\frac{1}{2}\left\| \theta \right\| ^2\]</span>进而，加入约束条件，得到间隔最大化问题的数学表达（与上一节类似）： <spanclass="math display">\[\begin{aligned}&amp;\min_{\theta ,\theta _0} \frac{1}{2}\left\| \theta \right\| ^2\\\mathrm{s}.\mathrm{t}\;\;y^{\left( i \right)}&amp;\left( \theta^Tx^{\left( i \right)}+\theta _0 \right) \geqslant 1\end{aligned}\]</span></p><h3 id="对偶问题-dual-problem">对偶问题 | Dual Problem</h3><p>称上式所述问题为<strong>原始问题</strong>（PrimalProblem），可以应用<strong>拉格朗日乘子法</strong>构造拉格朗日函数（LagrangeFunction）再通过求解其<strong>对偶问题</strong>（DualProblem）得到原始问题的最优解。</p><p>首先引入<strong>拉格朗日乘子</strong>（Lagrange Multiplier）<spanclass="math inline">\(\alpha^{(i)}\geqslant 0,i=1,2,\cdots,n\)</span>。则拉格朗日函数为： <span class="math display">\[L(\theta ,\theta _0,\alpha )=\frac{1}{2}\left\| \theta \right\|^2-\sum_{i=1}^n{\alpha ^{\left( i \right)}}\left[ y^{\left( i\right)}\left( \theta ^Tx^{\left( i \right)}+\theta _0 \right) -1\right]\]</span>如果所有样本点都线性可分，则上式中第二项<strong>必不为负</strong>，则：<span class="math display">\[\max_{\alpha} L(\theta ,\theta _0,\alpha )=\frac{1}{2}\left\| \theta\right\| ^2\]</span> 因此，优化问题等价于： <span class="math display">\[\min_{\theta ,\theta _0} \max_{\alpha} L(\theta ,\theta _0,\alpha )\]</span>根据<strong>拉格朗日对偶性</strong>，上述问题即原始问题的对偶问题是：<span class="math display">\[\max_{\alpha} \min_{\theta ,\theta _0} L(\theta ,\theta _0,\alpha )\]</span> 通过转化为对偶问题，我们只需<strong>优化一个变量</strong><spanclass="math inline">\(\alpha\)</span>，且<strong>约束条件也可以简化</strong>。</p><h3 id="极大化极小">极大化极小</h3><p>首先求<strong>内部的极小化问题</strong> <spanclass="math inline">\(\min_{\theta ,\theta _0} L(\theta ,\theta_0,\alpha )\)</span>，对拉格朗日函数求导并令导数为 <spanclass="math inline">\(0\)</span>： <span class="math display">\[\begin{aligned}&amp;\nabla _{\theta}L(\theta ,\theta _0,\alpha )=\theta-\sum_{i=1}^n{\alpha ^{\left( i \right)}y^{\left( i \right)}x^{\left( i\right)}}=0\Longrightarrow \theta =\sum_{i=1}^n{\alpha ^{\left( i\right)}y^{\left( i \right)}x^{\left( i \right)}}\\&amp;\nabla _{\theta _0}L(\theta ,\theta _0,\alpha)=-\sum_{i=1}^n{\alpha ^{\left( i \right)}y^{\left( i\right)}}=0\Longrightarrow \sum_{i=1}^n{\alpha ^{\left( i\right)}y^{\left( i \right)}}=0\end{aligned}\]</span> 将以上两式代入 <span class="math inline">\(L(\theta ,\theta_0,\alpha )\)</span> 并化简： <span class="math display">\[\begin{aligned}    L(\theta ,\theta _0,\alpha )&amp;=\frac{1}{2}\left\| \theta \right\|^2-\sum_{i=1}^n{\alpha ^{\left( i \right)}}\left[ y^{\left( i\right)}\left( \theta ^Tx^{\left( i \right)}+\theta _0 \right) -1\right]\\    &amp;=\frac{1}{2}\left( \sum_{i=1}^n{\alpha ^{\left( i\right)}y^{\left( i \right)}x^{\left( i \right)}} \right) \cdot \left(\sum_{j=1}^n{\alpha ^{\left( j \right)}y^{\left( j \right)}x^{\left( j\right)}} \right) -\left( \sum_{i=1}^n{\alpha ^{\left( i\right)}y^{\left( i \right)}x^{\left( i \right)}} \right) \cdot \left(\sum_{j=1}^n{\alpha ^{\left( j \right)}y^{\left( j \right)}x^{\left( j\right)}} \right) -\theta _0\sum_{i=1}^n{\alpha ^{\left( i\right)}y^{\left( i \right)}}+\sum_{i=1}^n{\alpha ^{\left( i \right)}}\\    &amp;=\sum_{i=1}^n{\alpha ^{\left( i \right)}}-\frac{1}{2}\left(\sum_{i=1}^n{\alpha ^{\left( i \right)}y^{\left( i \right)}x^{\left( i\right)}} \right) \cdot \left( \sum_{j=1}^n{\alpha ^{\left( j\right)}y^{\left( j \right)}x^{\left( j \right)}} \right)\\    &amp;=\sum_{i=1}^n{\alpha ^{\left( i\right)}}-\frac{1}{2}\sum_{i,j=1}^n{y^{\left( i \right)}}y^{\left( j\right)}\alpha ^{\left( i \right)}\alpha ^{\left( j \right)}\left(x^{\left( i \right)}\cdot x^{\left( j \right)} \right)\\\end{aligned}\]</span> 在求<strong>外部的极大化问题</strong>，等价于对上取负数后对<span class="math inline">\(\alpha\)</span> 求极小： <spanclass="math display">\[\begin{aligned}    \min_{\alpha} \frac{1}{2}&amp;\sum_{i,j=1}^n{y^{\left( i\right)}}y^{\left( j \right)}\alpha ^{\left( i \right)}\alpha ^{\left( j\right)}\left( x^{\left( i \right)}\cdot x^{\left( j \right)} \right)\,\,\\    &amp;\mathrm{s}.\mathrm{t}\;\;\sum_{i=1}^n{\alpha ^{\left( i\right)}y^{\left( i \right)}}=0\\\end{aligned}\]</span>不难发现这是一个<strong>二次规划</strong>问题，有现成的通用的算法来求解。此外也有一些更高效的算法，例如序列最小优化（SequentialMinimal Optimiation，SMO）算法。</p><h3 id="kkt-条件">KKT 条件</h3><p>Karush-Kuhn-Tucker（KKT）条件是非线性规划（NonlinearProgramming）最佳解的必要条件，这里暂不展开介绍。现在假设我们求出了以上问题的最优解<span class="math inline">\(\hat{\alpha}\)</span>，则可以求出最优 <spanclass="math inline">\(\theta\)</span>： <span class="math display">\[\hat{\theta}=\sum_{i=1}^n{\hat{\alpha}^{\left( i \right)}y^{\left( i\right)}x^{\left( i \right)}}\]</span> 因为至少存在一个 <spanclass="math inline">\(\hat{\alpha}^{(j)}&gt;0\)</span>（若不存在，即<span class="math inline">\(\hat{\alpha}\)</span> 全为 <spanclass="math inline">\(0\)</span>，则 <spanclass="math inline">\(\hat{W}=0\)</span>，即 <spanclass="math inline">\(\rho=\frac{2}{\|\hat{W}\|}=\infty\)</span>，显然不行），再根据KKT 条件，即： <span class="math display">\[\left\{ \begin{array}{l}    \text{乘子非负：}\alpha ^{\left( i \right)}\geqslant 0\\    \text{约束条件：}y^{\left( i \right)}\left( \theta ^Tx^{\left( i\right)}+\theta _0 \right) -1\geqslant 0\\    \text{互补条件：}\alpha ^{\left( i \right)}\left( y^{\left( i\right)}\left( \theta ^Tx^{\left( i \right)}+\theta _0 \right) -1\right) =0\\\end{array} \right.\]</span> 根据<strong>互补条件</strong>，至少存在一个 <spanclass="math inline">\(j\)</span>，使 <spanclass="math inline">\(y^{\left( j \right)}\left( \theta ^Tx^{\left( j\right)}+\theta _0 \right)-1=0\)</span>，即<strong>存在一个支持向量</strong>，可求得最优 <spanclass="math inline">\(\hat{b}\)</span> : <span class="math display">\[\begin{aligned}    \hat{b}&amp;=\frac{1}{y^{\left( j \right)}}-\theta ^Tx^{\left( j\right)}\\    &amp;=y^{\left( j \right)}-\theta ^Tx^{\left( j \right)}\\    &amp;=y_j-\sum_{i=1}^n{\hat{\alpha}^{\left( i \right)}y^{\left( i\right)}\left( x^{\left( i \right)}\cdot x^{\left( j \right)} \right)}\\\end{aligned}\]</span></p><p>此时分类的决策函数为： <span class="math display">\[f(x)=\mathrm{sign}\left( \sum_{i=1}^n{\hat{\alpha}^{\left( i\right)}y^{\left( i \right)}\left( x^{\left( i \right)}\cdot x\right)}+\hat{b} \right)\]</span>此外，根据互补条件，如果一个样本点<strong>不是支持向量</strong>，则其<spanclass="math inline">\(\hat{\alpha}^{(i)}=0\)</span>，说明<strong>此样本点对模型没有任何作用</strong>。</p><h2 id="实际使用">实际使用</h2><h3 id="软间隔-soft-margin">软间隔 | Soft Margin</h3><p>当训练数据不能线性可分但是可以<strong>近似线性可分</strong>时，可以允许SVM在少量样本上出错，即将之前的硬间隔最大化条件放宽一点，为此引入<strong>软间隔</strong>（SoftMargin）的概念。</p><p>此时，我们就不再不将正则化参数 <span class="math inline">\(C\)</span>设置得过大，则优化目标就变成： <span class="math display">\[\min_{\theta ,\theta _0} \frac{1}{2}\left\| \theta \right\|^2+C\sum_{i=1}^n{\max \left( 0,1-y^{\left( i \right)}\left( \theta^Tx^{\left( i \right)}+\theta _0 \right) \right)}\]</span>通过引入<strong>松弛变量</strong>改写优化目标后，得到的式子依然是一个<strong>凸二次规划</strong>问题，和硬间隔支持向量机类似，我们可以通过拉格朗日乘子法将其转换为对偶问题进行求解。这里不展开介绍。</p><h3 id="高斯核函数-gaussian-kernel">高斯核函数 | Gaussian Kernel</h3><p>对于非线性可分的数据，在之前的文章中我们直到可以引入<strong>高阶特征</strong>来进行<strong>多项式回归</strong>，而问题就是如何选择这些高阶特征？除了通过组合原有特征，在这里我们引入<strong>核函数</strong><span class="math inline">\(\phi:x\mapsto \phi(x)\)</span>，它将原来的<span class="math inline">\(n\)</span>维向量<strong>映射为更高维的向量</strong>，使得这些向量在该更高维空间中线性可分。</p><p><img src="/img/blog/ML-Note-9-images/kernel.png" alt="原始空间通过核函数映射到高维空间" width=70% /></p><p>在之前的例子中，可以认为是使用了<strong>线性核</strong> <spanclass="math inline">\(\phi(x)=x\)</span>，即不做任何改变；而接下来我们将介绍一种最常用的核函数——<strong>高斯核</strong>（GaussianKernel）： <span class="math display">\[\phi_i \left( x \right) =\mathrm{Sim}\left( x,l^{\left( i \right)}\right) =\exp \left\{ -\frac{\left\| x-l^{(i)} \right\| ^2}{2\sigma ^2}\right\}\]</span> 其中 <span class="math inline">\(\sigma\)</span>是可调节参数，<span class="math inline">\(l^{(i)}\)</span>被称为<strong>地标</strong>（Landmarks），是原始空间中选取的某些点，具有和样本特征<span class="math inline">\(x\)</span> 相同的维度。<spanclass="math inline">\(\left\| x-l^{(i)} \right\|\)</span>即为样本到地标的欧氏距离。</p><p>可以看出，高斯核与正态分布的形式有些许类似，绘制出函数图像：</p><p><img src="/img/blog/ML-Note-9-images/sigma.png" alt="不同参数下的高斯核函数图像" width=70%/></p><p>显然，<span class="math inline">\(x\)</span> 距离 <spanclass="math inline">\(l^{(i)}\)</span> 越远时，分子越大，函数值越接近<span class="math inline">\(0\)</span>；距离越近时，分子越接近 <spanclass="math inline">\(0\)</span>，函数值越接近 <spanclass="math inline">\(1\)</span>；当 <spanclass="math inline">\(x=l^{(i)}\)</span>时，函数值取到最大值。因此，我们自然而然地想到可以<strong>取训练集中的样本点作为地标</strong>，有<span class="math inline">\(m\)</span> 个样本就取 <spanclass="math inline">\(m\)</span>个地标。这里不需要区分正负样本点，因为在核函数 <spanclass="math inline">\(\phi^{\left( i \right)} \left( x \right)\)</span>前面还有一个可学习的 <spanclass="math inline">\(\theta^{(i)}\)</span>，对于不同样本会自动区分。注意到<span class="math inline">\(\sigma\)</span>较大时，变化也较为平缓，模型可能会欠拟合，造成高偏差；反之则可能会造成过拟合。</p><p>假设我们原来用 <span class="math inline">\(x^{\left( i\right)}=\left\{ 1,x_{1}^{\left( i \right)},x_{2}^{\left( i\right)},\cdots ,x_{n}^{\left( i \right)} \right\} \in \mathbb{R}^{n+1}\)</span> 来描述第 <span class="math inline">\(j\)</span>个样本的特征向量，则现在就是使用： <span class="math display">\[f\left( x^{\left( i \right)} \right)=\left[ \begin{array}{c}    1\\    \phi _1\left( x^{\left( i \right)} \right)\\    \phi _2\left( x^{\left( i \right)} \right)\\    \vdots\\    \phi _i\left( x^{\left( i \right)} \right)\\    \vdots\\    \phi _m\left( x^{\left( i \right)} \right)\\\end{array} \right] =\left[ \begin{array}{c}    1\\    \mathrm{Sim}\left( x^{\left( i \right)},l^{\left( 1 \right)}\right)\\    \mathrm{Sim}\left( x^{\left( i \right)},l^{\left( 2 \right)}\right)\\    \vdots\\    \mathrm{Sim}\left( x^{\left( i \right)},l^{\left( i \right)} \right)=1\\    \vdots\\    \mathrm{Sim}\left( x^{\left( i \right)},l^{\left( m \right)}\right)\\\end{array} \right]\in \mathbb{R} ^{m+1}\]</span></p><h3 id="scikit-learn-练习">Scikit-learn 练习</h3><p>在实际运用 SVM时，我们通常会调用现成的机器学习第三方库，比较经典的就有 Python 中的<strong>Scikit-learn</strong>，其调用<strong>支持向量分类器</strong>的函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sklearn.svm.SVC(C=<span class="hljs-number">1.0</span>, kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>, gamma=<span class="hljs-string">&#x27;auto&#x27;</span>, probability=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>该函数的实现基于 libsvm，其中二次规划问题的解决算法是SMO，通常需要调节的参数如下：</p><ul><li><code>C</code>：正则化参数，取值越大，对松弛变量（误分类的代价函数）的惩罚越大，适用于训练集几乎可分的情况，此时训练集上的<strong>准确率较高，但泛化能力弱</strong>（过拟合）。</li><li><code>kernel</code>：默认取 <code>rbf</code>径向基函数（高斯核），可选 <code>linear</code> 线性核、<code>poly</code>多项式核、<code>sigmod</code> 等。</li><li><code>gamma</code>：核函数系数，默认为auto，取样本特征数的倒数。</li><li><code>probablity</code>：是否启用概率估计，即最后的输出会包含属于每个类的概率值。</li></ul><hr /><p>下面以 <a href="https://www.coursera.org/">Coursera</a> 上的数据集<code>ex6data1.mat</code> 为例，这是一个线性可分的数据集：</p><p><img src="/img/blog/ML-Note-9-images/ex6data1.png" alt="数据集 ex6data1 可视化" width=50% /></p><p>使用如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data</span><br>data = loadmat(<span class="hljs-string">&#x27;ex6data1.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>]<span class="hljs-comment"># (51, 2)</span><br>y = data[<span class="hljs-string">&#x27;y&#x27;</span>].flatten()<span class="hljs-comment"># (51, )</span><br><br><span class="hljs-comment"># classifier define</span><br>clf = svm.SVC(C=<span class="hljs-number">1</span>, kernel=<span class="hljs-string">&#x27;linear&#x27;</span>)<br>clf.fit(X, y)<br><br><span class="hljs-comment"># dicision boundary</span><br>x1, x2 = np.meshgrid(np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">4.5</span>, <span class="hljs-number">100</span>), np.linspace(<span class="hljs-number">1.3</span>, <span class="hljs-number">4.5</span>, <span class="hljs-number">100</span>))<br>f = np.concatenate((x1.reshape(<span class="hljs-number">10000</span>, <span class="hljs-number">1</span>), x2.reshape(<span class="hljs-number">10000</span>, <span class="hljs-number">1</span>)), axis = <span class="hljs-number">1</span>)<br>res = clf.decision_function(f).reshape((<span class="hljs-number">100</span>,<span class="hljs-number">100</span>))<br><br><span class="hljs-comment"># draw</span><br>plt.xlabel(<span class="hljs-string">&#x27;x1&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;x2&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;C=100&#x27;</span>)<br>plt.plot(X[y==<span class="hljs-number">1</span>][:, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;+&#x27;</span>, color=<span class="hljs-string">&#x27;black&#x27;</span>)<br>plt.plot(X[y==<span class="hljs-number">0</span>][:, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;gold&#x27;</span>)<br>plt.contour(x1, x2, res, [<span class="hljs-number">0</span>])<br>plt.show()<br></code></pre></td></tr></table></figure><p>训练结果如下：</p><p><img src="/img/blog/ML-Note-9-images/ex6data1-train.png" alt="数据集 ex6data1 训练结果" width=90% /></p><p>此外还有一个非线性可分的数据集<code>ex6data2.mat</code>：</p><p><img src="/img/blog/ML-Note-9-images/ex6data2.png" alt="数据集 ex6data2 可视化" width=50% /></p><p>使用如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data</span><br>data = loadmat(<span class="hljs-string">&#x27;ex6data2.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>]<span class="hljs-comment"># (863, 2)</span><br>y = data[<span class="hljs-string">&#x27;y&#x27;</span>].flatten()<span class="hljs-comment"># (863, )</span><br><br><span class="hljs-comment"># classifier define</span><br>clf = svm.SVC(C=<span class="hljs-number">100</span>, kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>, gamma=<span class="hljs-number">10</span>, probability=<span class="hljs-literal">True</span>)<br>clf.fit(X, y)<br><span class="hljs-built_in">print</span>(clf.score(X, y))<br><br><span class="hljs-comment"># dicision boundary</span><br>x1, x2 = np.meshgrid(np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">100</span>), np.linspace(<span class="hljs-number">0.38</span>, <span class="hljs-number">1</span>, <span class="hljs-number">100</span>))<br>f = np.concatenate((x1.reshape(<span class="hljs-number">10000</span>, <span class="hljs-number">1</span>), x2.reshape(<span class="hljs-number">10000</span>, <span class="hljs-number">1</span>)), axis = <span class="hljs-number">1</span>)<br>res = clf.decision_function(f).reshape((<span class="hljs-number">100</span>,<span class="hljs-number">100</span>))<br><br><span class="hljs-comment"># draw</span><br>plt.xlabel(<span class="hljs-string">&#x27;x1&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;x2&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;C=100, gamma=10&#x27;</span>)<br>plt.plot(X[y==<span class="hljs-number">1</span>][:, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;+&#x27;</span>, color=<span class="hljs-string">&#x27;black&#x27;</span>)<br>plt.plot(X[y==<span class="hljs-number">0</span>][:, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;gold&#x27;</span>)<br>plt.contour(x1, x2, res, [<span class="hljs-number">0</span>])<br>plt.show()<br></code></pre></td></tr></table></figure><p>训练结果如下：</p><p><img src="/img/blog/ML-Note-9-images/ex6data2-train.png" alt="数据集 ex6data2 训练结果" width=50% /></p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #08 数据集划分与误差分析</title>
    <link href="/ML-Note-8.html"/>
    <url>/ML-Note-8.html</url>
    
    <content type="html"><![CDATA[<p>如何在实际情形中高效地使用机器学习模型？如何调参优化一个现有的模型？什么时候才需要获取更大的数据集？什么时候需要寻找更多的特征？什么时候需要改变正则化程度？</p><p>本节将介绍常用的<strong>机器学习诊断法</strong>（Machine LearingDiagnostic），高效地评估某种算法，并指导如何改进这个模型。</p><h2 id="数据集划分">数据集划分</h2><p>在以往的实验中，我们把所有数据集拿来训练一个模型，同时为了评估假设函数，我们还会用它进行测试。这显然不是一个好的做法，因为即便准确率很高，那也可能存在<strong>过拟合</strong>现象。</p><p>正确的做法应该是将数据集划分为训练集（TrainingSet）独立的<strong>测试集</strong>（Testing Set），通常划分比例为 <spanclass="math inline">\(7:3\)</span>。在训练集上收敛后，再到测试集进行测试，最终的准确率作为模型的准确率。这样做才能确保<strong>模型</strong>在不同数据上的<strong>泛化能力</strong>。</p><p>进一步，如果模型中含有超参数，例如正则化的参数 <spanclass="math inline">\(\lambda\)</span> 或神经网络的层数 <spanclass="math inline">\(L\)</span>，这是需要我们人工设置的。通常我们会设置多组参数对照，分别在训练集中训练到收敛，再在测试集中测试并选取最好的结果，也称<strong>交叉验证</strong>（CrossValidation）。于是又产生了同样的问题：测试集上的准确率不能代表模型的准确率。即：对测试集最优的<strong>超参数</strong>不一定对其他数据具有相同的<strong>泛化能力</strong>。</p><p>为此我们继续引入<strong>验证集</strong>（ValidationSet），即用验证集而非测试集去调参，最后在测试集上跑结果。通常划分比例为<spanclass="math inline">\(6:2:2\)</span>，测试集自始至终不参与模型的建立。这与常见的数据挖掘比赛的赛制异曲同工，只不过赛中测试集固定，只有验证集需要手动划分。</p><blockquote><p>值得一提的是，如果我们在训练过程中加入了正则项，那么在计算模型的代价函数（误差）的时候应该去掉正则项。这是因为加入正则项的目的是训练出一个更为合理的参数<span class="math inline">\(\theta\)</span>，而为了评价这个参数 <spanclass="math inline">\(\theta\)</span>的好坏，原本的代价函数才是真正的代价。</p><p>举个例子，当我们想知道正则化参数 <spanclass="math inline">\(\lambda\)</span> 取 <spanclass="math inline">\(0,1,2,\cdots, 9\)</span>中那个较好时，需要用训练集训练出 10个不同的模型，并在验证集上交叉验证，此时的代价函数就不应该包含正则化。</p></blockquote><h3 id="高偏差与高方差">高偏差与高方差</h3><p>在前文 <a href="https://hwcoder.top/ML-Note-5">ML学习笔记 #5过拟合与正则化</a>中，我们提到欠拟合（Under-Fitting）对应的模型是<strong>高偏差</strong>（highbias），而过拟合（Over-Fitting）对应的模型是<strong>高方差</strong>（highvariance）。</p><p>以多项式回归为例，随着多项式系数的增加，我们从欠拟合逐渐过渡到过拟合，训练集上的代价函数<span class="math inline">\(J_\text{train}(\theta)\)</span>逐渐减小，而验证集上的代价函数 <spanclass="math inline">\(J_\text{valid}(\theta)\)</span>先减小后增大，形成下图所示情况：</p><p><img src="/img/blog/ML-Note-8-images/cost_function.png" alt="训练集和验证集上的代价函数变化趋势" width = 50% /></p><p>观察发现：当 <span class="math inline">\(J_{\mathrm{valid}}(\theta)\approx J_{\mathrm{train}}(\theta )\)</span>时，模型处于欠拟合（高偏差）；当 <spanclass="math inline">\(J_{\mathrm{valid}}(\theta )\ggJ_{\mathrm{train}}(\theta )\)</span>时，模型处于过拟合（高方差）。实际中我们也可以通过计算误差来分析模型。</p><h3 id="学习曲线-learning-curve">学习曲线 | Learning Curve</h3><p>此外，作出学习曲线也有利于帮助我们分析模型拟合情况。学习曲线是误差函数关于「<strong>训练集规模</strong>」的曲线。</p><p>如果我们尝试用一条直线来拟合曲线数据，显然，此时模型欠拟合，<strong>无论训练集有多么大</strong>误差都不会有太大改观：</p><p><img src="/img/blog/ML-Note-8-images/high_bias.png" alt="高偏差的学习曲线" width = 70%  /></p><p>当训练集规模很小时，<spanclass="math inline">\(J_\text{train}(\theta)\)</span> 比较小，而 <spanclass="math inline">\(J_\text{valid}(\theta)\)</span>很大；随着训练集规模的增大，<spanclass="math inline">\(J_\text{train}(\theta)\)</span> 迅速增大，<spanclass="math inline">\(J_\text{valid}(\theta)\)</span>减小，但是减小的幅度不大；最后，当训练集规模很大时，二者<strong>基本相当且都比较大</strong>。</p><p>而如果我们使用一个非常高次的多项式模型，并且正则化非常小，显然，此时模型过拟合。可以看出，当交叉验证集误差远大于训练集误差时，往训练集<strong>增加更多数据可以提高模型的效果</strong>：</p><p><img src="/img/blog/ML-Note-8-images/high_variance.png" alt="高方差的学习曲线" width = 70% /></p><p>当训练集规模很小时，<spanclass="math inline">\(J_\text{train}(\theta)\)</span> 很小，而 <spanclass="math inline">\(J_\text{valid}(\theta)\)</span>很大；随着训练集规模的增大，<spanclass="math inline">\(J_\text{train}(\theta)\)</span>增大，但是增大的幅度不大，而 <spanclass="math inline">\(J_\text{valid}(\theta)\)</span>减小，但是减小的幅度也不大；最后，当训练集规模很大时， <spanclass="math inline">\(J_\text{train}(\theta)\)</span> 较小，但 <spanclass="math inline">\(J_\text{valid}(\theta)\)</span> 较大。</p><blockquote><p>学习曲线还可以用于判断一个模型的潜能。如果一个模型随着数据量增大，效果成指数形式上升，则说明增大数据量可能会达到更好的效果。但如果成对数形式上升，则说明不具有数据潜力。</p></blockquote><h3 id="神经网络的方差和偏差">神经网络的方差和偏差</h3><p>在文章的开头，我们提出了一个问题：什么时候需要寻找更多的特征？显然现在我们已经知道答案：当模型过拟合时，很可能是因为<strong>特征及参数过多</strong>导致模型<strong>将训练集完全拟合</strong>，因此可以尝试减少特征的数量；同理，当模型欠拟合时，可能是现有的<strong>特征太简单、不足以刻画训练集</strong>，因此最好尝试获得更多的特征。</p><p>此外，使用神经网络也可以免于复杂的特征工程，只需将原始特征作为输入层，通过网络自动适应即可。但是难道越复杂的神经网络就越好吗？神经网络也会出现过拟合。</p><p>对于一个较小的神经网络，可以类比为参数较少的情况，容易导致高偏差和欠拟合；而对于较大的神经网络，类比于参数较多的情况，容易导致高方差和过拟合。虽然后者计算代价比较大，但是可以通过<strong>正则化手段</strong>来减少其过拟合程度，并且通常会取得比前者更好的效果。</p><p>例如神经网络中的隐藏层的层数的选择、各层的宽度设置，通常从一层、较窄的网络开始，逐渐增加层数和宽度，并将数据集划分为训练集、验证集和测试集，针对不同规模的网络进行验证，选择验证集代价最小的模型作为最终模型。</p><h2 id="误差分析-error-evaluation">误差分析 | Error Evaluation</h2><p>除了通过观察学习曲线判断拟合情况，还可以人工检查<strong>验证集预测出错的样本</strong>（BadCases），分析这些错误样本是否存在某种系统化的趋势。例如，<strong>某个类别总是被预测错误、某些样本预测误差最大</strong>等，从而思考怎么改进分类器。通过加入不同特征、尝试不同模型、设计不同网络结构来针对性训练。</p><p>因此，快速开发出一个不完美但可用的模型是很有必要的，它可以指导你的下一步该怎么走，比起花费大量时间思考显然高效很多。当然，如果能有一个<strong>数值化的评价指标</strong>，那实验的进度也能大大加快。</p><h3 id="类偏斜的误差度量-skewed-classes">类偏斜的误差度量 | SkewedClasses</h3><p>过去我们常用预测的<strong>准确率</strong>（Accuracy）或<strong>错误率</strong>（ErrorRate）来评价一个分类模型，既适用于二分类任务，也适用于多分类任务。然而，Accuracy却无法满足所有的分类需求，例如以下情形：</p><ul><li>癌症患者识别任务，患者为正例，正常人为负例。显然患者的比例远小于正常人的比例，例如<spanclass="math inline">\(1\%\)</span>，如果我们将所有样例都识别为正常人，那么该模型的Accuracy 将高达 <spanclass="math inline">\(99\%\)</span>，但显然这不是我们想要的结果。</li><li>垃圾邮件识别任务，垃圾邮件为正例，正常邮件为负例。与患者识别类似，垃圾邮件的比例较低，而人们却更关心有多少垃圾邮件被识别了出来，而不是Accuracy。</li></ul><p>这些任务的特点是<strong>类别分布极不均衡</strong>，即存在<strong>类偏斜</strong>问题（SkewedClass），在多分类任务中也被称为<strong>长尾分布</strong>问题（LongTailed Distribution）。在 <ahref="https://hwcoder.top/IR-Note-5">IR学习笔记 #5 检索系统评价</a>曾提及<strong>混淆矩阵</strong>（ConfusionMatrix），下面我们探讨更一般化的情形：</p><table><thead><tr class="header"><th style="text-align: center;">/</th><th style="text-align: center;">实际为正例 <spanclass="math inline">\(P\)</span></th><th style="text-align: center;">实际为负例 <spanclass="math inline">\(N\)</span></th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><strong>预测为正例</strong> <spanclass="math inline">\(P\)</span></td><td style="text-align: center;"><spanclass="math inline">\(TP\)</span></td><td style="text-align: center;"><spanclass="math inline">\(FP\)</span></td></tr><tr class="even"><td style="text-align: center;"><strong>预测为负例</strong> <spanclass="math inline">\(N\)</span></td><td style="text-align: center;"><spanclass="math inline">\(FN\)</span></td><td style="text-align: center;"><spanclass="math inline">\(TN\)</span></td></tr></tbody></table><p>定义<strong>精确率</strong>（Precision）也称<strong>查准率</strong>、精度，从<strong>预测结果角度</strong>出发，所有预测为正例<span class="math inline">\(P\)</span> 的样本中，实际正例的占比： <spanclass="math display">\[Precision\triangleq \frac{TP}{TP+FP}\]</span>定义<strong>召回率</strong>（Recall）也称<strong>查全率</strong>，从<strong>实际结果角度</strong>出发，所有实际为正例<span class="math inline">\(P\)</span> 的样本中，被预测为正例的占比：<span class="math display">\[Recall\triangleq \frac{TP}{TP+FN}\]</span> 此外还可以得到与 Precision相对的<strong>误报率</strong>（Fallout），与 Recall相对的<strong>漏识率</strong>（Miss）。</p><h3 id="精确率和召回率的权衡">精确率和召回率的权衡</h3><p>Precision 和 Recall通常是一对矛盾、此消彼长的性能度量指标。一般来说，Precision越高时，Recall往往越低，反之亦然。还是以癌症患者识别任务为例，假设我们的算法会输出一个<span class="math inline">\([0,1]\)</span> 的概率值，默认以 <spanclass="math inline">\(0.5\)</span> 作为阈值。</p><p>如果将一个正常人诊断为癌症患者，则会使其承担不必要的治疗。因此我们可以在保持模型不变的情况下<strong>提高阈值</strong>，如<span class="math inline">\(0.7\)</span> 或 <spanclass="math inline">\(0.9\)</span>，进而<strong>提高精确率</strong>——即只在非常有把握的情况下诊断为癌症。然而，如果漏识了一个潜在的癌症患者，带来的灾难可能是更巨大的。因此我们也可以<strong>降低阈值</strong>，进而<strong>提高召回率</strong>——即让所有潜在病人都得到进一步地检查。</p><p>为此，我们定义了一个统一的指标来衡量模型的召回率与精确率，即： <spanclass="math display">\[\mathrm{F} \text {-score }=\left(1+\beta^2\right) \frac{\text {Precision } \cdot \text { Recall }}{\beta^2 \cdot \text { Precision}+\text { Recall }}\]</span> 其中 <span class="math inline">\(\beta\)</span>越大表示越强调精确率，反之则强调召回率。当 <spanclass="math inline">\(\beta=1\)</span> 时，得到我们最常用的 <spanclass="math inline">\(\mathrm{F1}\)</span> 值： <spanclass="math display">\[\mathrm{F1} \text {-score }=2 \cdot \frac{\text { Precision } \cdot\text { Recall }}{\text { Precision }+\text { Recall }}\]</span>对于<strong>一个模型</strong>来说，如果想要在精确率和召回率之间取得一个较好的平衡，最大化<span class="math inline">\(\mathrm{F1}\)</span>值是一个有效的方法。</p><p>如果我们有<strong>多个模型</strong>，如何利用精确率和召回率评估<strong>模型之间</strong>的优劣呢？通过固定模型后调整阈值大小，我们可以得到一系列精确率和召回率并绘制成一条曲线，而这条曲线就被称为<strong>PR-曲线</strong>（ Precision-Recall Curve）。</p><p><img src="/img/blog/ML-Note-8-images/PR-Curve.jpg" alt="PR-曲线" width = 50% /></p><p>在 PR Curve 的基础上，可以<strong>曲线下面积</strong>（Area Under theCurve, AUC）来得到一个模型整体的评估值，从上图中可以看出，高 AUC值也就意味着高精确率和高召回率，也就意味着模型的效果越好。</p><h3 id="多分类的精度和召回率">多分类的精度和召回率</h3><p>在 <a href="https://hwcoder.top/ML-Note-4">ML学习笔记 #4逻辑回归：二分类到多分类</a> 中，我们介绍到对于 <spanclass="math inline">\(N\)</span> 分类问题，可以将其转化为 <spanclass="math inline">\(N\)</span>个二分类问题，并将每个类别单独视为「正例」，其他类型视为「<strong>该类别的负例</strong>」。</p><p>此时的混淆矩阵可以这样统计：</p><table><thead><tr class="header"><th style="text-align: center;">/</th><th style="text-align: center;">实际为类 1</th><th style="text-align: center;">实际为类 2</th><th style="text-align: center;">实际为类 3</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><strong>预测为类 1</strong></td><td style="text-align: center;">43</td><td style="text-align: center;">5</td><td style="text-align: center;">2</td></tr><tr class="even"><td style="text-align: center;"><strong>预测为类 2</strong></td><td style="text-align: center;">2</td><td style="text-align: center;">45</td><td style="text-align: center;">3</td></tr><tr class="odd"><td style="text-align: center;"><strong>预测为类 3</strong></td><td style="text-align: center;">0</td><td style="text-align: center;">1</td><td style="text-align: center;">49</td></tr></tbody></table><p>此时类 1 的精确率 <spanclass="math inline">\(Precision=\frac{43}{43+5+2}=0.86\)</span>，召回率<spanclass="math inline">\(Recall=\frac{43}{43+2+0}=0.955556\)</span>。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #07 神经网络：反向传播</title>
    <link href="/ML-Note-7.html"/>
    <url>/ML-Note-7.html</url>
    
    <content type="html"><![CDATA[<p>上一节介绍了神经网络模型可以利用前向传播做预测，并引入了一系列符号来描述神经网络模型：</p><blockquote><ul><li><span class="math inline">\(x_i\)</span> 表示输入层的第 <spanclass="math inline">\(i\)</span> 个输入特征，其中 <spanclass="math inline">\(x_0\)</span> 偏置项省略；</li><li><span class="math inline">\(a^{(j)}_i\)</span> 表示第 <spanclass="math inline">\(j\)</span> 层的第 <spanclass="math inline">\(i\)</span> 个激活单元，其中 <spanclass="math inline">\(a_0^{(j)}\)</span> 偏置单元省略；</li><li><span class="math inline">\(s_j\)</span> 表示第 <spanclass="math inline">\(j\)</span>层的神经元数量（<strong>不包含偏置项</strong>），<spanclass="math inline">\(\hat a^{(j)}\in\mathbb R^{s_j+1}\)</span>表示<strong>加上偏置项</strong>后的 <spanclass="math inline">\(a^{(j)}\)</span>；</li><li><span class="math inline">\(\Theta^{(j)}\in\mathbbR^{s_{j+1}\times(s_j+1)}\)</span> 表示第 <spanclass="math inline">\(j\)</span> 层到第 <spanclass="math inline">\(j+1\)</span> 层的<strong>权重矩阵</strong>，<spanclass="math inline">\(\Theta\)</span>表示所有权重矩阵的<strong>集合</strong>。</li></ul></blockquote><p>其中的权重参数，与前面的回归问题类似，需要通过最小化代价函数来逼近。</p><h2 id="代价函数">代价函数</h2><p>为了描述代价函数，我们引入新的符号：</p><ul><li><span class="math inline">\(L\)</span>表示神经网络的<strong>层数</strong>，包含输入层、隐藏层、输出层，因此共有<span class="math inline">\(L-1\)</span> 次传播；</li><li><span class="math inline">\(K\geqslant 3\)</span>表示多分类问题的<strong>类别数</strong>，相应的有 <spanclass="math inline">\(s_L = K,\;\;h_{\Theta}(x)\in\mathbbR^{K}\)</span>；</li><li><span class="math inline">\(h_\Theta(x^{(i)})\in\mathbbR^K\)</span>，表示从第 <span class="math inline">\(i\)</span>个样本经过神经网络得到的<strong>输出</strong>结果，<spanclass="math inline">\(h_\Theta(x^{(i)})_k\)</span> 表示其第 <spanclass="math inline">\(k\)</span> 维。</li></ul><p>回顾正则化逻辑回归中的<strong>交叉熵代价函数</strong>： $$J()=-_{i=1}<sup>m{}+<em>{j=1}^n{</em>{j}</sup>{2}}\</p><p>$$</p><blockquote><p>在神经网络中，我们沿用交叉熵代价函数而不是均方误差，原因如下：</p><ol type="1"><li>我们使用了 <span class="math inline">\(\text{sigmoid}\)</span>函数作为激活函数引入非线性，其单增 <spanclass="math inline">\(S\)</span> 形曲线使其在 <spanclass="math inline">\(y=0\)</span> 和 <spanclass="math inline">\(1\)</span> 附近导数较小；</li><li>通常我们会<strong>随机初始化参数</strong>后进行梯度下降，如果使用<strong>均方误差代价函数</strong>，对<span class="math inline">\(\theta\)</span> 求偏导后含有 <spanclass="math inline">\(\text{sigmoid}\)</span>的<strong>导数项</strong>，会使前期的梯度下降十分缓慢；</li><li>如果使用交叉熵代价函数，则求偏导后不含 <spanclass="math inline">\(\text{sigmoid}\)</span>的导数项，前期收敛更快。</li></ol></blockquote><p>具体地，对于一个 <span class="math inline">\(K\)</span>分类的神经网络而言，可以看作是 <span class="math inline">\(K\)</span>个二分类的<strong>交叉熵代价函数之和</strong>： <spanclass="math display">\[J(\Theta )=-\frac{1}{m}\left[ \sum_{i=1}^m{\sum_{k=1}^K{y_{k}^{(i)}}}\ln\left( h_{\Theta}(x^{(i)})_k \right) +(1-y_{k}^{(i)})\ln \left(1-h_{\Theta}(x^{(i)})_k \right) \right]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}{\sum_{i=1}^{s_l}{\sum_{j=1}^{s_{l+1}}{\left(\Theta _{ji}^{(l)} \right) ^2}}}\]</span></p><p>注意这里有一个符号混用的地方：上标 <spanclass="math inline">\(^{(i)}\)</span> 在 <spanclass="math inline">\(x^{(i)}\)</span> 和 <spanclass="math inline">\(y^{(i)}\)</span> 中表示第 <spanclass="math inline">\(i\)</span> 个数据的输入（<spanclass="math inline">\(\in\mathbb R^{n+1}\)</span>）和输出（<spanclass="math inline">\(\in\mathbb R^{K}\)</span>），但是在 <spanclass="math inline">\(\Theta^{(l)}\)</span> 中表示第 <spanclass="math inline">\(l\)</span> 层的 <spanclass="math inline">\(\Theta\)</span>（<span class="math inline">\(\in\mathbb R^{s_{l+1}\times (s_l+1)}\)</span>）.</p><h2 id="反向传播算法-back-propagation">反向传播算法 | BackPropagation</h2><p>现在我们的目标就是最小化代价函数 <spanclass="math inline">\(J(\Theta)\)</span>，并找到使之最小的参数 <spanclass="math inline">\(\Theta\)</span>，我们仍使用梯度下降法解决最小化问题（尽管<span class="math inline">\(J(\Theta)\)</span>可能为非凸函数）。因此需要计算梯度 <spanclass="math inline">\(\begin{aligned} \frac{\partial J}{\partial\Theta_{ji}^{(l)}}\end{aligned}\)</span>，而计算的关键就是「<strong>反向传播</strong>」：从输出层开始逐层向前计算。</p><blockquote><p>为什么需要「反向传播」？我们知道梯度反映的是 <spanclass="math inline">\(J\)</span> 在 <spanclass="math inline">\(\Theta_{ji}^{(l)}\)</span>方向的变化率，即：当参数发生微小增量时，代价函数的增量。对于前几层网络，参数的增量需要经过多层<strong>全连接</strong>传播，才会影响到输出（代价函数）。</p><p>如果我们从输入层开始计算每个参数的梯度，则每个参数都要<strong>依赖</strong>其后续路径的传播，每个参数的计算都要先算一遍后续路径的梯度。而如果从输出层开始逐层向前计算并存储了梯度，则<strong>解耦</strong>了依赖关系，前一层可以利用后一层的计算结果。有点类似「动态规划」中「递归求解子问题」的思想。</p></blockquote><h3 id="数学推导问题转化">数学推导：问题转化</h3><p>为了推导方便，先假设只有一组数据 <spanclass="math inline">\((x,y)\)</span>，这样可以省去上标和求和的麻烦。并且令<span class="math inline">\(\lambda = 0\)</span>省略正则化项，等到最后的偏导结果再加上。</p><p>我们首先对第 <span class="math inline">\(l+1\)</span> 层的第 <spanclass="math inline">\(j\)</span>个神经元（<strong>偏置单元除外</strong>）定义一个 <spanclass="math inline">\(\text{delta}\)</span> 误差： <spanclass="math display">\[\delta_j^{(l+1)}=\frac{\partial J}{\partial z_j^{(l+1)}}\]</span> 其中 <spanclass="math inline">\(z_j^{(l+1)}=\sum\limits_{k=0}^{s_{l}}\Theta^{(l)}_{jk}a^{(l)}_{k}\)</span>，即<span class="math inline">\(a_j^{(l+1)}\)</span> 神经元取 <spanclass="math inline">\(\text{sigmoid}\)</span>激活前的输出值。换句话说，<spanclass="math inline">\(a^{(l+1)}_j=g\left(z_j^{(l+1)}\right)\)</span>。</p><p>于是根据<strong>链式求导法则</strong>，我们有： <spanclass="math display">\[\frac{\partial J}{\partial \Theta^{(l)}_{ji}}=\frac{\partial J}{\partialz_j^{(l+1)}}\cdot\frac{\partialz_j^{(l+1)}}{\partial\Theta_{ji}^{(l)}}=\delta_j^{(l+1)}a_i^{(l)}\]</span> 写作矩阵形式： <span class="math display">\[\boxed{\Delta ^{\left( l \right)}=\delta ^{\left( l+1 \right)}\cdot\left( a^{\left( l \right)} \right) ^T}\tag{1}\]</span> 显然，一旦求出当前层的 <spanclass="math inline">\(\text{delta}\)</span>误差，那么<strong>传入</strong>当前层的各参数的梯度也可求出。所以现在<strong>问题转化</strong>为求解<span class="math inline">\(\delta^{(l+1)}\)</span>。</p><h3 id="数学推导输出层误差">数学推导：输出层误差</h3><p>既然要递归求解子问题，首先就是要算输出层，即 <spanclass="math inline">\(\delta_j^{(L)}\)</span>： <spanclass="math display">\[\begin{aligned}    \delta _{j}^{\left( L \right)}&amp;=\frac{\partial J}{\partialz_{j}^{\left( L \right)}}=\frac{\partial J}{\partial a_{j}^{\left( L\right)}}\cdot \frac{\partial a_{j}^{\left( L \right)}}{\partialz_{j}^{\left( L \right)}}\\    &amp;=\frac{\partial J}{\partial a_{j}^{\left( L \right)}}\cdotg&#39;\left( z_{j}^{\left( L \right)} \right)\\    &amp;=-\left( \frac{y_j}{a_{j}^{\left( L\right)}}-\frac{1-y_j}{1-a_{j}^{\left( L \right)}} \right) \cdot \left(a_{j}^{\left( L \right)}\left( 1-a_{j}^{\left( L \right)} \right)\right)\\    &amp;=a_{j}^{\left( L \right)}-y_j\\\end{aligned}\]</span></p><p>写作矩阵形式： <span class="math display">\[\boxed{\delta^{(L)}=a^{(L)}-y}\tag{2}\]</span></p><blockquote><p>此处推导过程的一些注释：</p><p>关于第二行，请注意：$a^{( L )}=g( z^{( L )} ) $；</p><p>关于第三行，请注意 <spanclass="math inline">\(\text{sigmoid}\)</span> 函数的性质：<spanclass="math inline">\(g&#39;(z)=g(z)(1-g(z))\)</span>；</p><p>以及第三行偏导项的计算，请注意 <spanclass="math inline">\(a^{(L)}=h_\Theta(x)\)</span>，所以 <spanclass="math inline">\(J(\Theta)\)</span> 在现在的假设条件下可以写作：<span class="math display">\[J(\Theta)=-\left[\sum_{k=1}^Ky_k\ln(a^{(L)}_k)+(1-y_k)\ln(1-a_k^{(L)})\right]\]</span></p></blockquote><h3 id="数学推导隐藏层误差">数学推导：隐藏层误差</h3><p>下面计算第 <span class="math inline">\(l\)</span> 层（<spanclass="math inline">\(2\leqslant l&lt;L\)</span>）的 <spanclass="math inline">\(\delta_j^{(l)}\)</span>： <spanclass="math display">\[\begin{aligned}    \delta _{j}^{\left( l \right)}&amp;=\frac{\partial J}{\partialz_{j}^{\left( l \right)}}\\    &amp;=\frac{\partial J}{\partial z^{\left( l+1 \right)}}\cdot\frac{\partial z^{\left( l+1 \right)}}{\partial a_{j}^{\left( l\right)}}\cdot \frac{\partial a_{j}^{\left( l \right)}}{\partialz_{j}^{\left( l \right)}}\\    &amp;={\delta ^{\left( l+1 \right)}}^T\cdot \Theta _{\cdot,j}^{\left( l \right)}\cdot g&#39;\left( z_{j}^{\left( l \right)}\right)\\    &amp;={\delta ^{\left( l+1 \right)}}^T\cdot \Theta _{\cdot,j}^{\left( l \right)}\cdot \left( a_{j}^{\left( l \right)} \left(1-a_{j}^{\left( l \right)} \right) \right)\\\end{aligned}\]</span> 观察式子的前半部分，可以看出下一层所有结点的 <spanclass="math inline">\(\text{delta}\)</span>误差均会影响当前层的任一结点，而影响则是通过该结点<strong>链出</strong>的权重参数反向传播。</p><p>转置后，写作矩阵形式： <span class="math display">\[\boxed{\delta ^{\left( l \right)}=\left( {\Theta ^{\left( l\right)}}^T\delta ^{\left( l+1 \right)} \right) \odot \left( a^{\left( l\right)}\odot \left( 1-a^{\left( l \right)} \right) \right)}\tag{3}\]</span> 其中 <span class="math inline">\(\odot\)</span> 表示 Hadmard积，即两个向量对应位置相乘。</p><h3 id="步骤总结">步骤总结</h3><p>以上是对一组数据的推导，我们得到了三个重要的结果 <spanclass="math inline">\((1)(2)(3)\)</span>： <span class="math display">\[\boxed{\begin{aligned}    \Delta ^{\left( l \right)}&amp;=\delta ^{\left( l+1 \right)}\cdot\left( a^{\left( l \right)} \right) ^T&amp;&amp;1\leqslant l&lt;L\\    \delta ^{\left( L \right)}&amp;=a^{\left( L \right)}-y\\    \delta ^{\left( l \right)}&amp;=\left( {\Theta ^{\left( l\right)}}^T\delta ^{\left( l+1 \right)} \right) \odot \left( a^{\left( l\right)}\odot \left( 1-a^{\left( l \right)} \right) \right)&amp;&amp;2\leqslant l&lt;L\\\end{aligned}}\]</span> 而 <span class="math inline">\(m\)</span>组数据只需要在一些地方进行累加即可。设数据集为 <spanclass="math inline">\(\left\{ \left( x^{\left( i \right)},y^{\left( i\right)} \right) \mid 1\leqslant i\leqslant m\right\}\)</span>，则反向传播算法的步骤如下：</p><ol type="1"><li><p>所有 <span class="math inline">\(\Delta^{(l)}\)</span>置零；</p></li><li><p>遍历数据集，设当前数据为 <spanclass="math inline">\((x^{(i)},y^{(i)})\)</span>：</p><ol type="1"><li>以 <span class="math inline">\(x^{(i)}\)</span>为输入做前向传播，得到输出 <spanclass="math inline">\(a^{(L)}\)</span>；</li><li>公式 <span class="math inline">\((2)\)</span> 计算输出层误差，公式<span class="math inline">\((3)\)</span> 计算隐藏层误差；</li><li>公式 <span class="math inline">\((1)\)</span> 更新各层的 <spanclass="math inline">\(\Delta^{(l)}\)</span>矩阵，进行<strong>累加</strong>：<spanclass="math inline">\(\Delta^{(l)}:=\Delta^{(l)}+\delta ^{\left( l+1\right)}\cdot \left( a^{\left( l \right)} \right)^T\)</span>；</li></ol></li><li><p>计算 <span class="math inline">\(D\)</span>矩阵，对偏置项以外的参数进行<strong>正则化</strong>： <spanclass="math display">\[D_{ij}^{(l)}:=\begin{cases}\frac{1}{m}\left( \Delta _{ij}^{(l)}+\lambda \Theta _{ij}^{(l)}\right)&amp;        \mathrm{if}\; j\ne 0\\\frac{1}{m}\Delta _{ij}^{(l)}&amp;      \mathrm{if}\; j=0\\\end{cases}\]</span> 这就是最终的梯度矩阵：<spanclass="math inline">\(\begin{aligned}\frac{\partial J}{\partial\Theta_{ij}^{(l)}}=D^{(l)}_{ij}\end{aligned}\)</span>。</p></li></ol><p>现在，我们可以用 <span class="math inline">\(D_{ij}^{(l)}\)</span>做一次梯度下降了，整个步骤称为<strong>一代</strong>（<strong>epoch</strong>）。注意，我们的参数<span class="math inline">\(\Theta^{(l)}\)</span> 应该初始化为 <spanclass="math inline">\([-\epsilon,\epsilon]\)</span>中的<strong>随机值</strong>。</p><h2 id="代码实现">代码实现</h2><h3 id="参数展开-unrolling-parameters">参数展开 | UnrollingParameters</h3><p>实现过程中，由于每一层都有一个 <spanclass="math inline">\(\Theta^{(l)}\)</span> 和 <spanclass="math inline">\(D^{(l)}\)</span> 矩阵，总共有 <spanclass="math inline">\(2 \times L\)</span>个矩阵，为了方便存储和计算，我们最好都转换为向量，并将各层的向量拼接成一个长向量。这样做可以方便调用封装好的高级优化函数接口，如<code>fminunc</code> 或 <code>scipy.optimize.minimize</code>。</p><p>同样，函数输出的结果向量也可以通过 <code>reshape</code>转为矩阵，再应用于前向传播。</p><h3 id="梯度检验-gradient-checking">梯度检验 | Gradient Checking</h3><p>当我们为一个较为复杂的模型（例如神经网络）实现梯度下降算法时，可能会出现一些不易察觉的错误，导致虽然代价看上去在不断减小，但最终的结果可能并不是最优解，误差可能高出一个量级。</p><p>为了避免这样的问题，可采取<strong>梯度的数值检验</strong>方法（NumericalGradientChecking）：通过数值计算得到梯度的近似值，再和反向传播的结果比对，检验是否接近。</p><p>在任意一代计算后，得到一组 <spanclass="math inline">\(\theta\)</span> 向量及 <spanclass="math inline">\(D\)</span> 矩阵，通过<strong>差商</strong>近似<span class="math inline">\(J(\theta)\)</span> 的各偏导： <spanclass="math display">\[\frac{\partial}{\partial\theta_k}J(\theta)\approx\frac{J(\theta_1,\cdots,\theta_k+\epsilon,\cdots,\theta_n)-J(\theta_1,\cdots,\theta_k-\epsilon,\cdots,\theta_n)}{2\epsilon}\]</span></p><p>其中，<span class="math inline">\(\epsilon\)</span>是非常小的常数，为了避免计算误差，通常选取 <spanclass="math inline">\(10^{-4}\)</span>。现在我们可以比较这些偏导估计值与对应位置<span class="math inline">\(D_{ij}^{(l)}\)</span>的值，它们应该非常接近。</p><p>注意：梯度检验耗时巨大，<strong>复杂度远大于反向传播</strong>，一旦验证了神经网络反向传播的代码正确后，不应进行梯度检验（删掉or 注释掉）。</p><h3 id="参数随机初始化">参数随机初始化</h3><p>前文我们提到参数 <span class="math inline">\(\Theta^{(l)}\)</span>应该初始化为 <span class="math inline">\([-\epsilon,\epsilon]\)</span>中的<strong>随机值</strong>，而不是简单初始化为<strong>全零</strong>。如果将<span class="math inline">\(\Theta^{(l)}\)</span>初始化为全零，则下一层的 <span class="math inline">\(z^{(l+1)}\)</span>也为全零，则 <span class="math inline">\(a^{(l+1)}\)</span> 就全为 <spanclass="math inline">\(0.5\)</span>，且所有隐藏层都会得到这个结果；与此同时，反向传播公式<span class="math inline">\((3)\)</span> 中，由输出层到隐藏层时 <spanclass="math inline">\(\text{delta}\)</span>误差也会清零。这将导致网络无法传播！</p><p>那么，如果将参数 <span class="math inline">\(\Theta^{(l)}\)</span>全都初始化为同一个<strong>非零常量</strong>呢？由于全连接，同一层的<span class="math inline">\(a^{(l+1)}\)</span>将为相同值；在反向传播公式 <span class="math inline">\((3)\)</span>中，由输出层到隐藏层时 <span class="math inline">\(\text{delta}\)</span>误差也会变成相同值，则所有的 <spanclass="math inline">\(D^{(l)}\)</span> 矩阵也为常数矩阵，参数 <spanclass="math inline">\(\Theta^{(l)}\)</span>将再次被更新为新的相同常量。这样下去所有的激活单元都成了摆设，因为每一层都在重复计算着相同特征。</p><blockquote><p>上述问题也被称为<strong>对称权重</strong>问题（SymmetricWeights），解决方法就是随机初始化，实现时可以将 <spanclass="math inline">\([0,1]\)</span> 的随机数映射到 <spanclass="math inline">\([-\epsilon,\epsilon]\)</span>，这里的 <spanclass="math inline">\(\epsilon\)</span>与梯度检验中的<strong>无关</strong>。</p><p>实际应用中 <span class="math inline">\(\epsilon\)</span>的选取，通常与 <span class="math inline">\(\Theta^{(l)}\)</span>前后两层的<strong>神经元个数</strong>有关，一种常用的有效的取值是：<span class="math display">\[\epsilon ^{\left( l \right)}=\frac{\sqrt{6}}{\sqrt{s^{\left( l\right)}+s^{\left( l+1 \right)}}}\]</span></p></blockquote><h3 id="练习">练习</h3><p>下面以 <a href="https://www.coursera.org/">Coursera</a>上的多分类数据集 <code>ex4data1.mat</code>为例，这是一个手写数字的数据集，与上一节的数据集类似。共 <spanclass="math inline">\(5000\)</span> 组数据，每组数据输入是一个由 <spanclass="math inline">\(20\times20\)</span> 灰度矩阵压缩而来的 <spanclass="math inline">\(400\)</span> 维向量，输出是 <spanclass="math inline">\(0\)</span> 到 <spanclass="math inline">\(9\)</span> 之间的整数。</p><p>题目还提供一组训练好的权重参数<code>ex4weight.mat</code>，用以检验前向传播和代价函数的正确性，此处跳过。采用题目推荐的网络层数<span class="math inline">\(L=3\)</span>，则输入层有 <spanclass="math inline">\(401\)</span> 个单元（含偏置项），隐藏层有 <spanclass="math inline">\(26\)</span> 个单元（含偏置项） ，输出层有 <spanclass="math inline">\(10\)</span> 个单元（独热编码）。</p><p>先进行预处理，包括数据预处理、随机初始化参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> scipy.io <span class="hljs-keyword">as</span> scio<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report<br><br><span class="hljs-comment"># load data</span><br>data = scio.loadmat(<span class="hljs-string">&#x27;ex4data1.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>] <span class="hljs-comment"># (5000, 400), 这里不转置</span><br>y = data[<span class="hljs-string">&#x27;y&#x27;</span>].flatten()<span class="hljs-comment"># (5000, )</span><br>y[y==<span class="hljs-number">10</span>] = <span class="hljs-number">0</span><br><br><span class="hljs-comment"># parameter</span><br>(m, n) = (<span class="hljs-number">5000</span>, <span class="hljs-number">401</span>)<br>L = <span class="hljs-number">3</span>  <span class="hljs-comment"># layer of network</span><br>s = [<span class="hljs-number">0</span>, <span class="hljs-number">400</span>, <span class="hljs-number">25</span>, <span class="hljs-number">10</span>] <span class="hljs-comment"># size of each layer</span><br>lmd = <span class="hljs-number">1</span> <span class="hljs-comment"># for regularization</span><br>alpha = <span class="hljs-number">0.1</span><br>num_iters = <span class="hljs-number">5000</span><br><span class="hljs-comment">#J_history = []</span><br><br><span class="hljs-comment"># One-hot encoder</span><br>Y = np.zeros((m, s[L]))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>Y[i][y[i]] = <span class="hljs-number">1</span><br><br><span class="hljs-comment"># random init Theta, (25, 401) (10, 26)</span><br>Theta = [<span class="hljs-literal">None</span>] * L<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>Theta[l] = np.random.rand(s[l+<span class="hljs-number">1</span>], s[l]+<span class="hljs-number">1</span>)<br>eps = np.sqrt(<span class="hljs-number">6</span>) / np.sqrt(s[l+<span class="hljs-number">1</span>] + s[l])<br>Theta[l] = Theta[l] * <span class="hljs-number">2</span> * eps - eps<br></code></pre></td></tr></table></figure><p>再实现前向传播与代价函数计算，代价函数这里用于绘图、梯度检验：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">z</span>):</span><br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-z))<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forwardProp</span>():</span><br>a = [<span class="hljs-literal">None</span>] * (L+<span class="hljs-number">1</span>)<br><span class="hljs-comment"># input layer</span><br>a[<span class="hljs-number">1</span>] = np.c_[np.ones(m), X] <span class="hljs-comment"># (5000, 401)</span><br><span class="hljs-comment"># hidden layer</span><br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, L):<br>a[l] = sigmoid(a[l-<span class="hljs-number">1</span>] @ Theta[l-<span class="hljs-number">1</span>].T)<span class="hljs-comment"># (5000, 25)</span><br>a[l] = np.c_[np.ones(m), a[l]]<span class="hljs-comment"># (5000, 26)</span><br><span class="hljs-comment"># output layer</span><br>a[L] = sigmoid(a[L-<span class="hljs-number">1</span>] @ Theta[L-<span class="hljs-number">1</span>].T)<span class="hljs-comment"># (5000, 10)</span><br><span class="hljs-keyword">return</span> a<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">J</span>(<span class="hljs-params">a_L</span>):</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">a_L : output layer animation, (m, s[L])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>res = - (<span class="hljs-number">1</span> / m) * np.<span class="hljs-built_in">sum</span>(Y * np.log(a_L) + (<span class="hljs-number">1</span>-Y) * np.log(<span class="hljs-number">1</span>-a_L))<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>res += lmd / (<span class="hljs-number">2</span> * m) * np.<span class="hljs-built_in">sum</span>(np.power(Theta[l][:, <span class="hljs-number">1</span>:], <span class="hljs-number">2</span>))<br><span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure><p>实现反向传播算法，注意这里<strong>偏置单元</strong>的 <spanclass="math inline">\(\text{delta}\)</span>误差没有意义，算出的值要忽略：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backProp</span>():</span><br><span class="hljs-comment"># init Delta -&gt; 0, should be (s_&#123;l+1&#125;, s_l + 1)</span><br>Delta = [<span class="hljs-literal">None</span>] * L<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>Delta[l] = np.zeros((s[l+<span class="hljs-number">1</span>], s[l]+<span class="hljs-number">1</span>))<br><span class="hljs-comment"># init delta, should be (s_l, )</span><br>delta = [<span class="hljs-literal">None</span>] * (L+<span class="hljs-number">1</span>)<br><span class="hljs-comment"># step 1</span><br>a = forwardProp()<br><span class="hljs-comment">#J_history.append(J(a[L]))</span><br><span class="hljs-comment"># step 2</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>delta[L] = a[L][i, :] - Y[i, :]  <span class="hljs-comment"># 降维成 (s_L, )</span><br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(L-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):<br>delta[l] = ((Theta[l].T @ delta[l+<span class="hljs-number">1</span>]) * (a[l][i, :] * (<span class="hljs-number">1</span> - a[l][i, :])))[<span class="hljs-number">1</span>:]<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>Delta[l] += delta[l+<span class="hljs-number">1</span>][:,np.newaxis] @ a[l][i:i+<span class="hljs-number">1</span>, :]  <span class="hljs-comment"># 升维成 (s_l, 1)</span><br><span class="hljs-comment"># step 3</span><br>D = [<span class="hljs-literal">None</span>] * L  <span class="hljs-comment"># init D, same as Delta</span><br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>D[l] = (<span class="hljs-number">1</span> / m) * (Delta[l] + lmd * np.c_[np.zeros(s[l+<span class="hljs-number">1</span>]), Theta[l][:, <span class="hljs-number">1</span>:]])<br><span class="hljs-keyword">return</span> D<br></code></pre></td></tr></table></figure><p>实现梯度下降算法并检验结果，大约耗时 20 分钟：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br><span class="hljs-built_in">print</span>(i)<br>D = backProp()<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>Theta[l] -= alpha * D[l]<br><br><span class="hljs-comment"># Prediction，调库生成混淆矩阵</span><br>y_pred = np.argmax(forwardProp()[L], axis=<span class="hljs-number">1</span>)<span class="hljs-comment"># (5000, )</span><br><span class="hljs-built_in">print</span>(classification_report(y, y_pred, digits=<span class="hljs-number">3</span>))<br></code></pre></td></tr></table></figure><p>如果要进行梯度检验，只需要将下述函数插入到<code>D = backProp()</code> 后即可，正式训练时删去：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Gradient Check</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gradCheck</span>():</span><br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(s[l+<span class="hljs-number">1</span>]):<br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(s[l]+<span class="hljs-number">1</span>):<br>Theta[l][i, j] -= <span class="hljs-number">0.0001</span><br>J1 = J(forwardProp()[L])<br>Theta[l][i, j] += <span class="hljs-number">0.0002</span><br>J2 = J(forwardProp()[L])<br>Theta[l][i, j] -= <span class="hljs-number">0.0001</span><br><span class="hljs-built_in">print</span>(D[l][i, j], (J2 - J1) / <span class="hljs-number">0.0002</span>)<br></code></pre></td></tr></table></figure><p>5000 轮次后，最终的预测结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs apache">              <span class="hljs-attribute">precision</span>    recall  f<span class="hljs-number">1</span>-score   support<br><br>           <span class="hljs-attribute">0</span>      <span class="hljs-number">0</span>.<span class="hljs-number">950</span>     <span class="hljs-number">0</span>.<span class="hljs-number">984</span>     <span class="hljs-number">0</span>.<span class="hljs-number">967</span>       <span class="hljs-number">500</span><br>           <span class="hljs-attribute">1</span>      <span class="hljs-number">0</span>.<span class="hljs-number">946</span>     <span class="hljs-number">0</span>.<span class="hljs-number">978</span>     <span class="hljs-number">0</span>.<span class="hljs-number">962</span>       <span class="hljs-number">500</span><br>           <span class="hljs-attribute">2</span>      <span class="hljs-number">0</span>.<span class="hljs-number">944</span>     <span class="hljs-number">0</span>.<span class="hljs-number">914</span>     <span class="hljs-number">0</span>.<span class="hljs-number">929</span>       <span class="hljs-number">500</span><br>           <span class="hljs-attribute">3</span>      <span class="hljs-number">0</span>.<span class="hljs-number">921</span>     <span class="hljs-number">0</span>.<span class="hljs-number">914</span>     <span class="hljs-number">0</span>.<span class="hljs-number">918</span>       <span class="hljs-number">500</span><br>           <span class="hljs-attribute">4</span>      <span class="hljs-number">0</span>.<span class="hljs-number">948</span>     <span class="hljs-number">0</span>.<span class="hljs-number">940</span>     <span class="hljs-number">0</span>.<span class="hljs-number">944</span>       <span class="hljs-number">500</span><br>           <span class="hljs-attribute">5</span>      <span class="hljs-number">0</span>.<span class="hljs-number">919</span>     <span class="hljs-number">0</span>.<span class="hljs-number">904</span>     <span class="hljs-number">0</span>.<span class="hljs-number">911</span>       <span class="hljs-number">500</span><br>           <span class="hljs-attribute">6</span>      <span class="hljs-number">0</span>.<span class="hljs-number">949</span>     <span class="hljs-number">0</span>.<span class="hljs-number">964</span>     <span class="hljs-number">0</span>.<span class="hljs-number">956</span>       <span class="hljs-number">500</span><br>           <span class="hljs-attribute">7</span>      <span class="hljs-number">0</span>.<span class="hljs-number">950</span>     <span class="hljs-number">0</span>.<span class="hljs-number">942</span>     <span class="hljs-number">0</span>.<span class="hljs-number">946</span>       <span class="hljs-number">500</span><br>           <span class="hljs-attribute">8</span>      <span class="hljs-number">0</span>.<span class="hljs-number">935</span>     <span class="hljs-number">0</span>.<span class="hljs-number">924</span>     <span class="hljs-number">0</span>.<span class="hljs-number">930</span>       <span class="hljs-number">500</span><br>           <span class="hljs-attribute">9</span>      <span class="hljs-number">0</span>.<span class="hljs-number">926</span>     <span class="hljs-number">0</span>.<span class="hljs-number">924</span>     <span class="hljs-number">0</span>.<span class="hljs-number">925</span>       <span class="hljs-number">500</span><br><br>    <span class="hljs-attribute">accuracy</span>                          <span class="hljs-number">0</span>.<span class="hljs-number">939</span>      <span class="hljs-number">5000</span><br>   <span class="hljs-attribute">macro</span> avg      <span class="hljs-number">0</span>.<span class="hljs-number">939</span>     <span class="hljs-number">0</span>.<span class="hljs-number">939</span>     <span class="hljs-number">0</span>.<span class="hljs-number">939</span>      <span class="hljs-number">5000</span><br><span class="hljs-attribute">weighted</span> avg      <span class="hljs-number">0</span>.<span class="hljs-number">939</span>     <span class="hljs-number">0</span>.<span class="hljs-number">939</span>     <span class="hljs-number">0</span>.<span class="hljs-number">939</span>      <span class="hljs-number">5000</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #06 神经网络基础</title>
    <link href="/ML-Note-6.html"/>
    <url>/ML-Note-6.html</url>
    
    <content type="html"><![CDATA[<p>回顾之前学的模型，无论是线性回归还是逻辑回归都有一个缺点，即：当<strong>特征太多</strong>时，计算的负荷会非常大。而有时候，我们又希望用高次多项式来拟合更复杂的情形，此时特征数更是成倍增长。</p><p>在计算机视觉（ComputerVision）领域，数据的输入往往是一张张由像素（pixel）构成的图片。一张<span class="math inline">\(50\times50\)</span> 的图片中包含 <spanclass="math inline">\(2500\)</span> 个像素点，如果算上 RGB 色值则有<span class="math inline">\(7500\)</span>个特征，更别提包含平方、立方项特征的<strong>非线性假设</strong>了。而神经网络则是适合学习复杂的非线性假设的一类算法。</p><h2 id="神经网络-neural-network">神经网络 | Neural Network</h2><p>神经网络起源于科学家对人脑神经元的模拟，早期应用十分广泛，后来由于计算量过大而逐渐没落，直到近些年硬件的增强，大规模的神经网络才得以训练和应用。</p><h3 id="神经元模型-neuron-model">神经元模型 | Neuron Model</h3><p>神经网络模型建立在许多神经元之上，每个神经元也被称为<strong>激活单元</strong>（Activationunit），它采纳一些特征作为<strong>输入</strong>（input），并且根据本身的模型提供一个<strong>输出</strong>（output）。神经网络就是大量神经元相互连接形成的一个网络。</p><p><img src="/img/blog/ML-Note-6-images/Neuron-Model.png" alt="单个神经元模型" width=40% /></p><p>激活单元（图中黄色结点）就是一个<strong>函数</strong>，根据若干输入信息<span class="math inline">\(x=(x_0,x_1,\cdots,x_n)^T\)</span> 以及其权重<spanclass="math inline">\(\theta=(\theta_0,\theta_1,\cdots,\theta_n)^T\)</span>，得到一个输出信息<span class="math inline">\(h_\theta(x)\)</span>，即： <spanclass="math display">\[h_\theta(x)=f(\theta, x)\]</span> <span class="math inline">\(h_\theta(x)\)</span>也称作<strong>激活函数</strong>（Activation Function），一般可以采取<span class="math inline">\(\text{sigmoid}\)</span> 函数 <spanclass="math inline">\(h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}\)</span>.</p><blockquote><p>注：上图省略了 <span class="math inline">\(x_0\equiv 1\)</span>这一<strong>偏置项</strong>（Biasunit），偏置项不仅可以是<strong>输入层</strong>的人为特征，也可以是<strong>隐藏层</strong>的一个常量单元<span class="math inline">\(a_0 \equiv 1\)</span>。</p></blockquote><blockquote><p>为什么 <span class="math inline">\(\text{sigmoid}\)</span>函数可以作为激活函数？事实上激活函数有很多种，他们的共同特点都是引入了<strong>非线性假设</strong>。早期的神经网络使用<span class="math inline">\(\text{sigmoid}\)</span>的原因还有：输出可映射到 Bernoulli分布，可以作为概率解决分类问题；求导计算方便。</p></blockquote><h3 id="网络的表示-presentation">网络的表示 | Presentation</h3><p>神经网络是许多神经元按照不同层级组织起来的网络。第一层被称作<strong>输入层</strong>（InputLayer），最后一层被称作<strong>输出层</strong>（OutputLayer），中间其他层被称作<strong>隐藏层</strong>（HiddenLayer），意味着「不可见」。隐藏层和输出层具备激活函数功能，而输入层仅仅是特征的拷贝。</p><p><img src="/img/blog/ML-Note-6-images/Neural-Network.png" alt="多层神经元组成的神经网络" width=50% /></p><ul><li><span class="math inline">\(x_i\)</span> 表示输入层的第 <spanclass="math inline">\(i\)</span> 个输入特征，其中 <spanclass="math inline">\(x_0\)</span> 偏置项省略；</li><li><span class="math inline">\(a^{(j)}_i\)</span> 表示第 <spanclass="math inline">\(j\)</span> 层的第 <spanclass="math inline">\(i\)</span> 个激活单元，其中 <spanclass="math inline">\(a_0^{(j)}\)</span> 偏置单元省略；</li><li><span class="math inline">\(s_j\)</span> 表示第 <spanclass="math inline">\(j\)</span>层的神经元数量（<strong>不包含偏置项</strong>），<spanclass="math inline">\(\hat a^{(j)}\in\mathbb R^{s_j+1}\)</span>表示<strong>加上偏置项</strong>后的 <spanclass="math inline">\(a^{(j)}\)</span>；</li><li><span class="math inline">\(\Theta^{(j)}\in\mathbbR^{s_{j+1}\times(s_j+1)}\)</span> 表示第 <spanclass="math inline">\(j\)</span> 层到第 <spanclass="math inline">\(j+1\)</span> 层的<strong>权重矩阵</strong>，<spanclass="math inline">\(\Theta\)</span>表示所有权重矩阵的<strong>集合</strong>。</li></ul><p>所以，由上图我们可以列出： <span class="math display">\[\begin{aligned}a_{0}^{(2)}&amp;\equiv 1\\a_{1}^{(2)}&amp;=g\left(\Theta _{10}^{(1)}+\Theta _{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta _{13}^{(1)}x_3\right)\\a_{2}^{(2)}&amp;=g\left(\Theta _{20}^{(1)}+\Theta _{21}^{(1)}x_1+\Theta_{22}^{(1)}x_2+\Theta _{23}^{(1)}x_3\right)\\a_{3}^{(2)}&amp;=g\left(\Theta _{30}^{(1)}+\Theta _{31}^{(1)}x_1+\Theta_{32}^{(1)}x_2+\Theta _{33}^{(1)}x_3\right)\\h_{\Theta}(x)=a_{1}^{(3)}&amp;=g\left(\Theta _{10}^{(2)}+\Theta_{11}^{(2)}a_{1}^{(2)}+\Theta _{12}^{(2)}a_{2}^{(2)}+\Theta_{13}^{(2)}a_{3}^{(2)}\right)\end{aligned}\]</span> 简写作矩阵形式，可得到<strong>前向传播</strong>（ForwardPropagation）公式： <span class="math display">\[a^{(j+1)}=g\left(\Theta^{(j)}\hat a^{(j)}\right)\]</span></p><p>是不是像极了逻辑回归 <spanclass="math inline">\(h_\theta(x)=g\left(\theta^Tx\right)\)</span>，只不过特征<span class="math inline">\(x\)</span> 换成了上一层 <spanclass="math inline">\(\hat a^{(j)}\)</span>，假说 <spanclass="math inline">\(h_\theta(x)\)</span> 变成了当前层 <spanclass="math inline">\(a^{(j+1)}\)</span>。当然，由于每一层的关联性，最终的假说<span class="math inline">\(h_{\Theta}(x)\)</span> 依旧是特征 <spanclass="math inline">\(x\)</span> 的<strong>非线性组合</strong>。</p><p>而随着每一层的深入，特征会变得越来越「<strong>抽象</strong>」，这些新特征远比单纯<span class="math inline">\(x\)</span>的多项式来得强大，也能更好的预测数据。这就是神经网络相比于逻辑回归和线性回归的优势。</p><blockquote><p>有的地方会把偏置项对应的<strong>偏置向量</strong> <spanclass="math inline">\(b^{(j)}=\Theta_{i0}^{(j)}\in\mathbbR^{s_{j+1}}\)</span> 单独拿出来，使得 <spanclass="math inline">\(\Theta^{(j)}\in\mathbb R^{s_{j+1}\timess_j}\)</span>，以求形式的统一： <span class="math display">\[a^{(j+1)}=g\left(\Theta^{(j)}a^{(j)}+b^{(j)}\right)\]</span></p></blockquote><h3 id="实现逻辑门">实现逻辑门</h3><p>神经网络与逻辑门有何种联系？我们知道 <spanclass="math inline">\(\text{sigmoid}\)</span> 函数具有将数值映射到 <spanclass="math inline">\(0\)</span> 和 <spanclass="math inline">\(1\)</span>的能力，而逻辑门也是类似。那么就可以把逻辑门看作一个简化的<strong>二分类</strong>问题，用神经网络对其训练。通过这个例子能够直观地理解神经网络中参数的作用，首先来看最简单的<strong>与门</strong><span class="math inline">\(\text{AND}\)</span>：</p><p><img src="/img/blog/ML-Note-6-images/AND.png" alt="与门的实现" width=40% /></p><p>考虑一个输入层有 <span class="math inline">\(2\)</span> 个特征且取值<span class="math inline">\(x_1,x_2\in\{0,1\}\)</span>、输出层有 <spanclass="math inline">\(1\)</span> 个神经元且取值 <spanclass="math inline">\(h_\theta(x)\in\{0,1\}\)</span>的神经网络。如果我们将权重参数设置为：$^{(1)}=( -30,20,20 )$，那么我们有：</p><table><thead><tr class="header"><th style="text-align: center;">输入 1</th><th style="text-align: center;">输入 2</th><th style="text-align: center;">输出</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">0</td><td style="text-align: center;">0</td><td style="text-align: center;"><spanclass="math inline">\(g(-30)\approx 0\)</span></td></tr><tr class="even"><td style="text-align: center;">0</td><td style="text-align: center;">1</td><td style="text-align: center;"><spanclass="math inline">\(g(-10)\approx 0\)</span></td></tr><tr class="odd"><td style="text-align: center;">1</td><td style="text-align: center;">0</td><td style="text-align: center;"><spanclass="math inline">\(g(-10)\approx 0\)</span></td></tr><tr class="even"><td style="text-align: center;">1</td><td style="text-align: center;">1</td><td style="text-align: center;"><span class="math inline">\(g(10)\approx1\)</span></td></tr></tbody></table><p>这样实现了一个与门的功能，同理，或门 <spanclass="math inline">\(\text{OR}\)</span>、非门 <spanclass="math inline">\(\text{NOT}\)</span>也可以用两层网络（<strong>一个激活单元</strong>）实现。但是<strong>异或门</strong><span class="math inline">\(\text{XOR}\)</span>和<strong>同或门</strong> <spanclass="math inline">\(\text{XNOR}\)</span>则需要三层网络——由数理逻辑知，<spanclass="math inline">\(\text{XOR}\)</span> 可以表示成前三者的组合 $( x_1x_2 ) (x_1x_2 )$，那么就可以用<strong>三个激活单元</strong>实现异或。</p><p>同时，我们也可以发现，与前三种逻辑门不同，仅用一条直线是无法画出<span class="math inline">\(\text{XOR}\)</span> 决策边界的：</p><p><img src="/img/blog/ML-Note-6-images/boundary.png" alt="逻辑门的决策边界" width=60% /></p><p>这也意味着：需要更复杂的特征（更深层的网络）来表达更高级的模型。</p><h3 id="多分类问题">多分类问题</h3><p>在介绍 <a href="https://hwcoder.top/ML-Note-4">逻辑回归</a>时，我们曾说对于多分类问题，可以实现多个标准的逻辑回归分类器，每个分类器的输出作为「<strong>属于某类</strong>」的概率，取其最大值作为预测结果即可。</p><p>在神经网络中，输出层的<strong>每一个神经元</strong>也可以视为<strong>一个逻辑回归分类器</strong>，对于<span class="math inline">\(K\geqslant 3\)</span> 个类的问题（<spanclass="math inline">\(K=2\)</span> 时用一个神经元即可），最终可以得到<span class="math inline">\(h_{\Theta}(x)\in\mathbb R^{K}\)</span>的<strong>预测向量</strong>，取其最大值作为预测结果即可。</p><p><img src="/img/blog/ML-Note-6-images/multiclass.png" alt="K=4的多分类问题" width=60% /></p><p>同理，在训练时也需要把标签 <span class="math inline">\(y^{\left( i\right)}\)</span> 用<strong>独热</strong>（One-Hot）编码为向量 <spanclass="math inline">\(y^{\left( i \right)}=\left( 0,0,1,0 \right)^T\)</span> 的形式，仅有对应类的预测值为 <spanclass="math inline">\(1\)</span>，再通过反向传播从输出层开始计算。</p><h3 id="代码实现">代码实现</h3><p>下面以 <a href="https://www.coursera.org/">Coursera</a>上的多分类数据集 <code>ex3data1.mat</code>为例，这是一个手写数字的数据集。本节中忽略训练过程，使用题目提供的权重参数<code>ex3weight.mat</code> 作预测。</p><p>给定的数据为 <code>.mat</code> 格式，是 Matlab数据<strong>二进制存储</strong>的标准格式，在 Matlab 交互窗中输入<code>save xxx</code> 即可保存所有变量到 <code>xxx.mat</code>文件中。Python 中使用 <strong>SciPy</strong> 的 <code>loadmat</code>方法可以读入数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> scipy.io <span class="hljs-keyword">as</span> scio<br><span class="hljs-comment"># 导入 .mat 文件需要用到 scipy.io 模块，专门用于和 Matlab 交互</span><br>data = scio.loadmat(<span class="hljs-string">&#x27;ex3data1.mat&#x27;</span>)<br><span class="hljs-built_in">print</span>(data)<br><span class="hljs-built_in">print</span>(data[<span class="hljs-string">&#x27;X&#x27;</span>].shape)<br><span class="hljs-built_in">print</span>(data[<span class="hljs-string">&#x27;y&#x27;</span>].shape)<br></code></pre></td></tr></table></figure><p>读取的文件在 Python中以<strong>字典</strong>存储，将其打印出来为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">&#123;<span class="hljs-string">&#x27;__header__&#x27;</span>: <span class="hljs-string">b&#x27;MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011&#x27;</span>,<br> <span class="hljs-string">&#x27;__version__&#x27;</span>: <span class="hljs-string">&#x27;1.0&#x27;</span>,<br> <span class="hljs-string">&#x27;__globals__&#x27;</span>: [],<br> <span class="hljs-string">&#x27;X&#x27;</span>: array([[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, ..., <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, ..., <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, ..., <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>        ...,<br>        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, ..., <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, ..., <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, ..., <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]]),<br> <span class="hljs-string">&#x27;y&#x27;</span>: array([[<span class="hljs-number">10</span>],<br>        [<span class="hljs-number">10</span>],<br>        [<span class="hljs-number">10</span>],<br>        ...,<br>        [ <span class="hljs-number">9</span>],<br>        [ <span class="hljs-number">9</span>],<br>        [ <span class="hljs-number">9</span>]], dtype=uint8)&#125;<br>(<span class="hljs-number">5000</span>, <span class="hljs-number">400</span>)<br>(<span class="hljs-number">5000</span>, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>文件中一共有 <span class="math inline">\(5000\)</span>个样本，每个样本的输入是一个长为 <spanclass="math inline">\(400\)</span> 的向量，由 <spanclass="math inline">\(20\times 20\)</span>的<strong>灰度矩阵</strong>压缩而来；输出是一个数字，表示样本图像代表的数字。</p><blockquote><p>注意：为了更好地兼容 Octave/Matlab 索引（其中没有零索引），数字 <spanclass="math inline">\(0\)</span> 被标记为了 <spanclass="math inline">\(10\)</span>，使用时可以把 <spanclass="math inline">\(10\)</span> 换回成 <spanclass="math inline">\(0\)</span>，但题目给的 <code>ex3weight.mat</code>没有考虑转换；另外，数据是按列压缩的，还原回 <spanclass="math inline">\(20\times20\)</span>的矩阵后其实转置了一下，这里提前转置回去方便后续编码，但题目给的<code>ex3weight.mat</code> 也没有考虑。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">data = scio.loadmat(<span class="hljs-string">&#x27;ex3data1.mat&#x27;</span>)<br><span class="hljs-comment"># 获取字典键 &#x27;X&#x27;，对每行的 20x20 矩阵先转置处理，再恢复成向量</span><br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>]<br>X = np.transpose(X.reshape((<span class="hljs-number">5000</span>, <span class="hljs-number">20</span>, <span class="hljs-number">20</span>)), [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]).reshape(<span class="hljs-number">5000</span>, <span class="hljs-number">400</span>)<br><span class="hljs-comment"># 获取字典键 &#x27;y&#x27;，展开成一维，将标签 ‘10’ 转换为 ‘0’</span><br>y = data[<span class="hljs-string">&#x27;y&#x27;</span>].flatten()<br>y[y==<span class="hljs-number">10</span>] = <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>现在我们随机挑选 <span class="math inline">\(100\)</span>个图像显示出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_100_image</span>(<span class="hljs-params">X</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot; sample 100 image and show them</span><br><span class="hljs-string">    X : (5000, 400)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    sample_idx = np.random.choice(np.arange(X.shape[<span class="hljs-number">0</span>]), <span class="hljs-number">100</span>)<br>    sample_images = X[sample_idx, :]  <span class="hljs-comment"># 100x400</span><br><br>    fig, ax_array = plt.subplots(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, sharey=<span class="hljs-literal">True</span>, sharex=<span class="hljs-literal">True</span>, figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br><br>    <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>            ax_array[r, c].matshow(sample_images[<span class="hljs-number">10</span> * r + c].reshape((<span class="hljs-number">20</span>, <span class="hljs-number">20</span>)),<br>                                   cmap=matplotlib.cm.binary)<br>            plt.xticks(np.array([]))<br>            plt.yticks(np.array([]))<br><br>plot_100_image(X)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/blog/ML-Note-6-images/100.png" alt="随机挑选100张图像" width=40% /></p><p>下面搭建神经网络，利用已知的 <spanclass="math inline">\(\Theta\)</span> 实现前向传播预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> scipy.io <span class="hljs-keyword">as</span> scio<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report<br><br><span class="hljs-comment"># load data</span><br>data = scio.loadmat(<span class="hljs-string">&#x27;ex3data1.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>] <span class="hljs-comment"># (5000, 400)</span><br>y = data[<span class="hljs-string">&#x27;y&#x27;</span>].flatten()<span class="hljs-comment"># (5000, )</span><br>(m, n) = (<span class="hljs-number">5000</span>, <span class="hljs-number">401</span>)<br><br><span class="hljs-comment"># load weight</span><br>weight = scio.loadmat(<span class="hljs-string">&#x27;ex3weights.mat&#x27;</span>)<br>Theta1 = weight[<span class="hljs-string">&#x27;Theta1&#x27;</span>] <span class="hljs-comment"># (25, 401)</span><br>Theta2 = weight[<span class="hljs-string">&#x27;Theta2&#x27;</span>] <span class="hljs-comment"># (10, 26)</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">z</span>):</span><br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-z))<br><br><span class="hljs-comment"># Feed Forward Prediction</span><br>a1 = np.c_[np.ones(m), X].T  <span class="hljs-comment"># (401, 5000)</span><br>a2 = sigmoid(Theta1 @ a1) <span class="hljs-comment"># (25, 5000)</span><br>a2 = np.r_[np.ones((<span class="hljs-number">1</span>, m)), a2]<span class="hljs-comment"># (26, 5000)</span><br>a3 = sigmoid(Theta2 @ a2)<span class="hljs-comment"># (10, 5000)</span><br><br>y_pred = np.argmax(a3, axis=<span class="hljs-number">0</span>) + <span class="hljs-number">1</span><span class="hljs-comment"># (5000, )</span><br><br><span class="hljs-comment"># evaluation, 调库生成混淆矩阵</span><br><span class="hljs-built_in">print</span>(classification_report(y, y_pred, digits=<span class="hljs-number">3</span>))<br></code></pre></td></tr></table></figure><p>调用 SciKit-Learn 库中的 <code>metrics</code>，生成 <ahref="https://hwcoder.top/IR-Note-5">精度和召回率</a>，预测结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs text">              precision    recall  f1-score   support<br><br>           1      0.968     0.982     0.975       500<br>           2      0.982     0.970     0.976       500<br>           3      0.978     0.960     0.969       500<br>           4      0.970     0.968     0.969       500<br>           5      0.972     0.984     0.978       500<br>           6      0.978     0.986     0.982       500<br>           7      0.978     0.970     0.974       500<br>           8      0.978     0.982     0.980       500<br>           9      0.966     0.958     0.962       500<br>          10      0.982     0.992     0.987       500<br><br>    accuracy                          0.975      5000<br>   macro avg      0.975     0.975     0.975      5000<br>weighted avg      0.975     0.975     0.975      5000<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #05 过拟合与正则化</title>
    <link href="/ML-Note-5.html"/>
    <url>/ML-Note-5.html</url>
    
    <content type="html"><![CDATA[<p>在介绍多元线性回归时，我们知道线性回归并不适用于所有情形，因此可以人为构造<strong>高阶项</strong>并将其当作一个新的特征来使用，从而达到拟合非线性数据的目的，即<strong>多项式回归</strong>（PolynomialRegression）。</p><p>与此同时，由<strong>多项式插值</strong>的原理可知，我们可以构造一个唯一的<span class="math inline">\(m-1\)</span> 次多项式，使其完美地经过 <spanclass="math inline">\(m\)</span> 个样本点： <spanclass="math display">\[L_{n}(x)=\sum_{i=0}^{n} f\left(x_{i}\right) l_{i}(x)=\sum_{i=0}^{n}f\left(x_{i}\right) \prod_{j=0 \atop j \neq i}^{n}\frac{x-x_{j}}{x_{i}-x_{j}}\]</span> 上式即为 Lagrange 插值公式，<spanclass="math inline">\(f(x_i)\)</span>是每个样本的函数值。但这样得到的结果会是好的拟合吗？</p><h2 id="过拟合-over-fitting">过拟合 | Over-Fitting</h2><p>所谓过拟合，一般是指过度在训练集上进行优化，进而损害了测试集上的<strong>泛化能力</strong>（generalize）的现象，具有<strong>高方差</strong>（highvariance），对输入的变化更敏感。与之对应的概念是<strong>欠拟合</strong>（Under-Fitting），不能很好地适应训练集，使得算法具有<strong>高偏差</strong>（highbias）。</p><p>下图就是一个典型的回归案例：</p><p><img src="/img/blog/ML-Note-5-images/polynomial-case.png" alt="多项式回归" width=70% /></p><p>分类案例中也存在这样的问题：</p><p><img src="/img/blog/ML-Note-5-images/classification-case.png" alt="逻辑回归" width=70% /></p><p>解决过拟合的方法有很多，我将其分为三个层面：</p><ul><li>从模型层面，可以通过 Early Stop、L1/L2Regularization、Batchnorm、Dropout 等方法；</li><li>从特征层面，可以丢弃一些不能帮助我们正确预测的特征，通过手工筛选或PCA 等降维方法；</li><li>从数据层面，可以获取更大的数据集，也可以进行数据增强（DataAugmentation），通过一定规则来扩充数据。</li></ul><h2 id="正则化-regularization">正则化 | Regularization</h2><p>正则化是一种能够<strong>保留所有特征</strong>（不必降维而丢失信息）的有效解决过拟合的方法。其思想是在损失函数上加上某些规则（限制），限制参数的解空间，从而减少求出过拟合参数的可能性。</p><p>仍然以多项式回归为例，如果我们的假设函数为：<spanclass="math inline">\(h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4\)</span>，其中的高阶项<span class="math inline">\(\theta_3,\theta_4\)</span>导致了过拟合问题，那么我们自然希望 <spanclass="math inline">\(\theta_3\)</span> 和 <spanclass="math inline">\(\theta_4\)</span>越小越好。此时，我们只需要对代价函数做出一点修改： <spanclass="math display">\[J(\theta)=\frac{1}{2m}\left[\sum\limits_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2+1000\theta_3^2+10000\theta_4^2\right]\]</span> 这样当 <span class="math inline">\(J(\theta)\)</span>取得极值时，<span class="math inline">\(\theta_3\)</span> 和 <spanclass="math inline">\(\theta_4\)</span> 都接近于 <spanclass="math inline">\(0\)</span>，我们也就达到了目的。一般地，我们并不知道究竟应该对哪些参数做出「<strong>惩罚</strong>」，所以我们设代价函数为：<span class="math display">\[J(\theta)=\frac{1}{m}\sum_{i=1}^m\text{Cost}\left(h_\theta(x^{(i)})-y^{(i)}\right)+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2\]</span> 其中，<span class="math inline">\(\lambda\)</span>是<strong>正则化参数</strong>，<spanclass="math inline">\(\lambda\sum\limits_{j=1}^n\theta_j^2\)</span>是<strong>正则化项</strong>。即我们对<strong>除了 <spanclass="math inline">\(\theta_0\)</span>以外</strong>的参数都做「惩罚」，使得曲线更加光滑。如果 <spanclass="math inline">\(\lambda\)</span>很大，意味着正则化项占主要地位，有可能导致所有的 <spanclass="math inline">\(\theta_j\)</span> 都太小了而欠拟合；如果 <spanclass="math inline">\(\lambda\)</span>很小，意味着损失函数占主要地位，就有可能过拟合。因此选取合适的正则化参数<span class="math inline">\(\lambda\)</span>是非常重要的，通常要建立<strong>验证集</strong>进行网格搜索。</p><blockquote><p>为什么不对 <span class="math inline">\(\theta_0\)</span> 正则化？因为<span class="math inline">\(\theta_0\)</span>是人为设置的偏置项，对于<strong>输入的变化</strong>（训练集 or测试集）是不敏感的，不对模型的<strong>方差</strong>产生贡献，即使发生了过拟合，那也与<span class="math inline">\(\theta_0\)</span> 无关。</p></blockquote><blockquote><p>此外，根据正则项的形式，又可分为：二次正则项、一般正则项。<strong>二次正则项</strong>即为前文提到的形式，更一般的形式为<span class="math inline">\(\lambda \sum_{j=1}^n{\left| \theta _j\right|^q}\)</span>，当 <span class="math inline">\(q\)</span>取不同值<strong>等高线图</strong>的形状为：</p><p><img src="/img/blog/ML-Note-5-images/Dq.png" alt="正则项的边缘直观表示" width=70% /></p><p>从几何空间上来看，损失函数的碗状曲面和正则化项的曲面叠加之后，就是我们要求极值的曲面。特别地，当<span class="math inline">\(q=1\)</span> 时，称其为 <strong>L1</strong>正则化，也叫 <strong>Lasso 回归</strong>；当 <spanclass="math inline">\(q=2\)</span> 时，称其为 <strong>L2</strong>正则化，也叫<strong>岭回归</strong>。L2由于其处处可微的特性，在实际中更常用。</p></blockquote><h3 id="线性回归的正则化">线性回归的正则化</h3><p>线性回归的含有正则化项的代价函数为： <span class="math display">\[\begin{aligned}    J(\theta )&amp;=\frac{1}{2m}\left[ \sum_{i=1}^m{\left(h_{\theta}(x^{(i)})-y^{(i)} \right) ^2}+\lambda \sum_{j=1}^n{\theta_{j}^{2}} \right]\\    &amp;=\frac{1}{2m}\left[ \sum_{i=1}^m{\left( \theta^Tx^{(i)}-y^{(i)} \right) ^2}+\lambda \sum_{j=1}^n{\theta _{j}^{2}}\right]\\\end{aligned}\]</span> 对其求导： <span class="math display">\[\frac{\partial J}{\partial\theta_j}=\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}_j+[j\neq0]\frac{\lambda}{m}\theta_j,\quad j=0,1,\cdots,n\]</span> 所以梯度下降时，整个迭代更新过程为： <spanclass="math display">\[\begin{aligned}    \theta _0:&amp;=\theta _0-\alpha \frac{\partial J}{\partial \theta_j}\\    &amp;=\theta _0-\alpha \frac{1}{m}\sum_{i=1}^m{\left( \theta^Tx^{(i)}-y^{(i)} \right)}x_{0}^{(i)}\\    \theta _j:&amp;=\theta _j-\alpha \frac{\partial J}{\partial \theta_j}\\    &amp;=\theta _j-\alpha \left[ \frac{1}{m}\sum_{i=1}^m{\left( \theta^Tx^{(i)}-y^{(i)} \right)}x_{j}^{(i)}+\frac{\lambda}{m}\theta _j\right]\\    &amp;=\theta _j\left( 1-\alpha \frac{\lambda}{m} \right) -\alpha\frac{1}{m}\sum_{i=1}^m{\left( \theta ^Tx^{(i)}-y^{(i)}\right)}x_{j}^{(i)},\quad j=1,\cdots ,n\\\end{aligned}\]</span> 相当于每次迭代先将参数 <spanclass="math inline">\(\theta_j\)</span>缩小一点，再做原来的梯度下降。除了梯度下降，我们还可以直接用正规方程，即在数学上解它。为了记号的方便，我们先<strong>假定</strong>对<span class="math inline">\(\theta_0\)</span> 也进行「惩罚」。首先将<span class="math inline">\(J(\theta)\)</span>写作<strong>矩阵形式</strong>： <span class="math display">\[\begin{aligned}    J(\theta )&amp;=\frac{1}{2m}\left[ \left( X\theta -y \right)^T\left( X\theta -y \right) +\lambda \theta ^T\theta \right]\\    &amp;=\frac{1}{2m}\left[ \theta ^TX^TX\theta -\theta^TX^Ty-y^TX\theta +y^Ty+\lambda \theta ^T\theta \right]\end{aligned}\]</span> 再用 <ahref="https://hwcoder.top/ML-Note-3">矩阵的求导法则</a>，然后令 <spanclass="math display">\[\frac{\partial J}{\partial\theta}=\frac{1}{m}\left[X^TX\theta-X^Ty+\lambda\theta\right]=0\]</span> 则： <span class="math display">\[(X^TX+\lambda)\theta=X^Ty\]</span> 解得： <span class="math display">\[\theta=(X^TX+\lambda)^{-1}X^Ty\]</span> 现在把 <span class="math inline">\(j=0\)</span>的特殊情况考虑进去，那么最后的结果就是： <span class="math display">\[\theta =\left( X^TX+\lambda \left[ \begin{matrix}    0       &amp;       &amp;       &amp;       \\    &amp;       1       &amp;       &amp;       \\    &amp;       &amp;       \ddots  &amp;       \\    &amp;       &amp;       &amp;       1       \\\end{matrix} \right] _{n+1} \right) ^{-1}X^Ty\]</span></p><p>之前我们曾讨论过 <span class="math inline">\(X^TX\)</span>不可逆的情形，但在加入正则化项后，只要 <spanclass="math inline">\(\lambda&gt;0\)</span>，就<strong>一定可逆</strong>。</p><h3 id="逻辑回归的正则化">逻辑回归的正则化</h3><p>逻辑回归的含有正则化项的代价函数为： <span class="math display">\[\begin{aligned}    J(\theta )&amp;=-\frac{1}{m}\sum_{i=1}^m{\left[ y^{\left( i\right)}\ln \left( h_{\theta}\left( x^{\left( i \right)} \right) \right)+\left( 1-y^{\left( i \right)} \right) \ln \left( 1-h_{\theta}\left(x^{\left( i \right)} \right) \right)\right]}+\frac{\lambda}{2m}\sum_{j=1}^n{\theta _{j}^{2}}\\    &amp;=\frac{1}{m}\sum_{i=1}^m{\left[ y^{(i)}\ln \left( 1+e^{-\theta^Tx^{(i)}} \right) +\left( 1-y^{(i)} \right) \ln \left( 1+e^{\theta^Tx^{(i)}} \right) \right]}+\frac{\lambda}{2m}\sum_{j=1}^n{\theta_{j}^{2}}\\\end{aligned}\]</span> 同样地，对其求导： <span class="math display">\[\frac{\partial J}{\partial\theta_j}=\frac{1}{m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}_j+[j\neq0]\frac{\lambda}{m}\theta_j,\quad j=0,1,\cdots,n\]</span> 同样地，梯度下降时，整个迭代更新过程为： <spanclass="math display">\[\begin{aligned}    \theta _0&amp;:=\theta _0-\alpha \frac{1}{m}\sum_{i=1}^m{\left(h_{\theta}(x^{(i)})-y^{(i)} \right)}x_{0}^{(i)}\\    \theta _j&amp;:=\theta _j\left( 1-\alpha \frac{\lambda}{m} \right)-\alpha \frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}(x^{(i)})-y^{(i)}\right)}x_{j}^{(i)},\quad j=1,\cdots ,n\\\end{aligned}\]</span></p><h3 id="代码实现">代码实现</h3><p>实现线性回归的正规方程解法比较简洁，构造一个 <spanclass="math inline">\(n+1\)</span>阶的类单位矩阵即可。但实现梯度下降解法则会遇到一个问题：<spanclass="math inline">\(\theta_0\)</span> 的更新与 <spanclass="math inline">\(\theta_j\)</span> 不同步，需要分开计算。</p><p>下面以 <a href="https://www.coursera.org/">Coursera</a>上的二分类数据集 <code>ex2data2.txt</code>为例，首先看一下数据的分布，代码同上一篇：</p><figure><img src="/img/blog/ML-Note-5-images/preview_scatter.png"alt="数据分布散点图" /><figcaption aria-hidden="true">数据分布散点图</figcaption></figure><p>显然，需要引入<strong>多项式特征</strong>实现高次的<strong>曲线</strong>，这里将数据的两维都扩充为<span class="math inline">\(6\)</span> 次，形成有 <spanclass="math inline">\(28\)</span>维特征的数据，沿用上一篇的矩阵运算，实现逻辑回归如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (118, 3)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex2data2.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>x1 = data[:, <span class="hljs-number">0</span>].reshape((m, <span class="hljs-number">1</span>))<br>x2 = data[:, <span class="hljs-number">1</span>].reshape((m, <span class="hljs-number">1</span>))<br>y = data[:, -<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># map polynomial feature, X.shape = (118, 28)</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">map_feature</span>(<span class="hljs-params">x1, x2</span>):</span><br>X = np.ones(x1.size)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">6</span> + <span class="hljs-number">1</span>):<br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, i + <span class="hljs-number">1</span>):<br>X = np.c_[X, np.power(x1, i - j) * np.power(x2, j)]<br><span class="hljs-keyword">return</span> X<br><br>X = map_feature(x1, x2)<br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>lmd = <span class="hljs-number">1</span><br>num_iters = <span class="hljs-number">100000</span><br>theta = np.zeros(X.shape[<span class="hljs-number">1</span>])<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">z</span>):</span><br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-z))<br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>error = sigmoid(X @ theta) - y  <span class="hljs-comment"># error.shape = (118, )</span><br>theta[<span class="hljs-number">0</span>] -= (alpha / m) * (X[:,<span class="hljs-number">0</span>].T @ error)  <span class="hljs-comment"># X.T.shape = (28, 118)</span><br>theta[<span class="hljs-number">1</span>:] = (<span class="hljs-number">1</span> - alpha * lmd / m) * theta[<span class="hljs-number">1</span>:] - (alpha / m) * (X[:,<span class="hljs-number">1</span>:].T @ error)<br><br><span class="hljs-comment"># plot scatter</span><br>pos = np.where(y == <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]<br>neg = np.where(y == <span class="hljs-number">0</span>)[<span class="hljs-number">0</span>]<br>plt.scatter(X[pos, <span class="hljs-number">1</span>], X[pos, <span class="hljs-number">2</span>], marker=<span class="hljs-string">&quot;o&quot;</span>, c=<span class="hljs-string">&#x27;xkcd:sky blue&#x27;</span>)<br>plt.scatter(X[neg, <span class="hljs-number">1</span>], X[neg, <span class="hljs-number">2</span>], marker=<span class="hljs-string">&quot;x&quot;</span>, c=<span class="hljs-string">&#x27;xkcd:pink&#x27;</span>)<br><br><span class="hljs-comment"># plot decision boundary</span><br>u = np.linspace(-<span class="hljs-number">1</span>, <span class="hljs-number">1.25</span>, <span class="hljs-number">50</span>)<br>v = np.linspace(-<span class="hljs-number">1</span>, <span class="hljs-number">1.25</span>, <span class="hljs-number">50</span>)<br>U, V = np.meshgrid(u, v)  <span class="hljs-comment"># 生成 50x50 网格点矩阵</span><br>z = (map_feature(U.flatten(), V.flatten()) @ theta).reshape((<span class="hljs-number">50</span>, <span class="hljs-number">50</span>))  <span class="hljs-comment"># 计算等高线</span><br>plt.contour(u, v, z, [<span class="hljs-number">0</span>], colors=<span class="hljs-string">&#x27;r&#x27;</span>)  <span class="hljs-comment"># 绘制轮廓图，取高度为 0 的点</span><br><br>plt.title(<span class="hljs-string">f&quot;lambda = <span class="hljs-subst">&#123;lmd&#125;</span>&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Microchip Test 1&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Microchip Test 2&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>分别绘制出 <span class="math inline">\(\lambda=0,1,5\)</span>时的决策边界图如下：</p><figure><img src="/img/blog/ML-Note-5-images/lambda0.png"alt="lambda=0的决策边界" /><figcaption aria-hidden="true">lambda=0的决策边界</figcaption></figure><figure><img src="/img/blog/ML-Note-5-images/lambda1.png"alt="lambda=1的决策边界" /><figcaption aria-hidden="true">lambda=1的决策边界</figcaption></figure><figure><img src="/img/blog/ML-Note-5-images/lambda5.png"alt="lambda=5的决策边界" /><figcaption aria-hidden="true">lambda=5的决策边界</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #04 逻辑回归：二分类到多分类</title>
    <link href="/ML-Note-4.html"/>
    <url>/ML-Note-4.html</url>
    
    <content type="html"><![CDATA[<p>前文介绍了基本的线性回归问题，尝试预测一系列<strong>连续值</strong>属性。现在我们将介绍的分类问题则关注<strong>离散值</strong>属性的预测。</p><p>在分类问题中，我们尝试预测的结果是否属于某一个类，最基础的就是<strong>二元</strong>的分类问题（例如预测垃圾邮件、恶性肿瘤），更为复杂的则是预测<strong>多元</strong>的分类问题。</p><h2 id="二分类问题">二分类问题</h2><p>分类问题的样本与回归问题类似，由特征和目标构成，给定数据集： <spanclass="math display">\[\left\{\left(x^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\]</span> - <span class="math inline">\(x^{(i)}\)</span> 代表第 <spanclass="math inline">\(i\)</span> 个观察实例的 <spanclass="math inline">\(n+1\)</span> 维<strong>特征向量</strong> <spanclass="math inline">\(\left(x_0^{(i)},\cdots,x_n^{(i)}\right)^T\)</span>；- <span class="math inline">\(y^{(i)}\in\{0,1\}\)</span> 代表第 <spanclass="math inline">\(i\)</span>个观察实例的<strong>目标变量</strong>，在这里有 <spanclass="math inline">\(0\)</span> 或 <spanclass="math inline">\(1\)</span> 两类结果。</p><p>即对于输入的自变量 <spanclass="math inline">\(x^{(i)}\)</span>，因变量 <spanclass="math inline">\(y^{(i)}\)</span> 可能为 <spanclass="math inline">\(0\)</span> 或 <spanclass="math inline">\(1\)</span>。其中 <spanclass="math inline">\(0\)</span> 表示<strong>负向类</strong>（negativeclass），<span class="math inline">\(1\)</span>表示<strong>正向类</strong>（positive class）。</p><blockquote><p>我们不对「正向」和「负向」加以特殊区分，但在实际应用中「正向」通常表示「具有我们要寻找的东西」，如垃圾邮件、恶性肿瘤等。</p></blockquote><h3 id="线性回归的不足">线性回归的不足</h3><p>首先可能会自然而然地想到用之前的线性回归来解决——用一条直线拟合结果，当预测值大于<span class="math inline">\(0.5\)</span>时归为正向类，反之归为负向类。</p><p>这看似合理，然而，线性回归保留了 <spanclass="math inline">\(y^{(i)}\)</span>太多的「<strong>信息量</strong>」。对于某些「<strong>反常样本</strong>」，我们可能预测出一个远大于<span class="math inline">\(1\)</span> 或者远小于 <spanclass="math inline">\(0\)</span>的结果，同理，这些「反常样本」用于拟合直线时也会对其造成一定偏移，以至于正常样本被归为错误类别。</p><p><img src="/img/blog/ML-Note-4-images/linear-regression.png" alt="反常样本使得蓝线偏移" width=67% /></p><h2 id="逻辑回归-logistic-regression">逻辑回归 | LogisticRegression</h2><p>线性回归和逻辑回归都属于<strong>广义线性模型</strong>（GeneralizedLinearModel）的特殊形式，线性模型都可用于回归问题的求解。但由于<strong>逻辑函数</strong>（LosisticFunction）将结果映射到 <strong>Bernoulli分布</strong>，因此逻辑回归更常用于分类问题。</p><h3 id="假说表示-hypothesis-representation">假说表示 | HypothesisRepresentation</h3><p>回忆线性回归的假设函数：<spanclass="math inline">\(h_\theta(x)=\theta^Tx\)</span>，我们在其外套上<span class="math inline">\(\text{sigmoid}\)</span>函数，构造逻辑回归的假设函数为： <span class="math display">\[h_\theta(x)=g\left(\theta^Tx\right)=\frac{1}{1+e^{-\theta^T x}}\]</span></p><blockquote><p>所谓 <span class="math inline">\(\text{sigmoid}\)</span>函数（也即前面提到的<strong>逻辑函数</strong>）： <spanclass="math display">\[g(z)=\frac{1}{1+e^{-z}}\]</span><img src="/img/blog/ML-Note-4-images/sigmoid.png" alt="sigmoid函数" width=40% /></p><p>是一个介于 <span class="math inline">\((0,1)\)</span> 之间的单增<span class="math inline">\(S\)</span> 形函数，其导出需要用到 GLMs和指数分布族（The Exponential Family）的知识。</p></blockquote><p>也就是说，对于一个参数为 <span class="math inline">\(\theta\)</span>的逻辑回归模型，输入 <span class="math inline">\(x\)</span>，得到 <spanclass="math inline">\(h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}\)</span>的预测值。</p><p>我们可以把这个输出值视为 <span class="math inline">\(x\)</span>这个样本对应的 <span class="math inline">\(y\)</span> 等于 <spanclass="math inline">\(1\)</span> 的<strong>概率</strong>（estimatedprobablity），即 <span class="math inline">\(h_\theta \left( x\right)=P\left( y=1|x;\theta\right)\)</span>。针对分类情形，我们可以认为如果概率 <spanclass="math inline">\(\geqslant 0.5\)</span>，则分类为 <spanclass="math inline">\(1\)</span>，否则分类为 <spanclass="math inline">\(0\)</span>。</p><h3 id="决策边界-decision-boundary">决策边界 | Decision Boundary</h3><p>又根据 <span class="math inline">\(\text{sigmoid}\)</span>函数的性质: <span class="math display">\[h_\theta(x)\geqslant 0.5\iff \theta^Tx\geqslant0\]</span> 所以只要 <spanclass="math inline">\(\theta^Tx\geqslant0\)</span>，就会分类为 <spanclass="math inline">\(1\)</span>，否则分类为 <spanclass="math inline">\(0\)</span>；于是乎，<spanclass="math inline">\(\theta^Tx=0\)</span>解出的这条「<strong>线</strong>」（对于高维情形为<strong>超平面</strong>）被称作决策边界，它将整个空间划分成两块区域（region），各自属于一个分类。</p><p>下面看两个二维情形的例子：</p><p><img src="/img/blog/ML-Note-4-images/decision-boundary-1.png" alt="线性的决策边界" width=30%/></p><p>对于上述样本点的分布，用一条直线即可划分空间，对应的假设函数为 <spanclass="math inline">\(h_\theta(x)=g\left(\theta_0+\theta_1 x_1+\theta_2x_2\right)\)</span>。</p><p><img src="/img/blog/ML-Note-4-images/decision-boundary-2.png" alt="多项式的决策边界" width=30%/></p><p>而对于这种分布，我们必须选择二维曲线来划分空间，即使用<strong>多项式特征</strong>来确定曲线的参数，对应的假设函数为<span class="math inline">\(h_\theta(x)=g\left(\theta_0+\theta_1x_1+\theta_2 x_2+\theta_3 x_3^2+\theta_4x_4^2\right)\)</span>。当然，我们也可以用更复杂的多项式曲线来划分更复杂的分布。</p><h3 id="代价函数">代价函数</h3><p>现在，我们的任务就是从训练集中拟合逻辑回归的参数 <spanclass="math inline">\(\theta\)</span>。仍然采用代价函数的思想——找到使代价最小的参数即可。</p><p>广义上来讲，代价函数是这样的一个函数： <span class="math display">\[J(\theta)=\frac{1}{m}\sum_{i=1}^m\text{Cost}\left(h_\theta(x^{(i)}),y^{(i)}\right)\]</span> 也就是说用每个数据的估计值 <spanclass="math inline">\(h_\theta(x^{(i)})\)</span> 和真实值 <spanclass="math inline">\(y^{(i)}\)</span> 计算一个代价 <spanclass="math inline">\(\text{Cost}\left(h_\theta(x^{(i)}),y^{(i)}\right)\)</span>，比如线性回归中这个代价就是二者差值的平方。</p><p>理论上来说，我们也可以对逻辑回归模型沿用平方误差的定义，但当我们将<span class="math inline">\({h_\theta}\left( x\right)=\frac{1}{1+{e^{-\theta^{T}x}}}\)</span>代入到这样的代价函数中时，我们得到的将是一个<strong>非凸函数</strong>（non-convexfunction）。这意味着空间中会有许多<strong>局部最小值</strong>，使得梯度下降法难以寻找到<strong>全局最小值</strong>。</p><p>因此我们重新定义逻辑回归的<strong>代价函数</strong>： <spanclass="math display">\[\mathrm{Cost}\left( h_{\theta}(x),y \right) =\begin{cases}    -\ln \left( h_{\theta}\left( x \right) \right)&amp;     y=1\\    -\ln \left( 1-h_{\theta}\left( x \right) \right)&amp;       y=0\\\end{cases}\]</span> 绘制出的曲线大致呈这样：</p><p><img src="/img/blog/ML-Note-4-images/cost.png" alt="代价函数"  width=50% /></p><p>观察曲线，发现当 <spanclass="math inline">\(y=1\)</span>（样本的真实值为 <spanclass="math inline">\(1\)</span>）时，预测值 <spanclass="math inline">\(h_\theta(x)\)</span> 越接近 <spanclass="math inline">\(1\)</span> 则代价越小，越接近 <spanclass="math inline">\(0\)</span>则代价趋于无穷。譬如在肿瘤分类中，将实际为恶性的肿瘤以百分之百的概率预测为良性，带来的后果将不可估量。</p><p>与此同时，注意到代价函数也可以<strong>简写</strong>为： <spanclass="math display">\[\mathrm{Cost}\left( h_{\theta}\left( x \right) ,y \right) =-\left[ y\ln\left( h_{\theta}\left( x \right) \right) +\left( 1-y \right) \ln \left(1-h_{\theta}\left( x \right) \right) \right]\]</span> 它还有另外一个名称——<strong>二元交叉熵代价函数</strong>（BCE,Binary Cross-Entropy），它又蕴含着怎样的原理呢？</p><h3 id="代价函数的数学推导">代价函数的数学推导</h3><p>首先明确什么是一个<strong>好的代价函数</strong>——当参数 <spanclass="math inline">\(\theta\)</span> 使得 <spanclass="math inline">\(J(\theta)\)</span>取<strong>极小值</strong>时，这个 <spanclass="math inline">\(\theta\)</span>也能使模型拟合效果最好。这时我们回忆起 <ahref="https://hwcoder.top/PR-Note-3">极大似然估计</a> 的思想：当参数<span class="math inline">\(\theta\)</span> 使得 <spanclass="math inline">\(L(\theta)\)</span>取<strong>极大值</strong>时，这个 <spanclass="math inline">\(\theta\)</span>也能使得<strong>事件组</strong>最容易发生！</p><p>前文已经提到，我们用概率解释预测值 <spanclass="math inline">\(h_\theta(x)=P(y=1)\)</span>，于是 <spanclass="math inline">\(1-h_\theta(x)=P(y=0)\)</span>，故： <spanclass="math display">\[P\left( y=k \right) =\left[ h_{\theta}\left( x \right) \right] ^k\left[1-h_{\theta}\left( x \right) \right] ^{1-k},\quad k\in \left\{ 0,1\right\}\]</span> 而对于数据集 <spanclass="math inline">\(\left\{\left(x^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\)</span>下，将其视为已发生的一个<strong>事件组</strong>，则似然函数为： <spanclass="math display">\[L\left( \theta \right) =\prod_{i=1}^m{P}\left( y=y^{(i)} \right)=\prod_{i=1}^m{\left[ h_{\theta}\left( x^{(i)} \right) \right]^{y^{(i)}}}\left[ 1-h_{\theta}\left( x^{(i)} \right) \right]^{1-y^{(i)}}\]</span> 取对数得到： <span class="math display">\[\ln L(\theta )=\sum_{i=1}^m{\left\{ y^{(i)}\ln \left[ h_{\theta}\left(x^{(i)} \right) \right] +\left( 1-y^{(i)} \right) \ln \left[1-h_{\theta}\left( x^{(i)} \right) \right] \right\}}\]</span> 注意到，极大似然法的目标是找到 <spanclass="math inline">\(L(\theta)\)</span> 或 <spanclass="math inline">\(\ln L(\theta)\)</span>的极大值，而逻辑回归的目标是找到 <spanclass="math inline">\(J(\theta)\)</span> 的极小值，所以自然的，我们将<span class="math inline">\(\ln L(\theta)\)</span><strong>取反</strong>来定义 <spanclass="math inline">\(J(\theta)\)</span>： <span class="math display">\[\begin{aligned}J(\theta)&amp;=-\frac{1}{m}\ln L(\theta)\\&amp;=-\frac{1}{m}\sum_{i=1}^m{\left[ y^{\left(i\right)}\ln \left(h_{\theta}\left( x^{\left(i\right)} \right) \right) +\left(1-y^{\left(i\right)} \right) \ln \left( 1-h_{\theta}\left(x^{\left(i\right)} \right) \right) \right]}\end{aligned}\]</span> 其中 <span class="math inline">\(\frac{1}{m}\)</span> 对要求的<span class="math inline">\(\theta\)</span>没有影响，仅是取一下平均罢了。</p><blockquote><p>可以证明上述代价函数 <span class="math inline">\(J(\theta)\)</span>会是一个<strong>凸函数</strong>，并且没有局部最优值。凸性分析的内容不在本讲的范围，但是可以证明我们所选的代价函数会给我们带来一个<strong>凸优化</strong>问题（ConvexOptimization）。</p></blockquote><h3 id="梯度下降">梯度下降</h3><p>既然是凸函数，那么现在我们就可以进行梯度下降求解 <spanclass="math inline">\(\underset{\theta}{\arg\min }J\left( \theta\right)\)</span> 。</p><p>为了求偏导，我们先计算： <span class="math display">\[\begin{aligned}    \frac{\partial}{\partial \theta}\mathrm{Cost}\left( h_{\theta}\left(x \right) ,y \right) &amp;=\frac{\partial}{\partial \theta}\left[ -y\ln\left( h_{\theta}\left( x \right) \right) -\left( 1-y \right) \ln \left(1-h_{\theta}\left( x \right) \right) \right]\\    &amp;=\frac{\partial}{\partial \theta}\left[ y\ln \left(1+e^{-\theta ^Tx} \right) +(1-y)\ln \left( 1+e^{\theta ^Tx} \right)\right]\\    &amp;=\frac{-yxe^{-\theta ^Tx}}{1+e^{-\theta ^Tx}}+\frac{\left( 1-y\right) xe^{\theta ^Tx}}{1+e^{\theta ^Tx}}\\    &amp;=\frac{-yx+\left( 1-y \right) xe^{\theta ^Tx}}{1+e^{\theta^Tx}}\\    &amp;=\left( -y+\frac{1}{1+e^{-\theta ^Tx}} \right) x\\    &amp;=\left( h_{\theta}\left( x \right) -y \right) x\\\end{aligned}\]</span> 于是乎， <span class="math display">\[\frac{\partial J}{\partial \theta}=\frac{1}{m}\sum_{i=1}^m{\left(h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)}\right) x^{\left( i \right)}}\]</span> 没错，这个偏导的形式和线性回归完全相同！不同的只是 <spanclass="math inline">\({h_\theta}\left( x \right)=g\left( {\theta^T}X\right)\)</span> 的定义——多了一层 <spanclass="math inline">\(\text{sigmoid}\)</span>函数，正是因此，我们不能使用正规方程直接给出<strong>解析解</strong>，而必须使用梯度下降等方法。</p><p><span class="math display">\[\theta:=\theta-\alpha\cdot\frac{\partial J}{\partial \theta}\]</span></p><p>现在我们对其使用梯度下降即可。另外，在运行梯度下降算法之前，进行<strong>特征缩放</strong>依旧是非常必要的。</p><blockquote><p>除了梯度下降法，还有很多算法可以用来求解这个最优值：共轭梯度法（ConjugateGradient）、局部优化法（Broyden fletcher goldfarb shann,BFGS）、有限内存局部优化法（LBFGS）等。</p><p>这些算法通常不需要手动选择学习率 <spanclass="math inline">\(\alpha\)</span>，而是使用一个智能的内循环（线性搜索算法）来选择一个较好的<span class="math inline">\(\alpha\)</span>，甚至能为每次迭代选择不同的<spanclass="math inline">\(\alpha\)</span>。因此他们有着更优越的常数和时间复杂度，在大型机器学习项目中更加适用。</p></blockquote><h3 id="代码实现">代码实现</h3><p>下面以 <a href="https://www.coursera.org/">Coursera</a>上的二分类数据集 <code>ex2data1.txt</code>为例，首先看一下数据的分布：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (100, 3)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex2data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>X = data[:, :-<span class="hljs-number">1</span>]<br>y = data[:, -<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># preview data</span><br>pos = np.where(y == <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]<br>neg = np.where(y == <span class="hljs-number">0</span>)[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 返回索引</span><br>plt.scatter(X[pos, <span class="hljs-number">0</span>], X[pos, <span class="hljs-number">1</span>], marker=<span class="hljs-string">&quot;o&quot;</span>, c=<span class="hljs-string">&#x27;c&#x27;</span>)<br>plt.scatter(X[neg, <span class="hljs-number">0</span>], X[neg, <span class="hljs-number">1</span>], marker=<span class="hljs-string">&quot;x&quot;</span>, c=<span class="hljs-string">&#x27;r&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Exam 1 score&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Exam 2 score&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><figure><img src="/img/blog/ML-Note-4-images/preview_scatter.png"alt="数据分布散点图" /><figcaption aria-hidden="true">数据分布散点图</figcaption></figure><p>看起来用<strong>直线</strong>即可划分数据。此外，注意到如果每次都用<code>np.sum()</code> 计算 <spanclass="math inline">\(\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}_j\)</span>耗时较大，因此将求和化成<strong>矩阵形式</strong>： <spanclass="math display">\[\theta :=\theta -\alpha \frac{1}{m}X^T\left( g\left( X\theta \right) -y\right)\]</span> 实现逻辑回归如下，矩阵化后运行时间可缩短一半：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (100, 3)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex2data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>X = data[:, :-<span class="hljs-number">1</span>]<br>y = data[:, -<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># normalization</span><br>X = (X - X.mean(axis=<span class="hljs-number">0</span>)) / X.std(axis=<span class="hljs-number">0</span>, ddof=<span class="hljs-number">1</span>)<br>X = np.c_[np.ones(m), X] <span class="hljs-comment"># 增加一列 1</span><br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">10000</span><br>theta = np.zeros(n)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">z</span>):</span><br>    g = np.zeros(z.size)<br>    g = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-z))<br>    <span class="hljs-keyword">return</span> g<br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>error = sigmoid(X @ theta) - y  <span class="hljs-comment"># error.shape = (100, )</span><br>theta -= (alpha / m) * (X.T @ error)  <span class="hljs-comment"># X.T.shape = (2, 100)</span><br><br><span class="hljs-comment"># plot decision boundary</span><br>pos = np.where(y == <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]<br>neg = np.where(y == <span class="hljs-number">0</span>)[<span class="hljs-number">0</span>]<br>plt.scatter(X[pos, <span class="hljs-number">1</span>], X[pos, <span class="hljs-number">2</span>], marker=<span class="hljs-string">&quot;o&quot;</span>, c=<span class="hljs-string">&#x27;c&#x27;</span>)<br>plt.scatter(X[neg, <span class="hljs-number">1</span>], X[neg, <span class="hljs-number">2</span>], marker=<span class="hljs-string">&quot;x&quot;</span>, c=<span class="hljs-string">&#x27;r&#x27;</span>)<br><br>x_plot = np.array([np.<span class="hljs-built_in">min</span>(X[:, <span class="hljs-number">1</span>]), np.<span class="hljs-built_in">max</span>(X[:, <span class="hljs-number">1</span>])])<br>y_plot = (-<span class="hljs-number">1</span> / theta[<span class="hljs-number">2</span>]) * (theta[<span class="hljs-number">1</span>] * x_plot + theta[<span class="hljs-number">0</span>])<br>plt.plot(x_plot, y_plot)<br>plt.show()<br></code></pre></td></tr></table></figure><p>得到的 <span class="math inline">\(\left( \theta_0, \theta_1,\theta_2 \right)\)</span> 结果是：[1.2677 3.05552.8289]，绘制出决策边界的图像为：</p><figure><img src="/img/blog/ML-Note-4-images/decision_boundary.png"alt="决策边界（归一化）" /><figcaption aria-hidden="true">决策边界（归一化）</figcaption></figure><h2 id="多分类问题">多分类问题</h2><p>在实际情形中，我们还会使用逻辑回归来解决<strong>多元</strong>的分类问题。多分类的数据集和二分类相似，区别在于<strong>目标变量</strong><span class="math inline">\(y^{(i)}\)</span> 在这里不仅有 <spanclass="math inline">\(0\)</span> 或 <spanclass="math inline">\(1\)</span> 两类结果，还可以取 <spanclass="math inline">\(2\)</span>、<span class="math inline">\(3\)</span>等更多的数字。</p><figure><img src="/img/blog/ML-Note-4-images/multi-class.png"alt="二分类和多分类" /><figcaption aria-hidden="true">二分类和多分类</figcaption></figure><blockquote><p>对于接下来要介绍的方法，标签数字的顺序、取法，都不会影响最终的结果。但在某些分类模型中，数值可能具有实际意义，这时候使用<strong>独热码</strong>（One-Hot）或许是更好的选择。</p></blockquote><h3 id="一对余-one-vs.-rest">一对余 | One vs. Rest</h3><p>对于 <span class="math inline">\(N\)</span>分类问题，我们可以将其转化为 <span class="math inline">\(N\)</span>个二分类问题——只需创建 <span class="math inline">\(N\)</span>个「<strong>伪训练集</strong>」，每个训练集中仅包含一个类作为正向类，其他<span class="math inline">\(N-1\)</span> 个类均视为负向类。</p><p><img src="/img/blog/ML-Note-4-images/one-vs-rest.png" alt="One vs. Rest" width=60% /></p><p>接下来就可以训练 <span class="math inline">\(N\)</span>个标准的逻辑回归分类器，将其记为： <span class="math display">\[h_\theta^{\left( i \right)}\left( x \right)=P\left(y=i|x;\theta  \right) \;\; i=\left( 1,2,\cdots,N \right)\]</span>显然，每个分类器的输出都可以视为「<strong>属于某类</strong>」的概率，在预测时，我们只需要运行一遍所有分类器，然后取其最大值作为预测结果即可。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>How to Read a Paper ?</title>
    <link href="/Read-Paper.html"/>
    <url>/Read-Paper.html</url>
    
    <content type="html"><![CDATA[<blockquote><p>Researchers spend a great deal of time reading research papers.However, this skill is rarely taugh, leading to much wastedeﬀort<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[How to Read a Paper -Stanford](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf)">[1]</span></a></sup>.</p></blockquote><p>本文旨在通过学习前人的经验，形成自己的一套阅读文献的方法。</p><p>在开始阅读文献之前，首先要确定大致的研究方向，并对相关领域有<strong>一定的了解</strong>：</p><ul><li>该领域存在的价值是什么？</li><li>该领域有哪些基本概念、专有名词、Key Words？</li><li>该领域必要的 Preliminaries 有哪些？</li><li>该领域能 work 的基本原理是什么？</li><li>该领域最著名的工作、成果有哪些？</li></ul><p>以上内容大部分可以在<strong>中文博客</strong>中找到前人总结好的文章，我们要做的任务就是广泛搜集、大量阅读、形成自己的First Impression。此外，还可以在 <ahref="https://arxiv.org/">ArXiv</a>、<ahref="https://github.com/">GitHub</a>、<ahref="https://papers.labml.ai/papers/daily">Daliy Papers</a>、<ahref="https://towardsdatascience.com/">Towards Data Science</a>、<ahref="https://www.paperdigest.org/">Paper Digest</a>、<ahref="https://www.connectedpapers.com/">Connected Papers</a>等网站上找到有用的资源。</p><p>以上最好能输出<strong>第一篇</strong>自己的 Overview博客，接下来才是真正接触文献。</p><h2 id="读什么">读什么？</h2><h3 id="初学者">初学者</h3><p>作为一个刚踏入新领域的初学者，读文献的优先级应该是<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[科研大牛们怎么读文献？- 知乎](https://www.zhihu.com/question/21278186)">[2]</span></a></sup>：</p><ol type="1"><li>近一两年该领域<strong>有影响力</strong>的 Survey 或Review（视领域的发展速度而异）：<strong>综述</strong>一般会将各个流派以及最先进的方法整理出来，读完后会对该领域有更深的认识，同时可以完善此前的博客。</li><li>近五年内该领域的<strong>经典著作</strong>（高引用，具有启发性的）：就算读不懂也会有很多博客讲解，并且会有backbone的各种源码实现，最好结合起来看，阅读的同时输出若干篇<strong>精读笔记</strong>。</li><li>经典著作引用的<strong>更经典著作</strong>（引用量达到一定量级）：时间可能会过于久远，领域也可能不完全契合，但这些文章绝对值得一看。</li><li>近两年的顶会<strong>SOTA</strong>（有开源代码优先）：帮助你更快地掌握该领域最新的热点。同上，阅读的同时输出若干篇<strong>泛读笔记</strong>，泛读过程中对有启发的文章加以精读。</li><li>其他<strong>相近领域</strong>可能有启发的经典著作、综述。</li></ol><blockquote><p>学术圈唯 SOTA 论的风气严重，几乎每篇论文都会声称自己达到某某任务的SOTA，筛选时更要区分 Trick 和真正的 Contribution，不要在 Trick上花太多的时间。</p></blockquote><p>确定了优先级后，做好以下的事情：</p><ul><li>根据此前的积累，辨识并搜集有用的文献；</li><li>使用 <code>Zotero</code> 管理文献，使用 <code>Notion</code>管理阅读；</li><li>多和导师、师兄师姐沟通，不要开局就踩坑。</li></ul><h3 id="研究者">研究者</h3><p>如果你对当前领域已经有了较深的认识，甚至取得过一定的成果和产出，下面还有三种扩宽知识面的方法<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[6 Tips to Finding Research Paper Sources that Set You Apart - studyright.net](https://www.studyright.net/blog/research-paper-sources/)">[6]</span></a></sup>：</p><p><strong>读大牛</strong>：如果他们在领域经典著作中高频出现，我们可以在Google Scholar 主页找到更多的资料：</p><ul><li>他的从博士阶段至今的研究课题；</li><li>他的高被引经典著作、形成的写作方式；</li><li>他的研究方向变动，特别是在领域有重大突破的时间节点；</li><li>他的最新论文动态（可能指示着领域未来的热点）。</li></ul><p><strong>扫会议</strong>：最新的会议 AcceptedList，扫一遍耗时略久，但可以保持对领域的清晰认识：</p><ul><li>通过关键词云，判断其他人的研究动向；</li><li>从其他人的 method 中摘取灵感。</li></ul><p><strong>追踪论文</strong>：在 Google Scholar选择一篇你关注的文献（经典、强相关），添加引用提醒：</p><ul><li>进入文章的引用列表（cited by），点击 CreateAlert，新论文引用该论文时，就会有邮件通知；</li><li>进入作者主页，点击 CreateAlert，当作者维护自己的主页时（有新文章），就会有邮件通知；</li><li>有时候，论文还没发表，就已经能够通过引用关系找到该论文。</li></ul><h2 id="怎么读">怎么读？</h2><h3 id="一篇论文的构成">一篇论文的构成</h3><p>首先要了解论文各部分的重要性<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[How_to_Search_and_Read_a_Paper - GitHub](https://github.com/qiyuangong/How_to_Search_and_Read_a_Paper)">[3]</span></a></sup>：</p><ol type="1"><li>Abstract：论文<strong>最重要</strong>的部分，概括问题的提出到解决，一般由导师直接把关；</li><li>Introduction：重要性<strong>仅次于摘要</strong>，内容更加易懂，带有<strong>最关键</strong>的<strong>Figure</strong>，指出本文的 <strong>Motivation</strong> 和<strong>Novelty</strong>；</li><li>RelatedWork、Background：对领域内工作的分类和总结，面对一个生涩的主题时读者则需要阅读，但通常没什么干货；</li><li>Method、Approach：论文的模型算法细节，对于<strong>理解</strong>本论文非常重要；</li><li>Experiment、Evaluation：会提出<strong>重要</strong>的<strong>metric</strong>，results 中有会对比其他工作的<strong>Table</strong>；</li><li>Discussion、Outlook、Conclusion：全文总结和展望，很可能会指出文章的<strong>缺陷</strong>；</li><li>Reference：论文引用的文献列表；</li><li>Appendix：部分无法放到正文中的内容，如证明和算法。</li></ol><p>综上，无论精读还是泛读，看一篇文章的<strong>正确顺序</strong>应该是<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[How to Read A Scientific Paper - MIT](https://be.mit.edu/sites/default/files/documents/HowToReadAScientificPaper.pdf)">[7]</span></a></sup>：</p><ol type="1"><li>Abstract &amp; Conclusion</li><li>Introduction</li><li>Figure &amp; Tables</li><li>Evaluation metric</li><li>Others（如果要精读）</li></ol><h3 id="准备工作">准备工作</h3><ul><li>一篇初步完成的 Overview 博客</li><li>使用 <code>Zotero</code> 搜集整理好的文献</li><li>使用 <code>Notion</code> 创建一个 Paper To Read 的优先级 List</li><li>使用 <code>Notion</code> 创建一篇 Paper 的精读 or 泛读模板</li></ul><h3 id="泛读-pass-1">泛读 Pass 1</h3><p><strong>目的</strong>：在有限的时间里筛选出<strong>或许有用</strong>的论文，并<strong>简要记录</strong>在表格中</p><p><strong>顺序</strong>：</p><ol type="1"><li>Title：论文是不是跟自己目前的工作相关？</li><li>Abstract &amp;Conclusion：论文试图解决什么<strong>问题</strong>？为什么这篇文章能中？</li><li>Introduction：论文的<strong>动机</strong>是什么？故事讲的如何？</li><li>Figure &amp; Tables：解决方案之<strong>关键</strong>是什么？</li></ol><p><strong>要点</strong>：</p><ul><li>预计用时 10-15 分钟；</li><li>得出结论：这篇文章要不要继续读下去？优先级如何？</li></ul><h3 id="泛读-pass-2">泛读 Pass 2</h3><p><strong>目的</strong>：知道文章<strong>每一块</strong>在干什么，总结出<strong>一段泛读笔记</strong></p><p><strong>顺序</strong>：</p><ol type="1"><li>Abstract &amp;Conclusion：论文试图解决什么<strong>问题</strong>？为什么这篇文章能中？</li><li>Introduction：论文的<strong>动机</strong>是什么？<strong>故事</strong>讲的如何？有什么<strong>贡献</strong>？</li><li>RelatedWork、Background：有哪些相关研究？如何<strong>归类</strong>？</li><li>Method：解决方案是什么？图和方法怎么<strong>对应</strong>起来？值得学习的绘图技巧？</li><li>Experiment：实验是如何设计的？用于定量评估的<strong>指标</strong>和数据集是什么？</li><li>Table：作者提出的方法跟别人的方法是怎么<strong>对比</strong>的？差距在哪？</li><li>Reference：论文中多次引用、提出<strong>改进</strong>的重要文献，有哪些值得一看？</li></ol><p><strong>要点</strong>：</p><ul><li>预计一天看个几篇；</li><li>不用太注意公式特别的证明，或者一些很细节的处理；</li><li>如果还没有特别没有搞得懂在干什么，可以读作者引用的重要文献；</li><li>可以并行阅读，互相启发，加深理解；</li><li>得出结论：这篇文章值得精读吗？</li></ul><h3 id="精读">精读</h3><p><strong>目的</strong>：知道文章<strong>每一句话</strong>在干什么，总结出<strong>一篇精读笔记</strong>，简要回答以下<strong>八个问题</strong><sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="[专业的学术讨论社区 - ReadPaper](https://readpaper.com/)">[4]</span></a></sup>：</p><ol type="1"><li>论文试图解决什么<strong>问题</strong>？为什么这篇文章能中？</li><li>论文的<strong>动机</strong>是什么？<strong>故事</strong>讲的如何？有什么<strong>贡献</strong>？</li><li>论文中提到的解决方案之<strong>关键</strong>是什么？如果是<strong>自己来完成会怎么做</strong>？</li><li>论文中的实验是如何设计的？实验的<strong>难点在哪里</strong>？</li><li>论文中的实验结果和分析有没有<strong>很好地支持</strong>需要验证的科学假设？<strong>能否改进</strong>？</li><li>用于定量评估的指标和数据集是什么？代码有没有开源？<strong>能否复现</strong>？</li><li>论文提到的<strong>思路、模型、实现、工具、数据集</strong>，<strong>有哪些可以为我所用</strong>？</li><li>下一步呢？论文中提到的<strong>不足和拓展</strong>，有什么工作可以<strong>继续深入或跟进</strong>？</li></ol><blockquote><p>如果这篇论文足够重要，足以在组会中<strong>讨论、分享</strong>它，最好按照上面十个问题进行梳理，特别是<strong>前两个</strong>问题（介绍）和<strong>后两个</strong>问题（体会）。</p></blockquote><p><strong>要点</strong>：</p><ul><li>一天能看完一篇吗？</li><li>很多原因会造成理解不够深入甚至理解错误，需要<strong>多次阅读</strong>（如果有必要，<strong>多次讨论</strong>）来避免这类问题；</li><li>如果在阅读过程中出现了大量盲点，反思是不是 Preliminaries的掌握不够；</li><li>学会<strong>挑刺</strong>，不能顺着作者的逻辑，要换个角度<strong>尝试去反驳</strong>它，就像审稿人一样。要能够找出这篇论文的优缺点、有哪些改进的空间，这样才能在相对成熟的研究中发现新问题。</li></ul><h2 id="后续工作">后续工作</h2><p>如果此前已经有了足够的Preliminaries，且有过一定的文献阅读经验，到这一步时大概已经过了两三个月了。</p><p>此时你已经阅读了大量文献和源码，和导师、同门进行了几次讨论，这时候还留在筐子里ideas 很有可能是可行的，现在就需要把你的 idea实现了。这个阶段也不必孤军奋战，可以找同组的小伙伴一起<strong>合作</strong>。</p><p>又过了一两个月，当你的 idea基本证实有效之后，就可以开始动手写论文了。主要使用的工具为线上多人合作的Overleaf。</p><h2 id="参考">参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><ahref="https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf">Howto Read a Paper -Stanford</a><a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><ahref="https://www.zhihu.com/question/21278186">科研大牛们怎么读文献？-知乎</a> <a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><ahref="https://github.com/qiyuangong/How_to_Search_and_Read_a_Paper">How_to_Search_and_Read_a_Paper- GitHub</a> <a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:4"class="footnote-text"><span><a href="https://readpaper.com/">专业的学术讨论社区- ReadPaper</a><a href="#fnref:4" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><ahref="https://www.eecs.harvard.edu/~michaelm/postscripts/ReadPaper.pdf">Howto read a research paper - Harvard</a><a href="#fnref:5" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:6"class="footnote-text"><span><a href="https://www.studyright.net/blog/research-paper-sources/">6Tips to Finding Research Paper Sources that Set You Apart -studyright.net</a><a href="#fnref:6" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span><ahref="https://be.mit.edu/sites/default/files/documents/HowToReadAScientificPaper.pdf">Howto Read A Scientific Paper - MIT</a><a href="#fnref:7" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span><ahref="https://www.bilibili.com/video/BV1H44y1t75x">如何读论文【论文精读】-李沐</a> <a href="#fnref:8" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>科研笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>软件工程 应试笔记 #2</title>
    <link href="/Software-Engineering-2.html"/>
    <url>/Software-Engineering-2.html</url>
    
    <content type="html"><![CDATA[<p>该笔记（<strong>考点预测</strong>）是本人于哈尔滨工业大学（深圳）2021年秋季学期「软件工程」课程的笔记。</p><p>笔记是在考前复习时扫 PPT做的，内容偏<strong>应试风格</strong>，内容涵盖了所有考试内容（变异测试、性能测试不在考试范围，故不在此列）。主要包括：软件工程概论、软件开发过程管理、需求工程、软件设计、软件测试、软件维护。</p><p>2021年秋季学期的简答题内容基本涵盖于本题单中（少数内容漏掉了，考完后进行了补充），整份题单按照PPT顺序，可用于复习过程中的<strong>查缺补漏</strong>、<strong>自我检测</strong>。</p><blockquote><p>括号内是本人的答案助记，不保证其准确性，仅供参考！</p></blockquote><h2 id="考试回忆">考试回忆</h2><p>2021 年简答题考了题单里的 <code>6, 25, 29, 35</code>，大题考了<code>5, 8, 9, 10</code>。判断题、选择题也基本在题单覆盖范围内，已在文中<strong>加粗</strong>。</p><p>2020 年简答题考了题单里的 <code>23, 40, 42, 45</code>，大题考了<code>4, 5, 6, 8, 9, 10</code>。</p><h2 id="考点预测">考点预测</h2><h3 id="简答考点">简答考点</h3><ol type="1"><li>软件的本质特性有哪些？（复杂性、一致性、可变性、不可见性）</li><li>软件开发面临的挑战为什么会出现？（客观上、主观上）</li><li>软件工程的基本要素有哪些？（方法、工具、过程、范型）</li><li>软件开发过程的典型阶段（需求定义与分析、软件设计、软件实现、软件验证、软件运行与维护）</li><li>可以通过调整软件开发不同阶段的顺序使之适应不同的情况。（√）</li><li>五种软件开发模型（瀑布、增量、快速应用开发、<strong>快速原型开发</strong>、螺旋）及特点</li><li>软件开发管理的 4P（人员、产品、过程、项目）</li><li>列举软件项目参数估算的技术（代码行、功能点、COCOMO、用例点、故事点、机器学习）</li><li>软件项目进度如何安排？（工作量分配、任务依赖网络、关键路径分配时间、确定资源等）</li><li>功能性需求和非功能性需求的区别和举例。（描述了系统与其独立于系统实现环境之间的交互——功能上，描述了不直接关联到系统功能行为的系统的方方面面——性能上）</li><li>非功能性需求有哪些层次？（业务规则、外部接口需求、约束条件）</li><li>描述一个完整的需求工程（需求获取、需求分析、需求规格说明、需求验证）</li><li>基于场景的需求建模方法举例与说明（用户故事、用例图、活动图）</li><li>绘制一个完整的用例图的步骤（确定系统边界、识别并描述参与者、识别用例、识别通讯关联、给出用例描述、细化用例模型）</li><li>通讯关联不表示参与者和用例之间的信息流。（√）</li><li>什么是 SRS？（精确阐述FR、NFR、限制条件，法律效力，理解和交流的手段）</li><li>传统软件工程开发方法有哪些？特点是？（功能分解法、结构化方法、信息建模法）</li><li>传统开发方法和面向对象方法有什么区别、优势？（过程抽象/实体对象，分析与设计难以对应/直接映射问题域形成良好的对应）</li><li>面向对象方法的三步骤（分析、设计、编程）</li><li>面向对象设计有哪些基本概念？（对象、类、封装、消息、泛化继承、多态）</li><li>软件设计的原则有哪些？（抽象、封装、模块化、层次化、复用）</li><li>软件体系结构的要素有哪些？（构建+连接件+约束）</li><li>软件体系结构风格（数据流风格、以数据为中心的风格、调用和返回体系结构风格、面向对象体系结构风格、层次体系结构风格）</li><li>C/S+B/S层次体系结构具有什么优点？（将二者结合起来，企业内/外有别，内部交互性增强响应速度高，外部不直接访问数据更安全）</li><li><strong>CRC 面向对象方法</strong>的具体内容（类、责任、协作）</li><li>顺序图包含哪些<strong>组件</strong>？</li><li>协作图与顺序图的异同点和适用情形。（都是交互，顺序图偏重时间顺序，协作图偏重链接关系）</li><li>活动图与状态图的异同点和适用情形。（图形差不多，状态图只有单个对象，活动图侧重多个对象交互，可以描述并行行为）</li><li><strong>软件编程</strong>除了代码实现还包含哪些内容？（程序设计、代码审查、代码重构）</li><li>代码审查有哪些形式？（桌面检查、代码走查、代码审查）</li><li>代码重构和添加新功能有什么区别？（重构不改变功能和外部行为，仅提高代码的可理解性、可扩展性等，新功能不修改既有代码，仅增加代码。二者交替进行）</li><li>软件缺陷的演化过程（错误、缺陷、故障、失效）</li><li>软件测试的两种思维是什么？（正向：验证正常、反向：假定缺陷）</li><li>软件测试有哪些局限性？（不彻底性——不能说明错误不存在、不完备性——不能完全覆盖、间接性）</li><li><strong>按照测试对象分类</strong>的软件测试类型（单元、集成、系统、验收）</li><li>单元测试需要哪些模块？（驱动、桩模块）</li><li>集成测试有哪些方法？（整体集成方式、自顶向下、自底向上的增量集成）</li><li>回归测试是什么？什么时候需要回归测试？（任何一个阶段）</li><li>自动化测试和手工测试的区别？自动化测试能替代手工测试吗？</li><li>测试用例的重要性（指导系统地测试、提高测试效率、作为检验软件的度量标准、积累和传递测试的经验）</li><li>测试用例的组成要素（测试用例值、期望结果、前缀值、后缀值）</li><li>黑盒测试与白盒测试的区别（白盒利用程序的内部逻辑、对所有逻辑路径进行测试、考虑逻辑的覆盖程度，黑盒不考虑内部逻辑、在软件接口处进行测试、考虑需求的覆盖程度）</li><li>黑盒测试用于辅助白盒测试发现其他类型错误。（√）</li><li>测试用例的评价标准（覆盖度高——黑白不一样、用例少、冗余度低、用例的缺陷定位能力高）</li><li>3种黑盒测试方法（等价类测试、边界值测试、场景法测试）共同点都是不用考虑内部实现就可以的方法，前两中面向数据，后一种面向过程</li><li>等价类测试中等价类划分原则（分而不交、和而不变、类内等价）</li><li>「事件流」出现在了需求建模的用例模型中的用例描述里，也出现在黑盒测试的场景法测试里，一个是常规流+扩展流，一个是基本流+备选流</li><li>传统的软件开发模式和现在的软件开发模式有什么区别？（传统的侧重预见性的开发方法，如瀑布模型，但是难以预知所有的内容和风险；现在的开发方法采用弹性的管理方式，系统迭代演进来应对快速变化的需求，适应而非预测）</li><li>敏捷开发方法有哪些？（极限编程XP、Scrum开发方法）</li><li>Scrum制品中的产品订单和迭代订单有什么区别？（产品订单是从客户角度要求实现的所有功能的列表，迭代订单是从开发技术角度的每个迭代要完成的功能列表）</li><li>软件交付工作包含哪些方面？（项目实施、客户培训、项目验收）</li><li>软件维护和软件再工程有什么区别？（软件维护是为了修改软件的缺陷或者增加新功能，软件再工程是为了避免软件本身退化对软件的一部分进行重新设计、编码和测试）</li><li>软件维护有哪些类型？有什么区别？（改正性、适应性、<strong>完善性</strong>）</li></ol><h3 id="大题考点">大题考点</h3><ol type="1"><li>给定需求，写出增量模型的迭代功能、故事卡片</li><li>给定需求，画出用例图并写出用例描述（耗时）</li><li>绘制层级体系结构图（B/S 和 C/S）（耗时）</li><li>面向对象设计，写出类和类图（耗时）</li><li>结构化方法数据建模，画出 0 层和 1 层 <strong>DFD</strong>图（耗时）</li><li>结构化方法数据建模，找出 DFD 图里的错误</li><li>过程行为建模，绘制状态图（耗时）</li><li>过程行为建模，画出<strong>顺序图</strong>（耗时）</li><li>白盒测试<strong>用例设计</strong>，要求覆盖</li><li>白盒测试基本路径，画出<strong>流图</strong>算环复杂度</li><li>黑盒测试用例设计，等价类测试：划分等价类，设计测试用例来覆盖</li><li>黑盒测试用例设计，边界值测试</li><li>黑盒测试用例设计，场景法测试</li></ol>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>本科课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>软件工程 应试笔记 #1</title>
    <link href="/Software-Engineering-1.html"/>
    <url>/Software-Engineering-1.html</url>
    
    <content type="html"><![CDATA[<p>该笔记（<strong>思维导图</strong>）是本人于哈尔滨工业大学（深圳）2021年秋季学期「软件工程」课程的笔记。</p><p>笔记是随着上课过程逐渐整理的，笔记内容偏<strong>应试风格</strong>，内容涵盖了所有考试内容（变异测试、性能测试不在考试范围，故不在此列）。主要包括：软件工程概论、软件开发过程管理、需求工程、软件设计、软件测试、软件维护。</p><p>「软件工程」属于「<strong>理工科中的文科</strong>」，需要<strong>记忆和辨析</strong>的概念较多，考试也不考察硬核的内容，2021年秋季学期的考试内容大都涵盖于本笔记中，试卷的难点在于设计部分（画顺序图、数据流图），此外还有一些「坑点」，会在另一篇笔记中补充。</p><blockquote><p>图片较大，加载可能比较缓慢，推荐保存下来使用。如需分享，请注明出处！</p></blockquote><h2 id="软件工程概论">软件工程概论</h2><p>主要内容包含：</p><ul><li>软件的产生与发展</li><li>软件工程基本概念</li></ul><figure><img src="/img/blog/Software-Engineering-1-images/软件工程概论.png"alt="软件工程概论" /><figcaption aria-hidden="true">软件工程概论</figcaption></figure><h2 id="软件开发过程管理">软件开发过程管理</h2><p>主要内容包含：</p><ul><li>软件开发过程</li><li>软件开发管理</li></ul><figure><img src="/img/blog/Software-Engineering-1-images/开发过程管理.png"alt="开发过程管理" /><figcaption aria-hidden="true">开发过程管理</figcaption></figure><h2 id="软件需求工程">软件需求工程</h2><p>主要内容包含：</p><ul><li>软件需求与需求工程</li><li>需求获取与建模</li><li>需求规格说明书（SRS）</li></ul><figure><img src="/img/blog/Software-Engineering-1-images/需求工程.png"alt="需求工程" /><figcaption aria-hidden="true">需求工程</figcaption></figure><h2 id="软件设计">软件设计</h2><p>主要内容包含：</p><ul><li>软件开发与设计方法</li><li>软件体系结构设计</li><li>类、数据建模与设计</li><li>过程、行为建模与设计</li><li>物理建模与设计</li></ul><figure><img src="/img/blog/Software-Engineering-1-images/软件设计.png"alt="软件设计" /><figcaption aria-hidden="true">软件设计</figcaption></figure><h2 id="软件测试">软件测试</h2><p>主要内容包含：</p><ul><li>软件编程</li><li>软件测试概论</li><li>白盒测试、黑盒测试</li><li>变异测试、性能测试（不考）</li></ul><figure><img src="/img/blog/Software-Engineering-1-images/软件测试.png"alt="软件测试" /><figcaption aria-hidden="true">软件测试</figcaption></figure><h2 id="软件维护">软件维护</h2><p>主要内容包含：</p><ul><li>敏捷开发</li><li>软件实施、维护与演化</li></ul><figure><img src="/img/blog/Software-Engineering-1-images/软件维护.png"alt="软件维护" /><figcaption aria-hidden="true">软件维护</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>本科课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Everyone Has Their Own Time Zone</title>
    <link href="/Everyone-Has-A-Time-Zone.html"/>
    <url>/Everyone-Has-A-Time-Zone.html</url>
    
    <content type="html"><![CDATA[<p>一学期中最忙的时候，刚结束一堆DDL，考完了数据库，总算有点时间更新博客了，可惜下周又是一堆 DDL。</p><p>最近的情绪比较低落，仿佛总是被一些无聊的事情推着走——无聊的考试、无聊的作业、无聊的项目、无聊的课程。</p><p>好在前段时间联系了高老师，现在可能也慢慢走上科研的正轨了？至少看见了一丝曙光。</p><hr /><p>也是突然被提醒才想起这个博客原来是有分享心情的初衷，最近常常念叨一篇小短文，就分享一下叭~</p><h2 id="everyone-has-their-own-time-zone">Everyone Has Their Own TimeZone</h2><p>New York is 3 hours ahead of California,</p><p>but it does not make California slow.</p><p>Someone graduated at the age of 22,</p><p>but waited 5 years before securing a good job!</p><p>Someone became a CEO at 25,</p><p>and died at 50.</p><p>While another became a CEO at 50,</p><p>and lived to 90 years.</p><p>Someone is still single,</p><p>while someone else got married.</p><p>Obama retires at 55,</p><p>but Trump starts at 70.</p><p>Absolutely everyone in this world works based on their Time Zone.</p><p>People around you might seem to go ahead of you,</p><p>some might seem to be behind you.</p><p>But everyone is running their own RACE, in their own TIME.</p><p>Don’t envy them or mock them.</p><p>They are in their TIME ZONE, and you are in yours!</p><p>Life is about waiting for the right moment to act.</p><p>So, RELAX.</p><p>You’re not LATE.</p><p>You’re not EARLY.</p><p>You are very much ON TIME, and in your TIME ZONE Destiny set up foryou.</p>]]></content>
    
    
    <categories>
      
      <category>心情随笔</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>DB课程项目-校园食堂点餐系统-开发文档</title>
    <link href="/DB-Project.html"/>
    <url>/DB-Project.html</url>
    
    <content type="html"><![CDATA[<figure><img src="/img/blog/DB-Project-images/logo.png" alt="logo" /><figcaption aria-hidden="true">logo</figcaption></figure><p><code>Date</code>：2021-11</p><p><code>Description</code>：本文档作为 2021年秋季学期「数据库系统」课程 Project 校园食堂点餐系统的开发文档，基于MySQL+Django 实现。</p><p><code>Reference</code>：参考了一系列 Django入门教程，在文中相应部分有链接。</p><p><code>Code Repo</code>：<ahref="https://github.com/hewei2001/campus-canteen-ordering">https://github.com/hewei2001/campus-canteen-ordering</a></p><p><code>Acknowledgement</code>：感谢 <ahref="https://github.com/Yiwen-Ding">Yiwen-Ding</a> 小盆友 😭不然不可能肝完这么多内容。</p><p><code>Copyright</code>：© 2021 Hwcoder. All rights reserved.</p><h2 id="项目说明">项目说明</h2><p>本实验要求设计并实现一个小型数据库应用系统，实现<strong>前后台</strong>数据交互，并将<strong>数据保存</strong>在之前实验所设计的数据库中。</p><p>由于不限语言和实现方式，同学们可能采用的方案大致有：Cpp+Qt，Python+Flask，Python+Django，Python+PyQt等，还有其他 Java、PHP 一些我不太熟悉的方案。</p><p>本项目采用的是 Python+Django，是因为 Django具有功能强大的脚手架和诸多开箱即用的组件，用 Django 搭建 Web应用<strong>快速而又省力</strong>。相比于其他同学手敲 Qt 窗口，Django自带的 Admin已经基本把这些功能都实现了，甚至装了插件后还<strong>更美观</strong>，因此可以聚焦于前台交互的快速开发。</p><h3 id="项目背景">项目背景</h3><p>本项目是建立在之前的实验结果的基础（已经用 PowerDesigner设计并导出了数据库到 MySQL Workbench）上，再用 Django进行应用系统的开发的过程。</p><p>这里简单描述一下之前的实验：</p><ul><li>设计校园食堂点餐系统，要求具备功能：食堂管理、商户管理、菜品管理、订单管理、用户管理；</li><li>E-R 图至少包括 8 个实体和 7 个联系；</li><li>需要考虑关系完整性约束：主键约束、外键约束、空值约束；</li><li>设计至少 1 个视图、1 个索引、1 个触发器（在应用中不需要体现）。</li></ul><p>以及设计好的 PDM (物理数据模型)：</p><p><img src="/img/blog/DB-Project-images/PDM.png" alt="PDM" width=50% /></p><p>最终实现的功能：</p><figure><img src="/img/blog/DB-Project-images/user.png" alt="user" /><figcaption aria-hidden="true">user</figcaption></figure><figure><img src="/img/blog/DB-Project-images/admin.png" alt="admin" /><figcaption aria-hidden="true">admin</figcaption></figure><h3 id="django-基础">Django 基础</h3><p>如果你之前从未接触过 Web 框架，可以先看这一篇文章：<ahref="https://zhuanlan.zhihu.com/p/98788776">一杯茶的时间，上手 Django框架开发</a>，再往下阅读。</p><p>通过这篇文章，你可以了解到：</p><ol type="1"><li>Django 环境的安装、项目初始化</li><li>项目的骨架以及各个文件的作用</li><li>Django App 的创建、各个子文件的作用</li><li>App 中 <code>views.py</code> 的编写、路由 <code>urls.py</code>的使用</li><li>Django Template 的实现——模板生成的 HTML 文件将随数据变化</li><li>Django ORM 与数据库中的内容交互、迁移数据模型<code>models.py</code></li><li>Django Admin 实现应用与后台管理接口、使用超级用户登录后台</li></ol><h2 id="设计文档">设计文档</h2><h3 id="运行环境">运行环境</h3><p>操作系统：</p><ul><li>Windows 10</li></ul><p>开发工具：</p><ul><li><p>IDE：PyCharm Community Edition 2020.2</p></li><li><p>设计：PowerDesigner</p></li><li><p>数据库：MySQL Workbench 8.0 CE</p></li></ul><p>主要技术：</p><ul><li><p>环境：Django 3.8</p></li><li><p>前端样式：Bootstrap</p></li><li><p>后台样式：Django-SimpleUI</p></li></ul><h3 id="django-后台开发">Django 后台开发</h3><p>本部分参考资料有：</p><ul><li><p><ahref="http://c.biancheng.net/django/">http://c.biancheng.net/django/</a></p></li><li><p><ahref="https://www.liujiangblog.com/course/django/">https://www.liujiangblog.com/course/django/</a></p></li></ul><h4 id="新建项目">新建项目</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ django-admin startproject django_CCOS<br></code></pre></td></tr></table></figure><p>调整基础配置 <code>settings.py</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">LANGUAGE_CODE = <span class="hljs-string">&#x27;zh-Hans&#x27;</span><br>TIME_ZONE = <span class="hljs-string">&#x27;Asia/Shanghai&#x27;</span><br></code></pre></td></tr></table></figure><p>其他字段的意义可以参考：<ahref="http://c.biancheng.net/view/7475.html">http://c.biancheng.net/view/7475.html</a></p><p>为了存放图片等资源，需要在 <code>settings.py</code>中加上以下内容，：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 这个是设置静态文件夹目录的路径</span><br>STATICFILES_DIRS = (<br>    os.path.join(BASE_DIR, <span class="hljs-string">&#x27;static&#x27;</span>),<br>)<br><span class="hljs-comment"># 设置文件上传路径，图片上传、文件上传都会存放在此目录里</span><br>MEDIA_URL = <span class="hljs-string">&#x27;/media/&#x27;</span><br>MEDIA_ROOT = os.path.join(BASE_DIR, <span class="hljs-string">&#x27;media&#x27;</span>)<br></code></pre></td></tr></table></figure><p>同时，在 <code>TEMPLATES</code> 中加上如下字段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;django.template.context_processors.media&#x27;</span>,<br></code></pre></td></tr></table></figure><p>Python3 的 Django 默认的数据库类型是sqlite，由于之前的实验做的数据库是 MySQL 的，这里进行修改<code>settings.py</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">DATABASES = &#123;<br>    <span class="hljs-string">&#x27;default&#x27;</span>: &#123;<br>        <span class="hljs-string">&#x27;ENGINE&#x27;</span>: <span class="hljs-string">&#x27;django.db.backends.mysql&#x27;</span>,<br>        <span class="hljs-string">&#x27;NAME&#x27;</span>: <span class="hljs-string">&#x27;django_CCOS&#x27;</span>,  <span class="hljs-comment"># 连接的数据库</span><br>        <span class="hljs-string">&#x27;HOST&#x27;</span>: <span class="hljs-string">&#x27;127.0.0.1&#x27;</span>,    <span class="hljs-comment"># mysql的ip地址</span><br>        <span class="hljs-string">&#x27;PORT&#x27;</span>: <span class="hljs-number">3306</span>,           <span class="hljs-comment"># mysql的端口</span><br>        <span class="hljs-string">&#x27;USER&#x27;</span>: <span class="hljs-string">&#x27;root&#x27;</span>,         <span class="hljs-comment"># mysql的用户名</span><br>        <span class="hljs-string">&#x27;PASSWORD&#x27;</span>: <span class="hljs-string">&#x27;123456&#x27;</span>    <span class="hljs-comment"># mysql的密码</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>同时还要安装第三方库，并在 <code>__init__.py</code> 中加上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pymysql<br><br>pymysql.install_as_MySQLdb()<br></code></pre></td></tr></table></figure><h4 id="连接-mysql-数据库">连接 MySQL 数据库</h4><p>打开 MySQLWorkbench，创建新数据库，<code>create database CCOS</code>。将Powerdesigner 生成的 <code>crebas.sql</code> 打开并执行，注意要注释掉<code>drop</code> 等语句。此时我们的数据库就建好了。</p><p>打开项目，创建三个 App：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ python manage.py startapp canteen<br>$ python manage.py startapp customer<br>$ python manage.py startapp dish<br></code></pre></td></tr></table></figure><p>然后在 <code>INSTALLED_APPS</code>中加上对应字段，注意要按顺序，否则后面的指令都会报错。</p><p>接下来要反向导入我们的数据库，在 Django 中生成 Model，参考：<ahref="https://blog.csdn.net/diao1057/article/details/98472327">https://blog.csdn.net/diao1057/article/details/98472327</a>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ python manage.py inspectdb --database default canteen shop shop_manager &gt; canteen/models.py<br>$ python manage.py inspectdb --database default customer address  &gt; customer/models.py<br>$ python manage.py inspectdb --database default dish orders comments &gt; dish/models.py<br></code></pre></td></tr></table></figure><p>导入后如果出现如下报错：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gauss">Python Django ValueError: source <span class="hljs-built_in">code</span> <span class="hljs-keyword">string</span> cannot contain <span class="hljs-built_in">null</span> bytes<br></code></pre></td></tr></table></figure><p>很可能是因为生成的 <code>models.py</code> 的编码方式默认被改为 UTF-16了，需要修改回 UTF-8。</p><h4 id="模型重建">模型重建</h4><p>生成的 Model 不完整，还不能直接运行，据说是因为 Powerdesigner比较拉跨。因此需要在刚刚生成的 <code>models.py</code> 中修改<code>class</code> 的内容，并导出到数据库里，覆盖掉原来 Powerdesigner生成的数据库。<code>class</code> 各个字段的意义参考：<ahref="https://www.liujiangblog.com/course/django/95">https://www.liujiangblog.com/course/django/95</a>。</p><p>可以看到所有的属性在 <code>class</code> 中都有自己的Field，并传入了详细的定义，下面给出一些常用字段：</p><ul><li><code>verbose_name</code>：显示名</li><li><code>choices</code>：多选框</li><li><code>upload_to</code>：图片上传路径</li><li><code>max_length</code>：最大长度</li><li><code>blank=True, null=True,</code>：可以为空</li></ul><blockquote><p>特别要注意外键属性（ForeignKey），如果外键在当前 App内还好（跟定义顺序有关），要是在其他 App 里，则要<code>from [其他app].models import xxx</code>。此外，外键还有一个<code>on_delete</code> 属性，代表删除外键对本表记录的影响，参考：<ahref="https://www.liujiangblog.com/course/django/96">https://www.liujiangblog.com/course/django/96</a>。</p></blockquote><p>还有 <code>class Meta</code> 部分，可以设置对整体的操作，比如ordering 显示顺序等。注意要把之前反向导入产生的<code>managed = False</code> 字段删除。</p><p>在每一个 <code>class</code> 下加上 Python的一个魔法方法，用于返回实例对象的打印字符串。：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span>(<span class="hljs-params">self</span>):</span><br>    <span class="hljs-keyword">return</span> self.title  <span class="hljs-comment"># title 换成想显示的属性</span><br></code></pre></td></tr></table></figure><p>最后的成品举例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Dish</span>(<span class="hljs-params">models.Model</span>):</span><br>    dish_id = models.AutoField(primary_key=<span class="hljs-literal">True</span>, verbose_name=<span class="hljs-string">&#x27;菜品编号&#x27;</span>)<br>    shop = models.ForeignKey(<span class="hljs-string">&#x27;canteen.Shop&#x27;</span>, models.CASCADE, verbose_name=<span class="hljs-string">&#x27;窗口&#x27;</span>)<br>    dish_name = models.CharField(max_length=<span class="hljs-number">20</span>, verbose_name=<span class="hljs-string">&#x27;菜品名称&#x27;</span>)<br>    dish_detail = models.CharField(max_length=<span class="hljs-number">200</span>, blank=<span class="hljs-literal">True</span>, null=<span class="hljs-literal">True</span>, verbose_name=<span class="hljs-string">&#x27;菜品描述&#x27;</span>)<br>    dish_price = models.DecimalField(max_digits=<span class="hljs-number">5</span>, decimal_places=<span class="hljs-number">2</span>, verbose_name=<span class="hljs-string">&quot;菜品价格&quot;</span>)<br>    dish_photo = models.ImageField(upload_to=<span class="hljs-string">&#x27;image/dish&#x27;</span>, null=<span class="hljs-literal">True</span>, blank=<span class="hljs-literal">True</span>, verbose_name=<span class="hljs-string">&#x27;菜品照片&#x27;</span>)<br>    dish_active = models.IntegerField(choices = [(<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;销售中&#x27;</span>),(<span class="hljs-number">0</span>, <span class="hljs-string">&#x27;售罄&#x27;</span>)], verbose_name=<span class="hljs-string">&#x27;菜品状态&#x27;</span>)<br><br>    <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Meta</span>:</span><br>        ordering = [<span class="hljs-string">&#x27;dish_id&#x27;</span>]<br>        db_table = <span class="hljs-string">&#x27;dish&#x27;</span><br>        verbose_name = <span class="hljs-string">&quot;菜品信息&quot;</span><br>        verbose_name_plural = verbose_name<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> self.dish_name<br></code></pre></td></tr></table></figure><h4 id="数据库迁移">数据库迁移</h4><p>运行以下命令创建迁移文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ python manage.py makemigrations<br></code></pre></td></tr></table></figure><p>由于导出时会覆盖原来的 MySQL里的数据库，会创造很多乱七八糟的表，可能会有些许报错，建议在 MySQL重新创一个空数据库 <code>create database django_CCOS</code>，并修改<code>settings.py</code> 中的连接。</p><p>接着我们进行数据库迁移：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ python manage.py migrate<br></code></pre></td></tr></table></figure><h4 id="后台接口管理">后台接口管理</h4><p>数据库迁移完成后，我们就可以创建用于登录后台管理的超级用户：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ python manage.py createsuperuser<br></code></pre></td></tr></table></figure><p>按照提示填写用户名和密码即可。</p><p>在每个 App 下的 <code>admin.py</code> 添加后台管理接口如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> django.contrib <span class="hljs-keyword">import</span> admin<br><br><span class="hljs-keyword">from</span> .models <span class="hljs-keyword">import</span> Canteen, Shop, ShopManager<br><br>admin.site.register(Canteen)<br>admin.site.register(Shop)<br>admin.site.register(ShopManager)<br></code></pre></td></tr></table></figure><p>完成上述操作后，访问 <ahref="localhost:8000/admin">localhost:8000/admin</a>，进入后台系统的登录页面，填入刚才设置的用户名和密码，进入后台管理页面。此时已经可以看到所有的表单，且可以完成基础的管理操作。</p><h3 id="bootstrap-前端开发">BootStrap 前端开发</h3><p>完成了后台，现在来实现前端，这就需要用到 Django 的 templates模板。首先在<strong>主目录</strong>下新建 <code>templates</code>用于存放前端页面 html，再创建 <code>static</code>用来存放渲染静态页面需要的 css 资源。</p><p>参考博客：<ahref="https://www.cnblogs.com/qican/p/14626811.html">https://www.cnblogs.com/qican/p/14626811.html</a></p><h4 id="静态模板安装">静态模板安装</h4><p>首先下载 Bootstrap：<ahref="https://v3.bootcss.com/getting-started/#download">https://v3.bootcss.com/getting-started/#download</a>，下载下来后放入<code>static</code> 文件夹（也可以直接从别人的 Django项目中复制进来，可能样式更高级）。</p><p>修改 <code>setting.py</code>，前面已经加了 <code>static</code>内容，现在要在 <code>TEMPLATE</code>字典中加入如下，用以表示模板路径：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;DIRS&#x27;</span>: [BASE_DIR / <span class="hljs-string">&#x27;templates&#x27;</span>],<br></code></pre></td></tr></table></figure><h4 id="view.py">view.py</h4><p>在 canteen 的 <code>view.py</code> 中写入如下内容，用以 render数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> django.shortcuts <span class="hljs-keyword">import</span> render<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">index</span>(<span class="hljs-params">request</span>):</span><br>    <span class="hljs-keyword">return</span> render(request, <span class="hljs-string">&#x27;base.html&#x27;</span>)<br></code></pre></td></tr></table></figure><h4 id="html">HTML</h4><p>在 <code>templates</code> 中放入我们的<code>base.html</code>，作为展示的首页，内容如下：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-meta">&lt;!DOCTYPE <span class="hljs-meta-keyword">html</span>&gt;</span><br>&#123;% load static %&#125;<br><span class="hljs-tag">&lt;<span class="hljs-name">html</span> <span class="hljs-attr">lang</span>=<span class="hljs-string">&quot;en&quot;</span> <span class="hljs-attr">dir</span>=<span class="hljs-string">&quot;ltr&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">head</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">charset</span>=<span class="hljs-string">&quot;utf-8&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>校园食堂点餐系统<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">link</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;stylesheet&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;&#123;% static &#x27;css/master.css&#x27; %&#125;&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">head</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">body</span>&gt;</span><br>        <span class="hljs-comment">&lt;!-- 此处省略首页代码，如导航栏等 --&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">body</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">html</span>&gt;</span><br></code></pre></td></tr></table></figure><p>其中静态资源引用了 <code>static</code> 下面的<code>css/master.css</code>，来自 GitHub 的其他开源项目。</p><h4 id="url-路径配置">URL 路径配置</h4><p>修改 <code>urls.py</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 用于 include 其他模板所在的 url</span><br><span class="hljs-keyword">from</span> django.conf.urls <span class="hljs-keyword">import</span> url, include<br><br>urlpatterns = [<br>    path(<span class="hljs-string">&#x27;admin/&#x27;</span>, admin.site.urls),<br>    path(<span class="hljs-string">&#x27;&#x27;</span>, include(<span class="hljs-string">&#x27;canteen.urls&#x27;</span>)),<br>]<br></code></pre></td></tr></table></figure><p>由于加了 <code>canteen</code>，相应的也要在<code>canteen/urls.py</code> 中加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> django.urls <span class="hljs-keyword">import</span> path<br><span class="hljs-keyword">from</span> .views <span class="hljs-keyword">import</span> index<br><br>app_name = <span class="hljs-string">&#x27;canteen&#x27;</span><br>urlpatterns = [<br>    path(<span class="hljs-string">&#x27;&#x27;</span>, index),<br>]<br></code></pre></td></tr></table></figure><p>之后就可以看到首页啦，其他功能将在下一节介绍。</p><h3 id="交互功能开发">交互功能开发</h3><p>要求实现的功能：食堂管理、商户管理、菜品管理、订单管理、用户管理。</p><h4 id="用户登录注册">用户登录注册</h4><p>参考资料：<ahref="https://www.liujiangblog.com/course/django/102">https://www.liujiangblog.com/course/django/102</a></p><p>在 <code>customer</code> 下 <code>view.py</code>中加入用户登录的方法，其中 <code>locals()</code>代表返回当前方法里的全部变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> django.shortcuts <span class="hljs-keyword">import</span> render<br><span class="hljs-keyword">from</span> .forms <span class="hljs-keyword">import</span> LoginForm, RegisterForm <span class="hljs-comment"># 导入表单，下面将介绍</span><br><span class="hljs-keyword">from</span> django.contrib <span class="hljs-keyword">import</span> messages<br><span class="hljs-keyword">from</span> .models <span class="hljs-keyword">import</span> Customer<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">register</span>(<span class="hljs-params">request</span>):</span><br>    <span class="hljs-comment"># 写注册方法</span><br>    <span class="hljs-keyword">return</span> render(request, <span class="hljs-string">&#x27;customer/register.html&#x27;</span>, <span class="hljs-built_in">locals</span>())<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">login</span>(<span class="hljs-params">request</span>):</span><br>    <span class="hljs-comment"># 写登录方法</span><br>    <span class="hljs-keyword">return</span> render(request, <span class="hljs-string">&#x27;customer/login.html&#x27;</span>, <span class="hljs-built_in">locals</span>())<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">logout</span>(<span class="hljs-params">request</span>):</span><br>    <span class="hljs-comment"># 写登出方法</span><br>    <span class="hljs-keyword">return</span> render(request, <span class="hljs-string">&#x27;customer/index.html&#x27;</span>, <span class="hljs-built_in">locals</span>())<br></code></pre></td></tr></table></figure><p>具体访问策略如下：</p><ul><li>未登录人员，不论是访问 index 还是 login 和 logout，全部跳转到 login界面</li><li>已登录人员，访问 login 会自动跳转到 index 页面</li><li>已登录人员，不允许直接访问 register 页面，需先 logout</li><li>登出后，自动跳转到 login 界面</li></ul><p>于是，在主目录 <code>urls.py</code> 中加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">path(<span class="hljs-string">&#x27;customer/&#x27;</span>, include(<span class="hljs-string">&#x27;customer.urls&#x27;</span>)),<br></code></pre></td></tr></table></figure><p>在 <code>customer/urls.py</code> 中加入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> django.urls <span class="hljs-keyword">import</span> path<br><span class="hljs-keyword">from</span> .views <span class="hljs-keyword">import</span> index, login, register, logout<br><br>app_name = <span class="hljs-string">&#x27;customer&#x27;</span><br>urlpatterns = [<br>    path(<span class="hljs-string">&#x27;&#x27;</span>, index),<br>    path(<span class="hljs-string">&#x27;login/&#x27;</span>, login),<br>    path(<span class="hljs-string">&#x27;register/&#x27;</span>, register),<br>    path(<span class="hljs-string">&#x27;logout/&#x27;</span>, logout),<br>]<br></code></pre></td></tr></table></figure><p>实现登录时，需要用到表单 form，创建<code>cutomser/forms.py</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> django <span class="hljs-keyword">import</span> forms<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LoginForm</span>(<span class="hljs-params">forms.Form</span>):</span><br>    username = forms.CharField(label=<span class="hljs-string">&quot;用户名&quot;</span>, max_length=<span class="hljs-number">128</span>, widget=forms.TextInput(attrs=&#123;<span class="hljs-string">&#x27;class&#x27;</span>: <span class="hljs-string">&#x27;form-control&#x27;</span>, <span class="hljs-string">&#x27;placeholder&#x27;</span>: <span class="hljs-string">&quot;Username&quot;</span>,<span class="hljs-string">&#x27;autofocus&#x27;</span>: <span class="hljs-string">&#x27;&#x27;</span>&#125;),<br>                    error_messages=&#123;<span class="hljs-string">&#x27;required&#x27;</span>: <span class="hljs-string">&#x27;用户名不能为空&#x27;</span>,<span class="hljs-string">&#x27;min_length&#x27;</span>: <span class="hljs-string">&#x27;用户名最少为3个字符&#x27;</span>,<br>   <span class="hljs-string">&#x27;max_length&#x27;</span>: <span class="hljs-string">&#x27;用户名最不超过为20个字符&#x27;</span>&#125;,)<br>    password = forms.CharField(label=<span class="hljs-string">&quot;密码&quot;</span>, max_length=<span class="hljs-number">256</span>, widget=forms.PasswordInput(attrs=&#123;<span class="hljs-string">&#x27;class&#x27;</span>: <span class="hljs-string">&#x27;form-control&#x27;</span>,<span class="hljs-string">&#x27;placeholder&#x27;</span>: <span class="hljs-string">&quot;Password&quot;</span>&#125;))<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RegisterForm</span>(<span class="hljs-params">forms.Form</span>):</span><br>    username = forms.CharField(label=<span class="hljs-string">&quot;用户名&quot;</span>, max_length=<span class="hljs-number">128</span>, widget=forms.TextInput(attrs=&#123;<span class="hljs-string">&#x27;class&#x27;</span>: <span class="hljs-string">&#x27;form-control&#x27;</span>, <span class="hljs-string">&#x27;placeholder&#x27;</span>: <span class="hljs-string">&quot;Username&quot;</span>,<span class="hljs-string">&#x27;autofocus&#x27;</span>: <span class="hljs-string">&#x27;&#x27;</span>&#125;))<br>    password1 = forms.CharField(label=<span class="hljs-string">&quot;密码&quot;</span>, max_length=<span class="hljs-number">256</span>, widget=forms.PasswordInput(attrs=&#123;<span class="hljs-string">&#x27;class&#x27;</span>: <span class="hljs-string">&#x27;form-control&#x27;</span>,<span class="hljs-string">&#x27;placeholder&#x27;</span>: <span class="hljs-string">&quot;Password&quot;</span>&#125;))<br>    password2 = forms.CharField(label=<span class="hljs-string">&quot;确认密码&quot;</span>, max_length=<span class="hljs-number">256</span>, widget=forms.PasswordInput(attrs=&#123;<span class="hljs-string">&#x27;class&#x27;</span>: <span class="hljs-string">&#x27;form-control&#x27;</span>,<span class="hljs-string">&#x27;placeholder&#x27;</span>: <span class="hljs-string">&quot;Password&quot;</span>&#125;))<br>    tel = forms.CharField(label=<span class="hljs-string">&quot;电话&quot;</span>, widget=forms.TextInput(attrs=&#123;<span class="hljs-string">&#x27;class&#x27;</span>: <span class="hljs-string">&#x27;form-control&#x27;</span>, <span class="hljs-string">&#x27;placeholder&#x27;</span>: <span class="hljs-string">&quot;TEL&quot;</span>&#125;))<br></code></pre></td></tr></table></figure><p>其他诸如 session(会话) 的内容默认已经配置好了（session是用于保存当前在线用户信息的，用于各种与用户关联的查询），现在可以打开页面查看登录。</p><h4 id="食堂窗口菜品展示">食堂、窗口、菜品展示</h4><p>在相应的 views.py 中写好 ORM 方法，将数据库内的表元组全部传到Django，再到 templates 中写好 html，将传出的元组以循环遍历显示。</p><p>例如显示食堂和窗口，在 <code>canteen/views.py</code> 写入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> django.shortcuts <span class="hljs-keyword">import</span> render<br><span class="hljs-keyword">from</span> .models <span class="hljs-keyword">import</span> Canteen, Shop<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show_canteen</span>(<span class="hljs-params">request</span>):</span><br>    template_name = <span class="hljs-string">&#x27;canteen/canteen_list.html&#x27;</span><br>    context = &#123;<span class="hljs-string">&#x27;canteen_list&#x27;</span>: Canteen.objects.<span class="hljs-built_in">all</span>()&#125;<br>    <span class="hljs-keyword">return</span> render(request, template_name, context)<br></code></pre></td></tr></table></figure><p>再到 canteen_list.html 中写下：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs html">&#123;% block store_content %&#125;<br><span class="hljs-tag">&lt;<span class="hljs-name">div</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;container&quot;</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;row&quot;</span>&gt;</span><br>            &#123;% for i in canteen_list %&#125;<br>            <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;align-items-stretch&quot;</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;card cardmodify cardbackground&quot;</span>&gt;</span><br>                    <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;card-header&quot;</span>&gt;</span><br>                        <span class="hljs-tag">&lt;<span class="hljs-name">h5</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">b</span>&gt;</span>&#123;&#123; i.canteen_name &#125;&#125;<span class="hljs-tag">&lt;/<span class="hljs-name">b</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">h5</span>&gt;</span><br>                    <span class="hljs-tag">&lt;<span class="hljs-name">img</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;card-img-top&quot;</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;&#123;&#123; i.canteen_photo.url &#125;&#125;&quot;</span>&gt;</span><br>                        <span class="hljs-tag">&lt;<span class="hljs-name">h5</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">b</span>&gt;</span>&#123;&#123; i.canteen_active &#125;&#125;<span class="hljs-tag">&lt;/<span class="hljs-name">b</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">h5</span>&gt;</span><br>                        <span class="hljs-tag">&lt;<span class="hljs-name">h5</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">b</span>&gt;</span>卫生等级：&#123;&#123; i.sanitation_level &#125;&#125;<span class="hljs-tag">&lt;/<span class="hljs-name">b</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">h5</span>&gt;</span><br>                    <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>                <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>            <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>            &#123;% endfor %&#125;<br>        <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>&#123;% endblock %&#125;<br></code></pre></td></tr></table></figure><p>再修改相应的路由，就能在页面中以卡片的形式展示食堂了！样式代码来自GitHub 的其他开源项目。</p><p>然而，这样只能实现所有食堂、所有窗口、所有菜品在一个静态页面中的显示，无法根据用户选中某个食堂而显示其对应的窗口。于是，本项目中采用了一个取巧的方法，利用HTML 的锚定位和 <code>id</code> 属性，选中某个食堂跳转到窗口页的相应<code>id</code> 处，让用户「看似」进入了该食堂独立的页面。</p><h4 id="用户下单">用户下单</h4><p>这里用到了 <code>reverse</code>方法，也是全项目卡住最久的地方，需要在 Model 中定义如下内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_order_url</span>(<span class="hljs-params">self</span>):</span><br>    <span class="hljs-keyword">return</span> reverse(<span class="hljs-string">&quot;dish:get_order&quot;</span>, kwargs=&#123;<span class="hljs-string">&#x27;dish_id&#x27;</span>: self.dish_id&#125;)<br></code></pre></td></tr></table></figure><p>这意味着当用户点击按钮时，触发 <code>get_order_url</code>方法，该方法将传入的 <code>dish_id</code> 转发到<code>dish/views.py</code> 中的 <code>get_order</code>方法，再进行处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_order</span>(<span class="hljs-params">request, dish_id</span>):</span><br>    dish = get_object_or_404(Dish, dish_id=dish_id)<br>    user_id = request.session[<span class="hljs-string">&#x27;user_id&#x27;</span>]<br><br>    <span class="hljs-keyword">try</span>:<br>        user = Customer.objects.<span class="hljs-built_in">filter</span>(customer_id=user_id).first()<br>        order = Orders.objects.create(dish=dish, customer=user)<br>        order.order_price = order.dish.dish_price<br>        order.order_status = <span class="hljs-number">0</span><br>        order.save()<br>        messages.success(request, <span class="hljs-string">&#x27;下单成功！&#x27;</span>)<br>        <span class="hljs-keyword">return</span> redirect(<span class="hljs-string">&quot;dish:show_order&quot;</span>)<br><br>    <span class="hljs-keyword">except</span> ObjectDoesNotExist:<br>        messages.warning(request, <span class="hljs-string">&quot;你还没有订单哦~&quot;</span>)<br>        <span class="hljs-keyword">return</span> redirect(<span class="hljs-string">&quot;dish:show_order&quot;</span>)<br></code></pre></td></tr></table></figure><p>相应的按钮触发事件还需要在 HTML 标签中指出，这里不再赘述。</p><h4 id="其他页面">其他页面</h4><p>诸如订单页面、个人中心页面等，实现方法都是用 ORM筛选数据库中当前用户的数据，并用模板显示出来。需要注意的是，进入该界面前要先判断，如果未登录则重定向至登录界面：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> request.session.get(<span class="hljs-string">&#x27;is_login&#x27;</span>, <span class="hljs-literal">None</span>):<br>    messages.warning(request, <span class="hljs-string">&quot;请先登录顾客账户~&quot;</span>)<br>    <span class="hljs-keyword">return</span> redirect(<span class="hljs-string">&#x27;/customer/login/&#x27;</span>)<br></code></pre></td></tr></table></figure><h4 id="优化后台管理">优化后台管理</h4><p>在 <code>admin.py</code> 里修改显示内容，使后台展示更全面。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DishAdmin</span>(<span class="hljs-params">admin.ModelAdmin</span>):</span><br>    list_per_page = <span class="hljs-number">10</span><br>    search_fields = [<span class="hljs-string">&#x27;dish_name&#x27;</span>]<br>    list_display = [<span class="hljs-string">&#x27;dish_id&#x27;</span>, <span class="hljs-string">&#x27;dish_name&#x27;</span>, <span class="hljs-string">&#x27;shop&#x27;</span>, <span class="hljs-string">&#x27;dish_price&#x27;</span>, <span class="hljs-string">&#x27;dish_active&#x27;</span>]<br></code></pre></td></tr></table></figure><p>就可以在后台显示详细信息，并加入搜索框，可以根据菜品名搜索。</p><p>此外，开源资源中还有许多对 Django Admin 的 UI 优化插件，例如simpleui，只需在 app 中包含即可。</p><h2 id="页面展示">页面展示</h2><figure><img src="/img/blog/DB-Project-images/page1.png" alt="菜品展示界面" /><figcaption aria-hidden="true">菜品展示界面</figcaption></figure><figure><img src="/img/blog/DB-Project-images/page2.png" alt="用户登录界面" /><figcaption aria-hidden="true">用户登录界面</figcaption></figure><figure><img src="/img/blog/DB-Project-images/page3.png" alt="订单展示界面" /><figcaption aria-hidden="true">订单展示界面</figcaption></figure><figure><img src="/img/blog/DB-Project-images/page4.png" alt="后台管理界面" /><figcaption aria-hidden="true">后台管理界面</figcaption></figure><figure><img src="/img/blog/DB-Project-images/page5.png" alt="后台操作演示" /><figcaption aria-hidden="true">后台操作演示</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>项目经历</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Database</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据库系统 应试笔记</title>
    <link href="/Database-System.html"/>
    <url>/Database-System.html</url>
    
    <content type="html"><![CDATA[<p>该笔记（思维导图）是本人于哈尔滨工业大学（深圳）2021年秋季学期「数据库系统」课程的笔记，由于试卷较为变态，最后只考了 91分，教学班排名 2/99。</p><p>由于笔记是在考前复习时才逐渐整理的，笔记内容偏<strong>应试风格</strong>，主要按照课程学习的顺序和考试内容梳理（由于SQL部分在实验中考察，考试不涉及），涵盖了课程所学，主要包括：数据库基础概念、数据库设计、数据库实现、数据库事务处理。</p><p>为什么是<strong>思维导图</strong>？因为「数据库」这门课涵盖的内容太多了，简直是进阶版的「计算机导论」，甚至连数据结构、算法设计、操作系统都有涉及，线性地整理笔记似乎不太适合记忆。</p><p>当然，<a href="https://github.com/ailanxier">Ailanxier</a> 用Markdown 整理的 <ahref="https://github.com/ailanxier/Database-System-Note">笔记</a>也很棒，很多老师上课没讲清楚的点都有解释，配合本文的思维导图复习可能更高效。</p><blockquote><p>图片较大，加载可能比较缓慢，推荐保存下来使用。如需分享，请注明出处！</p></blockquote><h2 id="数据库基础概念">数据库基础概念</h2><p>主要内容包含：</p><ul><li>基本概念</li><li>关系模型与关系运算</li><li>完整性和安全性</li><li>SQL 语句（不考）</li></ul><figure><img src="/img/blog/Database-System-images/数据库基础.png"alt="数据库基础" /><figcaption aria-hidden="true">数据库基础</figcaption></figure><h2 id="数据库设计">数据库设计</h2><p>主要内容包含：</p><ul><li>数据建模</li><li>数据库设计过程</li><li>函数依赖及其公理定理</li><li>关系范式及模式分解</li></ul><figure><img src="/img/blog/Database-System-images/数据库设计.png"alt="数据库设计" /><figcaption aria-hidden="true">数据库设计</figcaption></figure><h2 id="数据库实现">数据库实现</h2><p>主要内容包含：</p><ul><li>数据库物理存储</li><li>数据库索引技术</li><li>查询实现算法</li><li>查询优化技术</li></ul><figure><img src="/img/blog/Database-System-images/数据库实现.png"alt="数据库实现" /><figcaption aria-hidden="true">数据库实现</figcaption></figure><h2 id="数据库事务处理">数据库事务处理</h2><p>主要内容包含：</p><ul><li>并发控制</li><li>故障处理</li></ul><figure><img src="/img/blog/Database-System-images/数据库事务处理.png"alt="数据库事务处理" /><figcaption aria-hidden="true">数据库事务处理</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>本科课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Database</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PR学习笔记 #5 判别式 vs. 生成式</title>
    <link href="/PR-Note-5.html"/>
    <url>/PR-Note-5.html</url>
    
    <content type="html"><![CDATA[<p>在机器学习中，我们可以将<strong>有监督学习</strong>可以将其分为两类模型：<strong>判别式模型</strong>（DiscriminativeModel）和<strong>生成式模型</strong>（GenerativeModel）。二者的建模对象不同，但最终目标都是使<strong>测试集正确标签的后验概率</strong>（在训练集的先验上）最大化。下面我们以简单的分类问题为背景，探讨二者的区别。</p><h2 id="问题描述">问题描述</h2><p>首先简单地定义训练集为 <spanclass="math inline">\((X,Y)\)</span>，其中 <spanclass="math inline">\(X\)</span> 为特征集合，<spanclass="math inline">\(Y\)</span>为标签集合，每一个样本的特征与标签一一对应。现在我们获取了一个新的样本<span class="math inline">\(x\)</span>，我们的目标是预测其标签 <spanclass="math inline">\(y\)</span> 属于类别 $i=1,2,,N $ 中的哪一类。</p><p>换句话说，我们想要通过已有的训练数据，计算出后验概率 <spanclass="math inline">\(P\left( y ^{\left( i \right)} \mid x\right)\)</span>，使之满足： <span class="math display">\[i =\underset{i=1,\cdots ,N}{\mathrm{arg}\max}\;P\left( y ^{\left( i\right)} \mid x \right)\]</span> 下面介绍两种模型针对此目标的思路：</p><p><img src="/img/blog/PR-Note-5-images/vs.jpg" alt="判别式模型 vs. 生成式模型" width = 70% /></p><h3 id="判别式-discriminative">判别式 | Discriminative</h3><p>如左图所示，判别式模型首先根据训练集数据，得到一个<strong>超平面</strong>将不同类别区分开，并且使得训练集上的分类效果最好，也就是分类错误的「<strong>代价</strong>」最小化！</p><p>以 <a href="https://hwcoder.top/ML-Note-4">逻辑回归</a>算法为例，我们假设边界函数为： <span class="math display">\[h_{\theta}\left( x \right) =\mathrm{sigmoid}\left( \theta _0+\theta_1x_1+\theta _2x_2+\theta _3x_1x_2+\cdots \right)\]</span> 令 <span class="math inline">\(Cost()\)</span>为交叉熵损失，则有代价函数： <span class="math display">\[J(\theta)=\frac{1}{m}\sum_{i=1}^m\text{Cost}\left(h_\theta(x^{(i)}),y^{(i)}\right)\]</span> 通过梯度下降，我们可以求解 <spanclass="math inline">\(\underset{\theta}{\arg\min }J\left( \theta\right)\)</span> ，找到训练集上最优的 <spanclass="math inline">\(\theta\)</span>，刻画出最佳的决策边界。当我们遇到一个新的样本时，只需将其输入<span class="math inline">\(h_{\theta}\left( x\right)\)</span>，就能得到一个预测值，当预测值超过一定的阈值（在 <spanclass="math inline">\(\mathrm{sigmoid}\)</span> 中是 <spanclass="math inline">\(0.5\)</span>）时，样本被预测为某一类。而这个预测值就可以用<strong>后验概率</strong>解释为：<span class="math display">\[h_{\theta}\left( x \right) = P\left( y = 1 \mid x \right)\]</span></p><h3 id="生成式-generative">生成式 | Generative</h3><p>如右图所示，生成式模型会对<strong>每一个类别建立一个模型</strong>——有多少个类别，就建立多少个模型。当获得一个新样本时，计算该样本与每一个类别的模型的<strong>联合概率分布</strong>，将样本归为联合概率最大的一类即可。</p><p>以 <a href="https://hwcoder.top/PR-Note-2">朴素贝叶斯</a>算法为例，从训练集可以学习到：</p><ul><li>先验概率：<span class="math inline">\(P\left( \omega ^{\left( j\right)} \right)\)</span>，表示对任意未知测试样例，将其归为类别 <spanclass="math inline">\(\omega ^{\left( j \right)}\)</span> 的概率。</li><li>似然概率：<span class="math inline">\(P\left( x \mid \omega ^{\left(j \right)} \right)\)</span>，表示在类别 <spanclass="math inline">\(\omega ^{\left( j \right)}\)</span>中，出现属性等同于测试样例的训练样例的概率。</li></ul><p>以上两个概率构成类别 $ ^{( j )}$的模型，利用概率乘法公式得到联合概率分布： <span class="math display">\[P(x,  \omega ^{\left( j \right)})=P\left( x \mid \omega ^{\left( j\right)} \right) P\left( \omega ^{\left( j \right)} \right)\]</span> 当然，也可以再算出后验概率： <span class="math display">\[P\left( \omega ^{\left( j \right)} \mid x \right) =\frac{P\left( x \mid\omega ^{\left( j \right)} \right) P\left( \omega ^{\left( j \right)}\right)}{P\left( x \right)}=\frac{P\left( x \mid \omega ^{\left( j\right)} \right) P\left( \omega ^{\left( j \right)}\right)}{\sum_{i=1}^N{P\left( x \mid \omega ^{\left( i \right)} \right)P\left( \omega ^{\left( i \right)} \right)}}\]</span> 选择所有类别中最大的 <span class="math inline">\(P\left(\omega ^{\left( j \right)} \mid x \right)\)</span> 作为样本的分类。</p><h3 id="小结">小结</h3><p>简单地说，判别式模型是直接对<strong>后验概率</strong>建模，而生成式模型则先对<strong>联合概率</strong>进行建模。但不管是哪种模型，在分类任务中最终还是<strong>使用后验概率进行类别选择</strong>。</p><p>显然，判别式模型更直接、更简单；而生成式模型多了联合概率的计算，更具有普适性，生成的联合概率分布还可以应用到其他场景，不局限于分类。</p><p>由生成式模型可以得到判别式模型，但由判别式模型得不到生成式模型。</p><h2 id="案例分析">案例分析</h2><p>假设有四个 samples：</p><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">sample 1</th><th style="text-align: center;">sample 2</th><th style="text-align: center;">sample 3</th><th style="text-align: center;">sample 4</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(x\)</span></td><td style="text-align: center;"><spanclass="math inline">\(0\)</span></td><td style="text-align: center;"><spanclass="math inline">\(0\)</span></td><td style="text-align: center;"><spanclass="math inline">\(1\)</span></td><td style="text-align: center;"><spanclass="math inline">\(1\)</span></td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(y\)</span></td><td style="text-align: center;"><spanclass="math inline">\(0\)</span></td><td style="text-align: center;"><spanclass="math inline">\(0\)</span></td><td style="text-align: center;"><spanclass="math inline">\(0\)</span></td><td style="text-align: center;"><spanclass="math inline">\(1\)</span></td></tr></tbody></table><p>判别式模型的视角下：<span class="math inline">\(\sum_y{P\left( y\midx \right) =1}\)</span></p><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;"><spanclass="math inline">\(y=0\)</span></th><th style="text-align: center;"><spanclass="math inline">\(y=1\)</span></th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(x=0\)</span></td><td style="text-align: center;"><spanclass="math inline">\(1\)</span></td><td style="text-align: center;"><spanclass="math inline">\(0\)</span></td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(x=1\)</span></td><td style="text-align: center;"><spanclass="math inline">\(\frac{1}{2}\)</span></td><td style="text-align: center;"><spanclass="math inline">\(\frac{1}{2}\)</span></td></tr></tbody></table><p>生成式模型的视角下：<span class="math inline">\(\sum{P\left( x,y\right) =1}\)</span></p><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;"><spanclass="math inline">\(y=0\)</span></th><th style="text-align: center;"><spanclass="math inline">\(y=1\)</span></th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(x=0\)</span></td><td style="text-align: center;"><spanclass="math inline">\(\frac{1}{2}\)</span></td><td style="text-align: center;"><spanclass="math inline">\(0\)</span></td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(x=1\)</span></td><td style="text-align: center;"><spanclass="math inline">\(\frac{1}{4}\)</span></td><td style="text-align: center;"><spanclass="math inline">\(\frac{1}{4}\)</span></td></tr></tbody></table><h3 id="分析">分析</h3><p><strong>判别式模型特点：</strong></p><ul><li>直接学习决策函数 <span class="math inline">\(y=f(x)\)</span>或条件概率 <span class="math inline">\(P\left( y\mid x\right)\)</span>；</li><li>捕捉不同类别特征的差异信息，不学习本身分布信息，无法反应数据本身特性；</li><li>学习成本较低，需要的计算资源较少；</li><li>需要的样本数可以较少，少样本也能很好学习、推断；</li><li>无法转换成生成式。</li></ul><p><strong>生成式模型的特点：</strong></p><ul><li>学习联合概率分布 <span class="math inline">\(P(x,y)\)</span>；</li><li>学习到的数据本身信息更多，能反应数据本身特性；</li><li>学习成本较高，需要更多的计算资源；</li><li>需要的样本数更多，样本较少时学习效果较差，无法对每一类准确建模；</li><li>一定条件下能转换成判别式。</li></ul><h2 id="二者所包含的算法">二者所包含的算法</h2><p><strong>判别式模型：</strong></p><ol type="1"><li>K 近邻（K-Nearest Neighbor，KNN）</li><li>线性回归（Linear Regression）</li><li>逻辑回归（Logistic Regression，LR）</li><li>神经网络（Neural Network，NN）</li><li>支持向量机（Support Vector Machines，SVM）</li><li>高斯过程（Gaussian Process）</li><li>条件随机场（Conditional Random Field，CRF）</li><li>CART（Classification and Regression Tree）</li></ol><p><strong>生成式模型：</strong></p><ol type="1"><li>朴素贝叶斯（Naive Bayes）</li><li>高斯混合模型（Gaussian Mixture Model，GMM）</li><li>隐马尔可夫模型（Hidden Markov Model，HMM）</li><li>贝叶斯网络（Bayesian Network），也称深度信念网络（Deep BeliefNetwork）</li><li>马尔可夫随机场（Markov Random Fields）</li></ol>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>模式识别</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
      <tag>PR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PR学习笔记 #4 概率密度：非参数估计</title>
    <link href="/PR-Note-4.html"/>
    <url>/PR-Note-4.html</url>
    
    <content type="html"><![CDATA[<p>上一节介绍了概率密度的 <ahref="https://hwcoder.top/PR-Note-3">参数估计</a>方法，并以最常见的正态分布为例推导了计算公式。然而，并不是所有的<strong>分布</strong>都可以提前假设的。</p><p>当我们观测样本后，发现其概率密度分布的形式不典型时，就需要用到<strong>非参数估计</strong>方法。</p><h2 id="非参数估计-nonparametric-estimation">非参数估计 | NonparametricEstimation</h2><p>非参数估计就是在概率密度<strong>形式未知</strong>，根据样本点 <spanclass="math inline">\(X_1,X_2,\cdots,X_n\)</span><strong>直接推断</strong>出概率密度<strong>函数本身</strong>的过程。</p><p>回顾前文的贝叶斯分类，我们知道如果属性 <spanclass="math inline">\(A_k\)</span> 是离散的，可以直接用 <spanclass="math inline">\(a_k\)</span>在类别中的比例估计之。这时，我们通常需要大量的样本与监督学习。</p><blockquote><p>这种思想也可以延申到连续属性的 Histogram Algorithm(直方图算法)：将连续的浮点特征值离散化为 <spanclass="math inline">\(K\)</span> 个整数，将特征值的取值范围分割为 <spanclass="math inline">\(K\)</span>个区间。遍历数据，根据浮点值落在区间的样本数在直方图中累计统计量。</p><p>这种方法在 LightGBM中得到应用，使得内存占用更小（只保存离散化后的值）和计算代价更小（决策树分裂的代价）。</p></blockquote><p>而在非参数估计中，我们也是基于<strong>统计</strong>的思想——若在概率密度函数<span class="math inline">\(p(x)\)</span> 中固定区间 <spanclass="math inline">\(R\)</span>，则随机变量 <spanclass="math inline">\(x\)</span> 落入区间 <spanclass="math inline">\(R\)</span> 的概率为： <spanclass="math display">\[P=\int_R{p\left( x \right) \mathrm{d}x}\]</span> 当区间 <span class="math inline">\(R\)</span>足够小时，我们可以认为 <span class="math inline">\(p(x)\)</span>在区间内保持不变，记<strong>区间体积</strong>为 <spanclass="math inline">\(V\)</span>，上式转化为： <spanclass="math display">\[P=\int_R{p\left( x \right) \mathrm{d}x}=p(x)V\]</span> 当然，上述概率是在已知 <spanclass="math inline">\(p(x)\)</span>的情况下求得。对于<strong>总样本数</strong>为 <spanclass="math inline">\(N\)</span>的数据集中，我们可以用<strong>落在区间中的样本数</strong> <spanclass="math inline">\(k\)</span> 来近似估计： <spanclass="math display">\[\hat{P}\approx \frac{k}{N}\]</span> 从而我们可以得到 <span class="math inline">\(p(x)\)</span>的近似估计： <span class="math display">\[\hat{p}\left( x \right) \approx \frac{k/V}{N}\]</span></p><blockquote><p>当样本数量趋于无限时，<span class="math inline">\(\hat{p}\left( x\right)\)</span> <strong>依概率收敛</strong>于 <spanclass="math inline">\(p(x)\)</span>，当且仅当满足下列条件： <spanclass="math display">\[\underset{N\rightarrow \infty}{\lim}V_N=0  ,\quad \underset{N\rightarrow\infty}{\lim}k_N=\infty ,\quad \underset{N\rightarrow\infty}{\lim}\frac{k_N}{N}=0\]</span></p></blockquote><p>下面介绍的两种方法，是非参数估计的具体实现，分别依赖于估计式中 <spanclass="math inline">\(V\)</span> 和 <spanclass="math inline">\(k\)</span>。</p><h2 id="parzen-窗法">Parzen 窗法</h2><p>Parzen 窗法的核心是：<span class="math inline">\(V\)</span>不变，<span class="math inline">\(k\)</span>可变。通过选取固定大小的「<strong>窗</strong>」，使其遍布整个空间，并估算每个窗的$( x ) $ 作为窗中心的概率密度。</p><p>假设 <span class="math inline">\(d\)</span>维空间中的<strong>样本点</strong>为 <spanclass="math inline">\(x=\left[u_{1}, u_{2}, \ldots,u_{d}\right]^{T}\)</span>，且空间中每个<strong>小窗</strong>是一个<strong>超立方体</strong>，它在每一维的棱长都为<span class="math inline">\(h\)</span>，则小窗的体积是: <spanclass="math display">\[V=h^d\]</span> 要计算每个小窗内落入的样本数目，可以定义如下的 <spanclass="math inline">\(d\)</span> 维<strong>单位方窗函数</strong>： <spanclass="math display">\[\varphi\left(\left[u_{1}, u_{2}, \ldots,u_{d}\right]^{T}\right)=\left\{\begin{array}{lc}1 &amp; \text { 若 }\left|u_{j}\right| \leqslant  \frac{1}{2}, j=1,2,\cdots, d \\0 &amp; \text { 其他 }\end{array}\right.\]</span><img src="/img/blog/PR-Note-4-images/window-1.png" alt="一维方窗函数" width=20% /></p><p>该函数在以<strong>原点</strong>为中心的 <spanclass="math inline">\(d\)</span> 维单位超立方体内取值为1，在其他地方取值为 0。对于每个样本点 <spanclass="math inline">\(x_i\)</span>，我们只需计算 $( ) $就可以知道其<strong>是否落在</strong>观测点 <spanclass="math inline">\(x\)</span> 为中心、<spanclass="math inline">\(h\)</span> 为棱长的方窗内。现在共有 <spanclass="math inline">\(N\)</span> 个样本点，那么就有： <spanclass="math display">\[k_{N}=\sum_{i=1}^{N} \varphi\left(\frac{x-x_{i}}{h}\right)\]</span> 将上式代入 <span class="math inline">\(\hat{p}\left( x \right)\approx \frac{k/V}{N}\)</span> 中有： <span class="math display">\[\hat{p}\left( x \right) \approx \frac{1}{N V} \sum_{i=1}^{N}\varphi\left(\frac{x-x_{i}}{h}\right)\]</span> 或者，我们将 <span class="math inline">\(V\)</span>放入求和号中，并定义非单位的<strong>方窗函数</strong>（<strong>核函数</strong>）如下：<span class="math display">\[K\left(x, x_{i}\right)=\frac{1}{V} \varphi\left(\frac{x-x_{i}}{h}\right)\]</span> 核函数反映了在<strong>方窗情形</strong>下，任意一个样本点<span class="math inline">\(x_i\)</span> 对于观测点 <spanclass="math inline">\(x\)</span>处的概率密度的「<strong>贡献</strong>」。而我们要计算的的概率密度函数，就可以看作是所有样本点对观测点的贡献进行平均：<span class="math display">\[\hat{p}\left( x \right) \approx \frac{1}{N} \sum_{i=1}^{N}K\left(x,x_{i}\right)\]</span></p><h3 id="核函数的选取">核函数的选取</h3><p>超立方体的棱角带来了一些数据处理上的不合理，如果改用<strong>超球体</strong>，则可以得到<strong>超球窗函数</strong>：<span class="math display">\[K\left(x, x_{i}\right)=\left\{\begin{array}{cc}V^{-1} &amp; \text { 若 }|| x-x_{i} \| \leqslant  \rho \\0 &amp; \text { 其他 }\end{array}\right.\]</span> 其中，距离的度量采用了<strong>范数</strong>，$$为超球体的半径。仅当样本点落在超球体内时，其对概率密度有贡献。</p><p>然而，方窗和超球窗都有一个不足之处：忽略了<strong>距离</strong>这一特征。所有落在区域内的样本点一视同仁，而其他点则都被忽视，这就导致了<span class="math inline">\(h\)</span> 和 <spanclass="math inline">\(\rho\)</span>的取值对结果的影响很大。宽度过大会使得<strong>分辨率</strong>变低，而宽度过小则<strong>不平滑效应</strong>明显。</p><p>因此，我们考虑用连续函数计算贡献，只需要满足以下两点：</p><ol type="1"><li>任意点处<strong>贡献不为负</strong>：<spanclass="math inline">\(K\left(x, x_{i}\right)\geqslant 0\)</span></li><li>任一样本点对整个空间的<strong>总贡献之和</strong>为 1：<spanclass="math inline">\(\int K\left(x,x_{i}\right)\mathrm{d}x=1\)</span></li></ol><p>这样可以确保最终估算出的 <span class="math inline">\(\hat{p}\left( x\right)\)</span> 的总和也为1。由此，我们引入<strong>正态窗函数</strong>（高斯窗函数）： <spanclass="math display">\[K\left(x, x_{i}\right)=\frac{1}{\sqrt{(2 \pi)^{d} \rho^{2 d}|Q|}} \exp\left\{-\frac{1}{2} \frac{\left(x-x_{i}\right)^{T}Q^{-1}\left(x-x_{i}\right)}{\rho^{2}}\right\}\]</span> 即以样本 <span class="math inline">\(x_i\)</span>为均值、协方差矩阵为 <span class="math inline">\(\Sigma=\rho^{2}Q\)</span> 的高维正态分布函数。</p><p><img src="/img/blog/PR-Note-4-images/window-2.png" alt="一维正态窗函数" width=20% /></p><p>同理，还有指数窗函数： <span class="math display">\[K\left(x, x_{i}\right)=\exp \left\{-\frac{1}{2} || x-x_{i} \|\right\}\]</span><img src="/img/blog/PR-Note-4-images/window-3.png" alt="一维指数窗函数" width=20% /></p><h2 id="knn-法">KNN 法</h2><p>K 近邻法的核心是：<span class="math inline">\(k\)</span> 不变，<spanclass="math inline">\(V\)</span>可变。固定每一个「窗」中落入的<strong>样本数</strong>，而窗本身的<strong>体积可变化</strong>，并估算每个窗的$( x ) $ 作为窗中心的概率密度。</p><p>假设 <span class="math inline">\(d\)</span>维空间中的<strong>样本点</strong>为 <spanclass="math inline">\(x=\left[u_{1}, u_{2}, \ldots,u_{d}\right]^{T}\)</span> ，具体的做法是先选定一个 <spanclass="math inline">\(k_N\)</span>，再在点 <spanclass="math inline">\(x\)</span> 的周围，不断扩大窗的体积，直到捕捉到<span class="math inline">\(k_N\)</span> 个样本为止。</p><blockquote><p>通常取 <spanclass="math inline">\(k_N=\sqrt{N}\)</span>，窗的形状选择超球窗，半径为第<span class="math inline">\(k_N\)</span> 个近邻点到样本点的距离。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>模式识别</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
      <tag>PR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PR学习笔记 #3 概率密度：参数估计</title>
    <link href="/PR-Note-3.html"/>
    <url>/PR-Note-3.html</url>
    
    <content type="html"><![CDATA[<p>在前文的 <a href="https://hwcoder.top/PR-Note-2">贝叶斯分类器</a>中，我们简单介绍了先验概率 <span class="math inline">\(P\left( \omega^{\left( j \right)} \right)\)</span> 和似然概率 <spanclass="math inline">\(P\left( x \mid \omega ^{\left( j \right)}\right)\)</span> 的估计方法。</p><p>其中似然概率，也称为<strong>类条件概率密度</strong>（ClassConditional ProbabilityDensity），即分布<strong>取决于类别状态</strong>的一种概率密度。</p><p>抛开「类别」不谈，「似然概率的估计」其实就是一个「<strong>概率密度的估计</strong>」问题。下面我们将介绍，对于连续取值的<spanclass="math inline">\(x\)</span>，如何刻画一个已知形式的概率密度函数。</p><h2 id="参数估计-parameter-estimation">参数估计 | ParameterEstimation</h2><p>参数估计是在概率密度的<strong>形式已知</strong>（假设其服从某种分布）的情况下，根据样本点<span class="math inline">\(X_1,X_2,\cdots,X_n\)</span>推断出其中未知参数 <span class="math inline">\(\theta\)</span>的过程。</p><p>用来估计未知参数 <span class="math inline">\(\theta\)</span>的<strong>统计量</strong> <spanclass="math inline">\(\hat{\theta}=\hat{\theta}\left(X_{1}, X_{2},\cdots, X_{n}\right)\)</span> 称为 <spanclass="math inline">\(\theta\)</span> 的<strong>点估计</strong>（PointEstimation），如果只是估计 <span class="math inline">\(\theta\)</span>的取值范围，也称<strong>区间估计</strong>（Interval Estimation）。</p><blockquote><p>在贝叶斯分类器中，每个类别都有自己对应的似然概率，因此<strong>每个类别</strong>都需要估计其概率密度函数（包括<span class="math inline">\(x\)</span>离散和连续的情形，本节只介绍连续情形，离散同理）。</p><p>此外，在有限数目的样本中，样本所属的类别还可能是未知的，即无监督估计，本节也不加以讨论。</p></blockquote><p>下面我们以<strong>一维正态分布</strong> <spanclass="math inline">\(N(\mu,\sigma^2)\)</span>的参数估计为例，介绍三种方法：矩估计、极大似然估计、贝叶斯估计。</p><blockquote><p>对于高维的 <spanclass="math inline">\(X\)</span>，即当每个样本有多个特征时，可以分别求出各个特征的似然概率，再朴素地相乘得到<span class="math inline">\(X\)</span>的似然概率；也可以假设其符合<strong>高维正态分布</strong>： <spanclass="math display">\[P\left( X\mid \omega ^{\left( j \right)} \right) =\frac{1}{(2\pi)^{\frac{d}{2}}|\mathbf{\Sigma }|^{\frac{1}{2}}}\exp \left[-\frac{1}{2}(X-\mu )^T\mathbf{\Sigma }^{-1}(X-\mu ) \right]\]</span></p><p>其中 <span class="math inline">\(\mathbf{\Sigma }\)</span>代表<strong>协方差矩阵</strong>，一个对称矩阵，对角线外的元素表示了<span class="math inline">\(X\)</span>的各列分量之间的协方差（线性相关程度），决定了高维正态分布的形状。</p></blockquote><h2 id="矩估计-moment-estimation">矩估计 | Moment Estimation</h2><p>矩估计是最原始最直观的估计方法，首先介绍有关「矩」的两组定义：</p><blockquote><p>设 <span class="math inline">\(X\)</span> 为随机变量，<spanclass="math inline">\(k\)</span> 为整数，设 <spanclass="math inline">\(\mathbb{E} X^{k}\)</span>，<spanclass="math inline">\(\mathbb{E}(X-\mathbb{E} X)^{k}\)</span>存在且有限，则我们分别称 <span class="math display">\[\mu_{k}=\mathbb{E} X^{k}, \quad \sigma_{k}=\mathbb{E}(X-\mathbb{E}X)^{k}\]</span> 为 <span class="math inline">\(X\)</span> 的 <spanclass="math inline">\(k\)</span> 阶总体（原点) 矩，<spanclass="math inline">\(k\)</span> 阶总体中心矩。</p><p>设 <span class="math inline">\(X_{1}, X_{2}, \ldots, X_{n}\)</span>为总体 <span class="math inline">\(X\)</span> 的一个样本，<spanclass="math inline">\(k\)</span> 为整数，则我们分别称 <spanclass="math display">\[A_{k}=\frac{1}{n} \sum_{i=1}^{n} X^{k}, \quad B_{k}=\frac{1}{n}\sum_{i=1}^{n}(X-\bar{X})^{k}\]</span> 为 <span class="math inline">\(X\)</span> 的 <spanclass="math inline">\(k\)</span> 阶样本 (原点) 矩，<spanclass="math inline">\(k\)</span> 阶样本中心矩。</p></blockquote><p>我们知道当 <span class="math inline">\(X\)</span> 的概率密度函数为<span class="math inline">\(f(x,\theta)\)</span> 时，<spanclass="math inline">\(k\)</span> 阶总体矩可以由参数决定： <spanclass="math display">\[\mu_{k}=\mathbb{E} X^{k}=\int x^{k} f(x, \theta) d x\]</span> 反之，通常参数 <span class="math inline">\(\theta\)</span>也可以表示为各阶总体矩 <span class="math inline">\(\mu_k\)</span>的函数，而根据大数定律，样本矩 <span class="math inline">\(A_k\)</span><strong>依概率收敛于</strong>总体矩 <spanclass="math inline">\(\mu_{k}\)</span>。因此参数可以用<strong>各阶样本矩</strong>来估计：<span class="math display">\[\hat{\theta}_{n}:=\theta\left(A_{1}, A_{2}, \cdots, A_{m}\right)\]</span> ### 正态分布的矩估计</p><p>现在回到正态分布 <span class="math inline">\(N(\mu,\sigma^2)\)</span>的问题，注意到总体矩： <span class="math display">\[\mu_{1}=\mathbb{E} X=\mu, \quad \mu_{2}=\mathbb{E}X^{2}=\mu_{1}^{2}+\sigma^{2}\]</span> 提出参数 <span class="math inline">\(\mu\)</span> 和 <spanclass="math inline">\(\sigma\)</span>，<strong>用样本矩代替总体矩</strong>，可以得到参数的矩估计量为：<span class="math display">\[\begin{aligned}\hat{\mu}&amp;=A_{1}=\bar{X} \\\hat{\sigma}^{2}&amp;=A_{2}-A_{1}^{2}=\frac{1}{n} \sum_{i=1}^{n}X_{i}^{2}-\bar{X}^{2}=\frac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\end{aligned}\]</span></p><h2 id="极大似然估计-maximum-likelihood-estimation">极大似然估计 |Maximum Likelihood Estimation</h2><p>极大似然估计的思想：参数 <span class="math inline">\(\theta\)</span>应是使得<strong>事件组</strong>最容易发生的值。因此对于已发生的事件组<spanclass="math inline">\((X_1,X_2,\cdots,X_n)=(x_1,x_2,\cdots,x_n)\)</span>，我们知道其发生的<strong>联合</strong>概率函数为：<span class="math display">\[\prod_{i=1}^{n} f\left(x_{i}, \theta\right)\]</span> 而我们要做的就是找出参数 <spanclass="math inline">\(\theta\)</span>，使得上述概率<strong>最大化</strong>。因此我们将之看作<span class="math inline">\(\theta\)</span>的函数，称之为<strong>极大似然函数</strong>： <spanclass="math display">\[L(\theta) = \prod_{i=1}^{n} f\left(x_{i}, \theta\right)\]</span>要求上式的极大值点，我们考虑其<strong>对数</strong>形式简化计算： <spanclass="math display">\[l(\theta)=\ln L(\theta)=\sum_{i=1}^{n} \ln f\left(x_{i}, \theta\right)\]</span>此时，我们可以用微积分方法求极值，即考虑求解<strong>极大似然方程</strong>：<span class="math display">\[\frac{\partial \ln l}{\partial \theta}=0\]</span> ### 正态分布的极大似然估计</p><p>现在回到正态分布 <span class="math inline">\(N(\mu,\sigma^2)\)</span>的问题，其似然函数为： <span class="math display">\[\begin{aligned}L\left(\mu, \sigma^{2}\right)&amp;=\prod_{i=1}^{n}\left[\frac{1}{\sqrt{2 \pi} \sigma} \exp\left(-\frac{\left(x_{i}-\mu\right)^{2}}{2 \sigma^{2}}\right)\right] \\&amp;=\left(2 \pi \sigma^{2}\right)^{-\frac{n}{2}} \exp\left(-\frac{1}{2 \sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}\right)\end{aligned}\]</span> 考虑 <span class="math inline">\(L\)</span> 的对数，有： <spanclass="math display">\[\ln L\left(\mu, \sigma^{2}\right)=-\frac{n}{2} \ln \left(2 \pi\sigma^{2}\right)-\frac{1}{2 \sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}\]</span> 于是，似然方程组为： <span class="math display">\[\left\{\begin{array}{l}\begin{aligned}\frac{\partial \ln L}{\partial \mu}=\frac{1}{\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)=0\end{aligned}\\\begin{aligned}\frac{\partial \ln L}{\partial \sigma^{2}}=-\frac{n}{2\sigma^{2}}+\frac{1}{2 \sigma^{4}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}=0\end{aligned}\end{array}\right.\]</span> 解得： <span class="math display">\[\begin{aligned}&amp;\hat{\mu}=\frac{1}{n} \sum_{i=1}^{n} x_{i}=\bar{x} \\&amp;\hat{\sigma}^{2}=\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\end{aligned}\]</span></p><h2 id="贝叶斯估计-bayes-estimation">贝叶斯估计 | Bayes Estimation</h2><p>贝叶斯估计的思想：将未知参数当作随机变量，不关注其值为多少，而是对其取值赋予相应的<strong>概率</strong>。如贝叶斯分类一般，我们首先假设参数<span class="math inline">\(\theta\)</span>在其<strong>取值空间</strong> $$ 上具有<strong>先验分布</strong> $( )$。</p><p>再从其样本集 <span class="math inline">\(X\)</span>中得到<strong>样本联合分布</strong>（似然分布）： <spanclass="math display">\[\rho \left( X\mid \theta \right) =\prod_{i=1}^n{\rho \left( x_i\mid\theta \right)}=\prod_{i=1}^n{f\left( x_i,\theta \right)}\]</span> &gt;注：这里好像有点循环论证的感觉了？但此时的似然分布用的是条件独立的概率乘法公式求得。</p><p>根据贝叶斯公式可以计算<strong>后验分布</strong>： <spanclass="math display">\[\rho (\theta \mid X)=\frac{\rho (X\mid \theta )\rho (\theta)}{\int_{\Theta}{\rho}(X\mid \theta )\rho (\theta)\mathrm{d}\theta}=\frac{\prod_{i=1}^n{\rho}\left( x_i\mid \theta\right) \rho (\theta )}{\int_{\Theta}{\prod_{i=1}^n{\rho}}\left( x_i\mid\theta \right) \rho (\theta )\mathrm{d}\theta}\]</span> 此后，我们可将 $arg{ ( X ) } $作为参数估计，也可借鉴<strong>最小风险</strong>的思想，假设决策的<strong>损失函数</strong>为$( ,) $，定义样本 <span class="math inline">\(x\)</span>下的<strong>条件风险</strong>： <span class="math display">\[R(\hat{\theta} \mid x)=\int_{\Theta} \lambda(\hat{\theta}, \theta)\rho(\theta \mid x)\mathrm{d}\theta\]</span> 那么，当样本 <span class="math inline">\(x\)</span>取遍<strong>样本空间</strong> <span class="math inline">\(E^d\)</span>时，其<strong>总体期望风险</strong>为： <span class="math display">\[R=\int_{E^{d}} R(\hat{\theta} \mid x) \rho(x) \mathrm{d}x\]</span> 但是实际中我们无法取遍 <spanclass="math inline">\(E^d\)</span>，只能对已有样本 <spanclass="math inline">\(X=(x_1,x_2,\cdots,x_n)\)</span>求其条件风险的最小化，即： <span class="math display">\[\theta^{*}=\arg \min _{\hat{\theta}} R(\hat{\theta} \mid X)=\arg \min_{\hat{\theta}} \int_{\Theta} \lambda(\hat{\theta}, \theta) \rho(\theta\mid X) \mathrm{d}\theta\]</span>在决策分类时，需要事先定义代价表，而在连续情况下，需要定义代价函数。最常用的则是平方误差代价函数，即：<span class="math display">\[\lambda(\hat{\theta}, \theta)=(\theta-\hat{\theta})^{2}\]</span> 经一系列计算可得，当参数估计量 <spanclass="math inline">\(\theta^{*}\)</span> 为样本下 <spanclass="math inline">\(\theta\)</span> 的条件期望时，代价最小，即： <spanclass="math display">\[\theta^{*}=\mathbb{E}\left[ \theta \mid X \right] =\int_{\Theta} \theta\rho(\theta \mid X) \mathrm{d}\theta\]</span></p><blockquote><p>贝叶斯估计较为复杂，就不展开论证正态分布的贝叶斯估计了，感兴趣的读者可以查阅这篇<ahref="https://blog.csdn.net/linjing_zyq/article/details/120393298">博客文章</a>。这里给出结论：当样本数目趋于无穷多时，贝叶斯估计的结果与极大似然估计相同。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>模式识别</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
      <tag>PR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #03 正规方程：多元线性回归</title>
    <link href="/ML-Note-3.html"/>
    <url>/ML-Note-3.html</url>
    
    <content type="html"><![CDATA[<p>前文提到线性回归问题时，我们说在数学上也可以用最小二乘法（LeastSquareMethod）来解决，实际上其思路也是<strong>最小化</strong>平方误差代价函数（也称<strong>残差函数</strong>）。</p><p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，最小二乘法的矩阵解法——正规方程则是更好的解决方案。</p><h2 id="正规方程-normal-equation">正规方程 | Normal Equation</h2><p>利用多元微分学知识，我们知道对于代价函数：</p><p><span class="math display">\[J(\theta)=J(\theta_0, \theta_1,\cdots, \theta_n)\]</span>如果它是<strong>连续</strong>的，则要求出它的最小值，只需要令各偏导为零：<span class="math display">\[\frac{\partial J}{\partial \theta_j}=0,\quad j=0,1,\cdots,n\]</span> 或写作向量形式： <span class="math display">\[\frac{\partial J}{\partial \theta}=\vec 0\]</span></p><p>就能解出令 <span class="math inline">\(J(\theta)\)</span> 最小化的<span class="math inline">\(\theta\)</span> 值。</p><p>由此，我们将代价函数转化为<strong>有确定解的代数方程组</strong>（其方程式数目正好等于未知数的个数），这个方程组就是正规方程（NormalEquation）。</p><h3 id="数学推导-1">数学推导 1</h3><p>下面我们就对多元线性回归的代价函数进行求解：</p><p><span class="math display">\[J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)^2\]</span> 于是其偏导函数为： <span class="math display">\[\frac{\partial J}{\partial\theta}=\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}\]</span> 要使之为<strong>零向量</strong>，只能是： <spanclass="math display">\[\theta^Tx^{(i)}=y^{(i)},\quad i=1,2,\cdots,m\]</span> 恒成立。写作矩阵为： <span class="math display">\[\theta ^TX^T=y^T\text{ ，或 }X\theta =y\]</span> 其中， <span class="math display">\[X_{(n+1)\times m}=\left[ \begin{matrix}    x_{0}^{(1)}&amp;        x_{1}^{(1)}&amp;        \cdots&amp;     x_{n}^{(1)}\\    x_{0}^{(2)}&amp;        x_{1}^{(2)}&amp;        \cdots&amp;     x_{n}^{(2)}\\    \vdots&amp;     \vdots&amp;     \ddots&amp;     \vdots\\    x_{0}^{(m)}&amp;        x_{1}^{(m)}&amp;        \cdots&amp;     x_{n}^{(m)}\\\end{matrix} \right] =\left[ \begin{array}{c}    {x^{(1)}}^T\\    {x^{(2)}}^T\\    \vdots\\    {x^{(m)}}^T\\\end{array} \right] ,\quad y=\left[ \begin{array}{c}    y^{(1)}\\    y^{(2)}\\    \vdots\\    y^{(m)}\\\end{array} \right]\]</span> 两边同时乘以 <span class="math inline">\(X^T\)</span>，假设<span class="math inline">\(X^TX\)</span> 可逆，解得： <spanclass="math display">\[\theta=(X^TX)^{-1}X^Ty\]</span> ### 数学推导 2</p><p>前面的推导中，在<strong>向量形式</strong>的偏导函数中发现了简化条件，将零向量提出来单独求解。下面介绍另一个<strong>纯矩阵形式</strong>的解法。</p><p>首先将代价函数表示为： <span class="math display">\[\begin{aligned}    J(\theta )&amp;=\frac{1}{2m}\left( X\theta -y \right) ^T\left(X\theta -y \right)\\    &amp;=\frac{1}{2}\left( \theta ^TX^T-y^T \right) \left( X\theta -y\right)\\    &amp;=\frac{1}{2}\left( \theta ^TX^TX\theta -\theta^TX^Ty-y^TX\theta +y^Ty \right)\\\end{aligned}\]</span> 接下来对 <span class="math inline">\(J(\theta )\)</span>求偏导，需要用到<strong>矩阵的求导法则</strong>（证明过程略去不表）：</p><blockquote><ol type="1"><li><p>当 <span class="math inline">\(f(x) = Ax\)</span> 时， <spanclass="math display">\[\frac{\partial f (x)}{\partial x^T}  = \frac{\partial (Ax)}{\partialx^T}  =A\]</span></p></li><li><p>当 <span class="math inline">\(f(x) = x^TAx\)</span> 时， <spanclass="math display">\[\frac{\partial f (x)}{\partial x}  = \frac{\partial (x^TAx)}{\partialx}  =Ax+A^Tx\]</span></p></li><li><p>当 <span class="math inline">\(f(x) = a^Tx\)</span> 时， <spanclass="math display">\[\frac{\partial a^Tx}{\partial x}  = \frac{\partial x^Ta}{\partial x}  =a\]</span></p></li><li><p>当 <span class="math inline">\(f(x) = x^TAy\)</span> 时， <spanclass="math display">\[\frac{\partial x^TAy}{\partial x}  = Ay\]</span></p></li></ol></blockquote><p>分别用法则 2、4、3 求导，得到： <span class="math display">\[\begin{aligned}    \frac{\partial J\left( \theta \right)}{\partial\theta}&amp;=\frac{1}{2m}\left( 2X^TX\theta -X^Ty-(y^TX)^T+0 \right)\\    &amp;=\frac{1}{2m}\left( 2X^TX\theta -X^Ty-X^Ty+0 \right)\\    &amp;=\frac{1}{m}\left(X^TX\theta -X^Ty\right)\\\end{aligned}\]</span> 令偏导为零，解得： <span class="math display">\[\theta =\left( X^TX \right) ^{-1}X^Ty\]</span></p><h3 id="梯度下降-vs.-正规方程">梯度下降 vs. 正规方程</h3><p>观察到在正规方程的结果中，<span class="math inline">\(X^TX\)</span>是一个 <span class="math inline">\((n+1)\times(n+1)\)</span>的矩阵，因此直接取逆计算 <span class="math inline">\(\theta\)</span>的复杂度是 <span class="math inline">\(O(n^3)\)</span> 。如果 <spanclass="math inline">\(n\)</span> 不是很大，这是有效的，但是如果 <spanclass="math inline">\(n\)</span> 达到了 <spanclass="math inline">\(10^4,10^5\)</span>或更高，就需要使用梯度下降了。</p><p>下面从其他方面对两种算法进行比较：</p><table><thead><tr class="header"><th style="text-align: center;">区别</th><th style="text-align: center;">梯度下降</th><th style="text-align: center;">正规方程</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">学习率 <spanclass="math inline">\(\alpha\)</span></td><td style="text-align: center;">需要选择</td><td style="text-align: center;">不需要</td></tr><tr class="even"><td style="text-align: center;">迭代</td><td style="text-align: center;">需要多次迭代</td><td style="text-align: center;">一次运算得出</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(n\)</span>的取值</td><td style="text-align: center;">当 <spanclass="math inline">\(n\)</span> 大时也能较好适用</td><td style="text-align: center;">当 <spanclass="math inline">\(n\)</span> 小于时 <spanclass="math inline">\(10^4\)</span> 还是可以接受的</td></tr><tr class="even"><td style="text-align: center;">特征缩放</td><td style="text-align: center;">特征取值范围相差大时需要</td><td style="text-align: center;">不需要缩放</td></tr><tr class="odd"><td style="text-align: center;">适用情形</td><td style="text-align: center;">适用于各种类型的模型</td><td style="text-align: center;">只适用于线性模型</td></tr></tbody></table><blockquote><p>这里提及适用情形，是因为随着问题的深入，算法将越发复杂。例如在分类算法中的逻辑回归等模型，就无法使用正规方程求解。</p></blockquote><h3 id="代码实现">代码实现</h3><p>下面仍以 <a href="https://www.coursera.org/">Coursera</a>上的多元线性回归数据集 <code>ex1data2.txt</code>为例实现，代码非常简洁：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># load data, data.shape = (47, 3)</span><br>data = np.genfromtxt(<span class="hljs-string">&quot;ex1data2.txt&quot;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>X = np.c_[np.ones(m), data[:, :-<span class="hljs-number">1</span>]]<br>y = data[:, -<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># Normal Equation</span><br>theta = np.linalg.inv(X.T @ X) @ X.T @ y<br><span class="hljs-built_in">print</span>(theta)<br><br><span class="hljs-comment"># predict</span><br>predict = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">1650</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(predict @ theta)<br></code></pre></td></tr></table></figure><p>很快就计算完了，预测在 $( x_1=1650,x_2=3 ) $ 时的房价为293081.46433489426。大约相当于 3000 次梯度下降迭代的精度。</p><h3 id="不可逆情形">不可逆情形</h3><p>前一节的推导基于 <span class="math inline">\(X^TX\)</span>可逆（Invertible）的假设，如若不可逆（Non-invertible，也称Singular），我们只需将代码中的 <code>inv()</code> 换成<code>pinv()</code> 求出<strong>伪逆矩阵</strong>即可。</p><p>通常导致矩阵不可逆的原因可能有：</p><ul><li>存在冗余特征（特征之间不相互独立）；</li><li>特征数 <span class="math inline">\(n\)</span> 远大于样本数 <spanclass="math inline">\(m\)</span>（样本数不足以刻画这么多特征）。</li></ul><p>解决方法对应为：</p><ul><li>删除冗余特征（线性相关特征只保留其一）；</li><li>削减非必要的特征，或<strong>正则化</strong>方法（Regularization），后文将介绍。</li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #02 梯度下降：多元线性回归</title>
    <link href="/ML-Note-2.html"/>
    <url>/ML-Note-2.html</url>
    
    <content type="html"><![CDATA[<p>在前文 <a href="https://hwcoder.top/ML-Note-1">一元线性回归</a>的基础上，我们引入多个特征变量，探讨梯度下降对多元线性回归的解法。此外，下一节将介绍正规方程在解多元线性回归中的应用。</p><h2 id="多元线性回归-multiple-linear-regression">多元线性回归 | MultipleLinear Regression</h2><p>现在我们的样本点 <span class="math inline">\(\left(x^{(i)},y^{(i)}\right)\)</span>有多个特征作为<strong>输入变量</strong>，即给定的数据集为： <spanclass="math display">\[\left\{\left(x_1^{(i)},x_2^{(i)},\cdots,x_n^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\]</span> - <span class="math inline">\(n\)</span>代表单个样本的特征数量； - <spanclass="math inline">\({x}^{(i)}\)</span> 代表第 <spanclass="math inline">\(i\)</span> 个观察实例的<strong>特征向量</strong>；- <span class="math inline">\(x^{(i)}_j\)</span> 代表第 <spanclass="math inline">\(i\)</span> 个观察实例的第 <spanclass="math inline">\(j\)</span> 个<strong>特征分量</strong>。</p><p>同时，回归方程 <span class="math inline">\(h\)</span> 也具有多个参数<span class="math inline">\(\theta_0,\theta_1,\cdots,\theta_n\)</span>：<span class="math display">\[h_\theta(x)=\theta_0+\theta_1x_1\cdots+\theta_nx_n\]</span> 为简化表达式，这里假定 <span class="math inline">\(x_0 \equiv1\)</span> ，并以<strong>向量</strong>（vector）表示参数和自变量：<spanclass="math inline">\(\theta=(\theta_0,\cdots,\theta_n)^T,\;x=(x_0,\cdots,x_n)^T\)</span>，得到：<span class="math display">\[h_\theta(x)=\theta^Tx\]</span></p><h3 id="多变量梯度下降">多变量梯度下降</h3><p>类似地，我们定义平方误差代价函数： <span class="math display">\[J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)^2\]</span>我们的目标和一元线性回归中一样，要找出使得代价函数最小的一系列参数。于是，<span class="math display">\[\frac{\partial J}{\partial\theta}=\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}\]</span> 梯度下降时，不断作迭代： <span class="math display">\[\theta:=\theta-\alpha\cdot\frac{\partial J}{\partial \theta}\]</span> 即可。</p><h3 id="特征缩放与标准化-standardization">特征缩放与标准化 |Standardization</h3><p>当不同自变量取值范围相差较大时，绘制的<strong>等高线图</strong>上的椭圆会变得瘦长，而梯度下降<strong>算法收敛</strong>将会很慢，因为每一步都可能会跨过这个椭圆导致<strong>振荡</strong>。这里略去数学上的证明。同理，所有依赖于「<strong>距离计算</strong>」的机器学习算法也会有此问题。</p><p>此时，我们需要把所有<strong>自变量</strong>（除了假定的 <spanclass="math inline">\(x_0\)</span>）进行缩放、标准化，使其落在 -1 到 1之间。最简单的方法是，置： <span class="math display">\[x_i^{(j)}:=\frac{x_i^{(j)}-\mu_i}{\sigma_i}\]</span> 其中，<spanclass="math inline">\(\mu_i=\frac{1}{m}\sum\limits_{j=1}^mx_i^{(j)}\)</span> 是样本<strong>均值</strong>（Mean Value），<spanclass="math inline">\(\sigma_i=\sqrt{\frac{\sum\limits_{j=1}^m\left(x_i^{(j)}-\mu_i\right)^2}{m-1}}\)</span>是样本<strong>无偏标准差</strong>（Unbiased StanderdDeviation），就完成了<strong>标准化</strong>（Standardization）。标准化后样本均值为0，方差为1，但不一定是标准正态分布（与其原始分布有关），根据中心极限定理可以推出。</p><p>需要注意的是，<strong>因变量</strong>不需要标准化，否则计算的结果将失真。且如果进行了标准化，对所有<strong>待测</strong>样本点也需要进行一样的操作，参数才能生效。</p><blockquote><p>此外，线性回归并不适用于所有情形，有时我们需要曲线来适应我们的数据，这时候我们也要对特征进行<strong>构造</strong>，如二次函数、三次函数、幂函数、对数函数等。构造后的新变量就可以当作一个新的特征来使用，这就是<strong>多项式回归</strong>（PolynomialRegression）。新变量的取值范围可能更大，此时，特征缩放就非常有必要！</p></blockquote><h3 id="归一化-normalization">归一化 | Normalization</h3><p>人们经常会混淆标准化（Standardization）与<strong>归一化</strong>（Normalization）的概念，这里也简单提一下：归一化的目的是找到某种映射关系，将原数据<strong>固定映射</strong>到某个区间<span class="math inline">\([a,b]\)</span> 上，而标准化则没有限制。</p><p>归一化最常用于把有量纲数转化为<strong>无量纲数</strong>，让不同维度之间的特征在数值上有一定比较性，比如Min-Max Normalization： <span class="math display">\[x_{i}^{(j)}:=\frac{x_{i}^{(j)}-\min \left( x_i \right)}{\max \left( x_i\right) -\min \left( x_i \right)}\]</span> 又比如 Mean Normalization： <span class="math display">\[x_{i}^{(j)}:=x_{i}^{(j)}-\mu _i\]</span>但是，在机器学习中，标准化是更常用的手段，归一化的应用场景是有限的。因为「<strong>仅由极值决定</strong>」这个做法过于危险，如果样本中有一个异常大的值，则会将所有正常值挤占到很小的区间，而标准化方法则更加「弹性」，会兼顾所有样本。</p><h3 id="学习率-alpha">学习率 <spanclass="math inline">\(\alpha\)</span></h3><p>上一节谈到，学习率（Learnigrate）的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。通过绘制<strong>迭代收敛曲线</strong>（ConvergenceGraph）可以看出学习率的好坏，也可以看出何时算法能收敛并及时<strong>终止算法</strong>。</p><p><img src="/img/blog/ML-Note-2-images/cost-iter.png" alt="代价函数-迭代次数" width=50% /></p><p>特别地，当 <span class="math inline">\(\alpha\)</span>取值过大时，曲线可能呈现<strong>上扬</strong>或<strong>波浪线</strong>型，解决办法都是选择更小的<span class="math inline">\(\alpha\)</span> 值。可以证明，只要 <spanclass="math inline">\(\alpha\)</span> 足够小，凸函数都会收敛于极点。</p><p>此外，还有一种终止算法的方法：判断在某次或连续 <spanclass="math inline">\(n\)</span> 次迭代后 <spanclass="math inline">\(J(\theta)\)</span> 的变化小于某个极小量，如 <spanclass="math inline">\(\varepsilon=1e^{-3}\)</span>，此时就可以认为算法终止。但这种办法则不能用于选择尽量大的<span class="math inline">\(\alpha\)</span> 值。</p><h3 id="代码实现">代码实现</h3><p>下面以 <a href="https://www.coursera.org/">Coursera</a>上的多元线性回归数据集 <code>ex1data2.txt</code> 为例实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (47, 3)</span><br>data = np.genfromtxt(<span class="hljs-string">&quot;ex1data2.txt&quot;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>X = data[:, :-<span class="hljs-number">1</span>]<br>y = data[:, -<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># normalization</span><br>mu = X.mean(axis=<span class="hljs-number">0</span>)<br>sigma = X.std(axis=<span class="hljs-number">0</span>, ddof=<span class="hljs-number">1</span>)<br>X = (X - mu) / sigma<br>X = np.c_[np.ones(m), X] <span class="hljs-comment"># 增加一列 1</span><br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">1500</span><br>theta = np.zeros(n)<br>J_history = np.zeros(num_iters)<br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>error = (X @ theta).flatten() - y  <span class="hljs-comment"># error.shape = (47, )</span><br>theta -= (alpha / m) * np.<span class="hljs-built_in">sum</span>(X * error[:, np.newaxis], <span class="hljs-number">0</span>)<br>J_history[i] = np.<span class="hljs-built_in">sum</span>(np.power(error, <span class="hljs-number">2</span>)) / (<span class="hljs-number">2</span> * m)<br><br><span class="hljs-comment"># predict</span><br>predict = (np.array([<span class="hljs-number">1650</span>, <span class="hljs-number">3</span>]) - mu) / sigma<br>predict = np.r_[<span class="hljs-number">1</span>, predict]<br><span class="hljs-built_in">print</span>(predict @ theta)<br><br><span class="hljs-comment"># plot the convergence graph</span><br>plt.plot(np.arange(J_history.size), J_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Number of iterations&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Cost J&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>得到的 <span class="math inline">\(\left( \theta_0, \theta_1,\theta_2 \right)\)</span> 结果是：[340412.6595 110631.0484-6649.4724]，预测在 $( x_1=1650,x_2=3 ) $ 时的房价为293101.0568574823。</p><p>绘制的迭代收敛曲线如下：</p><p><img src="/img/blog/ML-Note-2-images/Figure_1.png" alt="多元线性回归的迭代收敛曲线" width=50% /></p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PR学习笔记 #2 贝叶斯分类器</title>
    <link href="/PR-Note-2.html"/>
    <url>/PR-Note-2.html</url>
    
    <content type="html"><![CDATA[<p>在 <a href="https://hwcoder.top/IR-Note-4/">IR学习笔记 #4概率模型</a> 中曾提到朴素贝叶斯（NaiveBayes）的应用，这里正式学到。贝叶斯分类器基于概率论的原理，是最经典的分类算法之一。其解决的核心点在于根据<strong>已有概率信息</strong>，对未知事物发生结果的概率计算。</p><p><img src="/img/blog/PR-Note-2-images/bayes.jpg" alt="贝叶斯分类器" width = 50% /></p><h2 id="朴素贝叶斯-naive-bayes">朴素贝叶斯 | Naive Bayes</h2><p>概率论中有许多易混淆的<strong>数学表述</strong>，这里列举：</p><ul><li>条件概率公式：<span class="math inline">\(\begin{aligned} P(A \midB)=\frac{P(A B)}{P(B)}\end{aligned}\)</span></li><li>概率乘法公式：<span class="math inline">\(P(A B)=P(A \mid B)P(B)\)</span></li><li>全概率公式：<span class="math inline">\(\begin{aligned}P(A)=\sum_{i=1}^{n} P\left(A B_{i}\right)=\sum_{i=1}^{n} P\left(A \midB_{i}\right) P\left(B_{i}\right)\end{aligned}\)</span></li><li>贝叶斯公式：<span class="math inline">\(\begin{aligned} P\left(B_i\mid A \right) =\frac{P\left( AB_i \right)}{P\left( A\right)}=\frac{P\left( A\mid B_i \right) P\left( B_i\right)}{\sum_{i=1}^n{P\left( A\mid B_i \right) P\left( B_i\right)}}\end{aligned}\)</span></li></ul><h3 id="算法原理">算法原理</h3><p>下面引入训练集的定义： <span class="math display">\[D=\left\{ x^{\left( i \right)},i=1,2,\cdots ,m \right\} ,\quad C=\left\{x^{(i)}\in \omega ^{\left( j \right)},\;j=1,2,\cdots ,N \right\}\]</span></p><ul><li><span class="math inline">\(m\)</span>代表训练集中样本点的数量；</li><li><span class="math inline">\(x^{(i)}\)</span> 代表第 <spanclass="math inline">\(i\)</span> 个样本点，是一个 <spanclass="math inline">\(n\)</span> 维向量，<spanclass="math inline">\(n\)</span> 代表特征数；</li><li><span class="math inline">\(N\)</span>代表类别数，所有训练集中样本点被显式地标注；</li><li>样本向量展开为 $x^{( i )}={ A_1=a_{i1},A_2=a_{i2},,A_n=a_{in} }$。</li></ul><p>贝叶斯分类器要解决的问题就是，利用上述数据集，将一个新的测试样例 $x={A_1=a_{1},A_2=a_{2},,A_n=a_{n} } $ 归为 <spanclass="math inline">\(N\)</span>中的某一类。在这个基础上，我们有如下定义：</p><ul><li>先验概率：<span class="math inline">\(P\left( \omega ^{\left( j\right)}\right)\)</span>，表示对<strong>任意未知</strong>测试样例，将其归为类别<span class="math inline">\(\omega ^{\left( j \right)}\)</span>的概率，常用训练集中类别 <span class="math inline">\(\omega ^{\left( j\right)}\)</span> 占 <span class="math inline">\(m\)</span>的比例估计。满足 <span class="math inline">\(\sum_{j=1}^N{P\left( \omega^{\left( j \right)} \right)}=1\)</span>。</li><li>后验概率：<span class="math inline">\(P\left( \omega ^{\left( j\right)} \mid x\right)\)</span>，表示对<strong>已知特征</strong>测试样例，将其归为类别<span class="math inline">\(\omega ^{\left( j \right)}\)</span>的概率，就是贝叶斯分类器所要求的。满足 <spanclass="math inline">\(\sum_{j=1}^N{P\left( \omega ^{\left( j\right)}\mid x \right)}=1\)</span>。</li><li>似然概率：<span class="math inline">\(P\left( x \mid \omega ^{\left(j \right)} \right)\)</span>，表示在类别 <spanclass="math inline">\(\omega ^{\left( j \right)}\)</span>中，出现<strong>属性等同于测试样例的训练样例</strong>的概率，可用训练集类别<span class="math inline">\(\omega ^{\left( j \right)}\)</span> 中含<span class="math inline">\(x\)</span> 的比例估计。</li></ul><p>于是，利用贝叶斯公式导出： <span class="math display">\[P\left( \omega ^{\left( j \right)} \mid x \right) =\frac{P\left( x \mid\omega ^{\left( j \right)} \right) P\left( \omega ^{\left( j \right)}\right)}{P\left( x \right)}=\frac{P\left( x \mid \omega ^{\left( j\right)} \right) P\left( \omega ^{\left( j \right)}\right)}{\sum_{i=1}^N{P\left( x \mid \omega ^{\left( i \right)} \right)P\left( \omega ^{\left( i \right)} \right)}}\]</span></p><h3 id="似然概率">似然概率</h3><p>前文提到，在计算 <span class="math inline">\(P(x \mid \omega ^{\left(j \right)})\)</span> 时，我们可用训练集类别 <spanclass="math inline">\(\omega ^{\left( j \right)}\)</span> 中含 <spanclass="math inline">\(x\)</span>的比例估计。然而，这样做会遇到一个问题：当类别 <spanclass="math inline">\(\omega ^{\left( j \right)}\)</span> 中不存在 <spanclass="math inline">\(x\)</span> 样本点的时候，如果将概率视作0，那么结果则必为 0，显然不符合要求。</p><p>而上述这种情况经常会发生在<strong>多属性</strong>的样本分类中，因此，我们改用：<span class="math display">\[P\left(x \mid \omega ^{\left( j\right)}\right)=\prod_{k=1}^n{P\left(A_k=a_k \mid \omega ^{\left( j\right)}\right)}\]</span> 其中，<span class="math inline">\(P\left(A_k=a_k \mid \omega^{\left( j \right)}\right)\)</span> 表示在类别 <spanclass="math inline">\(\omega ^{\left( j \right)}\)</span> 中，第 <spanclass="math inline">\(k\)</span> 个属性值时 <spanclass="math inline">\(a_k\)</span> 的样本出现的概率。</p><blockquote><p>此处采用连乘的形式，是基于<strong>所有属性相互独立</strong>的假设（这正是「<strong>朴素</strong>」的由来），否则应当使用概率乘法公式，即：$P(A_1A_2A_3 ) =P( A_1 ) P( A_2 A_1 ) P( A_3 A_1A_2 ) $。</p></blockquote><p>对于 <span class="math inline">\(P\left(A_k=a_k \mid \omega ^{\left(j \right)}\right)\)</span> 的求法，则须分两种情况来讨论：</p><ol type="1"><li><p><span class="math inline">\(A_k\)</span>是<strong>离散</strong>属性，直接用 <spanclass="math inline">\(a_k\)</span>在类别中的比例估计即可。如果依然发生了零频问题，则需要考虑使用折扣法、插值法、退避法等进行<strong>平滑处理</strong>。</p></li><li><p><span class="math inline">\(A_k\)</span>是<strong>连续</strong>属性，通常我们假设 <spanclass="math inline">\(A_k\)</span> 在类别中是符合均值为 <spanclass="math inline">\(\mu\)</span>，标准差为 <spanclass="math inline">\(\sigma\)</span> 的正态分布（详见：<ahref="https://hwcoder.top/PR-Note-3">PR学习笔记 #3概率密度：参数估计</a>），则用其<strong>概率密度函数</strong>的值估计其概率：<span class="math display">\[P(A_k=a_k \mid \omega ^{\left( j\right)})=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(a_k-\mu )^2}{2\sigma^2}}\]</span></p></li></ol><h2 id="决策理论">决策理论</h2><p>有了后验概率，现在则需要考虑决策问题。</p><h3 id="最小错误率分类">最小错误率分类</h3><p>在贝叶斯分类器中，在我们计算出所有类别的后验概率之后，若有： <spanclass="math display">\[P\left( \omega ^{\left( j \right)} \mid x \right) =\max \left\{ P\left(\omega ^{\left( 1 \right)} \mid x \right) ,P\left( \omega ^{\left( 2\right)} \mid x \right) ,\cdots ,P\left( \omega ^{\left( N \right)} \midx \right) \right\}\]</span> 则认为 <span class="math inline">\(x\in \omega ^{\left( j\right)}\)</span>，这是基于<strong>最小化错误率</strong>的思想。当选择了类别<span class="math inline">\(\omega ^{\left( j\right)}\)</span>，错误分类的概率就是 <spanclass="math inline">\(x\notin \omega ^{\left( j \right)}\)</span>的概率： <span class="math display">\[P(\mathrm{error} \mid x)=1-\underset{j=1,\cdots ,N}{\max}\left\{ P\left(\omega ^{\left( j \right)} \mid x \right) \right\}\]</span></p><p>当然，这时在固定测试样本为 <span class="math inline">\(x\)</span>的情况下，如果要求对所有不同的 <span class="math inline">\(x\)</span>取值的平均错误率 <spanclass="math inline">\(P(\mathrm{error})\)</span>，则需要对整个特征空间进行加权求和。</p><p>此外，按照最小错误率来分类时，我们只需要比较出后验概率最大的一类就行，因此在计算过程中只需比较分子部分<span class="math inline">\(P\left( x \mid \omega ^{\left( j \right)}\right) P\left( \omega ^{\left( j \right)} \right)\)</span>的大小即可。</p><h3 id="最小风险分类">最小风险分类</h3><p>但在实际分类情形中，我们还应当考虑每次决策的代价——譬如将患者诊断为阴性的代价要远大于将正常人诊断为阳性。为此，我们补充如下定义：</p><ul><li>决策：<span class="math inline">\(\alpha_i\)</span>，表示将测试样例<span class="math inline">\(x\)</span> 分到类别 <spanclass="math inline">\(\omega ^{\left(i\right)}\)</span>。</li><li>代价：<span class="math inline">\(\lambda _{ij}=\lambda \left(\alpha _i \mid \omega _j\right)\)</span>，表示将<strong>真实</strong>类别 <spanclass="math inline">\(\omega ^{\left(j\right)}\)</span>中的样本分到<strong>预测</strong>类别 <span class="math inline">\(\omega^{\left(i\right)}\)</span> 的代价，当 <spanclass="math inline">\(i=j\)</span>时代价显然较小，其他时候代价较大。在决策分类的<strong>离散</strong>情况下，通常需要事先绘制代价矩阵<span class="math inline">\(\lambda _{N \times N}\)</span>。</li></ul><p>则可以得到下面的<strong>条件风险</strong>： <spanclass="math display">\[R\left( \alpha _i \mid x \right) =\mathbb{E}\left[ \lambda _{ij} \mid x\right] =\sum_{j=1}^N{\lambda _{ij}}P\left( \omega _j \mid x \right)\]</span>如果我们致力于<strong>最小化总体风险</strong>，则需要定义<strong>决策函数</strong>（最优分类器）：<span class="math display">\[\alpha \left( x \right) =\underset{i=1,\cdots,N}{\mathrm{arg}\min}R\left( \alpha _i\mid x \right)\]</span> 此时，在整个特征空间中，对所有不同的 <spanclass="math inline">\(x\)</span>取值，采取决策函数带来的<strong>期望风险</strong>为： <spanclass="math display">\[R=\int{R\left( \alpha \left( x \right) \mid x \right)}P\left( x \right)\mathrm{d}x\]</span></p><h2 id="半朴素贝叶斯分类">半朴素贝叶斯分类</h2><p>前文提到，朴素贝叶斯分类中采用了<strong>所有属性条件独立</strong>的假设，但在现实任务中这个假设往往很难成立。因此，我们对假设进行一定程度的放松，适当考虑<strong>部分属性</strong>间的相互依赖信息，因此称为「<strong>半朴素</strong>」。</p><p>其中，<strong>独依赖估计</strong>（One-DependentEstimator，ODE）是半朴素贝叶斯分类器最常用的一种策略，顾名思议，就是假设每个属性<strong>最多仅依赖于一个</strong>其他属性。</p><p>被依赖的属性称为<strong>父属性</strong>，用 <spanclass="math inline">\(pA_k\)</span> 来表示，则似然概率： <spanclass="math display">\[P\left( x\mid \omega ^{\left( j \right)} \right) =\prod_{k=1}^n{P\left(A_k=a_k\mid \omega ^{\left( j \right)},pA_k=pa_k \right)}\]</span>那么，问题的关键就转化为如何确定每个属性的父属性。最直接的做法就是假设所有属性都依赖于同一个父属性，称之为<strong>超父</strong>（Super-Parent），对应的方法称为<strong>SPODE</strong> (Super-Parent ODE)。</p><p>此外，还有更为复杂的 <strong>TAN</strong> (Tree Augmented NaiveBayes)，通过计算所有属性两两之间的<strong>条件互信息</strong>（ConditionalMutual Information）： <span class="math display">\[I\left( A_i,A_j\mid C \right) =\sum_{A_i,A_j;\omega ^{\left( j\right)}\in C}{P}\left( A_i,A_j\mid \omega ^{\left( j \right)} \right)\log \frac{P\left( A_i,A_j\mid \omega ^{\left( j \right)}\right)}{P\left( A_i\mid \omega ^{\left( j \right)} \right) P\left(A_j\mid \omega ^{\left( j \right)} \right)}\]</span> 绘制出带权无向完全图，任意两个结点之边的权重为 <spanclass="math inline">\(I\left( A_i,A_j\mid C\right)\)</span>。再构建此完全图的<strong>最大带权生成树</strong>，挑选根结点，将边置为有向，就得到了每个属性及其父属性的依赖关系。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>模式识别</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
      <tag>PR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PR学习笔记 #1 KNN 分类器</title>
    <link href="/PR-Note-1.html"/>
    <url>/PR-Note-1.html</url>
    
    <content type="html"><![CDATA[<p>该笔记是本人于哈尔滨工业大学（深圳）2021年秋季学期「模式识别」课程的笔记，授课教师为 <ahref="https://scholar.google.com/citations?hl=zh-CN&amp;user=zOVgYQYAAAAJ">徐勇</a>教授。姑且算是一门 CV 入门课程。</p><h2 id="pr-vs.-ml">PR vs. ML</h2><p>「模式识别」与「机器学习」作为学校开设的<strong>人工智能</strong>领域二选一的先修课程，想必是有其考虑的。作为一个初学者，或许需要对这二者有更系统的认识。</p><p>这里引用一篇知乎回答的内容：</p><p><img src="/img/blog/PR-Note-1-images/content.jpg" alt="参考自相关书籍目录" width = 80% /></p><p>从上面的目录大致看出模式识别跟机器学习之间有很大部分的<strong>重叠</strong>。但是两者之间还是有很明显的区别的：</p><ul><li>在模式识别中，模式即用来描述研究对象的<strong>特征</strong>。一般模式识别中的大部分分类器，都是在假定已经了用来描述对象性质的特征，系统的输入就是这些处理好的特征。因此，<strong>模式识别研究的是怎么样通过输入的特征对样本进行分类</strong>。</li><li>然而，如果特征与所研究的分类问题没有关系或者关系很弱，那么无论采用怎么样的分类器，都很难取得理想的分类效果。相比之下，<strong>机器学习更加关注的是特征抽取、特征分析，进而实现分类</strong>。</li></ul><p>此外，既然上述图中还提到了「深度学习」，那不妨探讨一下经典面试题「深度学习和机器学习有什么不同？」，同样与特征有关：</p><ul><li><strong>深度学习是一种特殊的机器学习</strong>，具有强大的能力和灵活性。它通过学习将世界表示为嵌套的层次结构，每个表示都与<strong>更简单的特征</strong>相关，而抽象的表示则用于计算<strong>更抽象的表示</strong>。</li><li>传统的机器学习侧重人工进行特征抽取、特征分析，而深度学习从数据中先学习简单的特征，并从其逐渐学习到更为复杂抽象的深层特征，<strong>不依赖人工的特征工程</strong>。</li></ul><h2 id="k-近邻算法-k-nearest-neighbor">K 近邻算法 | K-NearestNeighbor</h2><p>最近邻算法是解决<strong>分类</strong>问题的一种算法，是一种监督学习算法。它的思想是：如果一个<strong>样本点</strong>在特征空间中<strong>最相似的样本点</strong>中的属于某一个类别，则该样本也属于这个类别。</p><p>关于「<strong>相似</strong>」这个概念，涉及到<strong>样本距离</strong>的度量，常用的度量方式是<strong>二范数</strong>，对向量而言就是<strong>欧式距离</strong>（EuclideanDistance)： <span class="math display">\[D(x,y)=\sqrt{\sum_{i=1}^n{(}x_i-y_i)^2}=\left\| x-y \right\| _2\]</span>大多数情况下，欧式距离可以满足我们的需求，但有时候也需要了解其他方式：</p><ul><li>闵可夫斯基距离（Minkowski Distance）：<spanclass="math inline">\(\sqrt[p]{\sum\limits_{i=1}^{n}(|x_i-y_i|)^p}\)</span></li><li>曼哈顿距离（Manhattan Distance）：<spanclass="math inline">\(\sum\limits_{i=1}^{n}|x_i-y_i|\)</span>，相当于<span class="math inline">\(p=1\)</span></li><li>切比雪夫距离（Chebyshev Distance）：<spanclass="math inline">\(\underset{i=1,\cdots ,n}{\max}\left|x_{\boldsymbol{i}}-y_i \right|\)</span>，相当于 $p=$</li><li>汉明距离（Hamming Distance）：仅当特征值为 bool 值时使用</li><li>夹角余弦距离（Cosine Distance）：<spanclass="math inline">\(\begin{aligned}\cos \theta=\frac{\sum_{i=1}^n{x_i}\timesy_i}{\sqrt{\sum_{i=1}^n{x_{i}^{2}}\sum_{i=1}^n{y_{i}^{2}}}}\end{aligned}\)</span></li></ul><h3 id="算法步骤">算法步骤</h3><p>解决了度量问题，剩下的就很简单了，假设有训练集： <spanclass="math display">\[D=\left\{x^{(i)},\;i=1,2,\cdots,m\right\},\quad C=\left\{ x^{(i)}\in\omega ^{\left( j \right)},\;j=1,2,\cdots ,N \right\}\]</span></p><ul><li><span class="math inline">\(m\)</span>代表训练集中样本点的数量；</li><li><span class="math inline">\(x^{(i)}\)</span> 代表第 <spanclass="math inline">\(i\)</span> 个样本点，是一个 <spanclass="math inline">\(n\)</span> 维向量，<spanclass="math inline">\(n\)</span> 代表特征数；</li><li><span class="math inline">\(N\)</span>代表类别数，所有训练集中样本点被显式地标注。</li></ul><p>那么，最近邻算法可以表述为：</p><ol type="1"><li>计算测试样本点 <span class="math inline">\(x\)</span>与所有训练样本点 <span class="math inline">\(x^{(i)}\)</span> 的距离<span class="math inline">\(\left\| x-x^{(i)} \right\|_2\)</span>；</li><li>找出距离最小的情况 <span class="math inline">\(g_k\left( x \right)=\min \left\| x-x^{\left( k \right)} \right\| _2\)</span>；</li><li>若 <span class="math inline">\(x^{(k)}\in \omega ^{\left( j\right)}\)</span>，则测试样本 <span class="math inline">\(x \in \omega^{\left( j \right)}\)</span>。</li></ol><p>然而，上述方法有明显的缺陷：<strong>单个训练样本</strong>对分类的结果有较大的影响。如果在一个样本空间中有反常的样本点，那么它很可能会影响到周围测试样本的判别。</p><p>因此，<span class="math inline">\(\text{KNN}\)</span>选择了<strong>多数表决法</strong>作为改进，在做决策时选择特征空间中最相似的<span class="math inline">\(K\)</span>个样本点，并以其中<strong>大多数样本所属</strong>的类别作为测试样本的类别。故<span class="math inline">\(\text{KNN}\)</span> 算法可以改进为：</p><ol type="1"><li>计算测试样本点 <span class="math inline">\(x\)</span>与所有训练样本点 <span class="math inline">\(x^{(i)}\)</span> 的距离<span class="math inline">\(\left\| x-x^{(i)} \right\|_2\)</span>；</li><li>找出距离最小的前 <span class="math inline">\(K\)</span>个样本点；</li><li>统计其中最多样本点所属的类别 <span class="math inline">\(\omega^{\left( j \right)}\)</span>，则测试样本 <span class="math inline">\(x\in \omega ^{\left( j \right)}\)</span>。</li></ol><p>此外，<span class="math inline">\(\text{KNN}\)</span>算法还可以用于<strong>回归</strong>的决策，即用最近的 <spanclass="math inline">\(K\)</span> 个样本的平均值作为回归预测值。</p><h3 id="实现细节">实现细节</h3><p><span class="math inline">\(\text{KNN}\)</span>算法是一种<strong>懒散学习法</strong>（LazyLearing），在模型训练的过程中几乎不耗时，主要的计算量则花在预测时的计算，因此会比其他许多算法都要慢。</p><p>通过 Brute-Force 的方法找到最相近的前 <spanclass="math inline">\(K\)</span>个样本点，在样本量小、样本特征少的时候或许还能用，但实际运用中我们通常采用更高级的处理方法：</p><ul><li>KDTree (K-Dimension Tree)：根据样本的 <spanclass="math inline">\(n\)</span>维特征建树，选取<strong>特征取值方差最大者</strong>作为根结点，将所有结点以该特征值均分为左右子树（将特征空间以<strong>超平面</strong>分割），再以同样方法递归建树。测试时，只需以测试样本点为圆心绘制<strong>超球体</strong>，在所有与球面交割的<strong>超矩形体</strong>中搜索最近邻的点。</li><li>BallTree：KDTree中利用球面交割矩形体的方法搜索，对于不均匀数据集，空间中的超矩形体会有许多棱角，导致会因为<strong>超球体交割于棱角</strong>导致进行多余的搜索。本方法将分割块改为<strong>超球体</strong>，建立球树以避免问题的发生，搜索回溯的方法与KDTree 类似。</li></ul><p>当然，上述方法在建模时需要较大的内存，属于「空间换时间」的经典样例。</p><h3 id="超参数-k">超参数 K</h3><p>所谓超参数（Hyper-Parameter），就是在开始<strong>模型训练之前</strong>，就<strong>人为设置</strong>好的参数，这是相对其他<strong>模型训练得出</strong>的参数（Parameter）提出的一个概念。</p><p>作为本算法唯一的参数 <spanclass="math inline">\(K\)</span>，就需要根据经验进行优化，以提高学习的性能和效果。下面我们通过<strong>交叉验证</strong>（Cross-Validation）来分析这个问题。</p><blockquote><p>所谓的交叉验证，就是将样本数据按照一定比例，<strong>拆分</strong>出训练用的数据和验证用的数据，其中验证集不参与模型的构建。</p></blockquote><p>选取一个较小的 <span class="math inline">\(K\)</span>值，不断增加，并计算验证集的错误率，最终可以绘制出如下曲线：</p><p><img src="/img/blog/PR-Note-1-images/KNN-error.png" alt="K值与误差率" width=80%/></p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>模式识别</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
      <tag>PR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python笔记 #3 Matplotlib</title>
    <link href="/Python-Note-3.html"/>
    <url>/Python-Note-3.html</url>
    
    <content type="html"><![CDATA[<p>学习 Machine Learning的时候发现需要用许多矩阵运算和画图的库，本文将以<strong>实用主义</strong>的方式记录每次遇到的新用法。</p><p>2021 年贵系的暑培新增了「科学计算」内容，本文部分内容参考了清华 LZJ同学的教程。本文将持续更新。</p><h2 id="matplotlib-基础">Matplotlib 基础</h2><p>绘图时最常用的模块是 Matplotlib 中的 <strong>pyplot</strong>模块。绘图时先调用相关绘图函数，设置图像各种细节，最后调用<code>plt.show()</code> 打开新窗口显示图片，此时程序会暂停。如果使用<code>%matplotlib inline</code> 方法，可以使图片在 Cell中显示而不开新窗口（适用于 Notebook）。</p><p>这时会出现一个基于 Qt 实现的交互窗口，程序执行到<code>plt.show()</code>时阻塞。在交互窗口中可以<strong>进一步调整图片</strong>格式细节或<strong>保存图片</strong>，关闭窗口后程序继续运行。下面以一个例子说明：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>x = np.linspace(-<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">30</span>)  <span class="hljs-comment"># [-4, 4] 中长度为 30 的等差数列</span><br>y = np.sin(x)<span class="hljs-comment"># 获得 sin 值</span><br>plt.plot(x, y)<span class="hljs-comment"># 以 x 为自变量，y 为因变量，绘制折线图</span><br>plt.show()<span class="hljs-comment"># 显示图像，程序阻塞</span><br>plt.savefig(<span class="hljs-string">&#x27;myFig.png&#x27;</span>) <span class="hljs-comment"># 保存图像, 可以设置 dpi=1200/600/300 调整清晰度</span><br></code></pre></td></tr></table></figure><h3 id="格式字符串">格式字符串</h3><p>与 Matlab 相似，Matplotlib使用事先约定好的字符串代表<strong>绘图格式</strong>，将其写入<code>plt.plot()</code> 即可，如 <code>plt.plot(x, y, 'bo')</code>即蓝色圆圈标记。</p><ul><li><code>color</code>：绘制点的颜色，支持以下常用缩写（还有更多未列出的颜色全名），还可以用十六进制代码<code>c='#000'</code> 指定。</li></ul><table><thead><tr class="header"><th style="text-align: center;">字符</th><th style="text-align: center;">颜色</th><th style="text-align: center;">字符</th><th style="text-align: center;">颜色</th><th style="text-align: center;">字符</th><th style="text-align: center;">颜色</th><th style="text-align: center;">字符</th><th style="text-align: center;">颜色</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>b</code></td><td style="text-align: center;">蓝色</td><td style="text-align: center;"><code>r</code></td><td style="text-align: center;">红色</td><td style="text-align: center;"><code>m</code></td><td style="text-align: center;">洋红色</td><td style="text-align: center;"><code>k</code></td><td style="text-align: center;">黑色</td></tr><tr class="even"><td style="text-align: center;"><code>g</code></td><td style="text-align: center;">绿色</td><td style="text-align: center;"><code>c</code></td><td style="text-align: center;">青色</td><td style="text-align: center;"><code>y</code></td><td style="text-align: center;">黄色</td><td style="text-align: center;"><code>w</code></td><td style="text-align: center;">白色</td></tr></tbody></table><ul><li><code>color#xkcd</code>：xkdc调色盘总结了数百种最常用的颜色，比标准色更美观，使用<code>c='xkcd:pink'</code> 格式即可使用。</li></ul><table><thead><tr class="header"><th style="text-align: center;">全称</th><th style="text-align: center;">颜色</th><th style="text-align: center;">全称</th><th style="text-align: center;">颜色</th><th style="text-align: center;">全称</th><th style="text-align: center;">颜色</th><th style="text-align: center;">全称</th><th style="text-align: center;">颜色</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>pink</code></td><td style="text-align: center;">粉色</td><td style="text-align: center;"><code>sky blue</code></td><td style="text-align: center;">天蓝色</td><td style="text-align: center;"><code>orange</code></td><td style="text-align: center;">橘色</td><td style="text-align: center;"><code>light green</code></td><td style="text-align: center;">嫩绿色</td></tr><tr class="even"><td style="text-align: center;"><code>light purple</code></td><td style="text-align: center;">嫩紫色</td><td style="text-align: center;"><code>lavender</code></td><td style="text-align: center;">粉紫色</td><td style="text-align: center;"><code>tan</code></td><td style="text-align: center;">褐色</td><td style="text-align: center;"><code>aqua</code></td><td style="text-align: center;">海绿色</td></tr></tbody></table><ul><li><code>marker</code>：绘制点的形状。</li></ul><table style="width:100%;"><thead><tr class="header"><th style="text-align: center;">字符</th><th style="text-align: center;">标记</th><th style="text-align: center;">字符</th><th style="text-align: center;">标记</th><th style="text-align: center;">字符</th><th style="text-align: center;">标记</th><th style="text-align: center;">字符</th><th style="text-align: center;">标记</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>.</code></td><td style="text-align: center;">点标记</td><td style="text-align: center;"><code>&gt;</code></td><td style="text-align: center;">右三角标记</td><td style="text-align: center;"><code>p</code></td><td style="text-align: center;">五边形标记</td><td style="text-align: center;"><code>D</code></td><td style="text-align: center;">菱形标记</td></tr><tr class="even"><td style="text-align: center;"><code>,</code></td><td style="text-align: center;">像素标记</td><td style="text-align: center;"><code>1</code></td><td style="text-align: center;">三叉戟标记</td><td style="text-align: center;"><code>*</code></td><td style="text-align: center;">星形标记</td><td style="text-align: center;"><code>d</code></td><td style="text-align: center;">菱形标记</td></tr><tr class="odd"><td style="text-align: center;"><code>o</code></td><td style="text-align: center;">圆圈标记</td><td style="text-align: center;"><code>2</code></td><td style="text-align: center;">三叉戟标记</td><td style="text-align: center;"><code>h</code></td><td style="text-align: center;">六角形标记</td><td style="text-align: center;"><code>丨</code></td><td style="text-align: center;">竖线标记</td></tr><tr class="even"><td style="text-align: center;"><code>v</code></td><td style="text-align: center;">倒三角标记</td><td style="text-align: center;"><code>3</code></td><td style="text-align: center;">三叉戟标记</td><td style="text-align: center;"><code>H</code></td><td style="text-align: center;">六角形标记</td><td style="text-align: center;"><code>_</code></td><td style="text-align: center;">横线标记</td></tr><tr class="odd"><td style="text-align: center;"><code>^</code></td><td style="text-align: center;">正三角标记</td><td style="text-align: center;"><code>4</code></td><td style="text-align: center;">三叉戟标记</td><td style="text-align: center;"><code>+</code></td><td style="text-align: center;">+标记</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><code>&lt;</code></td><td style="text-align: center;">左三角标记</td><td style="text-align: center;"><code>s</code></td><td style="text-align: center;">正方形标记</td><td style="text-align: center;"><code>x</code></td><td style="text-align: center;">x标记</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr></tbody></table><ul><li><code>line</code>：绘制线的形状。</li></ul><table><thead><tr class="header"><th style="text-align: center;">字符</th><th style="text-align: center;">格式</th><th style="text-align: center;">字符</th><th style="text-align: center;">格式</th><th style="text-align: center;">字符</th><th style="text-align: center;">格式</th><th style="text-align: center;">字符</th><th style="text-align: center;">格式</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>-</code></td><td style="text-align: center;">实线</td><td style="text-align: center;"><code>--</code></td><td style="text-align: center;">虚线</td><td style="text-align: center;"><code>-.</code></td><td style="text-align: center;">点划线</td><td style="text-align: center;"><code>:</code></td><td style="text-align: center;">点线</td></tr></tbody></table><h2 id="特定类型图">特定类型图</h2><p>下面是一些常用的绘图函数。更多官网案例：https://matplotlib.org/stable/gallery/index.html。</p><h3 id="折线图-plot">折线图 <code>plot()</code></h3><p><code>plt.plot()</code>用于绘制<strong>折线图</strong>，需要一系列点作为因变量和自变量，函数接口如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">matplotlib.pyplot.plot(*args, scalex=<span class="hljs-literal">True</span>, scaley=<span class="hljs-literal">True</span>, data=<span class="hljs-literal">None</span>, **kwargs)<br></code></pre></td></tr></table></figure><p>观察接口，可以发现有以下的用法：</p><ul><li>传入<strong>两个等长</strong>数组或列表，前者是自变量，后者是因变量。</li><li>只传入<strong>一个</strong>数组或列表，自变量默认从 0开始整数递增。</li><li>传入两个列表，<strong>各包含两个值</strong>，可以绘制一条直线。</li></ul><h3 id="散点图-scatter">散点图 <code>scatter()</code></h3><p><code>plt.scatter()</code>用于绘制<strong>散点图</strong>，需要<strong>两个等长</strong>数组或列表，函数接口如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">matplotlib.pyplot.scatter(x, y, s=<span class="hljs-literal">None</span>, c=<span class="hljs-literal">None</span>, marker=<span class="hljs-literal">None</span>, **kwargs)<br></code></pre></td></tr></table></figure><p>其中只有 <code>x</code> 和 <code>y</code> 为必填项，<code>s</code>代表每个点的大小，可以是常数也可以是列表。</p><h3 id="轮廓图-contour">轮廓图 <code>contour()</code></h3><p><code>plt.contour()</code>用于绘制<strong>等高线图</strong>，也可绘制闭合的<strong>轮廓图</strong>，函数接口如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">matplotlib.pyplot.contour(x, y, Z, [levels], **kwargs)<br></code></pre></td></tr></table></figure><p>其中 <code>x</code> 和 <code>y</code>是一维的数组，代表绘制的网格点的横纵坐标；<code>Z</code> 是形状为<code>(len(X), len(Y))</code> 的二维数组，代表绘制点的高度。参数<code>levels</code>是一个列表，如果空缺则默认绘制<strong>一组等距的等高线</strong>，也可以传入<code>[0, 0.5, 1]</code>这样的列表，则只会<strong>显示对应高度</strong>的轮廓线。</p><p>网格的横纵坐标通常由 Numpy 中的 <code>merhgrid</code>生成，完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">u = np.linspace(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">50</span>)<br>v = np.linspace(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">50</span>)<br>U, V = np.meshgrid(u, v)  <span class="hljs-comment"># 生成 50x50 网格点矩阵</span><br>z = (map_feature(U.flatten(), V.flatten()) @ theta).reshape((<span class="hljs-number">50</span>, <span class="hljs-number">50</span>))  <span class="hljs-comment"># 计算等高线</span><br>plt.contour(u, v, z, [<span class="hljs-number">0</span>], colors=<span class="hljs-string">&#x27;r&#x27;</span>)  <span class="hljs-comment"># 绘制轮廓图，取高度为 0 的点</span><br></code></pre></td></tr></table></figure><h3 id="热力图-imshow">热力图 <code>imshow()</code></h3><p><code>plt.imshow()</code>本身是用于绘制<strong>二维数字图像</strong>，但图像有更好的工具，因此主要用于绘制<strong>热力图</strong>（heatmap），配套使用的还有<strong>色值柱</strong>，函数接口如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">matplotlib.pyplot.imshow(X, cmap=<span class="hljs-literal">None</span>, interpolation=<span class="hljs-literal">None</span>, **kwargs)<br>matplotlib.pyplot.colorbar()<br></code></pre></td></tr></table></figure><p>其中 <code>X</code>是二维数组，数组元素的值就是热力值，<code>cmap</code>为<strong>色彩风格</strong>，可选的值有：<code>plt.cm.hot</code>、<code>cool</code>、<code>gray</code>、<code>bone</code>、<code>white</code>、<code>spring</code>、<code>summer</code>、<code>autumn</code>、<code>winter</code>。</p><p>非必要的参数如，<code>interpolation</code>设置为<code>’nearest’</code>可以将相邻的<strong>相同的颜色连成片</strong>。最后调用<code>plt.colorbar()</code> 可以把色值柱附在图像旁边。</p><h3 id="直方图-hist">直方图 <code>hist()</code></h3><p><code>plt.hist()</code>用于绘制直方图（Histogram），一种特殊的柱状图。，函数接口如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">matplotlib.pyplot.hist(x, bins=<span class="hljs-literal">None</span>, <span class="hljs-built_in">range</span>=<span class="hljs-literal">None</span>, **kwargs)<br></code></pre></td></tr></table></figure><p>其中 <code>x</code> 为一维数组，可以是浮点数，<code>bins</code>为直方图的组数，会<strong>自动量化原始数据</strong>。非必要的参数<code>range</code> 可输入一个范围元组，默认为<code>(x.min(), x.max())</code>，如果设置了则会依据 <code>range</code>来划分直方图的组宽。</p><h2 id="子图布局">子图布局</h2><p>Matplotlib 有一个概念 subplot：包含在 Figure 对象中的小型 Axes对象。这允许我们在一幅图中创建很多个子图，方便对比数据。在前面绘制单张图时，可以不声明Figure对象作为<strong>所有内容的容器</strong>，但<strong>绘制子图时则必须声明</strong>。总共有三种方法。</p><h3 id="任意位置-add_axes">任意位置 <code>add_axes()</code></h3><p>先调用 <code>plt.figure()</code> 创建 Figure 对象，接着调用<code>fig.add_axes()</code> 在图表的任意位置添加子图，函数接口如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">ax = fig.add_axes(rect, projection=<span class="hljs-literal">None</span>, polar=<span class="hljs-literal">False</span>, **kwargs)<br></code></pre></td></tr></table></figure><p>其中只有 <code>rect</code> 是必要的，这是一个四个浮点数的列表<code>[left, bottom, width, height]</code>，分别代表子图<strong>左下角</strong>的坐标，子图的宽度和高度。这四个数字的取值范围都是0 到 1，代表<strong>相对位置和大小</strong>。</p><p>最后在每个子图里用各自的类型图进行绘制即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">fig = plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">7</span>))<br><br><span class="hljs-comment"># 创建子图</span><br>ax1 = fig.add_axes([<span class="hljs-number">0</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.45</span>, <span class="hljs-number">0.45</span>])  <span class="hljs-comment"># 在图表的左上角创建一个子图</span><br>ax2 = fig.add_axes([<span class="hljs-number">0.5</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0.45</span>, <span class="hljs-number">0.45</span>])  <span class="hljs-comment"># 在图表的右下方创建一个子图</span><br><br><span class="hljs-comment"># 左上角子图：曲线图</span><br>x1 = np.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>)<br>ax1.plot(x1, np.sin(x1), color=<span class="hljs-string">&quot;red&quot;</span>)<br><br><span class="hljs-comment"># 右下角子图：柱状图</span><br>x2 = [<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;c&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>, <span class="hljs-string">&quot;e&quot;</span>, <span class="hljs-string">&quot;f&quot;</span>]<br>y2 = [<span class="hljs-number">1.2</span>, <span class="hljs-number">1.3</span>, <span class="hljs-number">2.5</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1.56</span>]<br>ax2.bar(x2, y2, color=<span class="hljs-string">&quot;blue&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="对齐网格-subplot">对齐网格 <code>subplot()</code></h3><p><code>plt.subplot()</code>用于在一张图里绘制<strong>多个子图</strong>，最常用，函数接口如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">ax = matplotlib.pyplot.subplot(nrows=<span class="hljs-number">1</span>, ncols=<span class="hljs-number">1</span>, index, **kwargs)<br></code></pre></td></tr></table></figure><p>其中只有 <code>nrow</code> 和 <code>ncols</code>表示总共有多少子图，<code>index</code>代表其中第几个。如：<code>plt.subplot(2,2,1)</code>，也可以缩写为<code>plt.subplot(221)</code>。紧跟在 <code>plt.subplot()</code>语句后面的语句绘制的就是 <code>index</code> 所指向的图，绘制完再次使用<code>plt.subplot()</code> 语句切换到下一张子图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">fig = plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">7</span>))  <span class="hljs-comment"># 如果对整体有标题、标签，在此处添加</span><br><br>ax1 = plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>ax1.plot()  <span class="hljs-comment">#　如果对子图有标题、标签，在此处添加</span><br><br>ax2 = plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>ax2.plot()  <span class="hljs-comment">#　如果对子图有标题、标签，在此处添加</span><br></code></pre></td></tr></table></figure><p>但是，上述方法不适用于<strong>大量子图</strong>的绘制，如 100张子图拼接等，因此有另一个相似的方法 <code>plt.subplots()</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">fig, axes = matplotlib.pyplot.subplots(nrows=<span class="hljs-number">1</span>, ncols=<span class="hljs-number">1</span>, *, sharex=<span class="hljs-literal">False</span>, sharey=<span class="hljs-literal">False</span>, **fig_kw)<br></code></pre></td></tr></table></figure><p>同时返回一个固定的 Figure 对象（设置的参数一起写即可）和一个 Axes对象<strong>二维列表</strong>，通过循环可以遍历：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建一个图形对象，拆分为2*3的网格，包含6个坐标对象</span><br>fig, axes = plt.subplots(<br>    nrows=<span class="hljs-number">2</span>,  <span class="hljs-comment"># 定义行数</span><br>    ncols=<span class="hljs-number">3</span>,  <span class="hljs-comment"># 定义列数</span><br>    sharex=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># 是否共享x轴坐标</span><br>    sharey=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># 是否共享y轴坐标</span><br>    figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">7</span>)  <span class="hljs-comment"># 图像大小</span><br>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):<br>        axes[i, j].plot()<br></code></pre></td></tr></table></figure><h3 id="自由网格-gridspec">自由网格 <code>GridSpec</code></h3><p>如果想创建<strong>不规则的子图</strong>，部分子图更大，展示核心信息，有的子图较小，展现辅助信息，<code>plt.GridSpec()</code>可实现这一点，工作原理是先创建一个网格状的蓝图，然后<strong>合并部分子图</strong>。函数接口如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">grid = matplotlib.gridspec.GridSpec(nrows, ncols, figure=<span class="hljs-literal">None</span>, **kwargs)<br></code></pre></td></tr></table></figure><p>使用时调用 <code>plt.GridSpec</code>创建网格状的<strong>二维数组</strong><code>grid</code>，通过<strong>切片和索引</strong>按需求合并子图，最后调用<code>ax.plot()</code> 将数据映射到图表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">fig = plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">7</span>))<br><br>grid = plt.GridSpec(nrows=<span class="hljs-number">2</span>, ncols=<span class="hljs-number">3</span>, figure=fig)  <span class="hljs-comment"># 创建 2*3 的网格蓝图</span><br><br><span class="hljs-comment"># 合并子图</span><br>ax1 = plt.subplot(grid[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>])<br>ax2 = plt.subplot(grid[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>:])  <span class="hljs-comment"># 合并(0,1)和(0,2)位置的子图</span><br>ax3 = plt.subplot(grid[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>:<span class="hljs-number">2</span>])  <span class="hljs-comment"># 合并(1,0)和(1,1)位置的子图</span><br>ax4 = plt.subplot(grid[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br><br>x = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">30</span>)<br><br>ax1.plot(x, np.sin(x), <span class="hljs-string">&quot;-r&quot;</span>)<br>ax2.plot(x, np.cos(x), <span class="hljs-string">&quot;-ob&quot;</span>)<br>ax3.plot(x, np.sin(x + <span class="hljs-number">10</span>), <span class="hljs-string">&quot;-oy&quot;</span>)<br>ax4.plot(x, np.cos(x + <span class="hljs-number">10</span>), <span class="hljs-string">&quot;-g&quot;</span>)<br></code></pre></td></tr></table></figure><h2 id="装饰输出">装饰输出</h2><p>除了上述常用的绘图函数，Matplotlib还带有各种绘图组件，用于装饰输出。</p><h3 id="全局配置-rcparams">全局配置 <code>rcParams</code></h3><p>Matplotlib 使用 MRC（Matplotlib <strong>ResourceConfigurations</strong>）配置文件来自定义各种属性，我们称之为 rc配置或者 rc 参数。使用 <code>rcParams</code>可以控制几乎所有的默认属性：视图窗口、线条、颜色、样式、坐标轴、网格、文本、字体等属性。</p><p>通常在 Notebook 中使用，使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><br>%matplotlib inline<br>plt.rcParams[<span class="hljs-string">&#x27;figure.figsize&#x27;</span>] = (<span class="hljs-number">10.0</span>, <span class="hljs-number">8.0</span>)<br>plt.rcParams[<span class="hljs-string">&#x27;image.interpolation&#x27;</span>] = <span class="hljs-string">&#x27;nearest&#x27;</span><br>plt.rcParams[<span class="hljs-string">&#x27;image.cmap&#x27;</span>] = <span class="hljs-string">&#x27;gray&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="画板-figure">画板 <code>figure()</code></h3><h3 id="图例-legend">图例 <code>legend()</code></h3><p>注意 label 不要拼错了</p><h3 id="坐标轴">坐标轴</h3><p>去掉坐标轴的<strong>轴线</strong>、刻度、标签：<code>plt.axis('off')</code></p><p>去掉坐标轴的刻度、标签：<code>plt.xticks([])</code> 和<code>plt.yticks([])</code></p><p>去掉坐标轴的刻度，但保留标签：<code>plt.tick_params(left=False, bottom=False)</code></p><p>去掉坐标轴的标签，但保留刻度（适用于画 Attention 热力图）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">ax = plt.gca()<br>ax.axes.xaxis.set_ticklabels([])<br>ax.axes.yaxis.set_ticklabels([])<br></code></pre></td></tr></table></figure><h3 id="文字说明">文字说明</h3><ul><li><code>plt.xlabel('X Label')</code>：</li><li><code>plt.ylabel('Y Label')</code>：</li><li><code>plt.title('TITLE')</code>：</li></ul>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python笔记 #2 NumPy</title>
    <link href="/Python-Note-2.html"/>
    <url>/Python-Note-2.html</url>
    
    <content type="html"><![CDATA[<p>学习 Machine Learning的时候发现需要用许多矩阵运算和画图的库，本文将以<strong>实用主义</strong>的方式记录每次遇到的新用法。</p><p>2021 年贵系的暑培新增了「科学计算」内容，本文部分内容参考了清华 LZJ同学的教程，部分参考自 NumPy 中文官网 <ahref="https://www.numpy.org.cn/reference/">参考手册</a>。本文将持续更新。</p><h2 id="常用的库">常用的库</h2><p>计算机领域，有用的数据特指<strong>结构化的数据</strong>，即符合一定的语法规范，才能方便计算机处理。这些数据包括表格型的数据，多维数组型的数据，由键位列关联起来的多张表数据等等。针对这些数据，Python都有相应库去处理。</p><p><strong>数据处理</strong>：</p><ul><li><p><strong>NumPy</strong>：是 Numerical Python的缩写，用于处理<strong>大规模多维数组</strong>，<strong>矩阵计算</strong>等，有成熟的C 接口，很多 Python 的第三方库都基于 NumPy 实现（例如 Python的计算机视觉库 cv2 ）。</p></li><li><p><strong>Pandas</strong>：提供了更高级的数据结构和函数，适用于处理<strong>表格化</strong>、<strong>结构化</strong>的数据。</p></li></ul><p><strong>科学计算</strong>：</p><ul><li><strong>SciPy</strong>：Python的<strong>科学计算</strong>库，可以处理积分微分、线性代数、最优化问题、信号处理、统计学等问题。</li><li><strong>Scikit-learn</strong>：Python的<strong>机器学习</strong>工具包，可以处理机器学习的众多计算问题，如支持向量机，聚类，特征选择等问题。</li><li><strong>Statmodels</strong>：处理统计学和经济学问题的库，与scikit-learn 相比包含更多经典模型。</li></ul><p><strong>数据可视化</strong>：</p><ul><li><strong>Matplotlib</strong>：最经典的<strong>二维数据</strong>可视化库，结合NumPy 使用可以成为 Matlab 的有力替代品。</li></ul><p><strong>自动化办公</strong>：</p><ul><li><p><strong>docx</strong>：处理 Doc 文档。</p></li><li><p><strong>pptx</strong>：处理 Powerpoint 幻灯片。</p></li><li><p><strong>openpyxl</strong>：处理 Excel 表格。</p></li></ul><h2 id="numpy-基础">NumPy 基础</h2><p>NumPy 底层是用 C 写的，所以可以很方便的操作内存，这点与 Python的原生数据结构不同。因此，NumPy在处理大型数据时<strong>速度更快</strong>，内存<strong>开销也更小</strong>（和原生Python 相比）。</p><h3 id="ndarray-对象">ndarray 对象</h3><p>ndarray 是 NumPy 的核心数据结构，全称是 N-dimensional Array，N维数组，一个快速、灵活的大型数据集容器，内部储存<strong>同一类型</strong>的数据。</p><p>ndarray 有两个重要属性，<code>dtype</code> 和<code>shape</code>，分别表示元素的<strong>数据类型</strong>和<strong>形状参数</strong>，此外还有<code>ndim</code> (维数), <code>size</code> (元素个数),<code>itemsize</code> (每个元素字节数) 等属性。</p><ul><li><code>arr.dtype</code>、<code>np.dtype(arr)</code>：返回<code>arr</code> 的数据类型，常见类型如下：</li></ul><table><thead><tr class="header"><th style="text-align: center;">类型</th><th style="text-align: center;">描述</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">int16/uint16</td><td style="text-align: center;">有符号和无符号 16 位整数</td></tr><tr class="even"><td style="text-align: center;"><strong>int32</strong>/uint32</td><td style="text-align: center;">有符号和无符号 32 位整数</td></tr><tr class="odd"><td style="text-align: center;">int64/uint64</td><td style="text-align: center;">有符号和无符号 64 位整数</td></tr><tr class="even"><td style="text-align: center;">float32</td><td style="text-align: center;">标准单精度浮点数，兼容 C 的 float</td></tr><tr class="odd"><td style="text-align: center;"><strong>float64</strong></td><td style="text-align: center;">标准双精度浮点数，兼容 C 的 double</td></tr><tr class="even"><td style="text-align: center;">float128</td><td style="text-align: center;">拓展精度浮点数</td></tr><tr class="odd"><td style="text-align: center;">complex128</td><td style="text-align: center;">基于 float64 的复数</td></tr><tr class="even"><td style="text-align: center;">bool</td><td style="text-align: center;">布尔值，为 True 或 False</td></tr></tbody></table><ul><li><code>arr2 = arr1.astype(np.float64)</code>：改变原数据类型为目标类型。</li><li><code>arr.shape</code>、<code>np.shape(arr)</code>：返回<code>arr</code> 的形状参数，通常赋值给变量<code>(m, n)</code>，不同的参数含义如下：</li></ul><table><thead><tr class="header"><th style="text-align: center;"><code>arr</code></th><th style="text-align: center;"><code>arr.shape</code></th><th style="text-align: center;"><code>arr.dtype</code></th><th style="text-align: center;">含义</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">array(1)</td><td style="text-align: center;"><code>()</code></td><td style="text-align: center;">int32</td><td style="text-align: center;">单个整数</td></tr><tr class="even"><td style="text-align: center;">array(1.)</td><td style="text-align: center;"><code>()</code></td><td style="text-align: center;">float64</td><td style="text-align: center;">单个浮点数</td></tr><tr class="odd"><td style="text-align: center;">array([1, 2])</td><td style="text-align: center;"><code>(2,)</code></td><td style="text-align: center;">int32</td><td style="text-align: center;">一维数组，2 个元素</td></tr><tr class="even"><td style="text-align: center;">array([[1],<br/> [2]])</td><td style="text-align: center;"><code>(2, 1)</code></td><td style="text-align: center;">int32</td><td style="text-align: center;">二维数组，2 行 1 列</td></tr><tr class="odd"><td style="text-align: center;">array([[1, 2]])</td><td style="text-align: center;"><code>(1, 2)</code></td><td style="text-align: center;">int32</td><td style="text-align: center;">二维数组，1 行 2 列</td></tr><tr class="even"><td style="text-align: center;">array([[1, 2],<br/> [3, 4]])</td><td style="text-align: center;"><code>(2, 2)</code></td><td style="text-align: center;">int32</td><td style="text-align: center;">二维数组，2 行 2 列</td></tr><tr class="odd"><td style="text-align: center;">array([[[1, 1],<br/> [1, 1]],<br/> [[1,1],<br/> [1, 1]]])</td><td style="text-align: center;"><code>(2, 2, 2)</code></td><td style="text-align: center;">int32</td><td style="text-align: center;">三维数组</td></tr></tbody></table><ul><li><code>arr.shape[0]</code>、<code>arr.shape[1]</code>：返回<code>arr</code> 的第 0 维、第 1 维的长度。</li><li><code>arr.size</code>：返回元素个数，通常赋值给变量<code>m</code>，用于生成等长的数组。</li><li><code>np.set_printoptions(formatter=&#123;'float': '&#123;: 0.6f&#125;'.format&#125;)</code>：设置<strong>全局</strong>精度。</li></ul><h3 id="ndarray-构造">ndarray 构造</h3><p>根据不同的需求，有各种构造 ndarray 的方法。</p><ul><li><strong>已有容器转化数组</strong></li></ul><p>生成 ndarray 的基本函数是 <code>np.array()</code>，其参数可以是 list、tuple 或另一个ndarray，会<strong>自动识别</strong>类型。其函数接口如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    obj: 传入原数组</span><br><span class="hljs-string">    dtype: 指定数据类型，默认根据原数组推断为 int32 或 float64</span><br><span class="hljs-string">    copy: 是否拷贝（不共享内存）</span><br><span class="hljs-string">    order: 数组存储风格，C 按行优先，F 按列优先，K 自适应</span><br><span class="hljs-string">    ndmin: 最小维数</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>np.array(<span class="hljs-built_in">object</span>, dtype = <span class="hljs-literal">None</span>, copy = <span class="hljs-literal">True</span>, order = <span class="hljs-string">&#x27;K&#x27;</span>, subok = <span class="hljs-literal">False</span>, ndmin = <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>样例测试如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># list</span><br>arr1 = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<span class="hljs-comment"># arr1.shape = (3,)</span><br>arr2 = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]) <span class="hljs-comment"># arr2.shape = (2, 2)</span><br><span class="hljs-comment"># tuple</span><br>arr3 = np.array((<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<span class="hljs-comment"># arr3.shape = (3,)</span><br>arr4 = np.array(((<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)))<span class="hljs-comment"># arr4.shape = (2, 2)</span><br><span class="hljs-comment"># float</span><br>arr5 = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3.14</span>])<span class="hljs-comment"># arr5.dtype = float64</span><br>arr6 = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype = np.float64)<br><span class="hljs-comment"># merge</span><br>arr7 = np.array([arr1, arr5])<span class="hljs-comment"># arr7.shape = (2, 3)</span><br></code></pre></td></tr></table></figure><ul><li><strong>生成已初始化数组</strong></li></ul><p>NumPy 自带初始化函数，用来生成默认数组，以下为样例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">arr1 = np.zeros((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<span class="hljs-comment"># 全 0.  数组，arr1.shape = (2, 3)</span><br>arr2 = np.ones(<span class="hljs-number">10</span>)<span class="hljs-comment"># 全 1.  数组，arr2.shape = (10,)</span><br>arr3 = np.empty((<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>))<span class="hljs-comment"># 未初始化数组，arr3.shape = (3, 4, 5)</span><br></code></pre></td></tr></table></figure><p>需要注意的是，传入的实参为生成数组的<code>shape</code>，必须用<strong>元组</strong>表示！且生成的数组默认类型都为float64。</p><p>若要生成高维数组，除了用 <code>np.array()</code>拼接两个低维数组，通常都是用 <code>np.empty()</code> 实现。</p><ul><li><strong>基于范围生成数组</strong></li></ul><p>生成方式与 Matlab 类似，这里给出函数接口：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 范围在 [start, stop) 之间，缺省其一则视为 [0, stop)，步长为 step</span><br>np.arange(start=<span class="hljs-number">0</span>, stop, step=<span class="hljs-number">1</span>, dtype=int32)<br><span class="hljs-comment"># 生成等差数列，[start, stop] 之间，限定总数为 num，endpoint 表示是否包含 stop 端点</span><br>np.linspace(start, stop, num=<span class="hljs-number">50</span>, endpoint=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 生成等比数列，注意范围是 [base^start, base^stop] 之间,限定总数为 num</span><br>np.logspace(start, stop, num=<span class="hljs-number">50</span>, endpoint=<span class="hljs-literal">True</span>, base=<span class="hljs-number">10.0</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>生成随机数组</strong></li></ul><p>常用的有三种方式，这里给出函数接口：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 生成 0 到 1 之间的浮点数，参数为各个维度，注意不能传入元组！</span><br>np.random.rand(d0, d1, ..., dn)<br><span class="hljs-comment"># 生成 [low, high) 之间的整数，缺省其一则视为 [0, high)，size 用元组表示</span><br>np.random.randint(low, high, size=<span class="hljs-literal">None</span>, dtype=<span class="hljs-built_in">int</span>)<br><span class="hljs-comment"># 生成 均值为 0，方差为 1 的正态分布浮点数，参数为各个维度</span><br>np.random.randn(d0, d1, ..., dn)<br></code></pre></td></tr></table></figure><ul><li><strong>从外部文件导入</strong></li></ul><p>实践最常用的方式是从外部文件导入大量数据集，Numpy 自身的文件后缀为<code>.npy</code>，专用于 ndarray 的读写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">np.save(<span class="hljs-string">&quot;array&quot;</span>, arr)<br>arr_copy = np.load(<span class="hljs-string">&quot;array.npy&quot;</span>)<br></code></pre></td></tr></table></figure><p>此外，Numpy 还有对 .txt 类型文件的读写方法，这里列出样例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入 .txt 文件，每行作为数组的一行，分隔符为逗号，选中前 2 列</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex1data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>, usecols=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>x = data[:, <span class="hljs-number">0</span>] <span class="hljs-comment"># 第 0 列作为自变量</span><br>y = data[:, <span class="hljs-number">1</span>]<span class="hljs-comment"># 第 1 列作为因变量</span><br>m = y.size<span class="hljs-comment"># 元素组数定义为 m</span><br></code></pre></td></tr></table></figure><p>当 .txt文件有<strong>空缺数据</strong>时，需要使用复杂度更高的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入有空缺的 .txt 文件，每行作为数组的一行，分隔符为逗号，选中前 2 列</span><br>data = np.genfromtxt(<span class="hljs-string">&#x27;ex1data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>, usecols=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><p>导入其他格式文件或许需要用到其他辅助库：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> scipy.io <span class="hljs-keyword">as</span> scio<br><span class="hljs-comment"># 导入 .mat 文件需要用到 scipy.io 模块，这是一个二进制字典文件</span><br>data = scio.loadmat(<span class="hljs-string">&#x27;ex6data1.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>]<span class="hljs-comment"># 获取字典键 &#x27;X&#x27;</span><br>y = data[<span class="hljs-string">&#x27;y&#x27;</span>].flatten()<span class="hljs-comment"># 获取字典键 &#x27;y&#x27;，展开成一维</span><br>m = y.size<br></code></pre></td></tr></table></figure><h3 id="ndarray-索引">ndarray 索引</h3><p>ndarray容器的赋值主要用<strong>索引</strong>完成，其基本特性：<strong>浅拷贝</strong>，即多个对象<strong>共用一块内存</strong>。NumPy官方解释是，NumPy主要用于处理大量数据，所以不希望用缺省复制的方式，否则会引起各种内存问题。</p><p>索引语法和原生 Python类似，用<strong>数字和冒号</strong>表示<strong>左闭右开</strong>范围：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 索引 [5,8) 位</span><br>arr1 = np.arange(<span class="hljs-number">6</span>)<span class="hljs-comment"># 内存中 [0, 1, 2, 3, 4, 5]</span><br>arr2 = arr1[<span class="hljs-number">3</span>:<span class="hljs-number">4</span>]<br><span class="hljs-comment"># 缺省，索引全部值</span><br>arr2[:] = <span class="hljs-number">6</span><span class="hljs-comment"># 内存中 [0, 1, 2, 6, 6, 5]</span><br><span class="hljs-comment"># 如果不想索引原数组</span><br>arr3 = arr1[<span class="hljs-number">1</span>:<span class="hljs-number">2</span>].copy()<span class="hljs-comment"># 用 .copy() 可以强制拷贝</span><br></code></pre></td></tr></table></figure><p>对于高维数组，如果想通过赋值实现<strong>降维</strong>，可以使用<strong>切片索引</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">arr1 = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br><span class="hljs-built_in">print</span>(arr1[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>], arr1[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>])<span class="hljs-comment"># 4 4，前者为 Python 列表索引</span><br><span class="hljs-built_in">print</span>(arr1[:<span class="hljs-number">1</span>, <span class="hljs-number">0</span>:])<span class="hljs-comment"># [[1 2 3]]</span><br><span class="hljs-built_in">print</span>(arr1[:, <span class="hljs-number">0</span>])<span class="hljs-comment"># [1 4]</span><br><span class="hljs-built_in">print</span>(arr1[:, :<span class="hljs-number">1</span>])<span class="hljs-comment"># [[1] [4]]</span><br></code></pre></td></tr></table></figure><p>由第一个例子注意到，不同于原生 Python，高维数组还可以通过<code>[x,y,z]</code>的方式在不同维度之间索引，同时还可以用<strong>冒号加数字</strong>表示<strong>左闭右开</strong>范围。</p><p>特别注意最后两个例子，在第二维上虽然范围都是0，但前者只用了数字，后者还用了冒号，导致结果的维数并不相同。这说明：<strong>使用冒号不会降维</strong>，哪怕确实范围中只有一列。</p><p>此外，NumPy中还有一种<strong>布尔索引</strong>的有趣用法，不是本文重点，这里举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 按 50% 概率，随机抽取序列</span><br>arr = np.arange(<span class="hljs-number">10</span>)<span class="hljs-comment"># 生成 待抽取 的序列 0 - 9</span><br>p = np.random.rand(<span class="hljs-number">10</span>)  <span class="hljs-comment"># 生成 0 - 1 之间的随机小数</span><br><span class="hljs-built_in">print</span>(arr[p &lt; <span class="hljs-number">0.5</span>])<span class="hljs-comment"># 利用标量语法，转化为 bool 数组，再索引 arr</span><br><span class="hljs-comment"># 浓缩成一行</span><br><span class="hljs-built_in">print</span>(np.arange(<span class="hljs-number">10</span>)[np.random.rand(<span class="hljs-number">10</span>) &lt; <span class="hljs-number">0.5</span>])<br></code></pre></td></tr></table></figure><h3 id="ndarray-变形">ndarray 变形</h3><p>ndarray可通过一些自带的函数改变形状（维度），以便完成一些复杂的矩阵运算。</p><ul><li><code>arr.flatten()</code>：返回一份<strong>深拷贝</strong>的展开成<strong>一维</strong>的数组，默认按行优先展开<code>order='C'</code>，用于矩阵时默认展开到 <code>(n,1)</code>。</li><li><code>arr[:,np.newaxis]</code>：将一维 <code>arr</code>增加到二维数组<code>(m,1)</code>。常用于生成<strong>列向量</strong>，才能<strong>点乘</strong>系数矩阵。</li><li><code>arr[np.newaxis,:]</code>：将一维 <code>arr</code>增加到二维数组 <code>(1,m)</code>。</li><li><code>np.c_[arr1, arr2]</code>：将<strong>二维数组</strong>按列相连，要求<strong>行数一致</strong>。如果对象是<strong>一维</strong>数组，视作<code>(m,1)</code> 的二维数组。<ul><li><code>np.column_stack([arr1, arr2])</code>：类似 <code>np.c_</code>的函数形式，要求传入<strong>数组列表</strong>。</li><li><code>np.hstack([arr1,arr2])</code>：类似 <code>np.c_</code>的函数形式，要求传入<strong>数组列表</strong>。</li></ul></li><li><code>np.r_[arr1, arr2]</code>：将<strong>二维数组</strong>按行相连，要求<strong>列数一致</strong>。如果对象是<strong>一维</strong>数组或<strong>常数</strong>，则仍拼成<strong>一维</strong>数组。<ul><li><code>np.row_stack([arr1, arr2])</code>：类似 <code>np.r_</code>的函数形式，要求传入<strong>数组列表</strong>。</li><li><code>np.vstack([arr1,arr2])</code>：类似 <code>np.c_</code>的函数形式，要求传入<strong>数组列表</strong>。</li></ul></li><li><code>np.reshape(arr, (5,5))</code>：调整形状，默认<strong>按行展开</strong>后填到新形状，要求<strong>规模匹配</strong>。如果形状元组里某一维填<code>-1</code>，则会根据其他维度计算该维长度。如果只有一个<code>-1</code>，则效果同 <code>arr.flatten()</code>。</li><li><code>np.flip(arr, axis=0)</code>：将 <code>arr</code>在按行切片，并把<strong>行</strong>次序颠倒。如果不加 <code>axis</code>则会在<strong>所有维度</strong>都进行颠倒，常用于卷积核的翻转。</li><li><code>np.concatenate((a1,a2,...), axis=0)</code>：高效地拼接大规模数组，<code>axis</code>表示拼接时切片的维度，对于一维数组默认拼完还是一维。</li><li><code>np.pad(arr, width, mode=‘edge’)</code>：在 <code>arr</code>的每个轴边缘填充，填充的长度 <code>width</code> 为嵌套元组<code>((before_1, after_1), ..., (before_N, after_N))</code>，<code>mode</code>为填充的值，可以为边缘值、最大值、平均值等。</li></ul><h2 id="常用数学函数">常用数学函数</h2><p>ndarray的另一特点是支持<strong>类标量语法</strong>的计算，标量会作用在数组的<strong>每一个</strong>元素上，例如：<code>arr + 2</code>、<code>arr * 10</code>、<code>arr[:] = 2</code>、<code>arr &gt; 5</code>等。此外，还支持一系列运算函数，以下所有函数同样支持<code>数组变量.函数名(参数)</code> 形式调用：</p><h3 id="普通运算">普通运算</h3><ul><li><code>np.sum(arr[1,:])</code>：对一维数组<code>arr[1,:]</code>，求和，得到一个值。</li><li><code>np.sum(arr, 0)</code>：对二维数组<code>arr</code>，求每一列和，<strong>塌缩</strong>成<code>(列数,)</code>。</li><li><code>np.sum(arr, 1)</code>：对二维数组<code>arr</code>，求每一行和，塌缩成 <code>(行数,)</code>。</li><li><code>np.max(arr)</code>：对整个数组，求最大值，得到一个值。</li><li><code>np.max(arr, 0)</code>：对二维数组<code>arr</code>，求每一列最大值，塌缩成 <code>(列数,)</code>。</li><li><code>np.argmax(arr,0)</code>：对二维数组<code>arr</code>，求每一列最大值的索引，塌缩成<code>(列数,)</code>。</li><li><code>np.sin(arr)</code>、<code>np.cos(arr)</code>、<code>np.tan(arr)</code>：作用于每个元素求三角函数。</li><li><code>np.arcsin(arr)</code>、<code>np.arccos(arr)</code>、<code>np.arctan(arr)</code>：作用于每个元素求反三角函数。</li><li><code>np.arctan2(y, x)</code>：<code>(x, y)</code>为坐标系上的一点，该函数可求出该点与原点连线到 x 轴的夹角，范围为<code>[-pi, pi]</code>，通常使用<code>np.arctan2(y, x) * 180 / np.pi</code> 来求出夹角。</li><li><code>np.power(arr, n)</code>：作用于每个元素求 <code>n</code>次幂。</li><li><code>np.square(arr)</code>：作用于每个元素求平方，更快。</li><li><code>np.sqrt(arr)</code>：作用于每个元素开根号，更快。</li></ul><h3 id="矩阵运算">矩阵运算</h3><ul><li><p><code>arr1 @ arr2</code>：<strong>矩阵点乘</strong>，要求第一个矩阵的列数等于第二个矩阵的行数，一维数组视作二维数组的<strong>行向量或列向量</strong>。</p><ul><li><code>np.dot(arr1, arr2)</code>：同上，矩阵点乘</li><li><code>np.matmul(arr1, arr2)</code>：同上，矩阵点乘，支持高维矩阵的Broadcast。</li></ul></li><li><p><code>arr1 * arr2</code>：<strong>对应位置相乘</strong>，即Hadmard 积，要求两个矩阵<strong>各个维度长度相等</strong>。</p><ul><li><code>np.multiply(arr1, arr2)</code>：同上，对应位置相乘。</li></ul></li><li><p><code>arr1.T</code>：求数组<strong>转置</strong>，数组没有求逆、共轭的函数。</p></li><li><p><code>np.transpose(arr1)</code>：同上，<strong>不指定参数</strong>时求二维数组的转置，高维数组需用元组指定索引，表示相应的索引顺序<strong>交换</strong>。</p></li></ul><h3 id="线性代数">线性代数</h3><ul><li><code>np.linalg.inv(arr)</code>：求逆矩阵，要求二维数组。</li><li><code>np.linalg.pinv(arr)</code>：求伪（广义）逆矩阵，要求二维数组。</li><li><code>np.linalg.det(arr)</code>：求行列式（标量），要求二维数组。</li><li><code>np.linalg.solve(A,b)</code>：解形如 Ax=b 线性方程组。</li><li><code>w = np.linalg.eigvals(arr)</code>：求矩阵特征值，<code>w</code>为 (m,) 一维数组，存放特征值。</li><li><code>w,v = np.linalg.eig(arr)</code>：求矩阵特征值，<code>v</code>为 (m,m) 二维数组，每一列都是特征向量，与 <code>w</code> 对应。</li><li><code>np.linalg.svd(arr)</code>：求矩阵的 SVD 分解，<code>arr</code>为 (m,n) 二维数组，三个返回值：<code>U</code> 为(m,m)，<code>Sigma</code> 为(m,n)，<code>VT</code> 为 (n,n)。</li><li><code>np.linalg.norm(arr, axis=0, ord=1)</code>：按列求向量 1范数。</li><li><code>np.linalg.norm(arr, axis=0, ord=2)</code>：按列求向量 2范数，默认选项。</li><li><code>np.linalg.norm(arr, axis=0, ord=np.inf)</code>：按列求向量无穷范数。</li></ul><h3 id="统计运算">统计运算</h3><ul><li><code>np.mean(arr)</code>：求<strong>所有</strong>样本平均值。</li><li><code>np.mean(arr, 0)</code>：对二维数组<code>arr</code>，求每一列样本平均值，<strong>塌缩</strong>成<code>(列数,)</code>。</li><li><code>np.average(arr, weights=None)</code>：求样本加权平均值。</li><li><code>np.std(arr)</code>：求所有样本标准差。</li><li><code>np.std(arr, ddof=1)</code>：求所有样本<strong>无偏</strong>标准差。</li><li><code>np.std(arr, axis=0, ddof=1)</code>：对二维数组<code>arr</code>，求每一列样本<strong>无偏</strong>标准差，<strong>塌缩</strong>成<code>(列数,)</code>。</li><li><code>np.var(arr)</code>：求所有样本方差，相当于标准差的平方。</li><li><code>np.cov(arr)</code>：对一维数组<code>arr</code>，相当于求自身的协方差（退化为<strong>无偏</strong>方差），即<strong>无偏</strong>标准差的平方。</li><li><code>np.cov(arr)</code>：对二维数组<code>arr</code>，返回一个二维数组，元素 <code>[i,j]</code>代表<code>arr[i]</code> 与 <code>arr[j]</code> 两列元素的协方差。</li></ul><h2 id="numpy-进阶">NumPy 进阶</h2><h3 id="matrix-对象">matrix 对象</h3><p>除了 ndarray，NumPy针对二维数组还专门设置了一个<strong>矩阵</strong>对象。matrix的大部分性质与 ndarray 无异，但多了一些功能函数：</p><ul><li><code>np.mat([[1,2],[5,7]])</code>：声明矩阵，注意维数必须为2。</li><li><code>np.mat([1,2,3])</code>：会被强制转化为 <code>(1,3)</code>的二维矩阵。</li><li><code>np.mat(arr1)</code>、<code>np.asmatricx(arr1)</code>：从数组转到矩阵。</li><li><code>np.asarray(mat1)</code>：从矩阵转到数组。</li><li><code>mat1.T</code>、<code>mat1.H</code>、<code>mat1.I</code>：求转置矩阵、共轭矩阵、逆矩阵。</li><li><code>mat1 * mat2</code>：<strong>点乘</strong>，要求第一个矩阵的列数等于第二个矩阵的行数。</li><li><code>np.multiply(mat1, mat2)</code>：<strong>对应位置相乘</strong>，要求两个矩阵<strong>各个维度长度相等</strong>。</li></ul><h3 id="broadcast-广播机制">Broadcast 广播机制</h3><p>NumPy 中对于两个 ndarray的<strong>加减乘除</strong>都是标量操作，即对应位置元素之间的操作。并且，当两个数组的形状不同时，NumPy自带的 Broadcast 机制会<strong>自动扩展数组</strong>进行操作，这是 NumPy向量化运算的基石。</p><p>例如，在归一化变量时，我们使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 元素减去平均值，除以无偏标准差</span><br>data = (data - data.mean(<span class="hljs-number">0</span>)) / data.std(axis=<span class="hljs-number">0</span>, ddof=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>很明显，<code>data</code> 和塌缩后的 <code>np.mean(data, 0)</code>形状不相同，但仍可以相减。</p><p>广播兼容的条件有两种：</p><ul><li>两个<strong>不同维度</strong>的数组的<strong>后缘维度</strong>（trailingdimension，即从末尾开始算起的维度）的轴长度相符。如：<code>(4,3)</code>与 <code>(3,)</code>；<code>(5,6,7)</code> 与 <code>(6,7)</code>。</li><li>两个<strong>相同维度</strong>的数组，但其中在<strong>某一维度</strong>上仅为1。如 <code>(4,5,6)</code> 与<code>(4,1,6)</code>、<code>(4,5,1)</code>。</li></ul><h3 id="爱因斯坦求和">爱因斯坦求和</h3><p>爱因斯坦求和法 <code>np.einsum()</code>函数是一个非常强大的工具，用于执行各种复杂张量运算，属于是矩阵乘法的泛化版。它可以在<strong>不创建中间数组</strong>的情况下，高效地执行<strong>多种线性代数、张量操作和张量缩并操作</strong>。参数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.einsum(subscripts, *operands)<br></code></pre></td></tr></table></figure><ul><li><code>subscripts</code>：爱因斯坦求和<strong>约定字符串</strong>，用于指定操作的具体形式。</li><li><code>operands</code>：输入张量或数组。</li></ul><p>在爱因斯坦求和法中，我们约定：</p><ol type="1"><li>如果<strong>一个字母在两个输入</strong>中都出现了，那么就执行逐分量的乘法；</li><li>如果<strong>输出中不包含一个字母</strong>，则在该维度上执行求和。</li></ol><p>例如，当我们想实现一个<strong>内积</strong>操作时，可以简化为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">A = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br>B = np.array([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br><br><span class="hljs-comment"># C[i] = A[i, j] * B[j], np.dot(A, B) 或 A @ B 可以实现</span><br>C = np.einsum(<span class="hljs-string">&#x27;ij,j-&gt;i&#x27;</span>, A, B)<br></code></pre></td></tr></table></figure><p>对两个三维数组进行<strong>对应位置相乘</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">A = np.array([[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]], [[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]])<br>B = np.array([[[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>]], [[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>]]])<br><br><span class="hljs-comment"># A * B 可以实现</span><br>C = np.einsum(<span class="hljs-string">&#x27;ijk,ijk-&gt;ijk&#x27;</span>, A, B)<br></code></pre></td></tr></table></figure><p>执行张量缩并操作，例如计算矩阵的<strong>迹</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">A = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><br><span class="hljs-comment"># np.trace(A) 可以实现</span><br>result = np.einsum(<span class="hljs-string">&#x27;ii&#x27;</span>, A)<br></code></pre></td></tr></table></figure><p>在 Transformer 中，最复杂的运算操作是自注意力层，其中除了 softmax之外的一切都可以使用 einsum 表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># b: batch</span><br><span class="hljs-comment"># n: sequence length</span><br><span class="hljs-comment"># d: embedding dimension</span><br><span class="hljs-comment"># h: number of heads</span><br><span class="hljs-comment"># k: dimension of each head</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">multihead_attention</span>(<span class="hljs-params">X, W_q, w k, W_v, W_o</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">X: [b, n, d] (input array)</span><br><span class="hljs-string">W_q, W_k, W_v, W_o [h, d, k] (projection parameters)</span><br><span class="hljs-string">Y: [b, n, d] (output array)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>    Q = einsum(<span class="hljs-string">&quot;bnd,hdk-&gt;bhnk&quot;</span>, X, W_q)<br>    K = einsum(<span class="hljs-string">&quot;bnd,hdk-&gt;bhnk&quot;</span>, X, W_k)<br>    V = einsum(<span class="hljs-string">&quot;bnd,hdk-&gt;bhnk&quot;</span>, X, W_v)<br>    scores = einsum(<span class="hljs-string">&quot;bhnk,bhmk-&gt;bhnm&quot;</span>, Q, K)<br>    weights = softmax(scores)<br>    O = einsum(<span class="hljs-string">&quot;bhnm,bhmk-&gt;bhnk&quot;</span>, weights, V)<br>    Y = einsum(<span class="hljs-string">&quot;bhnk,hdk-&gt;bnd&quot;</span>, O, W_o)<br>    <span class="hljs-keyword">return</span> Y<br></code></pre></td></tr></table></figure><h3 id="c-api">C-API</h3><p>NumPy 提供了一个C-API，使用户能够扩展系统并快速访问<strong>数组对象</strong>，以便在其他例程中使用。真正理解C-API 的最好方法是阅读源代码，这里仅列举部分神奇的用法。</p><ul><li><p><code>np.argpartition(arr, n)</code>：高效地将数组中的所有元素<strong>分割</strong>，分割点为结果下标<code>n</code>的元素，左侧是较小数，右侧是较大数。注意仅返回分割后数组的<strong>索引</strong>，常用于检索算法中输出Top N 结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([<span class="hljs-number">9</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">9</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">0</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(np.argpartition(a, -<span class="hljs-number">5</span>))<br>[<span class="hljs-number">6</span> <span class="hljs-number">9</span> <span class="hljs-number">4</span> <span class="hljs-number">3</span> <span class="hljs-number">7</span> <span class="hljs-number">2</span> <span class="hljs-number">1</span> <span class="hljs-number">5</span> <span class="hljs-number">8</span> <span class="hljs-number">0</span>]  <span class="hljs-comment"># 结果的索引</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>a[np.argpartition(a, -<span class="hljs-number">5</span>)]<br>array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">9</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>])<span class="hljs-comment"># 真正的结果</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>a[np.argpartition(a, -<span class="hljs-number">5</span>)[-<span class="hljs-number">5</span>:]]<br>array([<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">9</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>])<span class="hljs-comment"># Top 5</span><br></code></pre></td></tr></table></figure></li><li><p><code>np.where(condition)</code>：找出满足条件<code>condition</code> 元素的坐标，返回一个 tuple，每一项是一个一维ndarray，分别对应符合条件元素的<strong>各维坐标</strong>。简单的条件用<strong>布尔索引</strong>也可替代，常用于绘制散点图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">pos = np.where(y == <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]<br>neg = np.where(y == <span class="hljs-number">0</span>)[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 返回 label 为 0 和 1 的样本点的索引</span><br><br>plt.scatter(X[pos, <span class="hljs-number">0</span>], X[pos, <span class="hljs-number">1</span>], marker=<span class="hljs-string">&quot;o&quot;</span>, c=<span class="hljs-string">&#x27;c&#x27;</span>)<br>plt.scatter(X[neg, <span class="hljs-number">0</span>], X[neg, <span class="hljs-number">1</span>], marker=<span class="hljs-string">&quot;x&quot;</span>, c=<span class="hljs-string">&#x27;r&#x27;</span>)  <span class="hljs-comment"># 绘制散点图</span><br></code></pre></td></tr></table></figure></li><li><p><code>X, Y = np.meshgrid(x, y)</code>：生成<strong>格点矩阵</strong>，<code>x</code>和 <code>y</code> 需是一维向量，输出的是一个 <code>x * y</code>格点矩阵的每个点的横纵坐标，<code>X</code> 和 <code>Y</code> 是形状为<code>(x, y)</code> 的二维矩阵。常用于绘制等高线图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">u = np.linspace(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">50</span>)<br>v = np.linspace(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">50</span>)<br>U, V = np.meshgrid(u, v)  <span class="hljs-comment"># 生成 50x50 网格点矩阵</span><br>z = (map_feature(U.flatten(), V.flatten()) @ theta).reshape((<span class="hljs-number">50</span>, <span class="hljs-number">50</span>))  <span class="hljs-comment"># 计算等高线</span><br>plt.contour(u, v, z, [<span class="hljs-number">0</span>], colors=<span class="hljs-string">&#x27;r&#x27;</span>)  <span class="hljs-comment"># 绘制轮廓图，取高度为 0 的点</span><br></code></pre></td></tr></table></figure></li><li><p><code>np.concatenate((np.ravel(a), np.ravel(b)))</code>：扁平化参数，其中<code>ravel(a)</code>是将多维数组变成一维数组，<code>concatenate((arr1, arr2, ...),axis=0)</code>是将一维数组首尾相接。有些 SciPy中的科学计算方法（梯度下降）需要一次性将所有参数线性输入，就可以用到这个。</p></li><li><p><code>np.cumsum(arr, axis=None)</code>：对一维、二维等数组求<strong>前缀和</strong>，如果不加<code>axis</code> 参数则默认按行展开为一维数组后求前缀和；如果为 0，按照行累加，为 axis=1，按照列累加。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>], [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.cumsum(a)<br>array([ <span class="hljs-number">1</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">6</span>, <span class="hljs-number">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">21</span>])  <span class="hljs-comment"># 展开成一维数组</span><br></code></pre></td></tr></table></figure></li><li><p><code>np.argsort(arr)</code>：返回的是元素值从小到大排序后的<strong>索引</strong>，内置快排实现，如果要从大到小，只需将参数改为<code>-arr</code>，或者在后面跟上 <code>[::-1]</code>表示逆序。常用于求完特征值和特征向量后进行排序，提取主成分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">5</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.argsort(a)<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])  <span class="hljs-comment"># 升序</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>np.argsort(-a)<br>array([<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>])  <span class="hljs-comment"># 降序</span><br><br><span class="hljs-comment"># PCA z</span><br>w, v = np.linalg.eig(M)<br>sorted_indices = np.argsort(-w)<br>eigenvalues = w[sorted_indices[:k]]<br>eigenvectors = v[:, sorted_indices[:k]].astype(np.float64).T<br></code></pre></td></tr></table></figure></li><li><p><code>grid = np.indices((H, W))</code>：返回一个给定<code>shape=(2,H,W)</code> 的<strong>序号网格数组</strong>，其中<code>grid[0]</code>里的每个元素为该位置元素的<strong>行</strong>序号，<code>grid[1]</code>为<strong>列</strong>序号，可以用于<strong>局部提取</strong>数组元素：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>x = np.arange(<span class="hljs-number">20</span>).reshape((<span class="hljs-number">5</span>,<span class="hljs-number">4</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>grid = np.indices((<span class="hljs-number">2</span>,<span class="hljs-number">3</span>))<br><span class="hljs-comment"># [[[0 0 0],</span><br><span class="hljs-comment">#   [1 1 1]],</span><br><span class="hljs-comment">#  [[0 1 2],</span><br><span class="hljs-comment">#   [0 1 2]]]</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>x[grid[<span class="hljs-number">0</span>], grid[<span class="hljs-number">1</span>]]<br><span class="hljs-comment"># [[0 1 2]</span><br><span class="hljs-comment">#  [4 5 6]]</span><br></code></pre></td></tr></table></figure></li><li><p><code>np.allclose(arr1, arr2)</code>：用于检查两个数组是否每个元素都相似，返回<strong>布尔值</strong>，默认在1e-5 的相对误差范围内。</p></li><li><p><code>np.nonzero(arr)</code>：用于得到 N维数组中非零元素的位置（数组索引），返回一个<strong>N元元组</strong>，每一个元素都为一个一维数组，<code>tuple[i][j]</code>代表第 <code>j</code> 个非零元素的第 <code>i</code> 维下标：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">a =[[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],<br>    [<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>],<br>    [<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>]]<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.nonzero(a)<br><span class="hljs-comment"># (array([0, 0, 0, 1, 2, 2], dtype=int64), array([0, 1, 2, 1, 1, 2], dtype=int64))</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>indices = np.stack(np.nonzero(a)).T<br><span class="hljs-comment"># array([[0,0], [0,1], [0,2], [1,1], [2,1], [2,2]])</span><br></code></pre></td></tr></table></figure></li><li><p><code>np.all()</code>、<code>np.any()</code>：用于判断整个数组中的元素是否<strong>全部满足</strong>条件、是否<strong>存在满足</strong>条件。本质上是实现了与、或运算。具体使用如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">a = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br>b = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span>(a == b).<span class="hljs-built_in">all</span>()<br><span class="hljs-comment"># False</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>(a == b).<span class="hljs-built_in">any</span>()<br><span class="hljs-comment"># True</span><br></code></pre></td></tr></table></figure></li><li><p><code>np.convolve(arr1, arr2)</code>：对两个<strong>一维数组</strong>的卷积运算（模拟信号操作），其中<code>arr2</code>会先翻转再平移。此外还可以选择模式设置是否存在边际效应。注：该方法实现简易，没有用到FFT 优化，速度较慢。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>np.convolve([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br><span class="hljs-comment"># array([4, 13, 28, 27, 18])</span><br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python笔记 #1 基础语法</title>
    <link href="/Python-Note-1.html"/>
    <url>/Python-Note-1.html</url>
    
    <content type="html"><![CDATA[<p>跟着贵系的暑培过了一遍语法，然而写代码时还是感到乏力，总觉得要打一遍才能记住，于是有了这篇博客。</p><p>本文部分内容参考了清华 AYF 同学的教程，部分参考了 Python 官方的一份<ahref="https://docs.python.org/zh-cn/3/tutorial/index.html">Tutorial</a>，函数用法参考了<a href="https://xyfjason.top">xyfJASON</a> 的博客。本文将持续更新。</p><h2 id="python-特性">Python 特性</h2><p>相比于 C 的编译型、弱类型、静态类型特点，Python则是一种解释型、强类型、动态类型的语言。</p><h3 id="交互式-vs-脚本">交互式 vs 脚本</h3><p>作为一种解释型语言，Python 不需要像 C一样编译运行，它可以<strong>逐行运行</strong>代码，因此又分为<strong>交互式窗口</strong>运行与<strong>脚本</strong>运行两种模式：</p><ul><li>交互式窗口：在 CLI 输入<code>python</code>，即可进入交互式窗口，输入一行代码即可运行。本文使用的交互式环境是<strong>IPython</strong>，输入 <code>ipython</code> 即可呼出。</li><li>脚本：在 CLI 输入<code>python xxx.py</code>，就会依次执行整个脚本文件。本文使用的 IDE 是<strong>PyCharm</strong>。</li></ul><h3 id="hello-world">Hello World</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Hello, world!&#x27;</span>) <span class="hljs-comment"># 打印输出</span><br>Hello, world!<br></code></pre></td></tr></table></figure><p>在交互式窗口，<code>&gt;&gt;&gt;</code> 作为提示符，在 IPython 中则是<code>In [1]:</code>。执行 <code>exit()</code> 或者按下<code>Ctrl+D</code> 就能退出窗口。</p><p>注意到，这行代码中没有 <code>;</code>分号，可以直接运行而无需编译，字符串用了单引号，注释用 <code>#</code>开始【与 C 不同】。</p><h2 id="简单数据类型">简单数据类型</h2><h3 id="变量类型">变量类型</h3><p>【与 C 不同】，Python<strong>不用</strong>声明变量类型，解释器自动解释。</p><ul><li><code>int</code>：变长整数，默认是 4字节，有需要时自动增长，用于高精度运算，还支持十六进制、八进制和二进制表示。</li><li><code>complex</code>：自带的复数类型，表示为<code>real + imag*1j</code> 的形式，虚部为 1 的时候不可省略 1，可以用<code>j</code> 也可以用 <code>J</code>。实虚部分别为一个<code>float</code>。</li><li><code>float</code>：8 字节浮点数，【相当于 C 的 double】。</li><li><code>bool</code>：True 和 False，注意首字母大写，用作数值计算时与 C一样视作 0 和 1。</li><li><code>NoneType</code>： None，空值，常用于返回值、特判。</li></ul><blockquote><p>需要单独说明的是，Python 会存储所有的 -5 到 256的整数，其他任何变量是这些值时，会被指向这个<strong>预先开好</strong>的内存，因此任何两个值为5 的 int 变量都指向同⼀内存地址。</p></blockquote><p>尽量用小写变量名（<strong>下划线法</strong>），这是 Python3的主流命名方式。</p><h3 id="运算符">运算符</h3><p>这里列出常见的运算符，运算符可以重载，重载需要修改其对应的定义函数。</p><p><strong>算术运算符</strong>：</p><ul><li><code>+</code> <code>-</code> <code>*</code>【与 C 相同】</li><li><code>%</code>：<code>35 % 4 == 3</code>，<code>-35 % 4 == 1</code>，<code>35 % -4 == -1</code>，<code>-35 % -4 == -3</code>，【与C 不同：负数对正数取模时返回正余数，而非像 C 那样返回负余数】</li><li><code>/</code>： <code>__trudiv__</code>，真除，得到<code>float</code> 结果</li><li><code>//</code>：<code>__floordiv__</code>，除后向下取整（不是舍弃小数），得到<code>int</code> 结果</li><li><code>**</code>： <code>__pow__</code>，幂运算</li></ul><p><strong>比较运算符</strong>：</p><ul><li><code>&lt;</code> <code>&lt;=</code> <code>&gt;</code><code>&gt;=</code> <code>==</code> <code>!=</code> 【与 C 相同】</li></ul><p><strong>位运算符</strong>:</p><ul><li><code>&amp;</code> <code>|</code> <code>^</code> <code>~</code><code>&lt;&lt;</code> <code>&gt;&gt;</code> 【与 C 相同】</li></ul><p><strong>赋值运算符</strong>：</p><ul><li><code>=</code>：赋值号，不能被重载</li><li><code>+=</code> <code>-=</code> <code>*=</code> <code>/=</code><code>%=</code> 【与 C 相同】</li><li>注意 Python 中没有 <code>++</code> <code>--</code>的运算符，只能通过 <code>+= 1</code> 实现</li></ul><p><strong>逻辑运算符</strong>：</p><ul><li><code>and</code> <code>or</code> <code>not</code> 【类似 C 中的<code>&amp;&amp;</code> <code>||</code><code>！</code>，具有<strong>短路机制</strong>】</li><li>对于 <code>and</code> 和 <code>or</code>，通常用于条件分支<code>bool</code> 的判断，如果非要连接 <code>int</code>变量，得到的结果不会直接转换为<code>bool</code>，而是返回<strong>能够得出结果</strong>的<strong>最后一个变量</strong>【与C 中的短路类似】</li></ul><p><strong>三目运算符</strong>：</p><ul><li><code>a if cond else b</code>：相当于 C 中的<code>cond ? a : b</code>，注意其参数顺序【与 C不同】，但更贴近自然语言</li></ul><p><strong>特殊条件运算符</strong>：</p><ul><li><code>in</code>：被包含于，详见下文「容器」，返回 bool</li><li><code>not in</code>：<code>in</code> 的否定，返回 bool</li><li><code>is</code>：判断两个变量的地址是否相同，不可重载，返回bool</li><li><code>is not</code>：判断两个变量地址是否不同，不可重载，返回bool</li></ul><h3 id="字符串">字符串</h3><p>Python将字符串封装成了<strong>基本类型</strong>并处理了多种运算，带来许多便利，注意基本类型本身是<strong>不可修改</strong>的，所谓修改其实是将<strong>重新生成</strong>另一个字符串，再将其<strong>赋值</strong>给目标。</p><p>此外，Python 中没有单独的字符类型，单个字符将被视为长度为 1的字符串。【与 C 不同】，可以用 <code>""</code> 或者 <code>''</code>括起字符串。</p><p>下面是一些常用函数，设 <code>str</code> 是一个字符串：</p><ul><li><code>str.title()</code>：返回单词首字母大写，其余字母小写（不管以前是不是大写）的字符串</li><li><code>str.upper()</code>、<code>str.lower()</code>：返回全大/小写的字符串</li><li><code>str1 + str2</code>：返回用加号拼接的字符串【与 C++ 的<code>string</code> 类似】</li><li><code>str * 3</code>：返回重复三遍拼接的字符串</li><li>制表符 <code>\t</code>，换行符 <code>\n</code>【与 C 相同】</li><li><code>str.lstrip()</code>、<code>str.rstrip()</code>、<code>str.strip()</code>:返回删除开头/末尾/两端空白的字符串</li><li><code>str.replace(str1, str2)</code>：将字符串中的单词<code>str1</code> 全部替换成 <code>str2</code></li><li><code>str.split(str1)</code>：以 <code>str1</code>为分隔符把字符串拆成子串，并返回包含子串的列表，默认分隔符为空格</li><li><code>str.zfill(n)</code>：将数字字符串补全<strong>前导零</strong>，<code>n</code>为总位数</li><li><code>' '.join("a", "b", "c")</code>：用 <code>' '</code>作为<strong>连接符</strong>，拼接所有子串得到一个长字符串</li></ul><p>此外还有一种跨行字符串，用 <code>’‘’</code> 或 <code>”“”</code>括起来，由于脚本中顺序执行时不会输出变量值，这类字符串常用于<strong>跨行注释</strong>，特别是函数头注释。</p><h3 id="输入输出与编码">输入输出与编码</h3><p>此处说明几个重要函数：</p><ul><li><code>len(obj)</code>：获取 <code>obj</code>的长度，常用于获取字符串（注意是 Unicode，因此中文和英文字符都占 1位）、字节串、容器的长度</li><li><code>str(a)</code>: 把数字（整型或浮点型） <code>a</code>转换成字符串</li><li><code>chr(0x4f60)</code>：将整型变量 <code>i</code>转化成单个字符</li><li><code>ord('你')</code>：获取单个字符的编码（Unicode），注意在Unicode 中，英文字母的编码有意设置与 ASCII 码一致</li></ul><p>最常用的输出语句<code>print</code>，本质上是将变量<strong>转化为字符串</strong>输出，在末尾自动<strong>换行</strong>。该函数可以有多个变量，变量间用<code>,</code> 分隔，输出时会用空格隔开。</p><p>如果要在一句话中插入许多变量，这条 <code>print</code>语句可能会很丑陋，因此 Python 中有三种格式化字符串的方法。</p><ul><li><code>%</code>：如 <code>print('I am %d' % (age))</code>，【与 C类似】</li><li><code>str.format()</code>：如<code>print('hello, &#123;&#125;'.format(name))</code></li><li><code>f-string</code>：如<code>print(f'hello,&#123;name&#125;.I am &#123;age&#125;')</code></li></ul><p>其中 <code>f-string</code> 是 Python3.6 的新特性，最为直观便利。</p><p>此外，<code>input('Press ENTER to continue')</code>是常见的一种输入语句，会显示对应的提示内容，读入内容以以字符串存储，可以用<code>int(input())</code> 或 <code>map(int, input().split())</code>处理。</p><h3 id="字节串">字节串</h3><p><code>bytes</code> 即是 Python中的字节串，它表示最纯粹的<strong>二进制</strong>数据，【类似 C 的<code>unsigned char *</code>】，但是在显示上它仅支持 ASCII显示，因此用肉眼看显得有些不伦不类，通常它只存在于数据的处理过程中。</p><p><code>bytes</code> 的构造与字符串类似，但是要加一个 <code>b</code>做前导，如 <code>print(b'\x41')</code>。</p><h2 id="容器">容器</h2><p>Python 提供了一系列内置容器，它们如同 C++ 的 STL ⼀样，不过比 STL的用法灵活得多。</p><p>同样先介绍几个重要函数：</p><ul><li><code>type(obj)</code>：可以获取参数 <code>obj</code> 的类型</li><li><code>isinstance(obj, class_or_tuple)</code>：可以判断<code>obj</code> 是不是类的实例</li><li><code>id(obj)</code>：获取 <code>obj</code> 的地址，<code>a is b</code> 等价于 <code>id(a) == id(b)</code></li></ul><h3 id="列表-list">列表 | List</h3><p>列表（list）是很常用的容器，常被看作 Python 中的数组，但实际上【与C++ 的 vector 类似】。设 <code>lst</code> 是一个列表：</p><h4 id="基础操作">基础操作</h4><ul><li>定义列表：<code>lst = [a, b, c, d]</code> ，其中<code>a,b,c,d</code> 等是列表的元素，<strong>类型可以不同</strong></li><li>构造空列表：直接写 <code>[]</code> 或 <code>list()</code></li><li>打印列表：<code>print(lst)</code>（会将列表中的元素列出，括在<strong>方括号</strong>里）</li><li>访问元素：<code>lst[3]</code> ， 下标从 <strong>0</strong>开始。此外，还支持<strong>负数</strong>索引，<code>-1</code>表示倒数第一个，<code>-2</code> 倒数第二个【与 C 不同】</li></ul><h4 id="修改添加删除元素">修改、添加、删除元素</h4><ul><li>修改：直接访问元素并赋值</li><li><code>lst.append(x)</code>：在列表末尾添加元素 <code>x</code></li><li><code>lst1.extend(lst2)</code>：在列表末尾拼接另一个列表，也可以用运算符<code>+=</code></li><li><code>lst.insert(idx, x)</code>：在列表索引 <code>idx</code>处插入一个元素 <code>x</code> （插入后，<code>x</code> 的索引是<code>idx</code>，其后的元素后移一格）</li><li><code>del lst[3]</code>：删除指定元素（删除后，其后元素前移一格）</li><li><code>lst.pop()</code>：弹出<strong>并返回</strong>最后一个元素</li><li><code>lst.pop(idx)</code>：弹出<strong>并返回</strong>指定元素</li><li><code>lst.remove(x)</code>：删除<strong>第一个</strong>值为<code>x</code> 的元素</li></ul><h4 id="组织列表">组织列表</h4><ul><li><code>lst.sort()</code>、<code>lst.sort(reverse = True)</code>：对列表排序，永久性修改顺序</li><li><code>sorted(lst)</code>、<code>sorted(lst, reverse = True)</code>：返回排序后的列表，但<strong>不改变</strong>列表原有顺序</li><li><code>lst.reverse()</code>：翻转列表，永久性修改顺序</li><li><code>len(lst)</code>：返回列表长度，即元素个数（不论类型）</li></ul><h4 id="遍历列表">遍历列表</h4><ul><li><p>从头到尾遍历列表：<code>for i in lst:</code> 循环表达式，【与 C不同】<code>i</code> 是列表<strong>元素</strong>，不是索引；循环结束后<code>i</code> 停留为<strong>最后一个</strong>元素</p></li><li><p>遍历列表时同时遍历下标：<code>for idx, value in enumerate(lst):</code></p></li><li><p>若要检查列表是否为空，可以用 <code>if lst:</code>条件表达式，返回 bool</p></li></ul><h4 id="列表切片">列表切片</h4><ul><li><code>lst[l:r]</code>：返回一个列表，元素依次是 <code>lst</code>列表的索引在<strong>左闭右开区间</strong>内的元素，省略 <code>l</code>或 <code>r</code> 则默认从头开始或到尾结束</li><li><code>lst[l:r:step]</code>：指定步长为 <code>step</code>切片，<code>step</code> 为 -1 时返回倒序，省略参数后写作<code>lst[::-1]</code></li><li>可以用循环遍历列表切片：<code>for i in lst[l:r]:</code></li><li>复制列表：在切片中同时省略 <code>l</code> 和<code>r</code>，即返回<strong>从头到尾</strong>的列表，如<code>lst2 = lst1[:]</code>，而非<code>lst2=lst1</code>，后者<code>lst1</code> 和 <code>lst2</code>实质是同一个列表【类似 C 的引用】</li></ul><h3 id="元组-tuple">元组 | Tuple</h3><p>元组就是元素值<strong>不可修改</strong>（弱意义上的，其中<strong>元素的元素</strong>可以被修改）的列表。设<code>tpl</code> 是一个元组：</p><h4 id="基础操作-1">基础操作</h4><ul><li>定义元组：<code>tpl = (a, b, c, d)</code>，把列表定义中的方括号改成圆括号<code>()</code> 即可</li><li>定义时的小括号有时候可以省略，可以直接用 <code>,</code>逗号构造元组【与 C 不同，没有逗号运算符】</li><li>构造单元组：<code>(1)</code> 会被理解成表达式，要用<code>(1,)</code></li><li>构造空元组，直接写 <code>()</code> 或 <code>tuple()</code>，但<code>(,)</code> 会报错</li><li>访问元素：<code>tpl[3]</code> ， 下标从 <strong>0</strong>开始。</li></ul><h4 id="遍历元组">遍历元组</h4><p><code>for i in tpl:</code> 和列表一样。</p><h4 id="修改元组">修改元组</h4><p>元组中元素的值不能修改，但是<strong>元组变量本身</strong>可以被赋值，这点与字符串类似。此外，如果元组的中的元素是可修改的，如List，则可以修改 List 内部的元素。</p><h4 id="元组解包">元组解包</h4><p>经典的 Python交换赋值代码：<code>a, b = b, a</code>，利用了解包和逗号构造。</p><p>其余的若干种解包方法待补充：https://zhuanlan.zhihu.com/p/351369448</p><h4 id="元组打包">元组打包</h4><p><code>zip()</code>函数用于将<strong>若干个迭代容器</strong>打包为元组（通常是两个List），返回一个 <code>zip</code> 对象。如果初始 List的大小不同，则会向最小的<strong>对齐</strong>。初始的 <code>zip</code>对象不占内存空间（Python 3.x 新特性），如果要展示，则需手动<code>list()</code> 转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span>b = [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span>zipped = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(a,b))     <span class="hljs-comment"># 打包为元组的列表</span><br>[(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">5</span>), (<span class="hljs-number">3</span>, <span class="hljs-number">6</span>)]<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(*zipped))          <span class="hljs-comment"># 与 zip 相反，*zipped 可理解为解包，返回二维矩阵</span><br>[(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>)]<br><span class="hljs-meta">&gt;&gt;&gt; </span>c, d = <span class="hljs-built_in">zip</span>(*zipped)<span class="hljs-comment"># 解包后利用逗号构造两个列表</span><br></code></pre></td></tr></table></figure><h3 id="集合-set">集合 | Set</h3><p>集合由一组无序、互不重复的元素构成（在数学上也是如此），在内部用哈希实现。设<code>st</code> 是一个集合：</p><h4 id="基础操作-2">基础操作</h4><ul><li>定义集合：<code>st = &#123;1, 2, 3&#125;</code>，把列表定义的方括号改成花括号<code>&#123;&#125;</code> 即可</li><li>构造空集合：只能用 <code>set()</code>，因为 <code>&#123;&#125;</code>的表达被空字典占用了</li><li>由列表转集合：<code>st = set([1, 1, 1, 2, 2, 3])</code>，会自动去重，常用于去重列表的遍历<code>for i in set(lst):</code></li><li>注意集合的元素必须是可 hash 的，不能为 list 这种可变容器变量</li></ul><h4 id="添加删除元素">添加、删除元素</h4><ul><li><code>st.add(4)</code>：添加一个元素</li><li><code>st.remove(2)</code>：删除一个元素</li></ul><h3 id="字典-dictionary">字典 | Dictionary</h3><p>字典是一系列「<strong>键值对</strong>」，用于存储一组有穷映射，可将任何Python 对象作为值，【类似于更高端版本的 C++ 的 map】。</p><h4 id="基础操作-3">基础操作</h4><ul><li>定义字典：<code>dic = &#123;'name': '张三', 'age': 18&#125;</code>，<strong>花括号</strong>括起一系列键值对，键与值之间冒号<code>:</code> 分隔，键值对之间逗号 <code>,</code> 分隔。</li><li>访问元素：<code>d['name']</code>，用键访问</li><li>注意字典的键必须是可 hash 的，不能为 list 或 set这种可变容器变量，但可以是 tuple</li></ul><h4 id="添加修改删除">添加、修改、删除</h4><ul><li>添加：直接赋值即可（即使键本来不存在），如：<code>dic['x'] = 0</code></li><li>修改：<code>dic['age'] = 18</code>，直接赋值即可</li><li>删除：<code>del dic['age']</code></li></ul><h4 id="遍历字典">遍历字典</h4><ul><li>遍历所有键值对：<code>for k, v in dic.items():</code>，其中<code>items()</code> 返回键值对的<strong>列表</strong></li><li>遍历所有键：<code>for k in dic.keys():</code>，其中<code>keys()</code> 返回键的<strong>列表</strong>，可省略</li><li>由于 <code>keys()</code>的本质是<strong>列表</strong>，各种对列表的操作也适用，如：<code>for k in sorted(dic.keys()):</code>或 <code>if 'age' in dic.keys():</code></li><li>遍历所有值：<code>for v in dic.values():</code>，其中<code>values()</code> 返回值的<strong>列表</strong></li></ul><h3 id="迭代器">迭代器</h3><p>前文提到的所有容器，包括字符串、字节串都是可迭代的，这意味着它们可以用<code>for</code>循环来遍历，也可以用<strong>生成式</strong>构造（下面介绍）。</p><p>但最为特殊的迭代器是 <code>range</code>类型，作为<strong>数值列表</strong>，与列表有相似之处，但它实际上<strong>不占用内存</strong>（用于大循环时很节省空间）。下面是一些常用方法：</p><ul><li><code>range(l, r)</code>：依次生成<strong>左闭右开区间</strong>中的整数【类似于C++ 的<code>for(int i = l; i &lt; r; i++)</code>】，如果省略左端，会默认以 0开始</li><li><code>range(l, r, step)</code>：指定步长为 <code>step</code>【类似于C++ 的 <code>for(int i = l; i &lt; r; i += step)</code>】</li><li><code>min(range(l, r))</code>、<code>max(range(l, r))</code>、<code>sum(range(l, r))</code>：返回数值列表的最小值、最大值、总和</li><li>在 <code>for i in range(10):</code> 的循环中改变了 <code>i</code>的值，如 <code>i += 1</code>，不会影响循环的次数，因为迭代器中的数值列表是<strong>不可修改</strong>的【与C 不同】</li></ul><h3 id="生成式">生成式</h3><p>使用生成式构造容器是非常常见、高效的操作，下面举几个例子：</p><ul><li><code>a = list(range(10))</code>：直接转化数值列表为基本列表</li><li><code>lst = [i ** 2 for i in a]</code>：列表生成式</li><li><code>st = &#123;x % 7 for x in a&#125;</code>：集合生成式</li><li><code>dic = &#123;x ** 2: x for x in a&#125;</code>：字典生成式，保留<code>:</code> 来分隔键与值</li><li><code>tpl = tuple(x + 1 for x in a)</code>：元组生成式，直接用<code>()</code> 会被当成表达式</li></ul><p>综上所述，生成式表达可以用三段式：<code>表达式 for 循环变量 in 迭代对象 if 筛选条件</code>，其中最后的筛选条件不一定要。</p><p>此外，还可以用更简单的 <code>map()</code>函数构造容器，其内置了一系列<strong>映射句柄</strong>，如<code>map(int, ['1', '2', '3'])</code>，或<code>map(square, [1, 2, 3])</code>。</p><h2 id="流程控制">流程控制</h2><p>Python中不用大括号来显式地分块，而是用<strong>冒号配合缩进</strong>（Indent）。代码块与代码块之间至少隔着一个空行表示结束。当一个代码块必须存在，又不想写任何语句的时候，可以写一个<code>pass</code> 作为占位符。</p><h3 id="条件分支">条件分支</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">a = <span class="hljs-number">1</span><br><span class="hljs-keyword">if</span> a == <span class="hljs-number">1</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;a is 1&#x27;</span>)<br><span class="hljs-keyword">elif</span> a == <span class="hljs-number">0</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;a is 0&#x27;</span>)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;wtf is a?&#x27;</span>)<br></code></pre></td></tr></table></figure><p>注意，<code>elif</code> 和 <code>else</code>后面也要有冒号配合缩进，如果有多个条件，用 <code>and</code> 和<code>or</code> 逻辑运算符连接。</p><h3 id="for-循环">for 循环</h3><p>前面所列的容器、数值列表都可以用于 <code>for</code>循环迭代，比较特别的是字符串也可以迭代：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a = <span class="hljs-string">&#x27;hello world&#x27;</span><br><span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> a:<br>    <span class="hljs-built_in">print</span>(c) <span class="hljs-comment"># 竖着打印出 hello world，因为 print 自带换行</span><br></code></pre></td></tr></table></figure><p>此外，如果在数值列表的迭代中用不到迭代变量<code>i</code>，仅作为迭代次数使用，可以用 <code>_</code> 变量表达。</p><h3 id="while-循环">while 循环</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">a = <span class="hljs-number">10</span><br><span class="hljs-keyword">while</span> <span class="hljs-number">2</span> &lt;= a &lt; <span class="hljs-number">20</span>:    <span class="hljs-comment"># 语法糖，这种写法是被建议使⽤的</span><br>    <span class="hljs-built_in">print</span>(a)<br>    a -= <span class="hljs-number">1</span>            <span class="hljs-comment"># 注意 Python 中没有⾃增 1 和⾃减 1 运算符</span><br></code></pre></td></tr></table></figure><p>跳出循环可以用 <code>break</code> 和 <code>continue</code>【与 C相同】</p><h3 id="异常控制">异常控制</h3><p>在 Python 中，我们可以使用 <code>try</code> 和 <code>except</code>语句来捕获和处理异常。异常处理使我们能够在程序出现错误时执行一些特定的操作，而不是让程序崩溃。下面是基本用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">try</span>:<br>    <span class="hljs-comment"># 可能会引发异常的代码块</span><br><span class="hljs-keyword">except</span> ExceptionType1:<br>    <span class="hljs-comment"># 处理特定类型的异常，这里需要填入异常类名</span><br><span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>    <span class="hljs-comment"># 处理未知异常，Exception 是大多数异常类的父类</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;发生了一个未知错误:&quot;</span>, e)<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-comment"># 当没有发生任何异常时执行的代码，例如打印成功信息</span><br><span class="hljs-keyword">finally</span>:<br>    <span class="hljs-comment"># 无论是否发生异常都会执行的代码，例如文件关闭</span><br></code></pre></td></tr></table></figure><blockquote><p>注意，如果这里存在 Exception 还无法捕获的异常，例如KeyboardInterrupt（在命令提示符中如果按下 Ctrl+C结束终止的键），则可以换成 BaseExpection，这是 Exception 的父类。</p></blockquote><p>另一种常用的方法是循环 <code>try</code> 语句，适用于 API接口的访问：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-comment"># 尝试某种操作</span><br>        <span class="hljs-keyword">pass</span><br>        <span class="hljs-comment"># 如果操作成功，跳出循环（如果操作失败则不会执行到 break）</span><br>        <span class="hljs-keyword">break</span><br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        <span class="hljs-comment"># 记录异常信息到日志文件</span><br>        logging.error(<span class="hljs-string">f&#x27;Error: <span class="hljs-subst">&#123;e&#125;</span>&#x27;</span>)<br>        <span class="hljs-comment"># 等待一段时间后再次尝试</span><br>        time.sleep(<span class="hljs-number">5</span>)  <span class="hljs-comment"># 5秒后重试</span><br></code></pre></td></tr></table></figure><h2 id="函数">函数</h2><h3 id="基础函数">基础函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fib</span>(<span class="hljs-params">n</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    这个函数用于计算斐波那契数 fib(n)</span><br><span class="hljs-string">    参数：</span><br><span class="hljs-string">    - n: 需要计算的第 n 个斐波那契数</span><br><span class="hljs-string">    返回值：</span><br><span class="hljs-string">    - 第 n 个斐波那契数 fib(n)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    current, then = <span class="hljs-number">0</span>, <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>        current, then = then, current + then<br>    <span class="hljs-keyword">return</span> current<br><br>fib(<span class="hljs-number">10</span>) <span class="hljs-comment"># 调用函数</span><br></code></pre></td></tr></table></figure><h4 id="函数传参">函数传参</h4><p>Python 是<strong>不允许</strong>程序员选择采用传值还是传址的。Python参数传递采用的肯定是「<strong>传对象引用</strong>」的方式【类似 C中传值和传址的结合】。如果函数收到的是一个<strong>可变对象</strong>（比如list、dict）的引用，就能修改对象的原始值；如果是一个<strong>不可变对象</strong>（比如int、float、str）的引用，就不能直接修改原始对象。</p><p>传入参数的数量可以<strong>不固定</strong>，但是必须指定默认值；也可以<strong>调换顺序</strong>，但必须指明对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add_numbers</span>(<span class="hljs-params">num1, num2 = <span class="hljs-number">0</span></span>):</span><br>    result = num1 + num2<br>    <span class="hljs-keyword">return</span> result<br>add_numbers(<span class="hljs-number">1</span>) <span class="hljs-comment"># 参数 num2 采用默认 0</span><br>add_numbers(b = <span class="hljs-number">2</span>, a = <span class="hljs-number">1</span>) <span class="hljs-comment"># 调换顺序，指明对象</span><br></code></pre></td></tr></table></figure><p>当然，也可以传递列表等容器，但传递的也是列表的地址，在函数中修改同样会改变原列表，如果不想修改原列表可以用<code>[:]</code> 传递切片。</p><h4 id="传递任意数量的参数">传递任意数量的参数</h4><ul><li>在形参前加星号 <code>*</code>，Python3会创建一个该形参名称的<strong>元组</strong>，本质上是一种元组解包</li><li>在形参前加双星号 <code>**</code>，Python3会创建一个该形参名称的<strong>字典</strong></li></ul><h4 id="返回值">返回值</h4><p>作为动态类型语言，函数返回值<strong>可以不固定</strong>，可以多个<code>return</code> 在不同情况下返回不同值，或者没有<code>return</code>（等价于 <code>return None</code>）。</p><p>值得注意的是，Python中<strong>虽然不需要指定参数和返回值的类型</strong>，但可以使用<strong>类型提示</strong>（TypeHints）来在函数定义时体现参数和返回值的类型。这是一种对函数参数和返回值进行注释的方法，它可以增加代码的可读性和可维护性，并提供了一种方便的方式来指定函数的输入和输出类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add_numbers</span>(<span class="hljs-params">num1: <span class="hljs-built_in">int</span>, num2: <span class="hljs-built_in">int</span> = <span class="hljs-number">2</span></span>) -&gt; <span class="hljs-built_in">int</span>:</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    这个函数用于计算两个整数的和，并返回结果。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    result = num1 + num2<br>    <span class="hljs-keyword">return</span> result<br></code></pre></td></tr></table></figure><h3 id="函数模块调用">函数模块调用</h3><p>函数可以被存储在模块中被调用，<strong>模块</strong>是扩展名为 .py的文件，包含函数的代码【类似于 C 的头文件】</p><ul><li>导入<strong>整个模块</strong>：使用 <code>import pizza</code>导入，调用时使用 <code>.</code>句点，如：<code>pizza.make(16, 'green peppers')</code></li><li>导入模块中<strong>特定函数</strong>：使用<code>from pizza import make, eat</code>，调用时无需句点，直接用函数名</li><li>导入特定函数<strong>别名</strong>：使用<code>from pizza import make as mk</code>，调用时无需句点，直接用别名</li><li>导入模块中<strong>所有函数</strong>：使用<code>from pizza import *</code>，调用时无需句点，但是会<strong>污染命名空间</strong>，不建议使用</li></ul><p>所有 <code>import</code> 都放在程序开头【类似于 C++ 的<code>#include&lt;&gt;</code>】。</p><h4 id="第三方模块">第三方模块</h4><p>作为一种工具语言，大部分情况我们都是通过「调包」完成任务，即导入第三方库。市面上的第三方库可以通过多种途径获取，包括<code>pip</code>、<code>conda</code> 等。在命令行中使用<code>pip list</code> 即可查看当前安装的所有库和版本信息，也可以结合管道<code>pip list | findstr numpy</code>（Windows）。</p><h4 id="模块封装">模块封装</h4><p>在 Python 的程序模板中经常看见<code>if __name__ = '__main__'</code>，这其实是 Python 中的一种 MagicMethod。</p><p>Python 中所有 <code>.py</code>文件都可被视为一个模块，当模块作为脚本被<strong>直接运行</strong>时，其<code>__name__</code> 的值变为<code>__main__</code>。相反，当模块被其他<strong>导入其他脚本运行</strong>时，其<code>__name__</code> 存放的就是模块的名字（类似环境变量）。</p><p>而作为一种解释型语言，Python会将所有导入的代码全部顺序执行一遍。如果我们在一个脚本中实现了某个算法，想将其作为模块在其他脚本中调用，我们并不希望它在<code>import</code>的那一刻就执行，因此，正确的做法是将其封装成函数。</p><p>但如果要单独运行该模块，譬如调试功能时，使用 <code>__name__</code>就可以完美解决：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">func</span>():</span><br><span class="hljs-comment"># 这里写函数代码</span><br>    <span class="hljs-keyword">return</span><br><br><span class="hljs-keyword">if</span> __name__ = <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    func() <span class="hljs-comment"># 这里调试函数</span><br></code></pre></td></tr></table></figure><h3 id="lambda-匿名函数">Lambda 匿名函数</h3><p>和很多语言⼀样，Python 中可以使用 Lambda匿名函数完成一些简单的逻辑，但比较特殊的地方在于，Python中匿名函数必须只由<strong>单个表达式</strong>构成，这个表达式的值也就是匿名函数的返回值。</p><p><code>lambda</code>关键字可以用来创建一个匿名函数，紧跟其后的是<strong>参数列表</strong>和用冒号<code>:</code> 分割开的单个表达式。如，<code>lambda x: 2 * x</code>是将任何输入的数乘 2，而 <code>lambda x, y: x+y</code>是计算两个数字的和。</p><p>使用匿名函数的经验准则是保持简单以及只在本地使用一次。一种常见却不推崇的做法是将其作为简单函数的另一种<strong>声明</strong>方式，赋值给<strong>变量</strong>，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>doubler = <span class="hljs-keyword">lambda</span> x: <span class="hljs-number">2</span> * x<br><span class="hljs-meta">&gt;&gt;&gt; </span>doubler(<span class="hljs-number">5</span>)<br><span class="hljs-number">10</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">type</span>(doubler)<br>&lt;<span class="hljs-class"><span class="hljs-keyword">class</span> &#x27;<span class="hljs-title">function</span>&#x27;&gt;</span><br></code></pre></td></tr></table></figure><blockquote><p>对 lambda函数命名的唯一作用可能是出于教学目的，其问题在于这使得调试不那么直观——错误信息只会提示某个<code>lambda</code> 函数存在问题，但不会提示哪个函数。</p></blockquote><p>正确的做法应该是将其作为<strong>参数</strong>传递，如<code>.sort</code> 函数、<code>sorted</code>函数等，此时单个表达式会被计算为一个值并且参与后续的计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>integers = [(<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), (<span class="hljs-number">5</span>, <span class="hljs-number">1</span>), (-<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)]<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">sorted</span>(integers, key=<span class="hljs-keyword">lambda</span> x: x[-<span class="hljs-number">1</span>])<br>[(<span class="hljs-number">3</span>, -<span class="hljs-number">3</span>), (<span class="hljs-number">5</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), (-<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)]<br><br><span class="hljs-meta">&gt;&gt;&gt; </span>nums = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span>nums.sort(key=<span class="hljs-keyword">lambda</span> a: <span class="hljs-built_in">abs</span>(<span class="hljs-number">5</span>-a))<br><span class="hljs-meta">&gt;&gt;&gt; </span>nums<br>[<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>, <span class="hljs-number">2</span>, <span class="hljs-number">8</span>, <span class="hljs-number">1</span>, <span class="hljs-number">9</span>]<br></code></pre></td></tr></table></figure><h2 id="文件操作">文件操作</h2><p><code>open()</code> 函数的第二个实参 <code>w</code>表示写入（自动创建或<strong>覆盖</strong>原内容），<code>r</code>表示只读，<code>a</code>表示附加（自动创建或<strong>添加到文件末尾</strong>），<code>r+</code>表示读写。如果不加第二个实参，则默认为 <code>r</code> 只读。</p><h3 id="读取文件">读取文件</h3><p>一种【与 C 相似】的操作是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">file = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;temp.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>)  <span class="hljs-comment"># 打开一个文件，返回表示文件的对象</span><br>contents = file.read()<span class="hljs-comment"># 读取整个文件</span><br><span class="hljs-built_in">print</span>(contents)<br>file.close()<span class="hljs-comment"># 关闭文件</span><br></code></pre></td></tr></table></figure><p>此外，还有一种更推崇的操作方式，使用 <code>with</code>上下文管理器，将<strong>打开的文件的对象</strong>存储在 <code>as</code>后的变量，这样可以避免由于忘记关闭文件导致丢失数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;temp.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>    contents = file.read()<br>    <span class="hljs-built_in">print</span>(contents)<br></code></pre></td></tr></table></figure><p>回到读取本身，方法 <code>read()</code>读取<strong>整个文件</strong>的内容并返回<strong>单个字符串</strong>，并包含每个<code>\n</code> 换行符。</p><p>但实际操作中读入<strong>一个长字符串</strong>是丑陋而且难以处理的，我们更倾向于逐行读入，用<code>line.rstrip()</code> 消除行末的 <code>\n</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;temp.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> file:  <span class="hljs-comment"># file 也可作为一个迭代器</span><br>        <span class="hljs-built_in">print</span>(line)<span class="hljs-comment"># 注意每一行末都会有一个 \n 被打印，而 print 本身又自带换行</span><br></code></pre></td></tr></table></figure><p>另一种逐行读入是创建一个包含文件各行内容的<strong>列表</strong>，每个元素是一行内容的字符串，包含行尾的<code>\n</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;temp.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>    line1 = file.readline()<span class="hljs-comment"># 读入第一行作为字符串</span><br>    lines = file.readlines()<span class="hljs-comment"># 读入第二行以后的行（文件指针被移动了），作为 List[]</span><br></code></pre></td></tr></table></figure><h3 id="写入文件">写入文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;temp.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>    file.write(<span class="hljs-string">&quot;I love programming.\n&quot;</span>) <span class="hljs-comment"># wirte 不会换行，要手动加</span><br></code></pre></td></tr></table></figure><p>方法 <code>write()</code>表示将<strong>字符串</strong>写入文件。如果要写入数值，应先用<code>str()</code> 将其转化为字符串。</p><p>同样，按行写入也有对应的函数<code>writelines()</code>，但要求传入的参数为字符串列表，同时每个字符串后面<strong>都要自带</strong>换行符，因此比较鸡肋。</p><p>如果要追加写入文件，则需要将 <code>w</code> 替换为 <code>a</code>模式，需要注意的是新文本可能会在旧文本之后立即添加，因此<strong>可能</strong>需要添加一个换行符<code>file.write("\n")</code>。</p><h2 id="类">类</h2><p>当涉及到面向对象编程（Object-Oriented Programming, OOP），Python提供了类（class）的概念，允许你创建对象、定义属性和方法。</p><h3 id="类与对象">类与对象</h3><p>在 Python中，类是创建对象的蓝图。<strong>类定义了对象的属性和方法，可以通过实例化（即创建对象）来使用类</strong>。下面介绍类与对象的基础用法。</p><h4 id="定义类">定义类</h4><p>可以使用 <code>class</code>关键字来定义一个类。以下是一个简单的类的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, name</span>):</span><br>        self.name = name<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">greet</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Hello, my name is <span class="hljs-subst">&#123;self.name&#125;</span>.&quot;</span>)<br></code></pre></td></tr></table></figure><p>类包含了一个特殊的方法<code>__init__</code>，它被称为<strong>构造函数</strong>（Constructor）。构造函数在创建对象时被调用，并用于初始化对象的属性。在构造函数中，<code>self</code>是一个特殊的参数，它代表<strong>实例化后的对象本身</strong>。我们可以使用<code>self</code> 来引用对象的属性和方法。</p><h4 id="创建对象">创建对象</h4><p>要创建类的实例（即对象），可以<strong>像调用函数一样</strong>使用类的名称并传递所需的参数。以下是一个创建<code>Person</code> 类的对象的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">person = Person(<span class="hljs-string">&quot;Alice&quot;</span>) <span class="hljs-comment"># 传递了名为 &quot;Alice&quot; 的参数</span><br></code></pre></td></tr></table></figure><h4 id="访问属性和调用方法">访问属性和调用方法</h4><p>要访问对象的属性和调用对象的方法，可以使用点号 <code>.</code>运算符。示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-built_in">print</span>(person.name)  <span class="hljs-comment"># 输出: Alice</span><br>person.greet()  <span class="hljs-comment"># 输出: Hello, my name is Alice.</span><br></code></pre></td></tr></table></figure><h3 id="继承与多态">继承与多态</h3><p>在面向对象编程中，继承和多态是重要的概念。继承允许创建一个新的类，它<strong>从现有的类派生，并继承其属性和方法</strong>。<strong>多态允许子类重写父类的方法</strong>，以实现不同的行为。</p><h4 id="继承">继承</h4><p>要创建一个继承自另一个类的子类，可以将父类<strong>作为子类定义中的参数</strong>。示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span>(<span class="hljs-params">Person</span>):</span> <span class="hljs-comment"># 继承的父类作为子类定义中的参数</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, name, student_id</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__(name) <span class="hljs-comment"># 相当于创建子对象时同时创建一个父对象</span><br>        self.student_id = student_id<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">study</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;self.name&#125;</span> is studying.&quot;</span>)<br></code></pre></td></tr></table></figure><p>这里定义了一个名为 <code>Student</code> 的子类，它继承自<code>Person</code> 父类。子类重写了父类的构造函数，并添加了一个名为<code>student_id</code> 的新属性和一个名为 <code>study</code>的新方法。使用 <code>super()</code>函数可以<strong>在子类中调用父类的方法</strong>。</p><h4 id="多态">多态</h4><p>多态允许不同的对象对相同的方法进行不同的实现。示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">introduce</span>(<span class="hljs-params">person</span>):</span><br>    person.greet()<br><br>person = Person(<span class="hljs-string">&quot;Alice&quot;</span>)<br>student = Student(<span class="hljs-string">&quot;Bob&quot;</span>, <span class="hljs-string">&quot;12345&quot;</span>)<br><br>introduce(person)  <span class="hljs-comment"># 输出: Hello, my name is Alice.</span><br>introduce(student)  <span class="hljs-comment"># 输出: Hello, my name is Bob.</span><br></code></pre></td></tr></table></figure><p>在上面的示例中，我们定义了一个名为 <code>introduce</code>的函数，它接受一个 <code>Person</code> 类的对象作为参数，并调用对象的<code>greet</code> 方法。通过向 <code>introduce</code>函数传递不同的对象，可以实现多态行为。</p>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #01 梯度下降：一元线性回归</title>
    <link href="/ML-Note-1.html"/>
    <url>/ML-Note-1.html</url>
    
    <content type="html"><![CDATA[<p>该笔记是观看斯坦福大学教授 <ahref="https://scholar.google.com/citations?hl=zh-CN&amp;user=mG4imMEAAAAJ">Andrew-Ng</a>在 <a href="https://www.coursera.org/">Coursera</a> 上开设的 <ahref="https://www.coursera.org/learn/machine-learning">Machine-Learning</a>网课所做的笔记。由于网上可见的该课程笔记繁多，而本文所记内容并无不同，仅供个人娱乐。BTW，做笔记的过程中还参考了<a href="https://xyfjason.top">xyfJASON</a>的博客，解答了我许多疑惑。</p><h2 id="概论">概论</h2><p>首先给出两个「机器学习」的定义，尽管对于相关从业者来说，也不存在一个被广泛认可的定义来准确定义机器学习是什么，但或许有助于回答诸如「机器学习是什么？」之类的面试问题：</p><ul><li>Arthur Samuel (1959). Machine Learning: Field of study that givescomputers the ability to learn <strong>without</strong> being explicitlyprogrammed.</li><li>Tom Mitchell (1998) Well-posed Learning Problem: A computer programis said to <em>learn</em> from <strong>experience</strong> E withrespect to some <strong>task</strong> T and some <strong>performancemeasure</strong> P, if its performance on T, as measured by P, improveswith experience E.</li></ul><h3 id="监督学习-supervised-learning">监督学习 | SupervisedLearning</h3><p>如果我们给学习算法一个数据集，这个数据集由「<strong>正确答案</strong>」组成，然后运用该算法算出更多可能正确的答案，这就是监督学习。</p><p>监督学习领域有两大问题：</p><ul><li>回归（Regression）：以一系列<strong>离散值</strong>（discrete），试着推测出这一系列<strong>连续值</strong>（continuous）属性。譬如根据已知房价预测未知房价。</li><li>分类（Classification）：以一系列<strong>离散值</strong>，试着推测出同样是<strong>离散值</strong>的属性。譬如预测肿瘤的恶性与否。</li></ul><p>对于分类问题，输出的结果可以大于两个（分出许多类），输入的<strong>特征</strong>（feature）也可以更多，例如通过患者年龄和肿瘤大小等特征预测肿瘤的恶性与否。</p><p>在绘制这些样本时，可以用一个<strong>多维空间</strong>中不同颜色的<strong>样本点</strong>来表示。在一些问题中，我们希望使用无穷多的特征来学习，显然电脑的内存肯定不够用，而后文将介绍的<strong>支持向量机</strong>（SVM，SupportVector Machine）巧妙地解决了这个问题。</p><h3 id="无监督学习-unsupervised-learning">无监督学习 | UnsupervisedLearning</h3><p>对于监督学习里的每条数据，我们已经清楚地知道，训练集对应的正确答案。而在无监督学习中，这些数据「<strong>没有任何标签</strong>」或是只有「<strong>相同的标签</strong>」。</p><p>因此，我们希望学习算法能够判断出数据具有不同的<strong>聚集簇</strong>（Cluster），这就是无监督学习领域的经典问题：<strong>聚类算</strong>（ClusterAlgorithm）。</p><p>譬如在谷歌新闻中，我们对收集的网络新闻分组，组成有关联的新闻；又如在鸡尾酒宴会问题中，两个位置不同的麦克风录到的两个音源，通过学习算法可以各自分离出来。</p><h2 id="一元线性回归-univariate-linear-regression">一元线性回归 |Univariate Linear Regression</h2><p>前文提到，回归是监督学习的一个经典问题，这里我们将以线性回归来一窥整个监督学习的模型建立过程。假设已经拥有了回归问题的<strong>数据集</strong>，我们将之描述为：<span class="math display">\[\left\{\left(x^{(i)}, y^{(i)}\right), i=1,2, \cdots, m\right\}\]</span></p><ul><li><span class="math inline">\(m\)</span> 代表训练集中实例的数量；</li><li><span class="math inline">\(x\)</span> 代表<strong>特征</strong> or输入变量；</li><li><span class="math inline">\(y\)</span> 代表<strong>目标变量</strong>or 输出变量；</li><li><span class="math inline">\(({x}^{(i)},{y}^{(i)})\)</span> 代表第<span class="math inline">\(i\)</span> 个观察实例；</li><li><span class="math inline">\(h\)</span>代表学习算法的解决方案或函数也称为<strong>假设</strong>（hypothesis）。</li></ul><p>为了解决这样一个问题，我们实际上是要将训练集「喂」给我们的学习算法，进而学习得到一个假设<span class="math inline">\(h\)</span>。在本文中，我们尝试用线性函数<span class="math inline">\(h_\theta \left( x \right)=\theta_{0} +\theta_{1}x\)</span> 拟合之。因为只含有一个特征 or输入变量，因此这样的问题叫作<strong>一元线性回归</strong>问题。</p><h3 id="代价函数-cost-function">代价函数 | Cost Function</h3><p>有了上文建立的基础模型，现在要做的便是为之选择合适的<strong>参数</strong>（parameters）<spanclass="math inline">\(\theta_{0}\)</span> 和 <spanclass="math inline">\(\theta_{1}\)</span>，即直线的斜率和在 <spanclass="math inline">\(y\)</span> 轴上的截距。</p><p>选择的参数决定了得到的直线相对于训练集的<strong>准确程度</strong>，模型所预测的值与训练集中实际值之间的差距就是<strong>建模误差</strong>（modelingerror）。我们要做的就是尽量选择参数使得误差降低，即最小化 <spanclass="math inline">\(h_{\theta}(x^{(i)})\)</span> 和 <spanclass="math inline">\(y^{(i)}\)</span>的距离。于是我们有了经典的<strong>平方误差代价函数</strong>： <spanclass="math display">\[J\left( \theta _0,\theta _1 \right) =\frac{1}{2m}\sum_{i=1}^m{\left(h_{\theta}(x^{(i)})-y^{(i)} \right) ^2}\]</span> 也即是每个数据纵坐标的预测值与真实值的差的平方之和的平均，除以2 仅是为了后续求导方便，没有什么本质的影响。</p><p>绘制一个三维空间，三个坐标分别为 <spanclass="math inline">\(\theta_{0}\)</span> 和 <spanclass="math inline">\(\theta_{1}\)</span> 和 <spanclass="math inline">\(J(\theta_{0}, \theta_{1})\)</span>，对每个 <spanclass="math inline">\(\left( \theta_0, \theta_1 \right)\)</span>对，代入训练集可以得到一个 <span class="math inline">\(J(\theta_{0},\theta_{1})\)</span>值，经过一系列计算，我们得到一个<strong>碗状曲面</strong>：</p><p><img src="/img/blog/ML-Note-1-images/3d-surface.png" alt="三维空间中的碗状曲面" width="67%" /></p><p>显然，在三维空间中存在一个使得 <spanclass="math inline">\(J(\theta_{0}, \theta_{1})\)</span>最小的点。为了更好地表达，我们将三维空间投影到二维，得到<strong>等高线图</strong>（Contourplot）：</p><p><img src="/img/blog/ML-Note-1-images/contour.png" alt="二维空间的等高线图" width="50%" /></p><p>这些同心椭圆的中心，就是我们要找到使得 <spanclass="math inline">\(J(\theta_{0}, \theta_{1})\)</span> 最小的点。</p><h3 id="梯度下降法-gradient-descent">梯度下降法 | Gradient Descent</h3><p>在数学上，我们知道最小二乘法可以解决一元线性回归问题。现在有了等高线图，我们需要的是一种有效的算法，能够自动地找出这些使代价函数<span class="math inline">\(J\)</span> 取最小值的 <spanclass="math inline">\(\left( \theta_0, \theta_1 \right)\)</span>对。当然并不是把这些点画出来，然后人工读出中心点的数值——对于更复杂、更高维度、更多参数的情况，我们很难将其可视化。</p><p>梯度下降是一个用来求<strong>一般函数最小值</strong>的算法，其背后的思想是：开始时<strong>随机选择</strong>一个参数组合<spanclass="math inline">\(\left( \theta_{0},\theta_{1},......,\theta_{n}\right)\)</span>，计算代价函数，然后一点点地修改参数，直到找到<strong>下一个</strong>能让代价函数值下降最多的参数组合。</p><p>持续这么做会让我们找到一个<strong>局部最小值</strong>（localminimum），但我们无法确定其是否就是<strong>全局最小值</strong>（globalminimum），哪怕只是稍微修改初始参数，也可能最终结果完全不同。当然，在一元线性回归问题中，生成的曲面始终是<strong>凸函数</strong>（convexfunction），因此我们可以不考虑这个问题。</p><p>在高等数学中，我们知道「下降最多」其实指的是函数 <spanclass="math inline">\(J\)</span>的<strong>梯度方向的逆方向</strong>，也即方向导数最大的方向，梯度定义为：<span class="math display">\[\nabla J=\mathrm{grad} J=\left\{ \frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta _1} \right\}\]</span> 所以不断迭代进行赋值： <span class="math display">\[\theta_j:=\theta_j-\alpha\cdot\frac{\partial}{\partial\theta_j}J(\theta_{0}, \theta_{1})\]</span> 直到收敛，即可找到一个<strong>极小值</strong>。其中，<spanclass="math inline">\(\alpha\)</span>就是这一步的<strong>步长</strong>，也称为<strong>学习率</strong>（Learnigrate）。学习率的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。</p><h3 id="数学推导">数学推导</h3><p>对代价函数求偏导：</p><ul><li><p><span class="math inline">\(j=0\)</span> 时：<spanclass="math inline">\(\frac{\partial}{\partial \theta _0}J(\theta_0,\theta _1)=\frac{1}{m}\sum_{i=1}^m{\left( \theta _1x^{(i)}+\theta_0-y^{(i)} \right)}\)</span></p></li><li><p><span class="math inline">\(j=1\)</span> 时：<spanclass="math inline">\(\frac{\partial}{\partial \theta _1}J(\theta_0,\theta _1)=\frac{1}{m}\sum_{i=1}^m{x^{(i)}\left( \theta_1x^{(i)}+\theta _0-y^{(i)} \right)}\)</span></p></li></ul><p>所以梯度方向为： <span class="math display">\[\mathrm{grad } J=\left\{ \frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta _1} \right\} =\left\{\frac{1}{m}\sum_{i=1}^m{\left( \theta _1x^{(i)}+\theta _0-y^{(i)}\right)},\frac{1}{m}\sum_{i=1}^mx^{(i)}\left( \theta _1x^{(i)}+\theta_0-y^{(i)} \right) \right\}\]</span></p><p>此外，我们注意到在<strong>每一步</strong>迭代过程中，我们都用到了<strong>所有的</strong>训练样本，如<span class="math inline">\(\sum_{i=1}^mx^{(i)}\)</span>、<spanclass="math inline">\(\sum_{i=1}^my^{(i)}\)</span>、<spanclass="math inline">\(\sum_{i=1}^mx^{(i)}y^{(i)}\)</span>等项，这也称为<strong>批量梯度下降</strong>（Batch GradientDescent）。</p><h3 id="代码实现">代码实现</h3><p>需要注意的是，迭代赋值的过程中，<span class="math inline">\(\left(\theta_0, \theta_1 \right)\)</span>的值要同时更新，否则就会与梯度方向有微小区别。用 Python元组解包赋值可以实现，用矩阵乘法也可实现。</p><p>为了用向量化加速计算，这里利用了一个小性质，就是当 <spanclass="math inline">\(a\)</span> 和 <spanclass="math inline">\(b\)</span> 都为向量时有 <spanclass="math inline">\(a^Tb=b^Ta\)</span>，所以有： <spanclass="math display">\[X\theta =\left[ \begin{array}{c}    -\left( x^{(1)} \right) ^T\theta -\\    -\left( x^{(2)} \right) ^T\theta -\\    \vdots\\    -\left( x^{(m)} \right) ^T\theta -\\\end{array} \right] =\left[ \begin{array}{c}    -\theta ^T\left( x^{(1)} \right) -\\    -\theta ^T\left( x^{(2)} \right) -\\    \vdots\\    -\theta ^T\left( x^{(m)} \right) -\\\end{array} \right]\]</span> 下面以 <a href="https://www.coursera.org/">Coursera</a>上的一元线性回归数据集 <code>ex1data1.txt</code> 为例实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (97, 2)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex1data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>, usecols=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>x = data[:, <span class="hljs-number">0</span>]<br>y = data[:, <span class="hljs-number">1</span>]<br>m = y.size<br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">5000</span><br>theta = np.zeros(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># X.shape = (97, 2), y.shape = (97, ), theta.shape = (2, )</span><br>X = np.c_[np.ones(m), x]  <span class="hljs-comment"># 增加一列 1 到 矩阵 X，实现多项式运算</span><br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>    error = (X @ theta).flatten() - y  <span class="hljs-comment"># error.shape = (97, )</span><br>    theta -= (alpha / m) * np.<span class="hljs-built_in">sum</span>(X * error[:, np.newaxis], <span class="hljs-number">0</span>) <span class="hljs-comment"># 0 表示每列相加</span><br><br><span class="hljs-comment"># plot</span><br><span class="hljs-built_in">print</span>(theta)<br>plt.scatter(x, y)<br>x_plot = np.array([np.<span class="hljs-built_in">min</span>(x), np.<span class="hljs-built_in">max</span>(x)])<br>y_plot = theta[<span class="hljs-number">0</span>] + x_plot * theta[<span class="hljs-number">1</span>]  <span class="hljs-comment"># NumPy Broadcast</span><br>plt.plot(x_plot, y_plot, c=<span class="hljs-string">&quot;m&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>得到的 <span class="math inline">\(\left( \theta_0, \theta_1\right)\)</span> 结果是：[-3.8953 1.1929]，作图如下：</p><p><img src="/img/blog/ML-Note-1-images/Figure_1.png" alt="一元线性回归" width="50%" /></p><blockquote><p><strong>向量化</strong>：在 Python 或 Matlab中进行科学计算时，如果有多组数据或多项式运算，转化成矩阵乘法会比直接用<code>for</code>循环高效很多，可以实现<strong>同时赋值</strong>且代码更简洁。</p><p>这是由于科学计算库中的函数更好地利用了硬件的特性，更好地支持了诸如<strong>循环展开、流水线、超标量</strong>等技术。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux学习笔记 #3 使用远程服务器</title>
    <link href="/Linux-Note-3.html"/>
    <url>/Linux-Note-3.html</url>
    
    <content type="html"><![CDATA[<p>在前文的基础上，介绍了<strong>远程控制</strong> Linux服务器的情景，以及在学习中常用的<strong>不间断会话</strong>服务。</p><p>本文大部分内容参考了清华 ZAH 同学的教程，部分参考了 刘遄 老师的《<ahref="https://www.linuxprobe.com">Linux 就该这么学</a>》，<ahref="https://www.runoob.com/linux/linux-tutorial.html">菜鸟教程-Linux</a>。</p><h2 id="远程控制服务">远程控制服务</h2><p>日常工作中，我们常常会使用远程服务器来进行开发/运维，连接远程服务器需要用到SecureShell（SSH）。</p><h3 id="初次连接">初次连接</h3><p>打开本地的终端模拟器，通过如下命令使用 SSH 连接到其他服务器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> ssh hewei@server.hitsz.edu.cn -p 30001</span><br></code></pre></td></tr></table></figure><p>在这个例子中，我们尝试以用户名 <code>hewei</code> 登陆服务器<code>server.hitsz.edu.cn</code>。服务器可以通过<strong>服务器主机名</strong>指定（如本例），也可以使用<strong>IP </strong>指定（如 <code>hewei@192.168.1.42</code>）。SSH的默认端口是 22，也可以通过 <code>-p port</code> 制定端口。</p><p>输入上述命令后，如果成功连接，会提示输入密码，该密码就是远程服务器中<strong>对应账户</strong>的密码。</p><blockquote><p>本文中由于没有其他服务器，就在本机上用 VirtualBox 创建了一个 Ubuntu<strong>虚拟机</strong>作为服务器，获取虚拟机IP，在主机中用终端连接。参考文章：<ahref="https://www.cnblogs.com/clqbolg/p/11729868.html">用vbox搭建Linux服务器</a>。</p><p>注意：用本方法需要确保主机接有<strong>网线</strong>，且虚拟机配置<strong>桥接网卡</strong>与主机以太网相同。</p><p>另一种方法是用公共集群作为临时服务器，可能不支持SSH，这时候要在远程终端手动激活<code>sudo service sshd start</code>。</p></blockquote><p>进入服务器后，可以看到命令行左侧的提示符变成了<code>hewei@hewei-VirtualBox ~&gt;</code>，其中 <code>~</code> 代表在<code>/home/hewei</code> 用户目录下，<code>&gt;</code>符号提示远程用户身份，不同服务器可能不一样。输入 <code>logout</code>可退出远程服务器。</p><h3 id="ssh-配置文件">SSH 配置文件</h3><p>上述<strong>连接</strong>远程服务器的命令还可以通过修改 SSH配置文件来简化，进入<strong>主机</strong>所在的用户文件夹下的<code>.ssh</code> 文件夹（SSH 密钥所在的文件夹），新建配置文件<code>config</code>，添加：</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-selector-tag">Host</span> <span class="hljs-selector-tag">training</span> (任意别名，用于助记)<br><span class="hljs-selector-tag">HostName</span> <span class="hljs-selector-tag">152</span><span class="hljs-selector-class">.136</span><span class="hljs-selector-class">.177</span><span class="hljs-selector-class">.53</span> (域名/IP)<br>    <span class="hljs-selector-tag">Port</span> <span class="hljs-selector-tag">30001</span> (端口号)<br>    <span class="hljs-selector-tag">User</span> <span class="hljs-selector-tag">hewei</span> (用户名)<br>    <span class="hljs-selector-tag">IdentityFile</span> ~/<span class="hljs-selector-class">.ssh</span>/<span class="hljs-selector-tag">id_rsa</span> (私钥路径，默认，可不填)<br></code></pre></td></tr></table></figure><p>配置完文件后，我们可以使用 <code>ssh test</code>这样的命令来连接服务器，有助于管理多个服务器的账户。</p><h3 id="免登录执行">免登录执行</h3><p>SSH的一个经常被忽视的特性是它可以在<strong>不登录服务器</strong>的状态下直接远程执行命令，例如在配置完<code>config</code> 后，使用 <code>ssh 别名</code> 命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> ssh training ls /home/hewei</span><br></code></pre></td></tr></table></figure><p>这步操作同样需要输入密码，但不会登录服务器，在主机环境中直接返回结果。</p><h3 id="ssh-鉴权">SSH 鉴权</h3><p>使用 SSH当然免不了基于<strong>密钥对</strong>的验证机制，我们只需要将<strong>公钥</strong>放在服务器上，每次登录时向服务器证明客户端持有对应的<strong>私钥</strong>。这样就可以避免每次登陆都输入密码了。</p><p>在 <a href="https://hwcoder.top/Git-Note-3/">Git学习笔记 #3远程仓库使用</a>中，同样讲述了这个问题，这里假设我们已经生成了密钥对。在<strong>本机</strong>终端下，可以通过如下命令把公钥传到服务器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> cat ~/.ssh/id_rsa.pub | ssh training <span class="hljs-string">&#x27;mkdir -p ~/.ssh &amp;&amp; cat &gt;&gt; ~/.ssh/authorized_keys&#x27;</span></span><br><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-comment"># 拷贝公钥，利用 ssh training 免登录操控主机，创建路径并重定向公钥到 authorized_keys</span></span><br></code></pre></td></tr></table></figure><p>如果<strong>本机</strong>是 Linux/MacOS，支持<code>ssh-copy-id</code> 命令，可以用下面这种更简单的方案：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> ssh-copy-id -i ~/.ssh/id_rsa.pub training</span><br></code></pre></td></tr></table></figure><p>当然，如果已经连上<strong>远程</strong>终端，也可以在远程直接操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> mkdir -p ~/.ssh</span><br><span class="hljs-meta">$</span><span class="bash"> vi  ~/.ssh/authorized_keys  <span class="hljs-comment"># 用 vi 编辑器打开，手动复制公钥进去</span></span><br><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;这里放你的公钥&quot;</span> &gt;&gt; ~/.ssh/authorized_keys  <span class="hljs-comment"># 这样可以不用 vi</span></span><br></code></pre></td></tr></table></figure><blockquote><p>如果服务器添加公钥后本机登录<strong>仍需要密码</strong>，则原因可能是<code>~/.ssh</code> 目录和 <code>~/.ssh/authorized_keys</code>文件的权限不对，使用以下命令即可：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> chmod 700 ~/.ssh</span><br><span class="hljs-meta">$</span><span class="bash"> chmod 644 ~/.ssh/authorized_keys</span><br></code></pre></td></tr></table></figure></blockquote><h3 id="scp-文件传输">scp 文件传输</h3><p>通常我们需要将文件的工程文件放到服务器上执行，如果采用了MobaXterm、Xshell这类增强终端，可以通过<strong>拖拽</strong>的方式直接实现文件互传。</p><p>但如果直接在<strong>本机</strong>终端中，可以通过如下指令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-comment"># 上传文件到服务器，参数一为本地文件</span></span><br><span class="hljs-meta">$</span><span class="bash"> scp test.zip training:/home/hewei/test.zip</span> <br><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-comment"># 从服务器下载文件，参数一为服务器文件</span></span><br><span class="hljs-meta">$</span><span class="bash"> scp training:/home/hewei/test.zip test.zip</span><br></code></pre></td></tr></table></figure><p>此外，使用 <code>scp -r</code>可以上传文件夹。如果需要更高级的功能，可以考虑使用<code>rsync</code>。</p><hr /><p>此方法也可以用于<strong>两台服务器</strong>之间传输文件，此时服务器 A就相当于主机，服务器 B 就相当于<strong>远程</strong>（B 的<code>.ssh/authorized_keys</code> 里需要有 A的公钥，如果没有则需要输入密码）。下面是一个基本的 <code>scp</code>命令的格式，用于从一个服务器传输文件到另一个服务器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scp [选项] 源文件 用户名@目标服务器:目标路径<br></code></pre></td></tr></table></figure><p>假设你已经在服务器 1.1.1.1 上，并且你想把本地文件<code>example.txt</code> 发送到服务器 2.2.2.2 上的<code>/home/username/</code> 目录，你可以使用以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scp -P 2222 example.txt hewei@2.2.2.2:/home/hewei/<br></code></pre></td></tr></table></figure><p>这里的参数解释如下：</p><ul><li><code>-P 2222</code>：指定 SSH 端口号为 2222（注意是大写的<code>P</code>）。</li><li><code>example.txt</code>：是你想传输的文件名。</li><li><code>hewei@2.2.2.2</code>：目标服务器的用户名和 IP 地址。</li><li><code>/home/hewei/</code>：目标服务器上的目标路径，你想把文件放在哪里。</li></ul><p>此外，如果目标服务器使用的是默认的 SSH 端口（22），则可以省略<code>-P</code>参数。如果你在执行命令时遇到权限问题，可能需要考虑是否有适当的读写权限或者使用<code>sudo</code>。</p><p>如果是首次连接到目标服务器，系统可能会要求你验证目标服务器的身份，并询问是否继续连接。此时需要输入<code>yes</code>，然后输入目标服务器的用户密码，除非已经设置了 SSH免密登录。</p><h3 id="利用-vs-code-连接">利用 VS Code 连接</h3><p>在今年的操作系统实验课上，又学到了一招连接远程服务器的办法，不得不说VS Code 太强大了！以下的内容需要 <code>Remote - SSH</code>插件支持。</p><p>安装插件后，在 <code>SSH TARGETS</code> 栏选择<code>Add New</code>，之后会弹出输入框，输入 <ahref="#初次连接">初次连接命令</a>，则会弹出提示自动设置<code>.ssh/config</code> 文件！添加成功后，左侧的<code>SSH TARGETS</code> 栏就会增加新的远程节点。</p><p>在该节点旁边，选中 <code>Connect to Host in New Window</code>即可打开一个 VS Code 窗口，新打开的窗口会要求你选择目标机器的平台，选择Linux，按下回车，再确定连接，输入<strong>密码</strong>，即可连接成功。观察到终端显示的提示符是<code>190110429@OSLabExecNode1:~$</code>，成功！</p><p>之后，点击 VS Code侧边栏的资源管理器，就可以直接打开远程的文件夹和文件，更方便<strong>编程和调试</strong>。此外，同样可以用SSH <strong>鉴权</strong>来简化输入密码的步骤，方法同上。在 VS Code中按下 <code>Ctrl+反引号</code> 可以快捷打开终端。</p><h2 id="预备知识">预备知识</h2><h3 id="vim-入门">Vim 入门</h3><p>通常我们将工程传到服务器上跑时，可能需要修改某些代码或参数，如果没有使用增强终端或者VSCode Remote 等插件，我们就只能在 CLI 界面进行交互。</p><p>要在 CLI 界面简单地<strong>编辑</strong>⼀个文件，可用的选择大概有vi, vim, nano 等，通常来说我们选择使用 vim。使用<code>vim &lt;filename&gt;</code> 命令打开文件进行编辑。</p><p>刚打开文件时，vim 处于一个 Normal 模式，在 Normal 模式下按<code>i</code> 进入 Insert模式，此时就可以像其他编辑器一样对文件进行编辑。编辑后按下<code>ESC</code> 键退回到 Normal 模式。</p><p>在 Normal 模式下按 <code>:</code> 进入 Command模式，在这个模式下可以使用命令进行操作，常用命令有：<code>:q</code>退出，<code>:w</code> 保存，<code>:wq</code> 保存并退出。</p><h3 id="软件包管理机制">软件包管理机制</h3><p>大多数现代类 Unix操作系统都提供了一个集中的<strong>软件包管理机制</strong>，以帮助用户搜索、安装和管理软件。而软件通常以包的形式存储在<strong>仓库</strong>（repository）中，对软件包的使用和管理被称为包管理。而Linux 包的基本组成部分通常有：共享库、应用程序、服务和文档。</p><p><strong>包管理器</strong>又称软件包管理系统，它是在电脑中自动安装、配制、卸载和升级软件包的工具组合，在各种系统软件和应用软件的安装管理中均有广泛应用。</p><p>几乎每⼀个系统发行版都有它的包管理器，Ubuntu 有<code>apt</code>，Arch 有 <code>pacman</code>，macOS 有<code>Homebrew</code>。</p><h3 id="包管理器换源">包管理器换源</h3><p>由于众所周知的原因，<strong>仓库</strong>在国内的访问速度是不佳的，往往需要更换为国内的镜像源，常见的镜像站点有清华的Tuna，其 Ubuntu源地址为：https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/ 。</p><p>Ubuntu 的软件配置文件时<code>/etc/apt/sources.list</code>，首先将系统自带的该文件备份：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="hljs-meta">$</span><span class="bash"> ls /etc/apt/source.list -lah <span class="hljs-comment"># 查看文件权限，发现需要 root 权限</span></span><br></code></pre></td></tr></table></figure><p>此时如果直接用 vim 打开文件，会发现无法进入 Insert模式，于是需要用：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> sudo vim /etc/apt/source.list <span class="hljs-comment"># 用 root 身份打开文件</span></span><br></code></pre></td></tr></table></figure><p>进入编辑器后，可以进入 Insert 慢慢删除，也可以直接在 Normal模式下输入 <code>ggyG</code>全选删除。然后将网站中的配置文件复制进去。</p><p>现在就可以使用镜像源下载软件包了，这里以下面要用的 Tmux 为例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-comment"># 由于包管理器是在系统中，也需要 sudo 来获得 root 权限，且需要密码</span></span><br><span class="hljs-meta">$</span><span class="bash"> sudo apt update       <span class="hljs-comment"># 检查已安装的软件包是否有可用的更新</span></span><br><span class="hljs-meta">$</span><span class="bash"> sudo apt search tmux  <span class="hljs-comment"># 在仓库中查找是否有该源文件，可以跳过</span></span><br><span class="hljs-meta">$</span><span class="bash"> sudo apt install tmux <span class="hljs-comment"># 安装软件包</span></span><br><span class="hljs-meta">$</span><span class="bash"> sudo apt remove tmux  <span class="hljs-comment"># 删除已安装软件包</span></span><br></code></pre></td></tr></table></figure><h3 id="从网络下载资源">从网络下载资源</h3><p>如果没有 <code>sudo</code>权限也想下载软件，或者想下载某些数据集时，就需要内容下载命令<code>wget</code> 或<code>curl</code>，其中前者由系统自带，后者需要用包管理器安装<code>sudo apt-get install curl</code>。两个命令的功能有所重叠，总体而言，后者更适合复杂的下载任务，具体用法如下：</p><ul><li><p>基础下载功能：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> curl -O https://hwcoder.top/index.html<span class="hljs-comment"># 大写 O，如果缺省则只会打印内容不会下载</span></span><br><span class="hljs-meta">$</span><span class="bash"> wget https://hwcoder.top/index.html<span class="hljs-comment"># 不用参数，直接下载到当前目录</span></span><br></code></pre></td></tr></table></figure></li><li><p>下载文件并重命名（指定路径）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> curl -o rename.html https://hwcoder.top/index.html<span class="hljs-comment"># 小写 o</span></span><br><span class="hljs-meta">$</span><span class="bash"> wget -O rename.html https://hwcoder.top/index.html<span class="hljs-comment"># 大写 O</span></span><br></code></pre></td></tr></table></figure></li><li><p>断点续传：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> curl -O -C https://hwcoder.top/index.html<span class="hljs-comment"># 大写 C</span></span><br><span class="hljs-meta">$</span><span class="bash"> wget -c https://hwcoder.top/index.html<span class="hljs-comment"># 小写 c</span></span><br></code></pre></td></tr></table></figure></li></ul><p>其中 <code>curl -#</code>还有进度条功能，支持正则表达式匹配批量下载。<code>wget</code>支持递归下载，常用于打包下载整个网页。</p><h2 id="tmux-不间断会话服务">Tmux 不间断会话服务</h2><p>命令行的典型使用方式是，打开一个<strong>终端窗口</strong>（terminalwindow，以下简称窗口），在里面输入命令。用户与计算机的这种临时的交互，称为⼀次「<strong>会话</strong>」（session）。</p><h3 id="服务中断">服务中断</h3><p>会话的⼀个重要特点是，窗口与其中启动的进程是<strong>绑定</strong>的——打开窗口，会话开始；关闭窗口，会话结束，会话内部的进程也会随之终止，不管有没有运行结束。</p><p>因此，使用 SSH远程控制服务时，当<strong>与远程主机的会话</strong>被关闭时，在远程主机上运行的命令也随之被中断。</p><p>如果运行某个重要的程序，中途是<strong>绝对不能</strong>关闭在本地打开的终端窗口或断开网络连接的，甚至连网速的波动都有可能导致任务中断，此时只能重新进行远程连接并重新开始任务。而我们需要在服务器上跑的东西恰恰就是比较<strong>占用资源</strong>的程序，通常需要执行较长时间，如果因为网速波动而中断就很难受。</p><h3 id="分离会话">分离会话</h3><p>为了解决这个问题，会话与窗口可以「<strong>解绑</strong>」：窗口关闭时，会话并不终止，而是继续运行，等到以后需要的时候，再让会话「<strong>重新绑定</strong>」其他窗口。</p><p>Terminal Multiplexer（终端复用器，简称为Tmux）就是一款能够实现<strong>多窗口远程控制</strong>的开源服务程序。简单来说就是为了<strong>解决网络异常中断</strong>或为了<strong>同时控制多个远程终端窗口</strong>而设计的程序。Unix系统自带，无需安装。</p><p>远程连接服务器后，输入 <code>tmux</code> 进入 Tmux窗口，窗口下方会出现<strong>绿色</strong>的一行。在 Tmux 窗口中，输入<code>tmux detach</code>命令，就会将当前<strong>会话与窗口分离</strong>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> tmux        <span class="hljs-comment"># 在终端窗口输入，进入 Tmux 窗口</span></span><br><span class="hljs-meta">$</span><span class="bash"> tmux detach <span class="hljs-comment"># 在 Tmux 窗口输入，退回终端窗口</span></span><br></code></pre></td></tr></table></figure><p>上述命令执行后，就会退回到本地终端窗口，但是会话和里面的进程仍然在后台运行。此时可以输入命令查看后台正在运行的Tmux 会话：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> tmux list-session</span><br><span class="hljs-meta">$</span><span class="bash"> tmux ls  <span class="hljs-comment"># 两条命令等价</span></span><br></code></pre></td></tr></table></figure><p>会弹出 <code>0: 1 windows (created Mon Sept 9 20:34:12 2021)</code>的提示信息，其中 <code>0</code> 表示该窗口标题（默认从 0开始递增），此外也可以<strong>命名窗口</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> tmux new -s &lt;session-name&gt;  <span class="hljs-comment"># 新建 Tmux 窗口并命名</span></span><br></code></pre></td></tr></table></figure><h3 id="接入会话">接入会话</h3><p>此时就算退出远程连接，会话也会保持运行，可以使用以下命令<strong>重新接入</strong>某个已存在的会话：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> tmux attach -t 0 <span class="hljs-comment"># 使用会话编号</span></span><br><span class="hljs-meta">$</span><span class="bash"> tmux attach -t &lt;session-name&gt;  <span class="hljs-comment"># 使用会话名称</span></span><br></code></pre></td></tr></table></figure><h3 id="杀死会话">杀死会话</h3><p>在终端窗口中，如果要<strong>彻底杀死</strong>一个运行中的会话，需要用到以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> tmux kill-session -t 0     <span class="hljs-comment"># 使用会话编号</span></span><br><span class="hljs-meta">$</span><span class="bash"> tmux kill-session -t &lt;session-name&gt;  <span class="hljs-comment"># 使用会话名称</span></span><br></code></pre></td></tr></table></figure><p>当然，也可以先用 <code>attach</code> 命令接入会话，再按下<code>Ctrl+D</code> 或者输入 <code>exit</code> <strong>直接退出</strong>Tmux 会话（注意退出与解绑不同）。</p><h3 id="管理多窗格">管理多窗格</h3><p>有时候需要同时执行多个程序，并观察输出，就需要多个会话窗口。利用 Tmux还可以将一个<strong>窗口切分</strong>为多个（并且独立执行会话，<strong>互不干扰</strong>）。只要使用如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> tmux split-window     <span class="hljs-comment"># 上下切割窗格</span></span><br><span class="hljs-meta">$</span><span class="bash"> tmux split-window -h  <span class="hljs-comment"># 左右切割窗格</span></span><br><span class="hljs-meta">$</span><span class="bash"> tmux select-pane -U / -D / -L / -R  <span class="hljs-comment"># 切换至 上/下/左/右 窗格</span></span><br></code></pre></td></tr></table></figure><p>在工作中，通过输入命令来切换窗格难免有些麻烦，Tmux为用户提供了一系列快捷键来执行窗格的切换：先同时按下 <code>Ctrl+B</code>组合键进入控制模式，松手后：</p><ul><li>按下 <code>%</code> 表示划分左右两个窗格；</li><li>按下 <code>"</code> 表示划分上下两个窗格；</li><li>按下 <code>&lt;方向键&gt;</code> 表示到上下左右相邻的一个窗格；</li><li>按下 <code>x</code> 表示关闭窗格。</li></ul>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux学习笔记 #2 高级的Shell与文件管理</title>
    <link href="/Linux-Note-2.html"/>
    <url>/Linux-Note-2.html</url>
    
    <content type="html"><![CDATA[<p>在前文的基础上，记录了 Linux 中如何充分利用 Shell命令，以及对<strong>文件权限</strong>的管理。</p><p>本文部分内容参考了清华 ZAH 同学的教程，部分参考了 刘遄 老师的《<ahref="https://www.linuxprobe.com">Linux 就该这么学</a>》，<ahref="https://www.runoob.com/linux/linux-tutorial.html">菜鸟教程-Linux</a>。</p><h2 id="高级的-shell">高级的 Shell</h2><p>Linux 中可以将简单的 Shell命令组合以发挥更大的作用，常见的如：与文件读写操作有关的重定向技术、处理命令输出值的管道命令符、通配符和转义字符等。这些技巧在Shell 脚本批处理中十分常用。</p><h3 id="输入输出重定向">输入输出重定向</h3><p>输入重定向是指把文件<strong>导入到命令</strong>中，而输出重定向则是指把原本要输出到屏幕的数据信息<strong>写入到指定文件</strong>中。</p><p>日常的学习和工作中，使用输出重定向的频率更高，所以又将输出重定向分为了<strong>标准输出</strong>重定向和<strong>错误输出</strong>重定向两种不同的技术，以及<strong>覆盖写入</strong>与<strong>追加写入</strong>两种模式。</p><blockquote><p>错误信息的输出对于执行一个<strong>自动化</strong>的 Shell脚本非常有用，它可以把整个脚本执行过程中的<strong>报错信息</strong>都记录到文件中，便于安装后的排错工作。</p></blockquote><p>对于输入重定向，格式如下：</p><table><thead><tr class="header"><th style="text-align: center;">格式</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>命令 &lt; 文件</code></td><td style="text-align: center;">将文件作为命令的标准输入</td></tr><tr class="even"><td style="text-align: center;"><code>命令 &lt;&lt; 分界符</code></td><td style="text-align: center;">从键盘输入中读入两个分界符 tag之间的内容</td></tr><tr class="odd"><tdstyle="text-align: center;"><code>命令 &lt; 文件A &gt; 文件B</code></td><td style="text-align: center;">将文件 A作为命令的输入并将标准输出到文件 B</td></tr></tbody></table><blockquote><p>通常用输入重定向能做的事情，直接执行该命令也能做（命令通常会自带文件输入的功能），因此使用频率较低。</p></blockquote><p>对于输出重定向，格式如下：</p><table><thead><tr class="header"><th style="text-align: center;">格式</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>命令 &gt; 文件</code></td><tdstyle="text-align: center;">将标准输出重定向到一个文件中（清空原有文件的数据）</td></tr><tr class="even"><td style="text-align: center;"><code>命令 &gt;&gt; 文件</code></td><tdstyle="text-align: center;">将标准输出重定向到一个文件中（追加到原有内容的后面）</td></tr><tr class="odd"><td style="text-align: center;"><code>命令 2&gt; 文件</code></td><tdstyle="text-align: center;">将错误输出重定向到一个文件中（清空原有文件的数据）</td></tr><tr class="even"><td style="text-align: center;"><code>命令 2&gt;&gt; 文件</code></td><tdstyle="text-align: center;">将错误输出重定向到一个文件中（追加到原有内容的后面）</td></tr></tbody></table><p>上述命令中的 <code>2</code> 就是<strong>文件描述符</strong>（FileDescriptor），必须和重定向符号<strong>紧贴</strong>才能生效。在 Linux中有三种文件描述符：</p><ul><li><code>0</code> 表示标准输入（STDIN）， <code>&lt;</code> 或<code>&lt;&lt;</code> 的默认输入</li><li><code>1</code> 表示标准输出（STDOUT），<code>&gt;</code> 或<code>&gt;&gt;</code> 的默认输出</li><li><code>2</code>表示标准错误输出（STDERR），通常需要重定向才能显示</li><li><code>&amp;</code> 放在 <code>&gt;</code>前可以<strong>同时表示</strong>标准输出和标准错误输出，意味着所有输出都会被重定向</li></ul><p>有时候会看见把 <code>2&gt;&amp;1</code>放在<strong>命令的最后</strong>的用法，表示「<strong>将标准错误输出重定向到标准输出</strong>」，其中<code>&amp;</code> 的作用是合并命令，如果只保留 <code>2&gt;1</code>则会将标准错误输出重定向到名为 1 的文件中。</p><blockquote><p>一个容易混淆的 <code>&amp;</code>用法是将其放在命令的最后，此时表示「<strong>在后台运行命令</strong>」。它允许你在执行命令后立即返回到终端或命令行界面，而不必等待命令完成。这对于运行长时间任务或需要持续运行的程序很有用。</p></blockquote><h3 id="管道命令符">管道命令符</h3><p>管道命令符 <code>|</code>用于把<strong>前一个命令</strong>原本要输出到屏幕的信息当作<strong>后一个命令</strong>的标准输入，就像一个「管道」连接了两条命令一样，其本质是创建一个<strong>临时文件</strong>用以读写。</p><p>下面用一些例子来展示管道命令符的巧妙之处：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> grep /sbin/nologin /etc/passwd | wc -l <span class="hljs-comment"># 抓取文件中含 /sbin/nologin 的行，统计行数</span></span><br><span class="hljs-meta">$</span><span class="bash"> ls -lah /etc/ | more <span class="hljs-comment"># 罗列目录下的文件信息，用翻页的形式查看</span></span><br><span class="hljs-meta">$</span><span class="bash"> ls -lah usr/bin | grep <span class="hljs-built_in">echo</span> <span class="hljs-comment"># 罗列目录下的文件信息，抓取 echo 文件</span></span><br><span class="hljs-meta">$</span><span class="bash"> ps aux | grep bash <span class="hljs-comment"># 显示全部进程，抓取 bash 的进程信息</span></span><br><span class="hljs-meta">$</span><span class="bash"> python run.py | tee -a run.log<span class="hljs-comment"># 将函数的标准输出附加到 log 文件</span></span><br></code></pre></td></tr></table></figure><p>此外，管道命令符在一条命令中还可以多次复用，完成更复杂的工作。</p><p>但是，由于管道运算符实现的本质，它默认会将前者的输出作为<strong>一整个临时文件</strong>传给后者，而有时候我们却需要拆散这个文件，<strong>依次输入</strong>到后者，这时就需要这个命令：</p><ul><li><code>xargs</code>：代表 eXtendedARGuments，管道命令传递参数的一个<strong>过滤器</strong>，格式为：<code>命令1 | xargs [参数] 命令2</code>。</li></ul><h3 id="通配符">通配符</h3><p>在 Git 学习中，已经介绍了一些常用的通配符，这里附上完整表格：</p><table><thead><tr class="header"><th style="text-align: center;">通配符</th><th style="text-align: center;">含义</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>*</code></td><td style="text-align: center;">任意字符</td></tr><tr class="even"><td style="text-align: center;"><code>?</code></td><td style="text-align: center;">单个任意字符</td></tr><tr class="odd"><td style="text-align: center;"><code>[a-z]</code></td><td style="text-align: center;">单个小写字母</td></tr><tr class="even"><td style="text-align: center;"><code>[A-Z]</code></td><td style="text-align: center;">单个大写字母</td></tr><tr class="odd"><td style="text-align: center;"><code>[a-Z]</code></td><td style="text-align: center;">单个字母</td></tr><tr class="even"><td style="text-align: center;"><code>[0-9]</code></td><td style="text-align: center;">单个数字</td></tr><tr class="odd"><td style="text-align: center;"><code>[[:alpha:]]</code></td><td style="text-align: center;">任意字母</td></tr><tr class="even"><td style="text-align: center;"><code>[[:upper:]]</code></td><td style="text-align: center;">任意大写字母</td></tr><tr class="odd"><td style="text-align: center;"><code>[[:lower:]]</code></td><td style="text-align: center;">任意小写字母</td></tr><tr class="even"><td style="text-align: center;"><code>[[:digit:]]</code></td><td style="text-align: center;">所有数字</td></tr><tr class="odd"><td style="text-align: center;"><code>[[:alnum:]]</code></td><td style="text-align: center;">任意字母加数字</td></tr><tr class="even"><td style="text-align: center;"><code>[[:punct:]]</code></td><td style="text-align: center;">标点符号</td></tr></tbody></table><h3 id="转义字符">转义字符</h3><p>转义字符通常用于避免命令的歧义，最常用的有如下几种：</p><ul><li>反斜杠<code>\</code>：使反斜杠后面的一个保留字符变为单纯的字符。</li><li>单引号 <code>‘’</code>：转义其中所有的变量为单纯的字符串。</li><li>双引号<code>“”</code>：保留其中的<strong>变量属性</strong>，不进行转义处理。</li></ul><h3 id="环境变量">环境变量</h3><p>回归一个最本质的问题，当我们在 Shell环境中输入一条命令时（我们知道这条命令其实是一个 C 编写的程序），Shell是如何找到这个程序的呢？</p><p>首先，Shell会用户是否以<strong>绝对路径或相对路径</strong>的方式输入命令（如/bin/ls），如果是绝对路径则直接执行。如果不是，系统则需要在<strong>多个路径</strong>中查找用户输入的命令文件，而定义这些路径的变量就叫作<strong>PATH</strong>。我们可以通过如下命令来查看：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-variable">$PATH</span>  <span class="hljs-comment"># 符号 $ 表示输出环境变量的值</span></span><br>/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/root/bin<br></code></pre></td></tr></table></figure><p>可以看到，PATH文件中含有多个量，都是这条命令可能存放的位置，于是解释器会在这些位置中逐个查找。</p><p>当然，为了 Linux 这样庞大系统的运行，类似的环境还有许多，我们可以使用<code>env</code> 命令来查看所有环境变量。这里列出常用的一些变量：</p><table><thead><tr class="header"><th style="text-align: center;">变量名称</th><th style="text-align: center;">含义</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">HOME</td><td style="text-align: center;">用户的主目录（即家目录）</td></tr><tr class="even"><td style="text-align: center;">SHELL</td><td style="text-align: center;">用户在使用的 Shell 解释器名称</td></tr><tr class="odd"><td style="text-align: center;">HISTSIZE</td><td style="text-align: center;">输出的历史命令记录条数</td></tr><tr class="even"><td style="text-align: center;">HISTFILESIZE</td><td style="text-align: center;">保存的历史命令记录条数</td></tr><tr class="odd"><td style="text-align: center;">MAIL</td><td style="text-align: center;">邮件保存路径</td></tr><tr class="even"><td style="text-align: center;">LANG</td><td style="text-align: center;">系统语言、语系名称</td></tr><tr class="odd"><td style="text-align: center;">RANDOM</td><td style="text-align: center;">生成一个随机数字</td></tr><tr class="even"><td style="text-align: center;">PS1</td><td style="text-align: center;">Bash 解释器的提示符</td></tr><tr class="odd"><td style="text-align: center;">PATH</td><td style="text-align: center;">定义解释器搜索用户执行命令的路径</td></tr></tbody></table><blockquote><p>PATH 是最重要的一个环境变量，许多黑客会在 /tmp 中存放一个与 ls 或 cd同名的木马文件，再把路径加入 PATH。</p></blockquote><h2 id="文件权限管理">文件权限管理</h2><p>Linux作为一个多用户、多任务的操作系统，具有很好的稳定性与安全性，在幕后保障Linux系统的安全则是一系列复杂的配置工作。这里暂不介绍<strong>用户身份</strong>相关的命令，只介绍<strong>文件权限与归属</strong>。</p><h3 id="目录详解">目录详解</h3><p>Unix/Linux中，<strong>一切皆文件</strong>。涉及权限操作时，不可避免地要接触各个目录，这里简单介绍一些常用目录的作用：</p><figure><img src="/img/blog/Linux-Note-2-images/directory.jpg"alt="树状目录结构" /><figcaption aria-hidden="true">树状目录结构</figcaption></figure><p><strong>系统启动项</strong>：</p><ul><li><p>/boot：存放的启动 Linux时使用的<strong>内核文件</strong>，包括连接文件以及镜像文件。</p></li><li><p>/etc：表示Etcetera，存放所有的系统需要的<strong>配置文件</strong>和<strong>子目录列表</strong>。</p></li></ul><p><strong>指令集合</strong>：</p><ul><li><p>/bin：存放着最常用的<strong>程序和指令</strong>，如<code>ls</code> 就是在 /bin/ls 目录下的。</p></li><li><p>/sbin：表示 SuperUser，只有系统管理员能使用的程序和指令。</p></li></ul><p><strong>账户</strong>：</p><ul><li><p>/root：<strong>系统管理员</strong>的用户主目录。</p></li><li><p>/home：用户的主目录，以各个用户的账号命名，<strong>支持多用户</strong>。</p></li><li><p>/usr：表示 Unix SharedResources，用户的很多<strong>应用程序和文件</strong>都放在这个目录下，类似Windows 下的 Program Files 目录。</p><ul><li>/usr/bin：系统用户使用的应用程序与指令。</li><li>/usr/sbin：超级用户使用的比较高级的管理程序和系统守护程序。</li><li>/usr/src：内核源代码默认的放置目录。</li></ul></li></ul><p><strong>其他</strong>：</p><ul><li>外部文件管理：/dev、/media、/mnt，挂载外部设备，如U盘，光驱等。</li><li>临时文件：/run，存储系统运行信息；/tmp，存储用户临时信息。</li><li>运行过程项：/var，存放日志等；/proc，管理<strong>内存空间</strong>，系统内存的映射。</li><li>扩展目录：/src，存放服务启动后需要提取的数据，不用<strong>服务器</strong>就是空。</li></ul><h3 id="权限类型">权限类型</h3><p>在 Linux系统中，每个<strong>文件和目录</strong>都有归属的所有者和所属组，并且规定了其<strong>所有者</strong>、<strong>所属组</strong>以及<strong>其他人</strong>所拥有的可读取（r）、可写入（w）、可执行（x，代表execute）的权限。</p><p>这三个权限对于文件和目录的含义各不相同：</p><table><thead><tr class="header"><th style="text-align: center;">权限</th><th style="text-align: center;">文件</th><th style="text-align: center;">目录</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">读取（r）</td><td style="text-align: center;">能读取文件</td><tdstyle="text-align: center;">能够读取目录内的<strong>文件列表</strong></td></tr><tr class="even"><td style="text-align: center;">写入（w）</td><td style="text-align: center;">能编辑、新增、修改、删除文件</td><td style="text-align: center;">能够在目录内新增、删除、重命名文件</td></tr><tr class="odd"><td style="text-align: center;">执行（r）</td><td style="text-align: center;">能运行一个<strong>脚本</strong>程序</td><td style="text-align: center;">能够<strong>进入该目录</strong></td></tr></tbody></table><p>三类对象，三种权限，总共需要<strong>九个标志位</strong>来限定一个文件或目录的权限范围，为了区分文件和目录等，还有<strong>一个附加标志位</strong>。权限的表示就分为<strong>字符</strong>表示和<strong>数字</strong>表示两种：</p><figure><img src="/img/blog/Linux-Note-2-images/file-permission.png"alt="文件权限的字符与数字表示" /><figcaption aria-hidden="true">文件权限的字符与数字表示</figcaption></figure><p>数字表示即在字符对应的<strong>二进制位</strong>填写 1 或0，表示授权或未授权，转换为<strong>十进制</strong>就是 4、2、1。</p><p>下面以一个例子来说明这两种表示法：现在有这样一个文件，其<strong>所有者</strong>拥有可读、可写、可执行的权限，其文件<strong>所属组</strong>拥有可读、可写的权限；<strong>其他人</strong>只有可读的权限。</p><ul><li>字符表示：<code>rwxrw-r--</code></li><li>数字表示：<code>111110100</code> 即 <code>764</code></li></ul><p>那么现在我们用 <code>ls -l</code> 查看文件的信息：</p><figure><img src="/img/blog/Linux-Note-2-images/ls-l.png"alt="通过ls命令查看到的文件属性信息" /><figcaptionaria-hidden="true">通过ls命令查看到的文件属性信息</figcaption></figure><p>这些信息依次为：<strong>文件类型与权限</strong>；硬链接数；所有者；所有组；文件大小（字节）；最后修改时间；文件名。</p><blockquote><p>文件类型有 7 种，最常见的是 <code>-</code>表示普通文件，<code>d</code> 表示目录文件。</p><p>硬链接数，对于普通文件，表示指向存储数据块中该文件的<strong>链接数</strong>（相当于Windows的快捷方式）；对于目录文件，表示所在<strong>子目录级数</strong>。</p></blockquote><p>在复杂的实际情况中，单纯设置 rwx 权限已无法满足需求，因此还有SUID、SGID 与 SBIT 三个特殊权限位，这里不多赘述。</p><h3 id="修改权限">修改权限</h3><p>通常来讲，修改权限的操作有以下两个：</p><ul><li><p><code>chmod</code>：代表 Change mode，格式为<code>chmod [参数] 文件名称</code>。其中参数为数字表示的权限。</p></li><li><p><code>chown</code>：代表 Change owner，格式为<code>chown 所有者:所有组 文件名</code>。</p></li></ul><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>chmod +x script.sh</code></td><tdstyle="text-align: center;">给脚本添加可执行权限（默认对于任何人）</td></tr><tr class="even"><td style="text-align: center;"><code>chmod 777 a.txt</code></td><td style="text-align: center;">赋予所有人<strong>完整权限</strong></td></tr><tr class="odd"><td style="text-align: center;"><code>chmod 000 a.txt</code></td><td style="text-align: center;">赋予<strong>最小权限</strong>，只有 root才能操作</td></tr><tr class="even"><td style="text-align: center;"><code>chown root:root a.txt</code></td><td style="text-align: center;">修改所有者和所有组为 root</td></tr></tbody></table><p>此外，它们还有一个特别的共性，就是针对目录进行操作时需要加上大写参数<code>-R</code> 来表示递归操作，即对目录内所有的文件进行整体操作。</p><h3 id="管理员权限">管理员权限</h3><p>Linux 系统为了安全性考虑，使得许多系统命令和服务只能被 root管理员来使用，通过一些命令可以<strong>切换或暂时切换</strong> root权限。</p><ul><li><code>su</code>：代表 Swapuser，不退出登录的情况下，<strong>切换</strong>用户身份，格式为<code>su - 用户名称</code>。</li></ul><p>注意到命令中间有个 <code>-</code>符号，这意味着<strong>完全切换</strong>到新的用户，即把环境变量信息也变更为新用户的相应信息，而不是保留原始的信息。用户名称可以是普通用户，也可以是root。由普通用户切换至 root 需要输入<strong>管理员密码</strong>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> su - root</span><br>Password:  #此处输入管理员密码<br></code></pre></td></tr></table></figure><ul><li><code>sudo</code>：代表 Superuserdo，给普通用户提供<strong>额外</strong>的权限，格式为<code>sudo [参数] 用户名称</code>。</li></ul><p>最常用的是为当前用户<strong>暂时</strong>提供 root权限，此时不需要输入管理员密码，只需要<strong>当前用户密码</strong>。该命令可以避免显式输入管理员密码，但有效时间仅有5 分钟。当然 sudo 命令也需要 root 的允许。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">$</span><span class="bash"> sudo su</span><br>Password:  #此处输入当前用户密码<br></code></pre></td></tr></table></figure><p>此外，<code>sudo</code>还可加在其他命令前面，以获取高权限的执行，常用的有包管理命令。</p>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux学习笔记 #1 终端与Shell</title>
    <link href="/Linux-Note-1.html"/>
    <url>/Linux-Note-1.html</url>
    
    <content type="html"><![CDATA[<p>继 Git 后贵系的另一个暑培项目，讲授 Linux的基本用法，恰好这学期「操作系统」课程实验需要用到Linux，而且实验室的服务器也需要学习相关用法，故学之。</p><p>本文部分内容参考了清华 ZAH 同学的教程，部分参考了 刘遄 老师的《<ahref="https://www.linuxprobe.com">Linux 就该这么学</a>》，<ahref="https://www.runoob.com/linux/linux-tutorial.html">菜鸟教程-Linux</a>。</p><h2 id="linux-简介">Linux 简介</h2><p>Linux，全称 GNU/Linux，是一套免费使用和自由传播的类 Unix操作系统。相比于其他系统，Linux更加稳定且有效率、更加安全、相对不耗资源……以至于几乎所有<strong>长期稳定运行的网站服务器</strong>上、在<strong>处理大数据的集群系统</strong>中，以及需要<strong>协同工作的服务器环境</strong>都采用Linux 系统。</p><h3 id="linux-发行版">Linux 发行版</h3><p>Linux严格来说是单指操作系统的<strong>内核</strong>，因操作系统中包含了许多用户图形接口和其他实用工具。如今Linux 常用来指「基于 Linux 的完整操作系统」，内核则改以「Linux内核」称之。</p><p>一些组织或厂商将 Linux内核与各种软件和文档<strong>包装</strong>起来，并提供系统安装界面和系统配置、设定与管理工具，就构成了Linux 的发行版本。</p><figure><img src="/img/blog/Linux-Note-1-images/Linux-release.jpg"alt="常见的Linux发行版" /><figcaption aria-hidden="true">常见的Linux发行版</figcaption></figure><h2 id="命令行与终端">命令行与终端</h2><p>在学习 Linux 的过程中，有几个易混淆的概念：</p><ul><li>图形用户界面（GUI）=使用<strong>图形</strong>方式进行交互的用户界面</li><li>命令行界面（CLI）=使用<strong>文本</strong>命令进行交互的用户界面</li><li>控制台（Console）= 物理意义的终端，电脑主机的一部分</li><li>终端（Terminal）= TTY (Teletypewriter，打字机) =<strong>文本输入输出环境</strong></li><li>终端模拟器（Terminal Emulator）=套壳程序，用来<strong>模拟传统终端</strong></li><li>Shell =一个程序，<strong>命令行解释器</strong>，执行用户输入的命令并返回结果</li></ul><h3 id="cli">CLI</h3><p>命令行界面（Command-LineInterface，CLI）是在图形用户界面得到普及之前使用最为广泛的用户界面，它通常不支持鼠标，用户通过键盘输入指令，计算机接收到指令后，予以执行。也有人称之为字符用户界面（CharacterUser Interface，CUI）。</p><p>一般来说，在<strong>服务器</strong>中较多采用的是 CLI界面，或许有以下几点原因：</p><ul><li>服务器的功能是对外提供服务，并不需要直观的 GUI界面来让用户日常使用，使用 GUI 界面反而浪费资源。</li><li>使用 CLI界面管理服务器，本质上是直接用命令控制服务器，这允许一系列的自动化脚本（Shell脚本）出现来节省时间。</li><li>方便自动化、规模化部署服务器。</li></ul><h3 id="shell">Shell</h3><p>Shell 是一个用 C 语言编写的程序，它是用户使用 Linux等系统的桥梁，如同「<strong>壳</strong>」一般。它的本质是一个<strong>命令解释器</strong>，将用户输入的命令（符合Shell语法）处理成对应<strong>操作系统的控制命令</strong>，处理完毕后再将结果反馈给用户。</p><p>不同操作系统下面的 Shell 种类众多，常见的有：</p><ul><li>Windows：cmd (Command Shell)、PowerShell；</li><li>Linux/macOS：sh、bash、zsh 等。</li></ul><p>Ken Thompson 的 sh 是第一种 Unix Shell，本教程关注的是 Bash，也就是Bourne Again Shell，Bash 也是大多数 Linux 系统默认的 Shell。</p><h3 id="terminal">Terminal</h3><p>终端(Terminal)，是一种用来让用户输入数据至计算机，以及显示其计算结果的机器。早期的终端通常就是一台<strong>电子打字机</strong>（Teletypewriter,TTY），后来随着计算机的发展，打字机被键盘和显示器取代，而 GUI界面也成了主流。</p><p>于是，这时候我们就需要一个程序来模拟传统终端的行为，即<strong>终端模拟器</strong>（TerminalEmulator），当用户打开终端模拟器时，实际上是进入一个<strong>会话进程</strong>（Session）。终端模拟器有很多，这里举几个经典的例子：</p><ul><li>GNU/Linux：gnome-terminal、Konsole；</li><li>macOS：Terminal.app、iTerm2；</li><li>Windows：Windows Terminal、Cmder 等。</li></ul><h2 id="shell-使用">Shell 使用</h2><p>在 Linux 系统中打开终端时，会看到一个提示符，通常类似<code>hewei@hewei-VirtualBox ~$</code>。在提示符下，命令会被 Shell环境<strong>解析并反馈</strong>到终端中。</p><p>提示符是 Shell最主要的<strong>文本接口</strong>。它告诉你，你的主机名是<code>hewei-VirtualBox</code>，你现在的身份是 <code>hewei</code>并且你当前的<strong>工作目录</strong>（Current working directory）是<code>~</code> （默认在 <code>/home/hewei/</code> 用户目录）。</p><p><code>$</code> 符号表示您现在的身份不是<code>root</code>，输入如下命令可以暂时切换到 <code>root</code>权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">hewei@hewei-VirtualBox ~$ sudo su  <span class="hljs-comment"># 获取 root 权限</span><br>[sudo] hewei 的密码:  <span class="hljs-comment"># 在此输入密码，Terminal 下密码都是不可见的</span><br></code></pre></td></tr></table></figure><p>输入密码后，可以看到提示符变成了<code>root@hewei-VirtualBox:/home/hewei#</code>，其中 <code>#</code>符号就是超级用户权限的标志。再输入 <code>exit</code>即可退回普通用户身份。</p><h3 id="终端快捷键">终端快捷键</h3><p>在 <a href="https://hwcoder.top/2021/08/28/Git-Note-1">Git学习笔记 #1基础知识介绍</a> 中，已经简单介绍了命令行界面的一些使用技巧，这些命令在Linux 系统的 Bash 中同样使用。这里罗列出 Linux 常用快捷键：</p><ul><li><code>Ctrl+Shift+T</code>：唤出一个终端窗口。</li><li><code>Tab</code>：实现对命令、参数或文件的内容补全。</li><li><code>Ctrl+C</code>：终止当前进程的运行，重获终端的控制权。</li><li><code>Ctrl+Z</code>：挂起当前进程。</li><li><code>Ctrl+D</code>：结束键盘输入，若正在使用 Shell则退出，相当于输入 <code>exit()</code>。</li><li><code>Ctrl+L</code>：清空当前终端中已有的内容，相当于输入<code>clear</code>。</li><li><code>&lt;上下键&gt;</code>：翻看使用过的命令记录。</li><li><code>Shift+PgUp/PgDn</code>：翻页终端。</li><li><code>home/end</code>：光标快速移动到当前行首或行尾。</li><li><code>Ctrl/Shift+Ins</code> ：复制粘贴文本。</li><li><code>‘’</code> 和<code>“”</code>：命令参数中若存在空格，需要用引号括起来。</li></ul><h3 id="命令格式">命令格式</h3><p>常见的执行 Linux命令的格式是这样的：<code>命令名称 [命令参数] [命令对象]</code>。其中，命令参数用于对命令进行调整，使之更好地贴近需求，参数分为<strong>长格式</strong>和<strong>短格式</strong>，如：<code>man --help</code>，<code>man -h</code>。短格式之间可以合并，合并后仅保留一个减号即可。</p><p>在 Linux相关的手册中，我们会约定俗成地将可选择的、非必需的参数使用<strong>中括号</strong>引起来，而命令所要求的、必须有的参数或对象值，则不带中括号。</p><p>此外，要注意 Linux系统中的命令、参数、对象都是<strong>严格区分大小写</strong>的。</p><h3 id="shell-脚本">Shell 脚本</h3><p>Shell除了是一个<strong>交互式</strong>（Interactive）的命令解释器，它还是一种<strong>程序设计语言</strong>（ShellScript）。它定义了各种变量和参数，并提供了许多在高级语言中才具有的控制结构，包括循环和分支。</p><p>用 Shell 编写的<strong>脚本文件</strong>即 <code>.sh</code>文件，它能在 Shell 环境下运行，fork出一个<strong>子进程</strong>，调用系统内核来执行<strong>批处理</strong>（Batch）的系统控制。在文件的第一行，通常是<code>#!/bin/bash</code>，这句话约定了这个脚本需要哪种 Shell环境来执行。</p><p>通过如下命令就可以执行一个 Shell 脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Shell"><span class="hljs-meta">#</span><span class="bash"> 通过将文件作为参数传递给 shell 以运行 shell 脚本</span><br><span class="hljs-meta">$</span><span class="bash"> bash example.sh<span class="hljs-comment"># 这种方法不需要有 x 权限</span></span><br><span class="hljs-meta">#</span><span class="bash"> 通过具体指定 shell 脚本的路径来执行脚本</span><br><span class="hljs-meta">$</span><span class="bash"> chmod u+x script.sh</span><br><span class="hljs-meta">$</span><span class="bash"> ./example.sh<span class="hljs-comment"># 注意这种方法必须有 ./ z</span></span><br></code></pre></td></tr></table></figure><h2 id="常用命令摘要">常用命令摘要</h2><p>下面罗列了部分常用指令与参数的介绍，更多功能请在帮助手册中检索。</p><h3 id="唤出帮助界面命令">唤出帮助界面命令</h3><ul><li><code>man</code>：代表 Manuel，查看帮助手册，格式为<code>man 命令名称</code>。会进入「<strong>导航</strong>」模式，使用如下快捷键进行查阅：</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">按键</th><th style="text-align: center;">作用</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>&lt;空格键&gt;</code></td><td style="text-align: center;">向下翻一页</td></tr><tr class="even"><td style="text-align: center;"><code>PaDn</code>/<code>PgUp</code></td><td style="text-align: center;">向下/上翻一页</td></tr><tr class="odd"><td style="text-align: center;"><code>home</code>/<code>end</code></td><td style="text-align: center;">直接前往首页/尾页</td></tr><tr class="even"><td style="text-align: center;"><code>/</code></td><td style="text-align: center;">从上至下搜索某个关键词，如<code>/linux</code></td></tr><tr class="odd"><td style="text-align: center;"><code>?</code></td><td style="text-align: center;">从下至上搜索某个关键词，如<code>?linux</code></td></tr><tr class="even"><td style="text-align: center;"><code>q</code></td><td style="text-align: center;">退出帮助文档</td></tr></tbody></table></blockquote><h3 id="常用系统工作命令">常用系统工作命令</h3><ul><li><code>echo</code>：输出<strong>字符串</strong>或<strong>变量</strong>提取后的值，格式为<code>echo [字符串] [$变量]</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>echo 'hello world'</code></td><tdstyle="text-align: center;">输出一个字符串参数，<code>hello world</code></td></tr><tr class="even"><td style="text-align: center;"><code>echo hello world</code></td><tdstyle="text-align: center;">输出两个<strong>空格隔开</strong>的字符串参数，<code>hello</code>和 <code>world</code></td></tr><tr class="odd"><td style="text-align: center;"><code>echo hello\ world</code></td><td style="text-align: center;">输出一个字符串参数，<code>\</code><strong>转义</strong>空格</td></tr><tr class="even"><td style="text-align: center;"><code>echo $PATH</code></td><td style="text-align: center;">输出环境变量 <code>PATH</code>的提取值</td></tr></tbody></table></blockquote><ul><li><code>date</code>：显示或设置系统的时间与日期，格式为：<code>date [+指定的格式]</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>date</code></td><td style="text-align: center;">显示<code>Sat Sep 5 09:13:45 CST 2020</code></td></tr><tr class="even"><tdstyle="text-align: center;"><code>date "+%Y-%m-%d %H:%M:%S"</code></td><td style="text-align: center;">按格式显示<code>2020-09-05 09:14:35</code></td></tr><tr class="odd"><tdstyle="text-align: center;"><code>date -s "20201101 8:30:00"</code></td><td style="text-align: center;">设置系统时间</td></tr></tbody></table></blockquote><ul><li><code>reboot</code>：重启系统，无格式，需要 <code>root</code>权限。</li><li><code>poweroff</code>：关闭系统，无格式，需要 <code>root</code>权限。</li><li><code>ps</code>：代表Processes，查看系统中的<strong>进程状态</strong>，格式为<code>ps [参数]</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>-a</code></td><td style="text-align: center;">显示所有进程（包括其他用户的进程）</td></tr><tr class="even"><td style="text-align: center;"><code>-u</code></td><td style="text-align: center;">显示用户以及其他详细信息</td></tr><tr class="odd"><td style="text-align: center;"><code>-x</code></td><td style="text-align: center;">显示没有控制终端的进程</td></tr><tr class="even"><td style="text-align: center;"><code>aux</code></td><td style="text-align: center;">合并三个参数，<code>ps</code> 命令可省略<code>-</code></td></tr></tbody></table><p>常用于定位 PID，例如 <code>ps aux | grep run.py</code>，定位后用于kill 等操作。</p></blockquote><ul><li><code>pstree</code>：以<strong>树状图</strong>形式展示进程之间关系，无格式。</li><li><code>top</code>：<strong>动态地监视</strong>进程活动及系统负载等信息，无格式。</li><li><code>htop</code>：更加<strong>直观、友好的资源监控</strong>界面，显示系统的负载情况，包括CPU、内存、交换分区等，按照 CPU使用率进行排序，显示进程树形结构，无格式。</li><li><code>nice</code>：调整进程的<strong>优先级</strong>，格式为<code>nice -n 优先级数字 服务名称</code>。取值为 -20 到19，数字越小，优先级越高。</li><li><code>pidof</code>：查询指定进程的 PID，格式为<code>pidof [参数] 服务名称</code>。</li><li><code>kill</code>：终止指定 PID 值的进程，格式为<code>kill [参数] PID</code>。参数 <code>-9</code>表示<strong>最高权限</strong>终止。</li><li><code>sudo</code>：以临时管理员身份（root）执行命令，格式为<code>sudo 命令</code>，需要被授予权限。</li><li><code>su</code>：代表 Switch User，更换其他使用者，除 root外，需要输入该使用者的密码。</li><li><code>tee</code>：读取标准输入的数据，并将其内容输出成文件，格式为<code>tee [参数] 目标文件</code>。</li></ul><blockquote><p>默认新建、覆盖目标文件，使用参数 <code>-a</code>可以附加到已有文件的后方，参数 <code>-i</code>忽略中断信号。目标文件可以是多个，用空格隔开。</p><p>使用命令后会提示用户输入数据，此时键入终端的内容将被读取。也可以结合<strong>管道运算符</strong>，将上一个任务的输出保存到目标文件：<code>python test.py | tee -a test.log</code>。</p><p><code>tee</code>实现的是复制标准输出，如果想要直接<strong>重定向所有输出</strong>，可以使用<code>python test.py &amp;&gt; test.log</code>。</p><p>注意，有时候输出内容并<strong>不会实时显示到文件</strong>中，这是因为Python 程序中的 <code>print()</code> 带有输出缓冲区，需要加上参数<code>flush=True</code>。</p></blockquote><h3 id="系统状态检测命令">系统状态检测命令</h3><ul><li><code>ifconfig</code>：代表 Interfaceconfig，获取网卡配置与网络状态等信息。</li><li><code>ipaddr</code>：代表 IP address，获取 IP 地址等信息。</li><li><code>uname</code>：代表 Unixname，查看系统<strong>内核</strong>版本与系统架构等信息。</li><li><code>uptime</code>：查看系统的<strong>负载信息</strong>（当前系统时间、已运行时间、启用终端数、平均负载值），无格式。</li><li><code>free</code>：显示当前系统中<strong>内存的使用量</strong>，格式为<code>free [-h]</code>，<code>-h</code> 为易读模式（humanreadable）。</li><li><code>who</code>：查看当前登入主机的<strong>用户终端信息</strong>，无格式。</li><li><code>ping</code>：测试主机之间的网络连通性，格式为<code>ping [参数] 主机地址</code>。</li><li><code>history</code>：显示执行过的命令历史，格式为<code>history [-c]</code>，<code>-c</code> 为清空（clear）。</li></ul><h3 id="查找定位文件命令">查找定位文件命令</h3><ul><li><code>pwd</code>：代表 Print workingdirectory，显示用户当前所处的工作目录，无格式。</li><li><code>cd</code>：代表 Change directory，切换当前的工作路径，格式为<code>cd [参数] [目录]</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>cd -</code></td><td style="text-align: center;">返回到上一次所处的目录</td></tr><tr class="even"><td style="text-align: center;"><code>cd .</code></td><td style="text-align: center;">进入目录（无变化）</td></tr><tr class="odd"><td style="text-align: center;"><code>cd ..</code></td><td style="text-align: center;">进入上级目录</td></tr><tr class="even"><td style="text-align: center;"><code>cd /</code></td><td style="text-align: center;">切换到根目录</td></tr><tr class="odd"><td style="text-align: center;"><code>cd ~</code></td><td style="text-align: center;">切换到当前用户的 <code>/home/</code>目录</td></tr><tr class="even"><td style="text-align: center;"><code>cd ~ &lt;username&gt;</code></td><td style="text-align: center;">切换到其他用户的 <code>/home/</code>目录</td></tr><tr class="odd"><td style="text-align: center;"><code>cd /bin</code></td><td style="text-align: center;">切换到绝对路径 <code>/bin</code> 下</td></tr></tbody></table></blockquote><ul><li><code>ls</code>：代表 List，显示目录中的文件信息，格式为<code>ls [参数] [文件名称]</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>ls -a</code></td><td style="text-align: center;">查看全部文件（包括隐藏文件）</td></tr><tr class="even"><td style="text-align: center;"><code>ls -l</code></td><tdstyle="text-align: center;">列表形式，查看文件的属性、大小等详细信息</td></tr><tr class="odd"><td style="text-align: center;"><code>ls -al</code></td><td style="text-align: center;">合并以上两个参数</td></tr><tr class="even"><td style="text-align: center;"><code>ls -alh</code></td><td style="text-align: center;">易读显示（human readable）</td></tr><tr class="odd"><td style="text-align: center;"><code>ls -ld</code></td><tdstyle="text-align: center;">查看当前路径文件夹的属性、大小等详细信息</td></tr></tbody></table></blockquote><ul><li><code>tree</code>：以树状图的形式列出<strong>目录内容及结构</strong>，无格式。</li><li><code>find</code>：指定条件查找文件所对应的位置，格式为<code>find [查找范围] 寻找条件</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><tdstyle="text-align: center;"><code>find /etc -name "host*"</code></td><td style="text-align: center;">查找 <code>/etc</code> 路径下<code>host</code> 开头的文件</td></tr><tr class="even"><td style="text-align: center;"><code>find / -perm -4000</code></td><td style="text-align: center;">整个系统中搜索权限包括 SUID 的文件</td></tr></tbody></table></blockquote><ul><li><p><code>updatedb</code>：生成文件索引库，无格式。和以下命令配合使用，快于<code>find</code>。</p></li><li><p><code>locate</code>：按照名称快速搜索文件所对应的位置，格式为<code>locate 文件名称</code>。</p></li><li><p><code>whereis</code>：按照名称快速搜索<strong>命令</strong>、<strong>源代码</strong>及<strong>帮助文件</strong>的位置，格式为<code>whereis 命令名称</code>。</p></li><li><p><code>which</code>：按照名称检索<strong>命令</strong>的位置，格式为<code>which 命令名称</code>，默认在 PATH变量所指定的路径中检索。</p></li><li><p><code>du</code>：diskusage，一个专门用于查看目录和文件磁盘使用情况的命令。</p></li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>du -h --max-depth=1</code></td><td style="text-align: center;">显示当前目录下的子目录大小</td></tr><tr class="even"><tdstyle="text-align: center;"><code>du -h --max-depth=1 /path/to/directory</code></td><td style="text-align: center;">指定目录显示</td></tr><tr class="odd"><tdstyle="text-align: center;"><code>du -h --max-depth=1 | sort -h</code></td><td style="text-align: center;">对输出进行排序</td></tr></tbody></table></blockquote><h3 id="文本统计编辑命令">文本统计编辑命令</h3><ul><li><code>cat</code>：代表Concatenate，查看<strong>内容较少</strong>的纯文本文件，格式为<code>cat [参数] 文件名称</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>cat a.txt</code></td><td style="text-align: center;">查看当前目录下文件内容</td></tr><tr class="even"><td style="text-align: center;"><code>cat -n a.txt</code></td><td style="text-align: center;">查看时显示行号</td></tr><tr class="odd"><td style="text-align: center;"><code>cat /etc/os-release</code></td><td style="text-align: center;">查看操作系统信息</td></tr></tbody></table></blockquote><ul><li><code>more</code>：查看<strong>内容较多</strong>的纯文本文件，格式为<code>more [参数] 文件名称</code>。用空格键或回车键向下滚动查看。</li><li><code>head</code>：查看纯文本文件的前 <em>N</em> 行，格式为<code>head -n [行数] 文件名称</code>。</li><li><code>tail</code>：查看纯文本文件的后 <em>N</em> 行，格式为<code>tail [参数] 文件名称</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>tail -n 10 a.txt</code></td><td style="text-align: center;">显示文件的后 10 行</td></tr><tr class="even"><tdstyle="text-align: center;"><code>tail -f /var/log/messages</code></td><td style="text-align: center;">查看并实时刷新，常用于日志文件</td></tr></tbody></table></blockquote><ul><li><code>tr</code>：代表 Translate，替换文本内容中的字符，格式为<code>tr [原始字符] [目标字符]</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><tdstyle="text-align: center;"><code>cat a.txt丨tr [a-z] [A-Z]</code></td><tdstyle="text-align: center;">把文本中的英文以全大写输出（经过管道运算符）</td></tr></tbody></table></blockquote><ul><li><code>wc</code>：统计指定文本文件的行数 <code>-l</code>、字数<code>-w</code> 或字节数 <code>-c</code>，格式为<code>wc [参数] 文件名称</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>wc -l /etc/passwd</code></td><td style="text-align: center;">统计 <code>passwd</code>行数，等价于求用户数</td></tr><tr class="even"><td style="text-align: center;"><code>wc 1.txt</code></td><td style="text-align: center;">不加参数，默认三个值都返回</td></tr></tbody></table></blockquote><ul><li><code>stat</code>：查看文件的具体存储细节和时间等信息，格式为<code>stat 文件名称</code>。</li><li><code>grep</code>：代表 Global Regular ExpressionPrint，按行提取文本内容，格式为<code>grep [参数] 查询内容 文件名称</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>-b</code></td><td style="text-align: center;">将可执行文件 (bin) 当作文本文件 (txt)来搜索</td></tr><tr class="even"><td style="text-align: center;"><code>-c</code></td><td style="text-align: center;">仅显示找到的行数</td></tr><tr class="odd"><td style="text-align: center;"><code>-i</code></td><td style="text-align: center;">忽略查询内容的大小写</td></tr><tr class="even"><td style="text-align: center;"><code>-n</code></td><td style="text-align: center;">同时显示行号</td></tr><tr class="odd"><td style="text-align: center;"><code>-v</code></td><td style="text-align: center;">反向选择，列出不包含查询内容的行</td></tr></tbody></table></blockquote><ul><li><code>diff</code>：比较多个文件之间内容的差异，格式为<code>diff [参数] 文件名称A 文件名称B</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><tdstyle="text-align: center;"><code>diff --brief diff_A.txt diff_B.txt</code></td><td style="text-align: center;">仅显示判断结果</td></tr><tr class="even"><tdstyle="text-align: center;"><code>diff -c diff_A.txt diff_B.txt</code></td><td style="text-align: center;">描述不同的行</td></tr></tbody></table></blockquote><ul><li><code>uniq</code>：代表 Unique，去除文本中连续的重复行，格式为<code>uniq [参数] 文件名称</code>。</li><li><code>sort</code>：对文本内容进行再排序，格式为<code>sort [参数] 文件名称</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>-f</code></td><td style="text-align: center;">忽略大小写</td></tr><tr class="even"><td style="text-align: center;"><code>-b</code></td><td style="text-align: center;">忽略缩进与空格</td></tr><tr class="odd"><td style="text-align: center;"><code>-n</code></td><td style="text-align: center;">以数值型排序</td></tr><tr class="even"><td style="text-align: center;"><code>-r</code></td><td style="text-align: center;">反向排序</td></tr><tr class="odd"><td style="text-align: center;"><code>-u</code></td><td style="text-align: center;">去除重复行</td></tr><tr class="even"><td style="text-align: center;"><code>-t</code></td><td style="text-align: center;">指定间隔符</td></tr><tr class="odd"><td style="text-align: center;"><code>-k</code></td><td style="text-align: center;">按照间隔符，指定第几列值</td></tr></tbody></table></blockquote><ul><li><code>less</code>：快速浏览文件，仅阅读模式，防止误修改，格式为<code>less 文件名称</code>，常用参数有 <code>-S</code>表示<strong>不自动换行</strong>。快捷键为：</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">滚屏</th><th style="text-align: center;">向上滚屏</th><th style="text-align: center;">向下滚屏</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">一页</td><td style="text-align: center;"><code>b</code>（back）</td><td style="text-align: center;"><code>空格</code></td></tr><tr class="even"><td style="text-align: center;">半页</td><td style="text-align: center;"><code>u</code>（undo）</td><td style="text-align: center;"><code>d</code>（down）</td></tr><tr class="odd"><td style="text-align: center;">一行</td><td style="text-align: center;"><code>y</code></td><td style="text-align: center;"><code>回车</code></td></tr><tr class="even"><td style="text-align: center;">跳跃</td><td style="text-align: center;"><code>g</code></td><td style="text-align: center;"><code>G</code></td></tr></tbody></table><table><thead><tr class="header"><th style="text-align: center;">搜索</th><th style="text-align: center;">说明</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>/pattern</code></td><td style="text-align: center;">向后搜索 pattern</td></tr><tr class="even"><td style="text-align: center;"><code>?pattern</code></td><td style="text-align: center;">向前搜索 pattern</td></tr><tr class="odd"><td style="text-align: center;"><code>n</code></td><td style="text-align: center;">跳跃到下一个 pattern</td></tr><tr class="even"><td style="text-align: center;"><code>N</code></td><td style="text-align: center;">跳跃到上一个 pattern</td></tr><tr class="odd"><td style="text-align: center;"><code>&amp;pattern</code></td><td style="text-align: center;">Display only matching lines</td></tr><tr class="even"><td style="text-align: center;"><code>!/pattern</code></td><td style="text-align: center;">Search for NON-matching lines</td></tr></tbody></table></blockquote><h3 id="文件目录管理命令">文件目录管理命令</h3><ul><li><code>touch</code>：创建空白文件或设置文件的时间，格式为<code>touch [参数] 文件名称</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>touch 1.txt</code></td><td style="text-align: center;">创建一个空白文件</td></tr><tr class="even"><tdstyle="text-align: center;"><code>touch -d "2020-05-04 15:44" 1.txt</code></td><td style="text-align: center;">设置文件上次修改时间</td></tr></tbody></table></blockquote><ul><li><code>mkdir</code>：代表 Make directory，创建空白的目录，格式为<code>mkdir [参数] 目录名称</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>mkdir mydir</code></td><td style="text-align: center;">在当前路径下新建文件夹</td></tr><tr class="even"><td style="text-align: center;"><code>mkdir -p a/b/c/d/e</code></td><td style="text-align: center;">创建层叠目录</td></tr></tbody></table></blockquote><ul><li><code>cp</code>：代表 Copy，复制文件或目录，格式为<code>cp [参数] 源文件名称 目标文件名称</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">源文件</th><th style="text-align: center;">目标文件</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">文件</td><td style="text-align: center;">目录</td><td style="text-align: center;">把源文件复制到该目录中</td></tr><tr class="even"><td style="text-align: center;">文件</td><td style="text-align: center;">文件</td><td style="text-align: center;">询问是否要覆盖它（需要<code>-i</code>）</td></tr><tr class="odd"><td style="text-align: center;">文件</td><td style="text-align: center;">不存在</td><tdstyle="text-align: center;">执行正常的复制操作（如<strong>备份</strong>）</td></tr><tr class="even"><td style="text-align: center;">目录</td><td style="text-align: center;">目录</td><td style="text-align: center;">递归持续复制（需要 <code>-r</code> 或<code>-a</code>）</td></tr></tbody></table></blockquote><ul><li><code>scp</code>：服务器之间拷贝数据，格式为<code>scp [参数] 源文件名称 目标文件名称</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">源文件</th><th style="text-align: center;">目标文件</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">文件</td><td style="text-align: center;">目录</td><td style="text-align: center;">把源文件复制到该目录中</td></tr><tr class="even"><td style="text-align: center;">文件</td><td style="text-align: center;">文件</td><td style="text-align: center;">询问是否要覆盖它（需要<code>-i</code>）</td></tr><tr class="odd"><td style="text-align: center;">文件</td><td style="text-align: center;">不存在</td><tdstyle="text-align: center;">执行正常的复制操作（如<strong>备份</strong>）</td></tr><tr class="even"><td style="text-align: center;">目录</td><td style="text-align: center;">目录</td><td style="text-align: center;">递归持续复制（需要 <code>-r</code> 或<code>-a</code>）</td></tr></tbody></table></blockquote><ul><li><code>mv</code>：移动文件，格式为<code>mv [参数] 源文件名称 目标文件名称</code>。不同于复制操作，默认会把源文件删除，只保留剪切后的文件。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>mv 1.txt 2.txt</code></td><td style="text-align: center;">相当于重命名文件</td></tr><tr class="even"><td style="text-align: center;"><code>mv 1.txt mydir</code></td><td style="text-align: center;">剪切到新目录下</td></tr></tbody></table></blockquote><ul><li><code>rm</code>：删除文件或目录，格式为<code>rm [参数] 文件名称</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>-f</code></td><td style="text-align: center;">强制执行，不询问</td></tr><tr class="even"><td style="text-align: center;"><code>-i</code></td><td style="text-align: center;">删除前询问（默认）</td></tr><tr class="odd"><td style="text-align: center;"><code>-r</code></td><td style="text-align: center;">递归删除目录</td></tr><tr class="even"><td style="text-align: center;"><code>-v</code></td><td style="text-align: center;">显示正在删除过程</td></tr></tbody></table></blockquote><ul><li><code>rmdir</code>：删除<strong>空</strong>目录，格式为<code>rmdir [参数] 目录名称</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>rmdir mydir</code></td><tdstyle="text-align: center;">删除当前路径的<strong>空</strong>目录</td></tr><tr class="even"><td style="text-align: center;"><code>rmdir -p a/b/c</code></td><td style="text-align: center;">删除层叠空目录</td></tr></tbody></table></blockquote><ul><li><code>file</code>：查看文件的类型，常用于无后缀文件，格式为<code>file 文件名称</code>。</li></ul><blockquote><p>Linux 中一切皆文件，而许多文件都是没有后缀的。</p><p>不像Windows，用户双击鼠标打开文件，此时系统会自行判断用户双击的文件是什么类型，因此需要有后缀进行区别。</p><p>而 Linux 系统则是根据用户执行的命令来调用文件，例如执行 cat命令查看文本等，所以也就不需要强制让用户给文件设置后缀了。</p></blockquote><ul><li><code>tar</code>：tar 文件进行打包压缩或解压，格式为<code>tar 参数 压缩包名称 要压缩的文件或目录</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>-c</code></td><td style="text-align: center;"><strong>创建</strong>压缩文件</td></tr><tr class="even"><td style="text-align: center;"><code>-x</code></td><td style="text-align: center;"><strong>解开</strong>压缩文件</td></tr><tr class="odd"><td style="text-align: center;"><code>-t</code></td><td style="text-align: center;">查看压缩包内有哪些文件</td></tr><tr class="even"><td style="text-align: center;"><code>-v</code></td><td style="text-align: center;">显示真正压缩或解压的过程</td></tr><tr class="odd"><td style="text-align: center;"><code>-C</code></td><td style="text-align: center;">指定解压到的目录</td></tr><tr class="even"><td style="text-align: center;"><code>-f</code></td><td style="text-align: center;">指定压缩包名称</td></tr><tr class="odd"><td style="text-align: center;"><code>-z</code></td><td style="text-align: center;"><strong>真正的「压缩」命令</strong></td></tr></tbody></table><p>需要注意的是 tar命令只是将文件或目录打包，并不具有压缩功能，因此压缩后的大小与源文件相同。常用的命令如下：</p><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>tar -cvf data.tar file</code></td><td style="text-align: center;">创建一个压缩包，压缩文件</td></tr><tr class="even"><tdstyle="text-align: center;"><code>tar -czvf data.tar.gz file1 file2 file3</code></td><td style="text-align: center;">创建一个真正的压缩包，压缩多个文件</td></tr><tr class="odd"><td style="text-align: center;"><code>tar -xvf data.tar</code></td><td style="text-align: center;">解压缩到当前目录</td></tr><tr class="even"><tdstyle="text-align: center;"><code>tar -Cxvf data.tar.gz ./data/</code></td><td style="text-align: center;">解压缩到指定目录</td></tr></tbody></table></blockquote><ul><li><code>zip/unzip</code>：zip 文件进行压缩、解压缩，格式为<code>zip/unzip [参数] 压缩包名称 [要压缩的文件或目录]</code>。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>-d 目标目录</code></td><tdstyle="text-align: center;"><strong>解压</strong>到指定目录下，默认当前目录</td></tr><tr class="even"><td style="text-align: center;"><code>-v</code></td><td style="text-align: center;">查看压缩文件的详细信息，但并不解压</td></tr><tr class="odd"><td style="text-align: center;"><code>-t</code></td><td style="text-align: center;">测试压缩文件有无损坏，但并不解压</td></tr><tr class="even"><td style="text-align: center;"><code>-o</code></td><tdstyle="text-align: center;">解压时覆盖已经存在的文件，并且无需用户确认</td></tr><tr class="odd"><td style="text-align: center;"><code>-n</code></td><td style="text-align: center;">解压时并不覆盖已经存在的文件</td></tr><tr class="even"><td style="text-align: center;"><code>-r</code></td><td style="text-align: center;">递归地压缩目录及其内容（仅适用于 .zip格式）</td></tr><tr class="odd"><td style="text-align: center;"><code>-z</code></td><td style="text-align: center;">使用 gzip 进行压缩（仅适用于 .tar 或.tar.gz 格式）</td></tr></tbody></table><p>常用命令如下：</p><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><tdstyle="text-align: center;"><code>zip data.zip file1 file2 file3</code></td><td style="text-align: center;">创建一个压缩包，压缩多个文件</td></tr><tr class="even"><tdstyle="text-align: center;"><code>zip -r data.zip ./data/*</code></td><td style="text-align: center;">递归压缩目录及内容</td></tr><tr class="odd"><td style="text-align: center;"><code>unzip data.zip</code></td><td style="text-align: center;">解压缩到当前目录</td></tr><tr class="even"><tdstyle="text-align: center;"><code>unzip data.zip -d ./data/</code></td><td style="text-align: center;">解压缩到目标目录</td></tr></tbody></table></blockquote><ul><li><code>du</code>：diskusage，一个专门用于查看目录和文件磁盘使用情况的命令。</li></ul><blockquote><table><thead><tr class="header"><th style="text-align: center;">样例</th><th style="text-align: center;">功能</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>du -h --max-depth=1</code></td><td style="text-align: center;">显示当前目录下的子目录大小</td></tr><tr class="even"><tdstyle="text-align: center;"><code>du -h --max-depth=1 /path/to/directory</code></td><td style="text-align: center;">指定目录显示</td></tr><tr class="odd"><tdstyle="text-align: center;"><code>du -h --max-depth=1 | sort -h</code></td><td style="text-align: center;">对输出进行排序</td></tr></tbody></table></blockquote>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git学习笔记 #4 GitHub合作开发</title>
    <link href="/Git-Note-4.html"/>
    <url>/Git-Note-4.html</url>
    
    <content type="html"><![CDATA[<p>接上文，本文介绍了 Git 基于 <strong>GitHub</strong>的使用，以及<strong>合作开发</strong>的注意事项。文章的最后还留下了一些待填的坑...</p><p>本文大部分内容参考了 RCY 同学的教程，部分参考了 <ahref="https://www.liaoxuefeng.com/wiki/896043488029600">廖雪峰教程-Git</a>，<ahref="https://www.runoob.com/git/git-tutorial.html">菜鸟教程-Git</a>，以及Git 官网文档 <ahref="https://git-scm.com/doc">Git-Documentation</a>。</p><h2 id="github-合作开发">GitHub 合作开发</h2><p>以下内容也适用于不同的服务器平台，如 Gitee、GitLab 等。</p><h3 id="新建远程仓库">新建远程仓库</h3><p>从 GitHub 上创建一个空仓库后，通常有三个选择：</p><ul><li><strong>直接克隆</strong>到本地，会生成一个仓库文件夹，里面只有一个<code>.git</code> 文件夹。可以在仓库文件夹中创作。</li><li>在<strong>本地新建</strong>一个同名文件夹，再创建本地仓库，添加远程仓库，推送并绑定上游分支。</li><li>对<strong>本地已有</strong>的一个仓库（确保同名），添加远程仓库，推送并绑定上游分支。</li></ul><p>对于第一种方法，只需要 <code>git clone</code>命令即可，对于第二种方法，GitHub 提供了一系列指引命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;# test&quot;</span> &gt;&gt; README.md<br>$ git init<br>$ git add README.md<br>$ git commit -m <span class="hljs-string">&quot;first commit&quot;</span><br>$ git branch -M main  <span class="hljs-comment"># 重命名本地主分支，避免冲突，新版本特性</span><br>$ git remote add origin https://github.com/hewei2001/test.git<br>$ git push -u origin main  <span class="hljs-comment"># 推送并绑定上游分支</span><br></code></pre></td></tr></table></figure><p>对于第三种情况，只需完成后面三步即可。</p><h3 id="gitignore">.gitignore</h3><p>在通过 GitHub 新建仓库时，我们会发现有个 <code>Add .gitignore</code>按钮，这有什么用呢？实际上我们知道，维护真正项目时可能有一次修改会涉及到多个文件，这个时候一般大家会倾向于使用<code>git add .</code> 的简单操作。</p><p>但是，并非所有文件都应该被放入 Git 仓库中，如：</p><ul><li>IDE / 编辑器 的<strong>配置文件</strong>（如 .vscode、.idea等）；</li><li>数据库文件；</li><li>带有敏感信息的文件。</li></ul><p>这和便利的命令形成了矛盾，为此出现了特殊文件<code>.gitignore</code>，它可以决定哪些的文件不需要添加到版本管理中。一个样例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Python3 的缓存文件（预编译模块）</span><br>__pycache__<br><span class="hljs-comment"># Vscode 的配置文件</span><br>.vscode<br><span class="hljs-comment"># Pycharm 等Jet的配置文件</span><br>.idea<br><span class="hljs-comment"># 忽略项目中所有 ckpt 目录</span><br>ckpt/<br><span class="hljs-comment"># 忽略项目根目录下的 data 目录</span><br>/data/<br><span class="hljs-comment"># 忽略项目根目录下的 foo 目录下所有的 model.pt</span><br>/foo/**/model.pt<br></code></pre></td></tr></table></figure><p>以行为单位，一行写一个规则，决定什么不被添加，书写规则也很简单：</p><ul><li>以 <code>#</code> 开头的一行是注释，用 <code>\</code>进行转义。</li><li>若规则不包含 <code>/</code>，会对 <code>.gitignore</code><strong>同路径下</strong>的文件和目录进行屏蔽；</li><li>若规则以 <code>/</code>结束，则会匹配项目中所有同名目录下的内容，但不匹配同名文件；</li><li>若规则以 <code>/</code> 开始，则会从项目根目录开始匹配；</li><li><code>*</code> 匹配<strong>除了 <code>/</code>以外</strong>的任意字符串，<code>**</code> 匹配完整路径名；</li><li><code>?</code> 匹配<strong>除了 <code>/</code>以外</strong>的任意单个字符；</li><li>区间表示，如<code>[a-zA-Z]</code>，可以用于匹配任何一个区间范围内的字符；</li><li><code>!</code>开头的模式标识<strong>否定</strong>，该文件将会再次被包含，通常用于在屏蔽的文件中添加<strong>特例</strong>；</li><li>不能识别<strong>中文</strong>，因为默认编码是 GBK。</li></ul><p>创建一个 <code>.gitignore</code> 文件最便捷的方法是，在创建仓库时勾选GitHub 自动创建，并在选项中选择合适的 <code>.gitignore</code>文件模板，再克隆到本地手动补充。</p><blockquote><p>在实际工程中，用 GitHub 创建仓库还可以便捷地添加合适的 LICENSE文件。</p></blockquote><h3 id="远程服务器拒绝">远程服务器拒绝</h3><p>如果你是在一个<strong>大合作团队</strong>中工作，很可能是 main被<strong>锁定</strong>了（只有 Collaborators 有权限推送），其他人需要Pull Request 流程来合并修改。</p><p>如果你直接提交到本地main，然后试图推送修改，你将会收到这样类似的信息:</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs less">! <span class="hljs-selector-attr">[远程服务器拒绝]</span> <span class="hljs-selector-tag">main</span> <span class="hljs-selector-tag">-</span>&gt; <span class="hljs-selector-tag">main</span> (<span class="hljs-attribute">TF402455</span>: 不允许推送(push)这个分支; 你必须使用pull request来更新这个分支.)<br></code></pre></td></tr></table></figure><p>这里推荐一个做法：<strong>Fork</strong>最新的仓库，此时你将拥有这个远程仓库的<strong>从分支</strong>——你的用户名就是从分支名。再将你的修改提交到这条分支，最后通过GitHub 网站来提交一个PR，请求主分支<strong>拉取并合并</strong>你的从分支。</p><blockquote><p>当然，如果只是几个人的<strong>小开发团队</strong>，还是建议在仓库的<code>Settings</code> 界面找到<code>Collaborators</code>，直接添加队友来得方便。</p></blockquote><p>此外，即使是在小开发团队中，也可能会遇到 push时远程服务器拒绝的情况，此时大概率是因为产生了<strong>分叉</strong>(Branch Diverged) ！具体的处理办法参见上一章。</p><h3 id="upstream">upstream</h3><p>在前文介绍 Push时，曾提过上游（upstream）的概念，对于个人开发而言，<code>upstream</code>只是标识了多个远程仓库或分支中的某个默认值。但在合作开发，特别是需要提PR 时，<code>upstream</code> 有其特定意义。</p><ul><li><code>upstream</code> 通常指代你所 fork的原始仓库，你无权修改。</li><li><code>origin</code> 则是你所 fork 下来的仓库，你有权修改。</li></ul><p>因此，当需要对一个大型项目贡献时，你的操作流程应该是：</p><ol type="1"><li><p>先在 GitHub 上 fork 原始仓库，得到你所有的仓库。</p></li><li><p>再 clone 你所有的仓库到本地，此时它默认为 <code>origin</code>分支。</p></li><li><p>将原始仓库添加到 <code>upstream</code>以保持联系，跟进其变化：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git remote add upstream git://github.com/&lt;aUser&gt;/&lt;aRepo.git&gt;<br>$ git remote -v  <span class="hljs-comment"># 查看远程仓库情况</span><br></code></pre></td></tr></table></figure><p>当原始仓库有变化时，可以拉取其更新：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git fetch upstream  <span class="hljs-comment"># 如果设置了 upstream 可以缺省</span><br>$ git merge<br></code></pre></td></tr></table></figure></li><li><p>当本地做出提交时，先推送到自己的 <code>origin</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git push origin  <span class="hljs-comment"># 如果不加 origin 会被远程服务器拒绝</span><br></code></pre></td></tr></table></figure><p>然后再通过 Pull Request 请求原始仓库拉取你的修改。</p></li></ol><h3 id="暂存本地修改">暂存本地修改</h3><p>有时候在本地修改进行到一半的时候，需要拉取远程版本，此时如果直接<code>git pull</code>则会和本地版本产生冲突。一个解决办法是先将本地的代码 <code>commit</code>再去 <code>fetch</code> 远程的并手动<code>merge</code>。但是这样则会提交一个半成品的节点。</p><p>Git 提供了一个 <code>git stash</code>命令来解决这个问题：将当前未提交的修改（包括工作区和暂存区的修改）<strong>先暂时储藏起来</strong>，这样工作区干净了后，就可以直接拉取线上更新。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 暂存修改</span><br>$ git stash<br><span class="hljs-comment"># 拉取更新</span><br>$ git pull<br><span class="hljs-comment"># 弹出之前暂存的内容，之后可能需要手动 merge 冲突文件</span><br>$ git stash pop<br></code></pre></td></tr></table></figure><p>如果需要多次暂存——「修改，暂存，再修改，再暂存」，则可以使用如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查看储藏记录列表</span><br>$ git stash list<br></code></pre></td></tr></table></figure><h2 id="多远程与多分支">多远程与多分支</h2><p>实践中遇到的最多的问题：</p><ul><li>创建空分支：https://www.jianshu.com/p/a18487d987ac</li><li>推送本地所有分支和拉取远程所有分支：https://blog.csdn.net/toopoo/article/details/85260277</li><li>将一个项目同时从本地推送到 GitHub 和Gitee：https://www.cnblogs.com/poloyy/p/12215199.html</li><li>将 upstream 的分支拉到本地后没有对应的同名分支：</li></ul><h2 id="高端的-git">高端的 Git</h2><p>高端的操作往往意味着危险的操作，以下将介绍前文未提及的一些操作，同时也挖下一些坑以后来填。</p><h3 id="版本回退-reset">版本回退 (Reset)</h3><p>有时候你可能希望放弃 Git仓库中的一部分提交，退到其他某处，这种情况下可以：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git reset &lt;Commit ID or Branch&gt;         <span class="hljs-comment"># 让 Git 仓库回退到某个记录或分支位置</span><br>$ git reset &lt;Commit Id or Branch&gt; --hard  <span class="hljs-comment"># 让所有区回退到某个记录或分支位置（将丢失你工作区和暂存区的数据）</span><br></code></pre></td></tr></table></figure><p>这种情况下可以带着分支一起回退，然后重新commit，走一条完全不同的道路，而放弃了部分数据。</p><p>但是，如果你 reset 完后又后悔了，该怎么办？没关系，只要你没玩 gc这样的危险指令，那么你 Git 仓库中的数据总有机会找回来。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git reflog <span class="hljs-comment"># 查看最近一些变动快照的操作与版本号</span><br></code></pre></td></tr></table></figure><p>在看到后面的快照的版本号后，则可以 reset 回去。</p><h3 id="cherry-pick">Cherry-Pick</h3><p>一个相对高端的分支管理命令，将一些提交复制到当前所在的位置（HEAD）下面，这些提交可以来自其他分支，但不一定是顺序的！</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git cherry-pick &lt;Commit ID&gt;<br></code></pre></td></tr></table></figure><h3 id="获取帮助">获取帮助</h3><p>无论是本博客，还是网上的文档，甚至官方的教程，都未必能把每个指令的每个参数、用法提到，因此Git 自带的帮助文档就很重要：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git add --<span class="hljs-built_in">help</span><br>$ git <span class="hljs-built_in">help</span> add<br><span class="hljs-comment"># 以上两个指令的效果等价，都会</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Git</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git学习笔记 #3 远程仓库使用</title>
    <link href="/Git-Note-3.html"/>
    <url>/Git-Note-3.html</url>
    
    <content type="html"><![CDATA[<p>接上文，本文介绍了 Git在<strong>远程仓库</strong>的使用，以及<strong>合作开发</strong>的简易教程。</p><p>本文大部分内容参考了 RCY 同学的教程，部分参考了 <ahref="https://www.liaoxuefeng.com/wiki/896043488029600">廖雪峰教程-Git</a>，<ahref="https://www.runoob.com/git/git-tutorial.html">菜鸟教程-Git</a>，以及Git 官网文档 <ahref="https://git-scm.com/doc">Git-Documentation</a>。</p><h2 id="远程的-git">远程的 Git</h2><p>尽管 Git本身是分布式的，但我们通常需要一个服务器来同步、传递我们的本地仓库数据，处于一种「<strong>伪集中式</strong>」的状态。但服务器上的内容不一定是最新的内容，只是一个用于传递的中间态而已。</p><p>因此有了一系列的相关网站，通常来说我们用以下两个：</p><ul><li>GitHub：⼀个基于 Git的代码托管服务平台，开源社区交流代码的重要网站，参考https://www.github.com/ 。</li><li>GitLab：类似Git，有完善的管理界面和权限控制，一般用于在企业、学校等内部网络搭建 Git私服，参考 https://www.gitlab.com/ 。</li></ul><p>从代码的私有性上来看，GitLab是一个更好的选择。但是对于开源项目而言，GitHub依然是代码托管的首选。</p><h3 id="鉴权">鉴权</h3><p>在使用远程仓库之前，我们要先解决鉴权问题：云服务器需要知道你是否有权力访问这个仓库。</p><p>鉴权有两种方法：</p><ol type="1"><li>用户名 + 密码</li><li>SSH 密钥对</li></ol><p>通常来说，使用密钥对比用户名密码更安全，这里不介绍相关原理，先简单讲下密钥对的配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ssh-keygen -t rsa -C <span class="hljs-string">&quot;631670924@qq.com&quot;</span> <span class="hljs-comment"># 获取 SSH Key</span><br><span class="hljs-comment"># 参数 -t 代表 type 设置，默认使用 SSH2d 的 rsa</span><br><span class="hljs-comment"># 参数 -C 代表 Comment 设置，提供一个新注释</span><br><span class="hljs-comment"># 其他参数用于生成更多功能的密钥，请自行查阅相关网站</span><br>$ cat ~/.ssh/id_rsa.pub <span class="hljs-comment"># 代表 concatenate，用于连接文件并打印到标准输出</span><br></code></pre></td></tr></table></figure><p>密钥对生成时会需要设置密码，但由于其本身就是加密的，密码只是为了适应更特殊的情景，这里可以直接回车<strong>跳过</strong>。</p><p><code>cat</code> 命令会把<strong>公钥</strong>放到标准输出，复制到GitHub 或 GitLab 上的对应位置即可。注意 <code>id_rsa.pub</code>是可以公开的，而<strong>私钥</strong>存储在 <code>id_rsa</code>文件，用于在每次操作远程仓库时与公钥进行匹配。</p><p>如果你有多台电脑或服务器，可以<strong>分别</strong>在各个主机上生成<strong>各自的</strong>SSH 密钥对，并将公钥复制到 GitHub 或GitLab，并区分命名。这样就可以实现在不同主机上的鉴权，方便多地办公。</p><h3 id="克隆远程仓库">克隆远程仓库</h3><p>远程仓库通常是由 clone 或 push 开始的，这里假设我们已经在 GitHub上有了一个仓库，现在要将其克隆到本地。</p><p>常见的克隆方法有两种：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git <span class="hljs-built_in">clone</span> &lt;仓库的 HTTPS URL 地址&gt;<br>$ git <span class="hljs-built_in">clone</span> &lt;仓库的 SSH Key&gt;<br></code></pre></td></tr></table></figure><p>前者不需要配置 SSH也能完成，但是每次操作远程仓库都需要用户名和密码。后者是在配置完 SSH Key后使用的，可以省去填写用户名和密码的步骤，也能帮助你克隆<strong>私有仓库</strong>。如果前面生成密钥对时设置了密码，这里就需要输入。</p><p>此外，如果要克隆一个较大的仓库（仓库有很长的提交历史或大量二进制文件），常用的一个方法是限制clone 的深度，只克隆最新的版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git <span class="hljs-built_in">clone</span> --depth=1 https://github.com/hewei2001/HITSZ-OpenCS<br></code></pre></td></tr></table></figure><h3 id="远程分支">远程分支</h3><p>现在对本地仓库查看 log，会发现除了 HEAD、main 这些原有的指针，还多了<code>origin/</code>的字样，这是<strong>远程仓库分支</strong>的默认标识。</p><p>这些远程分支与本地分支<strong>并存</strong>，反映了远程仓库在你上次和它「<strong>通信</strong>」时的状态，可以用前面的命令查看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git branch --all  <span class="hljs-comment"># 参数 --all 代表所有分支，包括远程分支</span><br></code></pre></td></tr></table></figure><p>同样，也可以用 <code>git checkout</code>切换到任意一个远程分支，只需要加上 <code>origin/</code>。</p><blockquote><p>远程分支有一个特别的属性，在你检出时自动进入一个「<strong>分离 HEAD状态</strong>」。在此状态下，不会有<code>HEAD -&gt; origin/main -&gt; node</code>，而是直接有<code>HEAD -&gt; node</code>。这样做是因为 Git不能直接在远程分支上进行操作，必须在本地操作后将 main分支同步到远程，origin/main 才会发生变动。</p><p>一旦发生了分离 HEAD 并提交修改，下次 push时本地仓库将与远程仓库发生分歧，可能需要 Rebase 操作。</p></blockquote><p>在实际操作中，我们<strong>只需要检出到本地分支</strong>即可。当克隆一个仓库时，它通常会自动地创建一个<strong>跟踪</strong>origin/main 的 main 分支。</p><p>但如果一个远程分支 origin/test 在本地没有对应分支 test，我们却要检出test，该命令会自动复制一个「<strong>跟踪分支</strong>」作为本地分支。</p><h2 id="本地与远程的交互">本地与远程的交互</h2><h3 id="下载-fetch">下载 (Fetch)</h3><p>所谓的拉取通常可由两个操作来完成，初学者往往分不清两个操作究竟分别做了什么：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git fetch  <span class="hljs-comment"># 下载--下载远程分⽀到本地</span><br>$ git pull   <span class="hljs-comment"># 拉取--使本地分支与远程分支同步</span><br></code></pre></td></tr></table></figure><p>下面我们用一组图来解释 <code>git fetch</code> 操作：</p><figure><img src="/img/blog/Git-Note-3-images/git-fetch-1.png"alt="使用fetch前的仓库" /><figcaption aria-hidden="true">使用fetch前的仓库</figcaption></figure><p>在这个例子中，虚线的结点代表远程仓库，它有两个我们本地仓库中没有的提交。而此时本地的远程分支origin/main还停留在你上次和它「<strong>通信</strong>」时的状态。如果此时使用<code>git fetch</code>，则会出现：</p><figure><img src="/img/blog/Git-Note-3-images/git-fetch-2.png"alt="使用fetch后的仓库" /><figcaption aria-hidden="true">使用fetch后的仓库</figcaption></figure><p>可以发现，本地仓库<strong>缺失的结点</strong> C2 和 C3被<strong>下载</strong>到了本地仓库，同时<strong>远程分支</strong>origin/main 也被更新，反映到了这一下载，但是<strong>本地分支</strong>main 依旧不变！</p><p>因此，我们可以知道：<code>git fetch</code>并不会改变本地仓库的状态。它不会更新你的 main分支，也不会修改你磁盘上原有的文件使其与远程「<strong>同步</strong>」，它只是将「同步」这一操作所需要的数据都下载下来了。</p><h3 id="拉取-pull">拉取 (Pull)</h3><p>那么，如何将这些数据真正完成「同步」呢？实际上有很多方法，上一节提到的<code>git merge origin/main</code> 和<code>git rebase origin/main</code> 等命令都可实现。</p><p>实际上，由于<strong>先抓取更新再合并到本地分支</strong>这个流程很常用，因此Git 提供了一个专门的命令来完成这两个操作。它就是我们要讲的<code>git pull</code>。</p><p>这里我们先用一个<strong>分叉</strong>的例子来演示：</p><figure><img src="/img/blog/Git-Note-3-images/git-pull-1.png"alt="使用pull前的仓库" /><figcaption aria-hidden="true">使用pull前的仓库</figcaption></figure><p><strong>分叉</strong> (Branch Diverged)是由于不同开发者进行了<strong>时空错位</strong>的提交导致的。观察该图，我们可以猜测本地开发者是在C1 时刻克隆的仓库，并再克隆后完成了一次 C2提交，而与此同时，另一个开发者在远程完成了 C3 提交。</p><p>此时如果想提交代码到远程，会提示 main分支发生了<strong>分歧</strong>——因为远程仓库包含了本地尚不存在的结点，无法通过<strong>快速前移</strong>直接合并。Git会提示你先将远程仓库拉取到本地解决分叉。</p><figure><img src="/img/blog/Git-Note-3-images/git-pull-2.png"alt="使用pull后的仓库" /><figcaption aria-hidden="true">使用pull后的仓库</figcaption></figure><p>可以看到，<code>git pull</code> 其实就是 <code>git fetch</code> 和<code>git merge origin/main</code> 两个指令的缩写。</p><p>需要注意的是，如果此时 C2 和 C3 有<strong>冲突</strong>，则<code>git pull</code> 不完全执行，需要<strong>手动维护</strong>后再merge，这个维护的过程对另一开发者是不可见的。</p><blockquote><p>在实际操作中，我们偶尔会用 <code>git fetch</code> 和<code>git rebase origin/main</code> 来避免一个不必要的合并。</p></blockquote><h3 id="推送-push">推送 (Push)</h3><p>相比于拉取，推送操作就尤为简单：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git push<br></code></pre></td></tr></table></figure><p>如果无分歧发生，远程仓库将会接收本地新增的结点，而远程仓库中的 main也会指向本地仓库中 main 的位置。此外，本地仓库的远程分支origin/main，会在这次「<strong>通信</strong>」的过程中，也移动到本地main 的位置。</p><p>再回顾一下上面的<strong>分叉</strong>的例子，如果我们已经 merge解决了冲突，这时再用 <code>git push</code> 则会有：</p><figure><img src="/img/blog/Git-Note-3-images/git-push-1.png"alt="分叉的第一种解决方案" /><figcaption aria-hidden="true">分叉的第一种解决方案</figcaption></figure><p>可以看到，远程仓库清楚地记录了这次 merge 的历程！</p><p>而如果我们用 <code>git fetch</code> 和<code>git rebase origin/main</code>的组合（虽然很繁琐），但是远程仓库历史将变得十分整洁，并且能清楚地体现<strong>提交顺序</strong>。</p><figure><img src="/img/blog/Git-Note-3-images/git-push-2.png"alt="分叉的第二种解决方案" /><figcaption aria-hidden="true">分叉的第二种解决方案</figcaption></figure><h3 id="push-的参数">Push 的参数</h3><p>上文介绍了最简单的 Push命令，但这个命令看似简单，实际上却是最「模糊」的，很容易出现报错。完整的Push 指令应该是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git push &lt;远程仓库名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt;  <span class="hljs-comment"># 冒号前后无空格</span><br><span class="hljs-comment"># 例如：</span><br>$ git push origin main:<span class="hljs-built_in">test</span>  <span class="hljs-comment"># 将本地 main 推送到 origin 的 test</span><br></code></pre></td></tr></table></figure><p>这个命令显式地指出了<strong>远程仓库名</strong>，如果只关联了一个远程，这个参数就可以<strong>缺省</strong>。但是如果关联了多个仓库（如GitHub 和 Gitee），则必须用这个参数指明。使用 <code>git remote -v</code>命令可以查看关联的远程仓库。</p><p>此外，它还指出了本地分支及其映射的<strong>上游分支</strong>，该命令可以用于不同名分支的推送，但实际工程中我们倾向于用本地远程<strong>同名分支</strong>（避免分歧），因此命令可以简化为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git push origin <span class="hljs-built_in">test</span>  <span class="hljs-comment"># 将本地 test 推送到 origin 的 test</span><br></code></pre></td></tr></table></figure><p>但是，如果远程仓库 origin 没有一个同名的 test分支，又会报错，此时我们需要为 test建立上游分支，并将其<strong>跟踪绑定</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git push --set-upstream origin <span class="hljs-built_in">test</span> <span class="hljs-comment"># 推送同时绑定上游分支</span><br>$ git push -u origin <span class="hljs-built_in">test</span>  <span class="hljs-comment"># 参数 -u 为上述命令的缩写</span><br></code></pre></td></tr></table></figure><p>注意，一次 Push默认<strong>只推送一个分支</strong>，因此如果<strong>不加分支名</strong>，会默认推送当前HEAD 所在的分支。因此指令还可以逐步缺省：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git push -u origin  <span class="hljs-comment"># 表示将 当前分支 推送并绑定 到 &lt;origin&gt;/同名上游分支</span><br>$ git push origin  <span class="hljs-comment"># 表示将 当前分支 推送 到 &lt;origin&gt;/同名上游分支（需先绑定)</span><br>$ git push  <span class="hljs-comment"># 表示将 当前分支 推送 到 &lt;s远程&gt;/同名分支（需先绑定且有权限)</span><br></code></pre></td></tr></table></figure><p>最后一条指令就是我们最早提及的推送命令，它需要满足：</p><ol type="1"><li>只有一个远程仓库；或者有多个仓库，但用 <code>-u</code>绑定其中一个。</li><li>只有一条分支；或者有多条分支，但当前分支与上游分支同名并绑定。</li></ol>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Git</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git学习笔记 #2 本地仓库使用</title>
    <link href="/Git-Note-2.html"/>
    <url>/Git-Note-2.html</url>
    
    <content type="html"><![CDATA[<p>接上文，本文在命令行的基础上介绍了常用指令的在<strong>本地的使用情景</strong>。</p><p>本文大部分内容参考了 RCY 同学的教程，部分参考了 <ahref="https://www.liaoxuefeng.com/wiki/896043488029600">廖雪峰教程-Git</a>，<ahref="https://www.runoob.com/git/git-tutorial.html">菜鸟教程-Git</a>，以及Git 官网文档 <ahref="https://git-scm.com/doc">Git-Documentation</a>。</p><h2 id="本地的-git">本地的 Git</h2><h3 id="预先准备">预先准备</h3><p>在正式的操作前，你需要先配置你的用户信息，这件事情通常在你的机器上只需要干一次即可，因为我们使用了全局配置。在任意⼀个目录打开你的终端，配置你的用户名和邮箱：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git config --global user.name &lt;username&gt; <span class="hljs-comment"># 替换为 GitHub 用户名</span><br>$ git config --global user.email &lt;email&gt;   <span class="hljs-comment"># 替换为绑定的邮箱</span><br><span class="hljs-comment"># username 中如果有空格，需要加双引号</span><br></code></pre></td></tr></table></figure><h3 id="基本操作">基本操作</h3><p>在想用 Git 管理的项目路径下右键进入 Bash终端，然后将当前目录<strong>初始化</strong>为 Git 仓库。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git init<br></code></pre></td></tr></table></figure><p>手动新建一个文件，随便写点什么，譬如一个hello.md，此时文件存在于工作区。下面将其跟踪并提交：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git add hello.md      <span class="hljs-comment"># 如果有更多文件，也可直接追加在后面</span><br>$ git commit -m <span class="hljs-string">&quot;Init&quot;</span>  <span class="hljs-comment"># -m: 表示 message，备注本次提交信息</span><br></code></pre></td></tr></table></figure><p>注意到，add后面是<strong>文件名</strong>，也可以是<strong>目录</strong>、<strong>通配符</strong>，因此也支持以下写法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git add .          <span class="hljs-comment"># 相对路径中 . 表示当前目录，因此这一选项会添加当前目录下所有文件</span><br>$ git add *.cpp   <span class="hljs-comment"># * 代表任意字符串，所有 .cpp后缀的文件被添加</span><br>$ git add <span class="hljs-built_in">test</span>?.cpp  <span class="hljs-comment"># ? 代表单个字符，不得为空，因此如 test1.cpp 将被添加，而 test.cpp 等不会</span><br>$ git add dir/   <span class="hljs-comment"># 添加路径下的 d 文件夹内全部文件， / 有没有皆可</span><br></code></pre></td></tr></table></figure><p>此外，为了体现三个区，我们可以使用一些指令来对比：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git status         <span class="hljs-comment"># 查看工作区、暂存区的文件，一般前者是红色，后者是绿色</span><br>$ git diff           <span class="hljs-comment"># 查看工作区和暂存区的差异，适用于暂存后又修改的内容</span><br>$ git diff --cached  <span class="hljs-comment"># 查看暂存区与 Git 仓库的差异，适用于提交前查看</span><br>$ git diff HEAD      <span class="hljs-comment"># 同时查看其他两个区和 Git 仓库的差异</span><br></code></pre></td></tr></table></figure><figure><img src="/img/blog/Git-Note-2-images/git-diff.jpg"alt="Diff用法图解" /><figcaption aria-hidden="true">Diff用法图解</figcaption></figure><blockquote><p>在工程中，通常我们不用命令行查看差异，而是用 VSCode、GitHubDesktop、GitLab 等可视化工具。</p></blockquote><p>如果对于已暂存的文件后悔了，也可以取消暂存：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git reset &lt;filename&gt;  <span class="hljs-comment"># 对于该命令，如果不带参数则会清空暂存区</span><br>$ git restore --stage &lt;filename&gt;<br></code></pre></td></tr></table></figure><p>在完成了几次提交后，可以查看提交历史：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git <span class="hljs-built_in">log</span><br></code></pre></td></tr></table></figure><p>提交历史中，每个提交记录都有对应的 commit hash值，唯一标识了这次提交，这是 Git 用 <code>SHA-1 hash</code>生成的加密字符串。</p><p>注：如果 log比较长或者窗口比较小，这会触发「<strong>导航</strong>」模式，很多人第⼀次见到可能不知所措，不会退出该页面，此时可以：</p><ul><li>上下键移动或 Page Up / Down 翻页；</li><li>输入 <code>\</code> 接字符来全局查找 ；</li><li>输入 <code>q</code> 退出，与其他系统中的导航模式类似；</li><li>其他操作可以通过查询关键字「Linux less 导航」来查到。</li></ul><h2 id="分支-branch">分支 (Branch)</h2><p>有了前面的知识，我们已经在脑海中想象⼀副快照变更图了，本节中我们将快照称作「<strong>结点</strong>」，若干结点组织成了<strong>版本树</strong>——Git本身正是使用了红黑树对结点进行高效管理。</p><blockquote><p>以下内容强烈推荐结合 <ahref="https://learngitbranching.js.org/?locale=zh_CN">Git分支在线教程</a> 来学习！</p></blockquote><p>目前我们的结点树基本是串行的（除了回退、重新提交会导致<strong>分叉</strong>），那么所谓的并行开发如何体现呢？</p><p>注意到了反复有⼀个单词 main 出现在命令行，这即是 Git默认的分支：<strong>主分支</strong>（旧版本叫 master）。main即是这个分支的名字，也是一个<strong>指针</strong>，指向了该分支的最新结点。</p><p>和它⼀起的还有⼀个单词HEAD，这表示<strong>头指针</strong>，指向<strong>当前所处</strong>的结点。在你做分支相关的操作前，会有<code>HEAD -&gt; main -&gt; 最新结点</code>，直到你将它们分开。</p><h3 id="创建分支">创建分支</h3><p>与初始化仓库类似，创建分支也很方便：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git branch <span class="hljs-built_in">test</span>  <span class="hljs-comment"># 创建⼀个名为 test 的分⽀</span><br></code></pre></td></tr></table></figure><p>注意：创建完后分支的即会指向当前所在的结点，因此当前最新结点同时被main、test、HEAD 指向，即<code>HEAD -&gt; main(test) -&gt; 最新结点</code>。</p><p>对于已有的分支，也可以通过以下命令查看、切换：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git branch <span class="hljs-comment"># 查看本地分支，带 * 的为 HEAD 指向的当前分支</span><br>$ git checkout <span class="hljs-built_in">test</span>  <span class="hljs-comment"># 检出，表示切换到 test 分支</span><br><br><span class="hljs-comment"># 上述的创建、切换操作也可以被缩减为一步操作</span><br>$ git checkout -b <span class="hljs-built_in">test</span> <span class="hljs-comment"># 创建⼀个名为 test 的分⽀并且切换过去</span><br></code></pre></td></tr></table></figure><p>切换分支后，可以看到出现在命令行右侧的 main 已经变成了 test。查看 log也可发现 <code>HEAD -&gt; test(main) -&gt; 最新结点</code>。</p><h3 id="合并分支">合并分支</h3><p>如果此时我们已经在不同的分支提交了不同修改 C2 和C3，那么如何将分支合并到一起，使得并行开发结果汇总呢？</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git checkout main  <span class="hljs-comment"># 切换到 main 分⽀</span><br>$ git merge <span class="hljs-built_in">test</span>     <span class="hljs-comment"># 让 main 分⽀合并 test 分⽀的结点</span><br></code></pre></td></tr></table></figure><p>对于分支的合并，Git有专门的<strong>图形化输出</strong>命令来进行查看版本树（也可以用其他可视化工具）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git <span class="hljs-built_in">log</span> --graph --oneline --all<br><span class="hljs-comment"># oneline 表示用行来显示记录，最上方的是最新的提交</span><br><span class="hljs-comment"># all 表示显示所有分支，如果没有则只显示当前分支及其祖先</span><br><br><span class="hljs-comment"># 当然，还有其他许多参数可以美化版本树，这里贴一个大神的版本</span><br>$ git <span class="hljs-built_in">log</span> --graph --all --pretty=format:<span class="hljs-string">&#x27;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#x27;</span> --abbrev-commit --date=relative<br></code></pre></td></tr></table></figure><p>为展示方便，这里使用在线教程里的图例：</p><figure><img src="/img/blog/Git-Note-2-images/git-merge-1.png"alt="main分支合并test分支" /><figcaption aria-hidden="true">main分支合并test分支</figcaption></figure><p>可以看到，合并后产生了一个新结点C4，该结点具有<strong>双父结点</strong>，指向原来的 C2 和C3。需要注意的是，该结点属于 main 分支，是 main 分支的最新结点（被指针main 所指），而 test 仍指向旧的 C2。</p><p>如果要把 test 分支也<strong>同步</strong>到新结点，只需要让 test分支合并 C4，也就是合并<strong>当前</strong>的 main 分支：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git checkout <span class="hljs-built_in">test</span>  <span class="hljs-comment"># 切换到 test 分⽀</span><br>$ git merge main     <span class="hljs-comment"># 让 test 分⽀合并 main 分⽀的结点</span><br></code></pre></td></tr></table></figure><p>而由于 main 分支的最新结点 C4 继承自 C2，此时的 Git不会有任何操作，只是简单地将指针 test 移动到指针 main所在位置，即<strong>快速前移</strong>（fast-forward）：</p><figure><img src="/img/blog/Git-Note-2-images/git-merge-2.png"alt="test分支合并main分支" /><figcaption aria-hidden="true">test分支合并main分支</figcaption></figure><blockquote><p>多数情况下，我们会先用<strong>主分支合并从分支</strong>，如果之后需要再从分支继续开发，才会把<strong>从分支快速前移</strong>！</p></blockquote><h3 id="冲突的合并">冲突的合并</h3><p>如果 C2 和 C3 修改的代码不在同一文件的同一处，上述的 merge是没有问题的，但是一旦发生冲突，<code>git merge test</code>命令时就会提示错误。</p><p>此时如果我们用 <code>git status</code>查看，会发现<strong>工作区</strong>里有一个新的状态「<strong>未合并的路径</strong>」，里面就是冲突的文件。打开该文件，会发现Git 已经在里面标记出了双方修改的内容（用 VSCode 等 IDE将看得更清楚）。</p><p>而我们只需要<strong>手动维护</strong>冲突，将该文件手动加入暂存区，最后再提交，就会生成一个新的结点，该结点无异于直接使用<code>git merge test</code> 命令得到的。</p><h3 id="变基-rebase">变基 (Rebase)</h3><p>一个来回穿插的版本树是有点凌乱的，此时不得不提到第二种合并分支的办法：变基(rebase) 操作。Rebase实际上就是取出<strong>从分支</strong>的提交记录，「<strong>复制</strong>」它们，然后在<strong>主分支</strong>逐个的放下去。</p><p>Rebase的优势就是可以创造更<strong>线性</strong>的提交历史。如果两个分支没有冲突，如上文提到的第一种情况，直接Merge会出现一个新的结点（实际上该结点并没有做出实质的修改，反而使版本树变得冗长）。</p><p>此时如果我们用 Rebase 操作，则可以简化版本树：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git checkout <span class="hljs-built_in">test</span>  <span class="hljs-comment"># 切换到 test 分⽀</span><br>$ git rebase main    <span class="hljs-comment"># 让 test 分⽀以 main 为基，因此 test 成为 main 的后代</span><br></code></pre></td></tr></table></figure><blockquote><p>注意，Rebase 操作通常是让<strong>从分支变基到主分支</strong>，这与Merge 操作是相反的！</p></blockquote><figure><img src="/img/blog/Git-Note-2-images/git-rebase.png"alt="test分支变基到main" /><figcaption aria-hidden="true">test分支变基到main</figcaption></figure><p>观察该图，我们可以发现 test 分支上的工作在 main的最顶端，同时我们也得到了一个更线性的提交序列。</p><p>而提交记录 C2 依然存在（树上那个半透明的节点），而 C2’ 是我们 Rebase到 main 分支上的 C2 的副本，它们具有不同的 hash 值，可以通过 log查看。</p><p>之后我们也可以通过类似的操作，把 main<strong>快速前移</strong>到最新的结点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git checkout main  <span class="hljs-comment"># 切换到 main 分⽀</span><br>$ git rebase <span class="hljs-built_in">test</span>    <span class="hljs-comment"># 让 main 分⽀以 test 为基，快速前移</span><br><span class="hljs-comment"># 当然，z也可以用前面的 merge 操作</span><br></code></pre></td></tr></table></figure><blockquote><p>Rebase操作在没有冲突时将非常舒适，可以避免没有意义的合并结点（尤其是涉及到远程仓库时），但是一旦发生了<strong>冲突</strong>，操作将十分繁琐！这里不再赘述，具体工程中如果遇到了请根据Git 的自动提示逐步操作。</p></blockquote><p>而对于 Rebase后的从分支上的结点，就变成了所谓的「<strong>悬垂结点</strong>」，这些结点的访问将十分复杂。此外，如果这个从分支被<strong>废弃</strong>，我们也可以用以下指令将其删去：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git branch -d <span class="hljs-built_in">test</span>  <span class="hljs-comment"># 删除名为 test 的分⽀，用于合并后废弃的分支</span><br></code></pre></td></tr></table></figure><h2 id="分叉-branch-diverged">分叉 (Branch Diverged)</h2><p>分叉是分支的一种特殊情况，往往是因为某些「不友好」操作而产生，最终被废弃掉。对于<strong>单人操作</strong>的仓库，其产生原因可能是：</p><ul><li>Reset后旧分支：版本回退后重新提交，这种情况下往往是要弃用旧分支。但如果旧的分支仍有需要保留的更改，则需要Cherry-Pick 等操作。</li><li>Rebase 后从分支：上文介绍到，Rebase将创造更线性的主分支，但这样做的代价是从分支将被废弃，成为一个无用的分叉。</li></ul><p>上述情形的发生往往可以人为进行控制，而对于<strong>多人操作</strong>的仓库，如果不同开发者同时对一个结点进行了更改，将会造成「不可控」的分叉，具体情形及解决方法将在下一节介绍。</p>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Git</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git学习笔记 #1 基础知识介绍</title>
    <link href="/Git-Note-1.html"/>
    <url>/Git-Note-1.html</url>
    
    <content type="html"><![CDATA[<p>无意中发现了清华贵系科协的暑培项目，内容都十分贴合计算机系学生的学习需求，正好下学期有「软件工程」课，就决定先把Git 入门了。</p><p>本文大部分内容参考了 RCY 同学的教程，部分参考了 <ahref="https://www.liaoxuefeng.com/wiki/896043488029600">廖雪峰教程-Git</a>，<ahref="https://www.runoob.com/git/git-tutorial.html">菜鸟教程-Git</a>，以及Git 官网文档 <ahref="https://git-scm.com/doc">Git-Documentation</a>。</p><h2 id="版本控制软件">版本控制软件</h2><p>版本控制是合作开发、工程项目中必要的手段，考虑以下情况：</p><ul><li>需求模糊：不得不准备两种方案。</li><li>伟大尝试：如果尝试失败，可以回到旧的版本。</li><li>组员弃坑：删除掉难以开发的代码。</li><li>并行开发：需要合并代码时，人工比对容易遗漏。</li></ul><p>为了让这样繁琐的工作自动化，就有了以下的工具：</p><ul><li>Git：行业内最为流行的版本控制软件，采用<strong>分布式</strong>。</li><li>SVN(Subversion)：较早期的版本控制软件，采用<strong>集中式</strong>。</li></ul><h2 id="关于-git">关于 Git</h2><p>事实上，Git 仅在过去十年里，就以极快地速度占据了市场。与 SVN的集中式不同的是，Git采用的分布式，除了少数的交互步骤外，版本控制操作、历史记录都在本地，这保证了它极高的操作效率。</p><blockquote><p>所谓集中式，即所有人都把代码放到一个远程的服务器，要写代码时将远程资源拉取到本地，用一个同一的仓库来实现协同。</p><p>所谓分布式，即每个人都有自己的仓库，每个仓库都可能存放着最新的代码，需要修改时直接在本地修改（即使断网），需要同步时再与远程仓库交互。</p></blockquote><h3 id="模式">模式</h3><p>多数版本控制软件是基于<strong>差异</strong>的控制，即不同版本间记录的是文本的「<strong>变化</strong>」，版本切换时再根据delta 前进和后退。</p><figure><img src="/img/blog/Git-Note-1-images/deltas.png" alt="Delta-based" /><figcaption aria-hidden="true">Delta-based</figcaption></figure><p>而 Git采用的是「<strong>快照</strong>」的方法，对每个版本记录全部文件快照并建立索引（特别地，没有变动的文件只是创建一个<strong>链接</strong>指向之前的快照）。</p><figure><img src="/img/blog/Git-Note-1-images/snapshots.png"alt="Snapshot-based" /><figcaption aria-hidden="true">Snapshot-based</figcaption></figure><p>这样做的好处是，Git<strong>切换版本和分支</strong>的速度远快于第一种模式。因为 Delta-based模式下，切换版本相当于一个线性遍历的过程，而 Git相当于是在文件系统层面做了一个哈希索引。</p><p>此外，快照并不是以整个文件为单位的，否则修改一行就需要增量整个文件的大小，在Git 中是以<strong>行</strong>为单位进行 diff 的。因此对于二进制文件，Git的功能就相对较差。</p><h3 id="基本状态">基本状态</h3><p>Git 中用三个状态来区分文件，生成版本：</p><ul><li>已修改 (modified)：你的文件<strong>修改</strong>了，但是没和 Git发生关系，如同没有 Git 存在⼀般。</li><li>已暂存 (staged)：已修改文件被<strong>标记</strong>后的状态，下次提交(commit) 时将为它生成快照，多数情况下处于暂存态后紧接着便被提交。</li><li>已提交(committed)：已经安全地保存到<strong>本地数据库</strong>的数据，通常来说不手动清理则不会丢失。</li></ul><p>这三个状态对应到了 Git 的三个区域：工作区、暂存区、Git 仓库：</p><figure><img src="/img/blog/Git-Note-1-images/areas.png" alt="三个区域" /><figcaption aria-hidden="true">三个区域</figcaption></figure><p>如果修改了一个已暂存的文件，它会同时出现在工作区和暂存区两个区域，此时可以选择<strong>再次暂存</strong>，覆盖掉暂存区的文件；也可以选择<strong>恢复工作</strong>，撤销后来的修改，覆盖掉工作区的文件（但暂存区的仍然保留）。</p><p>因此，我们可以对这三个区域加以定义：</p><ul><li>工作区是可以直接看到的文件，包括已修改和未修改；</li><li>暂存区本身是个文件，记录了下次要提交（快照）的文件列表信息；</li><li>Git 仓库包含了数据库、元数据等，是 Git最重要的部分，它的全部数据存储在项目根目录的 .git路径下，其大小甚至会超过其他文件之和。</li></ul><h2 id="使用-git">使用 Git</h2><p>在官网的文档中有详细的介绍，这里仅介绍部分常用内容。</p><h3 id="命令行界面">命令行界面</h3><p>Git 同时提供了 GUI 和 Bash的操作方式，但图形界面功能不够齐全，同时容易误触造成无法逆转的文件改动。</p><p>使用命令行界面 (Command Line Interface) 意味着：</p><ul><li>几乎放弃鼠标功效，最大化键盘作用；</li><li>放弃炫酷的界面显示，用文本配合着色传递信息；</li><li>你所学的操作到了服务器上仍然能流畅使用。</li></ul><p>对 Windows 而言，cmd 、Power Shell、WSL、Git都可以提供命令行界面，为了执行下列 Git 指令，需要使用 Git提供的终端，在文件夹中右键选中 <code>Git Bash Here</code> 即可。</p><p>关于命令行界面，有这些常用快捷键：</p><ul><li><p><code>Tab</code>：实现对命令、参数或文件的内容补全。</p></li><li><p><code>Ctrl+C</code>：终止当前进程的运行，重获终端的控制权。</p></li><li><p><code>Ctrl+D</code>：结束键盘输入，即退出终端，相当于输入<code>exit()</code>。</p></li><li><p><code>Ctrl+I</code>：清空当前终端中已有的内容，相当于输入<code>clear</code>。</p></li><li><p><code>&lt;上下键&gt;</code>：翻看使用过的命令记录。</p></li><li><p><code>Shift+PgUp/PgDn</code>：翻页终端。</p></li><li><p><code>Ctrl/Shift+Ins</code> ：复制粘贴文本，有的系统下可用<code>Ctrl+Shift+C/V</code>。</p></li><li><p><code>‘’</code> 和<code>“”</code>：命令是以空格来作为间隔符的，参数中若存在空格，必须用<strong>引号</strong>括起来。</p></li></ul><p>一些在 Git 使用中会用到的 Shell 命令有：</p><ul><li><code>ls</code>：List，罗列当前文件夹下的<strong>文件和下级文件夹</strong>，并且会用颜色区分二者，如果用<code>dir</code> 目录命令则无区分。</li><li><code>cd</code>：ChangeDirectory，<strong>改变目录</strong>，这里再介绍一下路径的表示：<ul><li><code>.</code> 代表当前目录，也可代表当前目录下所有文件。</li><li><code>..</code> 代表上级目录，<code>cd ..</code> 即回退上级。</li><li><code>/</code> 代表盘符根目录，<code>cd /</code>即切换到根目录。</li><li>输入当前目录下的文件夹名，即可进入该文件夹，可以用 <code>Tab</code>补全。</li></ul></li><li><code>mkdir</code>：MakeDirectory，在当前目录下<strong>新建文件夹</strong>。</li><li><code>touch</code>：<strong>新建空文件</strong>或改变已有文件的时间戳属性。</li><li><code>echo</code>：EchoProtocol（应答协议），强制显示文字，直接使用会将字符串显示在终端。<ul><li>利用该命令也可新建文件，如：<code>echo ‘test’ &gt; 1.txt</code>，强制输出内容到一个记事本文件，如果没有该文件则强制创建一个。</li><li>此外，新建文件还可以用其他<strong>重定向</strong>命令辅助完成，如<code>cd . &gt; 1.txt</code>，<code>type NUL &gt; 1.txt</code>，<code>copy NUL &gt; 1.txt</code>。</li></ul></li></ul><h3 id="git-基本指令">Git 基本指令</h3><p>这里先列举一些基本指令，它们的使用格式基本都是<code>git &lt;instruction&gt; &lt;other-params-if-needed&gt;</code>。以下列举的都是 <code>&lt;instruction&gt;</code>项，对应的参数在后面介绍。</p><p>本地基本操作：</p><ul><li><code>init</code>：告诉 Git 你要让它接管这个文件夹。</li><li><code>config</code>：做相关配置，告诉 Git 你是谁。</li><li><code>add</code>：文件从<strong>工作区</strong>转到<strong>暂存区</strong>，即「跟踪」。</li><li><code>commit</code>：文件从<strong>暂存区</strong>转到 Git<strong>仓库</strong>，即「提交」。</li></ul><p>远程基本操作：</p><ul><li><code>clone</code>：克隆远程仓库到本地。</li><li><code>push</code>：本地数据库同步到远程，即「推送」。</li><li><code>pull</code>：远程数据库同步到本地，即「拉取」。</li></ul><p>分支操作：</p><ul><li><code>branch</code>：分支基本操作。</li><li><code>checkout</code>：分支、版本的切换。</li></ul><p>状态切换：</p><ul><li><code>stash</code>：⼀个栈式额外存储区。</li><li><code>reset</code>：版本回退。</li></ul><p>信息获取：</p><ul><li><code>log</code>：查看记录。</li><li><code>reflog</code>：查看操作记录（常用于 reset后撤销操作时寻找版本号）</li><li><code>status</code>：查看<strong>已修改</strong>和<strong>已暂存</strong>的文件。</li><li><code>diff</code>：查看不同 <strong>区/版本</strong> 间的差异。</li></ul>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>Git</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数理逻辑 应试笔记</title>
    <link href="/Mathematical-Logic.html"/>
    <url>/Mathematical-Logic.html</url>
    
    <content type="html"><![CDATA[<p>该笔记是本人于哈尔滨工业大学（深圳）2021年春季学期「数理逻辑」课程的笔记，最终课程成绩是满分。授课教师为任世军，是本部经验丰富的老教师。</p><p>由于笔记是在考前复习时才逐渐整理的，笔记内容偏<strong>应试风格</strong>，标题顺序也是按照哈工大「数理逻辑」课程一贯的出题模板来的，涵盖了课程所学，主要包括：命题逻辑、三大系统中定理的证明、谓词逻辑。</p><h2 id="命题逻辑部分">命题逻辑部分</h2><h3 id="求主析取合取范式">求(主)析取/合取范式</h3><p>一般会涉及到三个原子命题，唯一方法——列真值表。</p><p>列真值表的顺序可以依照语法分析树。</p><p>主合取看 0，主析取看1，直接写。列真值表一定要<strong>反复检查细节</strong>。蕴含的前后件不要看反了。</p><h3 id="用完备联结词组表示公式">用完备联结词组表示公式</h3><p>通常最终结果要化为与非式、或非式。</p><p>先把原式子中的「蕴含、等价」全换成「与、或、非」。（<strong>可以用等价式化简，但没必要，除非很明显的吸收律</strong>）</p><ul><li><strong>与非</strong>：</li></ul><p>用德摩根律把 <span class="math inline">\(\lor\)</span> 全消掉，再把<span class="math inline">\(\land\)</span> 用</p><p><span class="math display">\[p\land q\Leftrightarrow \lnot \left( p\uparrow q \right)\]</span></p><p>替换，再把所有的 <span class="math inline">\(\lnot\)</span>用下式替换，注意此时 p 可能是个很长的式子。 <spanclass="math display">\[\lnot p\Leftrightarrow p\uparrow p\]</span></p><ul><li><strong>或非</strong>：</li></ul><p>同理，也是消掉另一个符号，再把或替换了，再替换非。</p><h3 id="判断逻辑蕴含逻辑等价的正确性">判断逻辑蕴含/逻辑等价的正确性</h3><ol type="1"><li><strong>逻辑蕴含</strong>，一般前提会有多个</li></ol><p>通常做法是，给前提均指派为真，利用公式法，推出一系列公式，然后推得右边的公式的值恒为1，则<strong>成立</strong>。</p><p>如果推不到右边，可以先假设右边为 0（反证法），推出左边的值可以均为1，则<strong>不成立</strong>。</p><p><strong>注意</strong>：假设右边为 1 没用，不充分。</p><ol start="2" type="1"><li><strong>逻辑等价</strong>，一般是一对一，两边都比较长</li></ol><p>理论上可以用<strong>真值表</strong>，但是语法分析树太长了。</p><p>如果可以一眼判断出<strong>不成立</strong>，那么枚举一个反例也行。</p><p>通常做法是，两边利用公式法化简，最后推出的赋值式相等，则对于任意的指派<span class="math inline">\(v\)</span>，都有 <spanclass="math inline">\(A^v = B^v\)</span>，故成立。</p><p>化简过程中，有一些较长的非、蕴含符号可以替换成与非，如： <spanclass="math display">\[\begin{aligned}\lnot p\rightarrow q\,\,&amp;\text{换成 } p\lor q\\\lnot \left( p\rightarrow \lnot q \right) \,\,&amp;\text{换成 } p\land q\end{aligned}\]</span> 避免列赋值式太长。</p><p>注意：化简到最后可能形式上不相等，这时候可以两个式子<strong>联立消项</strong>，看最后是不是<strong>永真式</strong>，如果不是，同样可以举反例（举反例不好直接想到，因此还是要推理，可以先用PC 的方法变形看看）。</p><h2 id="pc-中定理的证明">PC 中定理的证明</h2><p>依次使用以下方法。</p><h3 id="逆向推理法直接法">1 逆向推理法（直接法）</h3><p>对待证明定理进行等价转化，转变成熟悉的形式。常用工具：</p><ul><li><p>加前件定理3：去相同前件</p></li><li><p>加后件定理4：去相同后件</p></li><li><p>逆否转换：四种情况</p></li><li><p>前件互换定理2：前件移到里面后</p></li><li><p>公理2：提出前件</p></li><li><p>补前件定理：去单个前件，只需证出后件就可以</p></li><li><p>三段论定理8：需要多次加前件、后件时</p></li></ul><p>例题：</p><ol type="1"><li><strong>二难推理</strong>（后面发现用定理18更好做）</li></ol><p><span class="math display">\[\vdash \left( A\rightarrow C \right) \rightarrow \left( \left(B\rightarrow C \right) \rightarrow \left( \left( A\lor B \right)\rightarrow C \right) \right)\]</span></p><p>观察形式，发现三项都有共同后件 <spanclass="math inline">\(C\)</span>，则三项全取逆否 <spanclass="math display">\[\left( \lnot C\rightarrow \lnot A \right) \rightarrow \left( \left(\lnot C\rightarrow \lnot B \right) \rightarrow \left( \lnot C\rightarrow\lnot \left( \left( A\lor B \right) \right) \right) \right)\]</span> 利用公理 2 及 加前件定理可以去掉前件 <spanclass="math inline">\(\lnot C\)</span> <span class="math display">\[\lnot A\rightarrow \left( \lnot B\rightarrow \lnot \left( A\lor B\right) \right)\]</span> 再用逆否转换，前件互换等定理即可逆向</p><p><strong>类似题</strong>： <span class="math display">\[\vdash \left( A\rightarrow C \right) \rightarrow \left( \left(B\rightarrow C \right) \rightarrow \left( \left( \left( A\rightarrow B\right) \rightarrow B \right) \rightarrow C \right) \right)\]</span></p><p>也必须用逆否来做。</p><ol start="2" type="1"><li><strong>作业题5</strong>（常规做法是演绎定理，其他都很麻烦）</li></ol><p><span class="math display">\[\vdash \left( A\rightarrow \left( B\rightarrow C \right) \right)\rightarrow \left( \left( C\rightarrow D \right) \rightarrow \left(A\rightarrow \left( B\rightarrow D \right) \right) \right)\]</span></p><p>先前件互换可使得后件两项都含 <span class="math inline">\(A\)</span><span class="math display">\[\left( C\rightarrow D \right) \rightarrow \left( \left( A\rightarrow\left( B\rightarrow C \right) \right) \rightarrow \left( A\rightarrow\left( B\rightarrow D \right) \right) \right) \,\,\]</span> 发现去掉前件 <span class="math inline">\(A\)</span>后还可以去掉前件 <span class="math inline">\(B\)</span></p><p>但是再用一次前件互换即可得到定理4的形式 <span class="math display">\[\left( C\rightarrow D \right) \rightarrow \left( \left( B\rightarrow C\right) \rightarrow \left( B\rightarrow D \right) \right)\]</span></p><ol start="3" type="1"><li><strong>砍头操作</strong></li></ol><p>砍头操作有两种办法：将 <span class="math display">\[\left( \lnot A\rightarrow B \right) \rightarrow \left( \left( \lnotA\rightarrow \lnot B \right) \rightarrow A \right)\]</span> 变成 <span class="math display">\[B\rightarrow \left( \left( \lnot A\rightarrow \lnot B \right)\rightarrow A \right)\]</span> <strong>方法一</strong>：先添前件（用公理1再分离）变成 <spanclass="math display">\[B\rightarrow \left( \left( \lnot A\rightarrow B \right) \rightarrow\left( \left( \lnot A\rightarrow \lnot B \right) \rightarrow A \right)\right)\]</span> 再用公理2分离，前件是公理1的形式，后件自然成立，总计要4行</p><p><strong>方法二</strong>：直接用三段论定理8，因为 <spanclass="math display">\[B\rightarrow \left( \lnot A\rightarrow B \right)\]</span>显然成立，用三段论定理8接上，只需要2行，但是要分条件（有可能不允许使用定理8，如作业题PC1）</p><ol start="4" type="1"><li><strong>反公理2</strong>（19年试卷的答案，思路清奇，没用定理18）</li></ol><p><span class="math display">\[\vdash \left( \left( A\rightarrow B \right) \rightarrow \left(A\rightarrow C \right) \right) \rightarrow \left( A\rightarrow \left(B\rightarrow C \right) \right)\]</span></p><p>一开始观察三项前件都有 <spanclass="math inline">\(A\)</span>，以为可以用类似二难推理的办法做。</p><p>但是！注意到整体的形式是「两项推一项」，与二难推理不同，因此不能用公理2来提出前件，再去掉前件。</p><p>观察发现，后件 <span class="math inline">\(A\to(B\to C)\)</span>可以用一次前件互换定理，得到 <span class="math display">\[\vdash \left( \left( A\rightarrow B \right) \rightarrow \left(A\rightarrow C \right) \right) \rightarrow \left( B\rightarrow \left(A\rightarrow C \right) \right)\]</span> 此时有共同的后件，可以用加后件定理快速解决。</p><h3 id="利用定理18">2 利用定理18</h3><p>定理18证明的简便性和方法1相近，尝试方法1失败后，若形式符合<spanclass="math inline">\((A \to B)\to C\)</span>，可以考虑定理18</p><p>将原式子拆成 <span class="math inline">\(\lnot A \to C\)</span> 和<span class="math inline">\(B \to C\)</span> ，再用方法1</p><p>例题：</p><ol type="1"><li><strong>反公理2</strong>（书上用演绎定理证明，很简短）</li></ol><p><span class="math display">\[\vdash \left( \left( A\rightarrow B \right) \rightarrow \left(A\rightarrow C \right) \right) \rightarrow \left( A\rightarrow \left(B\rightarrow C \right) \right)\]</span></p><p>如果看前两项，想用逆推的方法解决，但是发现加前件定理用不了，因为接不上。直接考虑定理18。</p><ul><li>拆成</li></ul><p><span class="math display">\[\left( A\rightarrow C \right) \rightarrow \left( A\rightarrow \left(B\rightarrow C \right) \right)\]</span> 只需前件互换，去掉后件即可证明。</p><ul><li>拆成</li></ul><p><span class="math display">\[\lnot \left( A\rightarrow B \right) \rightarrow \left( A\rightarrow\left( B\rightarrow C \right) \right)\]</span> 比较复杂，需要前件互换，<strong>再摘掉 A</strong></p><p>（<strong>注意：这一步可以不需要前件互换，只需用公理1+三段论，直接砍后件头</strong>）</p><p>（<strong>附注</strong>：砍后件头，倒过来书写时其实是加后件头，与前面的砍头操作不同）<span class="math display">\[\lnot \left( A\rightarrow B \right) \rightarrow \left( B\rightarrow C\right)\]</span> 然后再换前件，因为前面的 <strong><spanclass="math inline">\(\lnot\)</span>是在整体上的，必须要和单体进行一次逆否转换</strong> <spanclass="math display">\[B\rightarrow \left( \lnot \left( A\rightarrow B \right) \rightarrow C\right)\]</span> 再对里面做逆否，此时 <spanclass="math inline">\(\lnot\)</span> 已经转移到单独的 <spanclass="math inline">\(C\)</span> 上了 <span class="math display">\[B\rightarrow \left( \lnot C\rightarrow \left( A\rightarrow B \right)\right)\]</span> 需要前件互换，<strong>再摘掉 <span class="math inline">\(\lnotC\)</span></strong></p><p>（<strong>注意：这一步可以不需要前件互换，只需用公理1+三段论，直接砍后件头</strong>）</p><ul><li>补充</li></ul><p><span class="math display">\[\lnot \left( A\rightarrow B \right) \rightarrow \left( B\rightarrow C\right)\]</span></p><p>前面的 <strong><span class="math inline">\(\lnot\)</span>是在整体上的，还有另一种办法可以消去</strong></p><p>因为 <span class="math inline">\(\lnot (A \to B)\)</span> 可以推出<span class="math inline">\(A\)</span> 或 <spanclass="math inline">\(\lnot B\)</span>，利用<strong>三段论</strong>定理可以接上去。</p><p>此题可以用 <span class="math inline">\(\lnotB\)</span>，转化后的形式是定理6。</p><ol start="2" type="1"><li><strong>二难推理</strong></li></ol><p><span class="math display">\[\vdash \left( A\rightarrow C \right) \rightarrow \left( \left(B\rightarrow C \right) \rightarrow \left( \left( A\lor B \right)\rightarrow C \right) \right)\]</span></p><p>二难推理系列的题目，比思维流更好的办法其实是定理18，并且套路十分明显。</p><ul><li>拆成</li></ul><p><span class="math display">\[\lnot A\rightarrow \left( \left( B\rightarrow C \right) \rightarrow\left( \left( A\lor B \right) \rightarrow C \right) \right)\]</span></p><p>首先第一部分，由于 <span class="math inline">\(C\)</span>在后件的两个后件，此时用加后件定理可以轻松消去 <spanclass="math inline">\(C\)</span>。 <span class="math display">\[\lnot A\rightarrow \left( \left( A\lor B \right) \rightarrow B \right)\]</span> 再用一次前件互换定理，发现就是定理1。</p><ul><li>（<strong>这一步纯套路，每题都是一样</strong>）拆成</li></ul><p><span class="math display">\[C\rightarrow \left( \left( B\rightarrow C \right) \rightarrow \left(\left( A\lor B \right) \rightarrow C \right) \right)\]</span></p><p>此时不能直接用加后件了，否则最前面的 <spanclass="math inline">\(C\)</span> 消不掉。直接前件互换， <spanclass="math display">\[\left( B\rightarrow C \right) \rightarrow \left( C\rightarrow \left(\left( A\lor B \right) \rightarrow C \right) \right)\]</span> 发现后件是公理1，于是摘掉前件，成立。</p><p><strong>类似题</strong>： <span class="math display">\[\vdash \left( A\rightarrow C \right) \rightarrow \left( \left(B\rightarrow C \right) \rightarrow \left( \left( \left( A\rightarrow B\right) \rightarrow B \right) \rightarrow C \right) \right)\]</span></p><p>也可以用同样的套路秒做。</p><h3 id="反证法">3 反证法</h3><p>前两种方法都想不出来的情况下考虑，书写过程会相对繁琐。</p><ol type="1"><li>基本方法</li></ol><p>假定原定理为0，按照依次将式子标上0和1，直到推出矛盾（一般会有多处，找容易的）。</p><p>证明时显而易见的（前几条）可以不用加注释，可以理解成真值表的体现。</p><p>后面的推理过程可以按标0和1的顺序来，通常都是由大后件的结论往前件找矛盾。</p><p><strong>最常使用：三段论定理</strong>，用于把 <spanclass="math inline">\(\lnot P\)</span> 接上去，最后才能转到矛盾。</p><ol start="2" type="1"><li>推演技巧</li></ol><p>最无脑的方法是把所有单项都用 <span class="math inline">\(\lnotP\)</span> 推出来，再进行排列组合出矛盾。</p><p>高级的方法是抓住「<strong>矛盾块</strong>」（通常是原式的一端），再把「<strong>另一个矛盾块</strong>」通过逆向推理法（直接法）证到原式的另一端，即可。</p><h3 id="演绎定理">4 演绎定理</h3><p>没啥好说的，一般不给用，对三个以上变量比较友好，提取出所有前件后就可以用直接法证明。</p><h2 id="nd-中定理的证明">ND 中定理的证明</h2><h3 id="思维流">思维流</h3><p>本质是<strong>逆向推理法</strong>，逆推的同时可以注意左端项的逻辑规律（赋1，看矛盾）</p><p>结合常见形式及解决套路可以更快解决。</p><ol type="1"><li><strong>当右端项包含 <span class="math inline">\(\lnot\)</span>时：</strong></li></ol><p><span class="math display">\[\lnot A\land \lnot B\vdash \lnot \left( A\lor B \right)\]</span></p><p>用 <span class="math inline">\(\lnot +\)</span>规则把右端项挪到左边时，要导出矛盾，往往这个矛盾是右端项的<strong>部分</strong>，如析取、合取的部分。</p><p>此时的惯常做法是将这两个矛盾合取，如 <spanclass="math inline">\(A\land \lnot A\)</span>，这个式子必定为0，事实上左边此时的两个式子已经自相矛盾。</p><p>当然不一定要用 <span class="math inline">\(A\land \lnotA\)</span>，因为左边矛盾的情况下，赋1可以推出很多对矛盾，只是这个最常见。</p><ol start="2" type="1"><li><strong>当左端项有 <span class="math inline">\(\lor\)</span>时：</strong></li></ol><p><span class="math display">\[\lnot A\lor \lnot B\vdash \lnot \left( A\land B \right)\]</span></p><p>充分利用析取，将其两项分别放到左端，用 <spanclass="math inline">\(\in\)</span> 规则把两项先推出来。</p><p>左端多出来的两项最终也需要消去。</p><ul><li>这就需要 <span class="math inline">\(\lor-\)</span>规则，因此上面推出来的两式要最后导出到同一个目标，通常是<spanclass="math inline">\(A\land \lnot A\)</span>，这类式子。</li><li>也有可能是需要 <span class="math inline">\(\to +\)</span>规则，把小项拿到右边当前件。</li></ul><p><strong>注意</strong>：如果 <span class="math inline">\(\lor\)</span>是含在某个大的式子里，也可以将小项直接拿到外面来推。</p><ol start="3" type="1"><li><strong>当右端项有 <span class="math inline">\(\lor\)</span>时：</strong></li></ol><p><span class="math display">\[\lnot \left( A\land B \right) \vdash \lnot A\lor \lnot B\]</span></p><p>一般可以用 <span class="math inline">\(\lor -\)</span>简化，只需证一半，但是保留哪一半要根据左边的情况推理，即思维流。</p><p>但有时也不能上来就简化，因为左边的条件可能太宽泛了（太少了），因此可能要加上一些小项再看。</p><p>这时候常用的是 <span class="math inline">\(-\)</span>规则。这也是最难用的一个规则。</p><ol start="4" type="1"><li><strong>当左端项有 <span class="math inline">\(\land\)</span>时：</strong></li></ol><p><span class="math display">\[A\land \left( {B}\lor {C} \right) \vdash \left( {A}\land {B} \right)\lor \left( {A}\land {C} \right)\]</span></p><p>利用 <span class="math inline">\(\land -\)</span>规则，将两个子命题拆出来。</p><p>当然不能白拆，要和前几条综合起来用，往往时加入了很多项后，用这个来助攻。</p><ol start="5" type="1"><li><strong>当右端项有 <span class="math inline">\(\land\)</span>时：</strong></li></ol><p>可能是要把两个细分都推出来。 <span class="math display">\[\lnot \left( {A}\lor {B} \right) \vdash \lnot {A}\land \lnot {B}\]</span> 也可能是要整个推出，即生成未出现过的项，常用 <spanclass="math inline">\(\lor-\)</span> 规则。 <spanclass="math display">\[\left( A\land {B} \right) \lor \left( {A}\land {C} \right) \vdash{A}\land \left( {B}\lor {C} \right)\]</span></p><ol start="6" type="1"><li><strong>在右端生成从未出现过的项：</strong></li></ol><p>常用 <span class="math inline">\(\lor-\)</span>规则，但是这种必须有理有据的推出，而且左端要含 <spanclass="math inline">\(\lor\)</span>。</p><p>另一个方法是 <span class="math inline">\(\lnot-\)</span>，矛盾可以推任意。</p><ul><li>通常的用法是：</li></ul><p><span class="math display">\[\begin{aligned}XXX, A, \lnot A&amp;\vdash A\\XXX, A, \lnot A&amp;\vdash \lnot A\\XXX, A, \lnot A&amp;\vdash B\end{aligned}\]</span></p><ul><li>然后再想办法消掉多出来的单项，如</li></ul><p><span class="math display">\[XXX, \lnot A\vdash A\rightarrow B\]</span></p><ul><li>或者再利用 <span class="math inline">\(\lor-\)</span>，二者结合，如</li></ul><p><span class="math display">\[\begin{aligned}XXX, A, \lnot A&amp;\vdash B\\XXX, A, B&amp;\vdash B\\XXX, A&amp;\vdash \lnot A\lor B\\XXX, A&amp;\vdash B\end{aligned}\]</span></p><p>例：（需要用到两种引入方法） <span class="math display">\[\left( A\lor B \right) \land \left( \lnot B\lor C \right) \vdash A\lor C\]</span> 按照思维流推断，发现最开始可以引入 <spanclass="math inline">\(B\)</span>，<span class="math inline">\(\lnotB\)</span> 的矛盾推任意。</p><p>而右端项的析取，一次只能推一半。</p><p>推一半的时候，还是需要引入新项，此时用 <spanclass="math inline">\(\lor-\)</span>，因为左端项还有个析取（隐藏在合取中）可以用，将两个子项单独拿出来放到左边，即可推出。</p><ol start="7" type="1"><li><strong>当左端项有 <span class="math inline">\(\to\)</span>时：</strong></li></ol><p>由于ND中没有分离规则，所以要利用好这个 <spanclass="math inline">\(\to\)</span> 需要先在左端加上前件，用两次 <spanclass="math inline">\(\in\)</span> 规则。</p><p>最后再用 <span class="math inline">\(\to -\)</span>规则把后件推到右端。</p><ol start="8" type="1"><li><strong>当左端项包含 <span class="math inline">\(\lnot\)</span>时：</strong></li></ol><p><span class="math display">\[\lnot ( A\to B ) \vdash A\land \lnot B\]</span></p><p>想按照逻辑推出东西基本不可能，最好的办法是利用已有的一个 <spanclass="math inline">\(\lnot\)</span>，制造矛盾，使得矛盾可以推任意，或者 <span class="math inline">\(\lnot+\)</span>。</p><p>但是如果要制造矛盾，必须引入新的变量，这样左边就会多一个元素，即使<span class="math inline">\(\lnot -\)</span>推出了任意，但左边也不符合。</p><p>因此可以用 <span class="math inline">\(\lnot +\)</span>规则，在左边引入一个<strong>待求右端项</strong>的 <spanclass="math inline">\(\lnot\)</span>，推出矛盾后再用反证法把<strong>待求变量</strong>移到右边，此时可能要用<span class="math inline">\(\lnot \lnot-\)</span> 规则。</p><h2 id="fc-中定理的证明">FC 中定理的证明</h2><h3 id="两种书写格式">两种书写格式</h3><ol type="1"><li><p>用 PC 中的公式序列，最后再补上 <spanclass="math inline">\(\Gamma\)</span>，不用序号，用「<strong>从而</strong>...」连接结论。</p><p>常用于 <strong>FC反证法定理8</strong>、<strong>双向证明</strong>（先证后证）</p></li><li><p>用 ND中的演绎序列，直接证出结果（书写内容多），如果此时<strong>要用 PC 或者FC 的定理</strong>，需要在公式前面写 <spanclass="math inline">\(\vdash\)</span>，并在下一条用分离规则导出（<strong>注意</strong>：不用写用若干次<span class="math inline">\(+\)</span> 规则）。</p><p>常用于 <strong>FC 存在消除定理10</strong></p></li></ol><p>具体证明过程依次使用以下方法。</p><h3 id="存在消除定理">存在消除定理</h3><p>当式子左端有 <span class="math inline">\(\exists xA\)</span> 或 <spanclass="math inline">\(\exists x(A\to B)\)</span>的量词时，优先考虑用存在消除定理10。</p><p>在左端补上 <span class="math inline">\(A\)</span> 或 <spanclass="math inline">\((A\to B)\)</span> ，并用 ND中的规则推出想要的右端项。</p><p>接着就可以用存在消除定理把左端补上的东西消除了。 <spanclass="math display">\[\begin{aligned}&amp;\left( 1 \right) \exists v\left( B\rightarrow A \right) ,B\rightarrow A, B\vdash A\\&amp;\left( 2 \right) \exists v\left( B\rightarrow A \right) , B\vdash\exists v\left( B\rightarrow A \right)\\&amp;\left( 3 \right) \exists v\left( B\rightarrow A \right) , B\vdashA\,\,\end{aligned}\]</span> <strong>注意</strong>：虽说「存在」消除定理，但是最后消掉的是<span class="math inline">\(\exists x\)</span> 里面的东西。</p><p><strong>警告</strong>：使用该方法时，(3) 式中左右端项都不能有 <spanclass="math inline">\(v\)</span> 的自由出现！</p><h3 id="反证法定理">反证法定理</h3><p>反证法不一定是最优解，但是是最 FC的做法，一般标准答案会用。当右端的整体上有个 <spanclass="math inline">\(\lnot\)</span> 打头，或者右端有个 <spanclass="math inline">\(\exists x\)</span> 时，建议使用。</p><p><span class="math display">\[\begin{aligned}&amp;\text{从而}\varGamma \vdash A\,\,\text{且} \varGamma \vdash \lnot A\\&amp;\text{从而公式集}\varGamma \text{不一致，由}FC\text{反证法定理}8\\&amp;\text{得}\forall vB\rightarrow A\vdash \lnot \forall v\lnot \left(B\rightarrow A \right)\end{aligned}\]</span></p><p>注意：如果左右都有 <span class="math inline">\(\existsx\)</span>，优先用存在消除定理。</p><h3 id="思维流-1">思维流</h3><p>前两种方法都不可以使用的时候考虑，跟 ND中的证明没啥区别。可能会用到一些 PC中的定理，如三段论定理8、逆否定理、以及一些小套件（定理6，定理9）。</p><h3 id="其他小技巧">其他小技巧</h3><ol type="1"><li>FC 定理 9 平时用的少，但是有时有妙用。</li></ol><p><span class="math display">\[\begin{aligned}&amp;\left( 1 \right) \exists v\left( B\rightarrow A \right) , B\vdash A\\&amp;\left( 2 \right) \exists v\left( B\rightarrow A \right) , \forallvB\vdash A\end{aligned}\]</span></p><ol start="2" type="1"><li><p>FC也有<strong>演绎定理</strong>，并且考试中可以<strong>直接使用</strong>。（省步骤，不然就得用ND 中复杂的规则来代替）</p><p>比如往左放就要用 <span class="math inline">\((+)\)</span> 再用 <spanclass="math inline">\((\to-)\)</span> 规则，往右放就要用 <spanclass="math inline">\((\to +)\)</span> 规则。</p></li><li><p>全称推广定理 和 定理1 是互逆的关系，是 PC/ND 系统与 FC联系的关键。常用于导出待证明式子右端项。</p></li><li><p>补充：使用 FC 定理时，有的定理有限制。</p></li></ol><ul><li>公理4 （项 t 对 v 可代入）</li><li>公理6 （v 在 A 中无自由出现）</li><li><strong>全称推广定理5</strong> （v 在 <spanclass="math inline">\(\Gamma\)</span> 中无自由出现）</li><li>定理9 （v 在 <span class="math inline">\(\Gamma\)</span>中无自由出现）</li><li><strong>存在消除定理10</strong> （v 在 <spanclass="math inline">\(\Gamma\)</span> 和 B中无自由出现）（在最后一行前后都无自由出现）</li></ul><h2 id="谓词演算部分">谓词演算部分</h2><h3 id="构造语义和指派">构造语义和指派</h3><p>语义就是 <span class="math inline">\(U=&lt;D,I&gt;\)</span>即论域和解释，其中论域部分令 <span class="math inline">\(D=\{1,2\}\)</span>。</p><p>解释部分 <span class="math inline">\(I\)</span>要加横杠，分别对常元、函词、谓词做一个映射。<strong>构造时根据要求，巧妙选择</strong>。<span class="math display">\[\begin{aligned}&amp;\bar{a}=1\\&amp;\bar{f}\left( 1,1 \right) =1,\bar{f}\left( 1,2 \right)=1,\bar{f}\left( 2,1 \right) =1,\bar{f}\left( 2,2 \right) =1\\&amp;\bar{R}=\left\{ 1,2 \right\} ,\bar{P}=\left\{ \left( 1,1 \right)\right\} ,\bar{Q}=\oslash\end{aligned}\]</span><strong>注意</strong>：函词是<strong>个体</strong>映射到<strong>个体</strong>，谓词是取这些<strong>个体</strong>时值为<strong>True</strong>。<strong>一元谓词跟函词很容易混淆</strong>。</p><p>指派 <span class="math inline">\(s\)</span> 就是给变元取值。 <spanclass="math display">\[\bar{x}=2,\bar{y}=1\]</span>注意：书写时展开函词的时候，<strong>横杠</strong>都要加上。特别是多层函词时。<span class="math display">\[\overline{f\left( x,y \right) }=\bar{f}\left( \bar{x},\bar{y} \right)\]</span> 判断真假的时候，简单的直接写 T 或 F就可以（需要横杠），表示映射。</p><p>但是含有<strong>任意、存在量词</strong>的必须用标准写法（不用横杠）<strong>拆去</strong>。<span class="math display">\[\begin{aligned}&amp;\models _U\left( \forall x \right) P\left( x \right) \left[ s\right] \,\, iff.\models _UP\left( x \right) \left[ s\left( x|1 \right)\right] \,\,\text{且} \models _UP\left( x \right) \left[ s\left( x|2\right) \right]\\&amp;\models _U\left( \exists x \right) P\left( x \right) \left[ s\right] \,\, iff.\models _UP\left( x \right) \left[ s\left( x|1 \right)\right] \,\,\text{或} \models _UP\left( x \right) \left[ s\left( x|2\right) \right]\end{aligned}\]</span></p><h3 id="构造自然语句的形式化">构造自然语句的形式化</h3><p>格式要注意，在开头要令出谓词、函词，一般不需要论域（除非真的只有一种个体词）。</p><p><strong>每个出现的个体词</strong>都应该有对应的谓词，如「作家」「作品」「小说」「学生」「实数」。</p><p>一个基本原则：$$</p><p>常用短语：</p><ul><li>并非所有：<span class="math inline">\(\lnot \forall x\)</span></li><li>有且仅有、唯一：$x( R( x ) u( R( u ) E( x, u ) ) ) $</li><li>不是就是、要么要么：$x( R( x ) P( x ) Q( x ) ) $</li></ul><p>例题：形式化表示第二数学归纳法。 <span class="math display">\[\begin{aligned}&amp;\text{令谓词：}\\&amp;F\left( x \right) \text{表示当}n=x\text{时，命题成立；}\\&amp;L\left( x,k \right) \text{表示}x&lt;k\text{；}\\&amp;P\left( x \right) \text{表示}x\text{是正整数。}\\&amp;\left( 1 \right) F\left( 1 \right)\\&amp;\left( 2 \right) \forall k\left( \left( P\left( k \right) \land \forallx\left( P\left( x \right) \land L\left( x,k \right) \rightarrow F\left(x \right) \right) \right) \rightarrow F\left( k \right) \right)\\&amp;\left( 3 \right) \forall x\left( P\left( x \right) \rightarrow F\left( x\right) \right)\end{aligned}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>本科课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IR课程项目-文学检索-开发文档</title>
    <link href="/IR-Project.html"/>
    <url>/IR-Project.html</url>
    
    <content type="html"><![CDATA[<p><img src="/img/blog/IR-Project-images/logo.png" alt="logo" style="zoom: 33%;" /></p><p><code>Date</code>：2021-08</p><p><code>Description</code>：本文档作为 2021 年夏季学期 IR 课程 Project搜索引擎搭建的开发文档，检索主题为「<strong>文学</strong>」。</p><p><code>Reference</code>：参考自 CHH12 的 <ahref="https://gitee.com/CHH12/IR-project-pioneer-search">实现方案</a>，在其基础上改进了算法，适配了现版本工具并完善文档。</p><p><code>Copyleft</code>：© 2021 Hwcoder. Some rights reserved.</p><h2 id="系统说明">系统说明</h2><p>中国文学有数千年悠久历史，以特殊的内容、形式和风格构成了自己的特色，有自己的审美理想，有自己的起支配作用的思想文化传统和理论批判体系，是世界文学宝库中光彩夺目的瑰宝。</p><p>本搜索引擎主要收录了与<strong>中国文学</strong>有关的网页文档，包括但不限于民族文学、宗教文学、语言文学、诗词文学、中国外国文学等，爬取网站均来自中国社科院等官方机构。</p><h3 id="系统架构">系统架构</h3><figure><img src="/img/blog/IR-Project-images/system.png" alt="系统架构" /><figcaption aria-hidden="true">系统架构</figcaption></figure><p>本搜索引擎系统主要架构如上图所示。</p><p>检索的源文档由网络爬虫从给定的种子页面开始爬取，爬取到本地后利用Python 中的 BeautifulSoup4 模块进行网页的解析，并写入文档数据库中。之后利用搜索引擎解决方案 Xunsearch（迅搜）构建搜索引擎，利用索引器Indexer 对文档数据进行索引，并构建基于 Xapian的后端搜索服务器与前端的搜索应用，向用户展示出搜索功能。</p><h3 id="模块介绍">模块介绍</h3><h4 id="网络爬虫larbin">网络爬虫：Larbin</h4><p>Larbin 是一种开源的网络爬虫，用 C++语言实现。其设计简单，具有高度的可配置性，能够配置抓取深度、间隔、并发度、代理，并支持通过后缀名对抓取网页进行过滤。Larbin具有非常高的效率，可以轻易获取单个网站的所有链接，自动扩展 url页面并抓取与保存，从而为搜索引擎提供广泛的数据来源。</p><p>然而，Larbin只是一个爬虫，<strong>只抓取网页</strong>，并不负责网页的解析、数据库的存储以及索引的建立，也不支持分布式系统。另外，Larbin已经较长时间不再更新，因而不支持 https协议，这在今天很大程度上造成了网页页面的局限性。</p><p>这里采用由国人在基于原版 Larbin2.6.3 版本上继续开发并发布于 GitHub 的Larbin2.6.5 版本进行搜索引擎系统的构建。</p><p>项目网址：https://github.com/ictxiangxin/larbin</p><h4 id="网页解析beautifulsoup4-bs4">网页解析：BeautifulSoup4 (bs4)</h4><p>BeautifulSoup4 是一个可以从 HTML 或 XML 文件中提取数据的 Python库。</p><p>由于 HTML 和 XML文件本身是<strong>结构化的文本</strong>，有一定的规则，通过它的结构可以简化信息提取。类似的网页信息提取库还有Lxml 和 Pyquery 等，但是 bs4 相比其他的库更加简单易用。</p><p>本项目就采用 BeautifulSoup4 快速对网页文档内容进行解析和格式化。</p><p>官方文档：https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/</p><h4 id="前端搜索引擎xunsearch-xapian">前端&amp;搜索引擎：Xunsearch &amp;Xapian</h4><p>Xunsearch（迅搜）是一款以 GPL协议开源发布的高性能、全功能的<strong>全文检索</strong>解决方案，并针对<strong>中文</strong>深度优化和处理，用于帮助开发者针对海量数据快速建立搜索引擎。</p><p>Xunsearch采用结构化分层设计，包含后端服务器和前端开发包两大部分。后端是用 C/C++基于 Xapian 搜索库、SCWS 中文分词、libevent 等开源库开发，借鉴了 nginx的多进程多线程混合工作方式，是一个可承载高并发的高性能服务端。前端则是使用流行的脚本语言编写了开发工具包（SDK)。</p><p>本项目采用 Xunsearch 还考虑了以下特点：</p><ul><li>具有为搜索而自主开发 SCWS中文分词库，支持复合分词、自定义补充词库，保障查全率、准确率。</li><li>索引接口齐全，索引添加简便，支持实时搜索，支持多种数据源（SQL、JSON、CSV等）。</li><li>除通用搜索引擎功能外，还内置支持拼音检索、分面搜索、相关搜索、同义词搜索、搜索纠错建议等专业功能。</li></ul><p>官方地址：http://www.xunsearch.com/</p><p>此外，Xunsearch 的高速响应能力还离不开 Xapian 这一搜索引擎库。</p><p>Xapian是一个允许开发人员轻易地添加<strong>高级索引和搜索功能</strong>到他们的应用系统的高度可修改的工具，它在支持概率论检索模型的同时也支持布尔型操作查询集。</p><p>Xapian 相比 Lucene 有更多的优势：基于 C++开发的强可移植性（可以运行在 Linux, MacOS, Windows系统上），丰富的查询机制（概率性搜索排名、相关度反馈、邻近搜索、布尔搜索、词干提取、通配符查询、别名查询、拼写纠正等）和较强的检索性能。</p><h2 id="设计文档">设计文档</h2><h3 id="运行环境">运行环境</h3><p>主机：</p><ul><li>系统：Windows 10</li><li>带宽：40 Mbps</li></ul><p>虚拟机：</p><ul><li>软件：Oracle VM VirtualBox 6.1.26</li><li>配置设置：<ul><li>内存：2GB</li><li>磁盘：20GB</li><li>处理器：Intel(R) Core(TM) i5-9300H CPU @ 2.40GHz（核心数：1）</li></ul></li><li>系统：Linux Ubuntu 20.04</li><li>依赖环境：<ul><li>Apache 2.4.41</li><li>PHP 7.4.3</li><li>Python 3.8.10</li></ul></li></ul><h3 id="总体设计流程">总体设计流程</h3><ol type="1"><li>安装 Ubuntu 虚拟机，配置环境，安装 Larbin 和 Xunsearch。</li><li>配置 Larbin 爬虫选项，选定种子页面，爬取文档。</li><li>利用 BeautifulSoup4 模块编写脚本 Parser.py 解析文档，存储为 csv文件。</li><li>生成 Xunsearch 配置文件，构建索引，生成搜索框架。</li><li>部署至 Apache HTTP Server，前后端代码再开发，优化搜索页面。</li></ol><h3 id="各模块设计细节">各模块设计细节</h3><h4 id="网络爬虫">网络爬虫</h4><p>网络爬虫模块使用 Larbin2.6.5 进行网页文档的爬取。在按照配置进行爬取30 分钟后，最终获得 10965 个文档（约 409MB）。由于爬虫自身的限制，这些文档均来自 http 站点。</p><p>以下是关键的配置项及说明：</p><ul><li>是否锁定种子站点：<strong>否</strong>。取消锁定才能根据链接爬取到更多内容。</li><li>是否使用扩展链接：<strong>否</strong>。大部分网页为了提高排名，都会引入无关的友链，由于本项目是专题检索，故关闭此功能避免进入其他域名。</li><li>同一个服务器的两次请求的间隔时间：<strong>无限制</strong>。通常基于礼貌原则，我们应该限制访问频率，但由于关闭了扩展链接，爬虫只会在少数域名中爬取，如果限制了间隔时间会使得爬取效率极低。</li><li>是否哈希页面以去重：<strong>是</strong>。由于站点固定，去重可以减少大量数据冗余。</li></ul><p>以下是种子网页地址：</p><ul><li>http://philosophychina.cssn.cn/ （中国哲学网-中国社科网子域名）</li><li>http://literature.cssn.cn/ （中国文学网-中国社科网子域名）</li><li>http://cel.cssn.cn/#story1 （中国民族文学网-中国社科网子域名）</li><li>http://ling.cssn.cn/ （中国语言文学网-中国社科网子域名）</li><li>http://iwr.cssn.cn/ （中国宗教文学网-中国社科网子域名）</li><li>http://ifl.cssn.cn （中国外国文学网-中国社科网子域名）</li><li>http://www.china-language.edu.cn/ （中国语言学网）</li><li>http://www.huaxiawen.com/ （华夏古代文学网）</li><li>http://www.zgwenxue.com/ （中国文学网）</li><li>http://www.eduwx.com/ （教育文学网）</li><li>http://www.wgwxzz.cn/ （外国文学网）</li><li>http://www.zhexue.org/ （哲学网）</li><li>http://www.52shici.cn/ （吾爱诗词网）</li></ul><p>在终端运行爬虫过程如下：</p><p><img src="/img/blog/IR-Project-images/Larbin1.png" alt="Larbin运行过程" style="zoom: 67%;" /></p><p>打开 localhost:8081 查看统计结果：</p><p><img src="/img/blog/IR-Project-images/Larbin2.png" alt="Larbin运行结果" style="zoom:67%;" /></p><p>从统计结果可以看出，共收到 13783 个 URL，访问了 13783个页面，最终成功爬取 10965 个页面。</p><p><img src="/img/blog/IR-Project-images/Larbin3.png" alt="爬取速度变化" style="zoom:67%;" /></p><p>从上图可以具体地得出爬虫的爬取效率，可见爬虫爬取的速度随着时间的推移会发生较大的变化。</p><h4 id="网页解析">网页解析</h4><p>解析器模块利用 Python 的 BeautifulSoup4 和 Pandas模块编写，前者可以对网页文档进行解析，后者便于将数据以 csv文件形式保存，便于后续搜索引擎使用。</p><p>调用 BeautifulSoup4 的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(path + file, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    soup = BeautifulSoup(f.read(), <span class="hljs-string">&#x27;html.parser&#x27;</span>)<br>    <span class="hljs-keyword">if</span> soup.title == <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> soup.title.string == <span class="hljs-literal">None</span>:<br>        title.append(<span class="hljs-string">&#x27;&#x27;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        title.append(soup.title.string.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>).replace(<span class="hljs-string">&#x27;\r&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>))<br>    body.append(soup.get_text().replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>).replace(<span class="hljs-string">&#x27;\r&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>))<br></code></pre></td></tr></table></figure><p>调用 Pandas 的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">data = &#123;<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-built_in">id</span>, <span class="hljs-string">&#x27;title&#x27;</span>: title, <span class="hljs-string">&#x27;body&#x27;</span>: body, <span class="hljs-string">&#x27;urls&#x27;</span>: urls&#125;<br>frame = pd.DataFrame(data)<br><span class="hljs-keyword">if</span> k == <span class="hljs-number">0</span>:<br>    frame.to_csv(<span class="hljs-string">&#x27;data_u.csv&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8-sig&#x27;</span>, index=<span class="hljs-literal">False</span>)<br><span class="hljs-keyword">else</span>:<br>    frame.to_csv(<span class="hljs-string">&#x27;data_u.csv&#x27;</span>, mode=<span class="hljs-string">&#x27;a+&#x27;</span>, header=<span class="hljs-literal">False</span>, encoding=<span class="hljs-string">&#x27;utf-8-sig&#x27;</span>, index=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>处理结果如下：</p><p><img src="/img/blog/IR-Project-images/data.png" alt="处理后的数据（部分）" style="zoom: 67%;" /></p><p>可以看到每个文档被分出 id, title, body, urls 四个字段，存储在一个 csv文件中。</p><h4 id="前端搜索引擎">前端&amp;搜索引擎</h4><p>搜索引擎模块利用 Xunsearch提供的开发工具即可实现。首先确定运行环境正常，然后编写配置文件对格式化的csv 文件建立索引，并生成搜索骨架代码，即可实现基本检索功能。</p><p>首先利用工具包中的 RequiredCheck 检查当前环境是否满足 Xunsearch的运行条件：</p><p><img src="/img/blog/IR-Project-images/xun1.png" alt="迅搜运行条件检查" style="zoom:50%;" /></p><p>环境正常后，利用工具包中的配置文件生成工具 IniWizzard，在 Web交互页面中即可完成各个字段的设计：</p><p><img src="/img/blog/IR-Project-images/xun2.png" alt="配置生成工具" style="zoom: 50%;" /></p><p>其中，各个字段的含义如下：</p><ul><li>id 为主键，作为每个文档的标识符。</li><li>title 为文档的标题，赋予较高权重。</li><li>body 为文档的内容，截取 300 个字符作为搜索结果的摘要显示。</li><li>urls 作为每个文档在展示时的附属信息，不需要进行索引。</li></ul><p>此后，就可以用工具包中的索引管理器 Indexer 批量建立索引：</p><p><img src="/img/blog/IR-Project-images/xun3.png" alt="批量建立索引" style="zoom: 67%;" /></p><p>在 db 文件夹下，可以看到建立的索引文件：</p><p><img src="/img/blog/IR-Project-images/xun4.png" alt="建立的索引文件" style="zoom: 67%;" /></p><p>此时，可以用工具包中配备的测试工具Quest，在当前索引中进行测试搜索，测试给定查询词的返回的数据。</p><p><img src="/img/blog/IR-Project-images/xun5.png" alt="测试搜索" style="zoom:50%;" /></p><p>可以看到搜索功能已经可以使用了，这时需要用到工具包中的骨架代码生成工具SearchSkel，生成前端代码。</p><p><img src="/img/blog/IR-Project-images/xun6.png" alt="骨架代码生成" style="zoom: 67%;" /></p><p>最后，运行 Apache2.0，将生成的 search 目录放到 /var/www/html中，即可在本地服务器 localhost 访问搜索页面，实现搜索引擎的功能。</p><figure><img src="/img/blog/IR-Project-images/xun7.png" alt="基础页面" /><figcaption aria-hidden="true">基础页面</figcaption></figure><p>点击文档标题，会返回对应的文档主键（id）：</p><p><img src="/img/blog/IR-Project-images/xun8.png" alt="点击标题后的弹框" style="zoom: 67%;" /></p><h4 id="页面再开发">页面再开发</h4><p>可以看到，直接生成的框架文件有如下的不足：</p><ul><li>前端较为简陋，重点不够突出。</li><li>由于使用了国外的 cdn，访问速度较慢。</li><li>搜索结果点击后无法直接跳转至页面。</li></ul><p>开发 style.css 和 search.tpl 文件后，可以得到新的页面：</p><figure><img src="/img/blog/IR-Project-images/xun9.png" alt="再开发后的页面" /><figcaption aria-hidden="true">再开发后的页面</figcaption></figure><h2 id="页面展示">页面展示</h2><p>网站部署到 web可访问目录后，可以通过虚拟机的端口转发设置，实现在主机上访问，也可以购买HTTP 映射服务后在给定域名访问。这里选择最简单的在虚拟机中访问。</p><h3 id="首页">首页</h3><p>使用搜索前：</p><p><img src="/img/blog/IR-Project-images/page0.png" alt="使用搜索前" style="zoom: 67%;" /></p><p>使用搜索后：</p><p><img src="/img/blog/IR-Project-images/page1.png" alt="使用搜索后" style="zoom:67%;" /></p><p>在使用了搜索功能后，搜索引擎将分析<strong>搜索日志</strong>，从而提供<strong>热门搜索词</strong>。此外，可以看到搜索时可以选择多种方式，页脚有标明版权、处理时间以及作者的信息。</p><h3 id="搜索页面">搜索页面</h3><p>点击搜索后，会反馈查询结果的标题、文档摘要以及文档对应的原网址，并进行对查询词进行高亮表示（标题中红字、摘要中加粗），并会返回检索条目、检索时间、检索相似度等信息。</p><p>特别地，在页脚还提供了相关的搜索内容的联想，下图是在搜索「散文」后的联想：</p><p><img src="/img/blog/IR-Project-images/page2.png" alt="相关搜索" style="zoom:67%;" /></p><h3 id="部分功能">部分功能</h3><ol type="1"><li>查询联想</li></ol><p><img src="/img/blog/IR-Project-images/page3.png" alt="查询联想" style="zoom:67%;" /></p><ol start="2" type="1"><li>拼音联想</li></ol><p><img src="/img/blog/IR-Project-images/page4.png" alt="拼音联想" style="zoom:67%;" /></p><ol start="3" type="1"><li>搜索纠错</li></ol><p><img src="/img/blog/IR-Project-images/page5.png" alt="搜索纠错" style="zoom:67%;" /></p><ol start="4" type="1"><li>拼音纠错</li></ol><p><img src="/img/blog/IR-Project-images/page6.png" alt="拼音纠错" style="zoom:67%;" /></p><ol start="5" type="1"><li>布尔检索</li></ol><p><img src="/img/blog/IR-Project-images/page7.png" alt="未使用布尔检索的结果" style="zoom:67%;" /></p><p>插入否定连接词后：</p><p><img src="/img/blog/IR-Project-images/page8.png" alt="插入布尔连接词后的结果" style="zoom:67%;" /></p><h2 id="性能评估">性能评估</h2><p>下表随机选取 10 个与本搜索引擎相关的查询词进行检索，评估搜索引擎的Top5 查准率（precision@5）与平均响应时间（mean responding time）。</p><table><thead><tr class="header"><th style="text-align: center;">查询</th><th style="text-align: center;">查准率@5</th><th style="text-align: center;">响应时间/s</th><th style="text-align: center;">匹配文档总数</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">校园文学</td><td style="text-align: center;">4</td><td style="text-align: center;">0.0219</td><td style="text-align: center;">2000</td></tr><tr class="even"><td style="text-align: center;">宗教文学</td><td style="text-align: center;">5</td><td style="text-align: center;">0.0186</td><td style="text-align: center;">1000</td></tr><tr class="odd"><td style="text-align: center;">汉语</td><td style="text-align: center;">5</td><td style="text-align: center;">0.0210</td><td style="text-align: center;">990</td></tr><tr class="even"><td style="text-align: center;">藏语</td><td style="text-align: center;">3</td><td style="text-align: center;">0.0229</td><td style="text-align: center;">68</td></tr><tr class="odd"><td style="text-align: center;">季羡林</td><td style="text-align: center;">1</td><td style="text-align: center;">0.0173</td><td style="text-align: center;">50</td></tr><tr class="even"><td style="text-align: center;">鲁迅</td><td style="text-align: center;">4</td><td style="text-align: center;">0.0188</td><td style="text-align: center;">501</td></tr><tr class="odd"><td style="text-align: center;">毛泽东诗词</td><td style="text-align: center;">5</td><td style="text-align: center;">0.0221</td><td style="text-align: center;">300</td></tr><tr class="even"><td style="text-align: center;">马克思主义哲学中国化</td><td style="text-align: center;">5</td><td style="text-align: center;">0.0356</td><td style="text-align: center;">100</td></tr><tr class="odd"><td style="text-align: center;">散文作品</td><td style="text-align: center;">4</td><td style="text-align: center;">0.0128</td><td style="text-align: center;">800</td></tr><tr class="even"><td style="text-align: center;">小说集</td><td style="text-align: center;">3</td><td style="text-align: center;">0.0225</td><td style="text-align: center;">120</td></tr></tbody></table><p>计算得：平均查准率为 3.9，平均响应时间为 0.02135s。</p><p>从结果上看，该搜索引擎在对应领域有着良好的表现，且响应迅速，但是对著名人物作品的查询则稍显不足。</p><p>此外，在部分检索结果中，有些无关文档由于重复出现了查询词而被赋予较高的「相似度」，但实际上Top 10 返回结果的「相似度」都很高，可以考虑用 PageRank等算法优化网站排名，提高 MRR 等指标。</p>]]></content>
    
    
    <categories>
      
      <category>项目经历</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IR</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IR学习笔记 #11 问答系统</title>
    <link href="/IR-Note-11.html"/>
    <url>/IR-Note-11.html</url>
    
    <content type="html"><![CDATA[<p>在传统的搜索引擎中，通常是以关键词为索引，通过匹配而返回相似的文档。然而，有时用户在搜索引擎中检索，并非想得到若干个网页，而是直接返回询问的答案，免去用户人工定位信息的过程。</p><p><strong>问答式信息检索</strong>，是一种允许用户以自然语言方式询问，系统从单语或多语文档集中查找并返回<strong>确切答案或者蕴含答案文本片断</strong>的新型信息检索的方式。</p><p>要做到这一点，就需要先做到：</p><ol type="1"><li>理解问句中的查询意图。</li><li>根据分析结果去检索匹配文档，尽量缩小范围。</li><li>在返回的文档中提取答案信息或定位答案文本。</li></ol><p>当然，早期还有一种做法是构建 FAQ (Frequently AskedQuestions，常用问题解答)库，将用户的问句与问题库中的问句进行相似度匹配，直接返回对应的答案。</p><h2 id="askmsr-shallow-approach">AskMSR: Shallow approach</h2><p>AskMSR 是 Microsoft Research提出的是一种简易的基于检索的问答系统。</p><p>其大体步骤分为六步：</p><ol type="1"><li>Rewrite Query：重写查询，并将问句类型归类。</li><li>Search Engine：将所有重写结果输入到搜索引擎，返回前若干个文档。</li><li>Mine N-grams：文档分词，N 通常枚举1，2，3，根据出现频率给出置信度。</li><li>Filter N-grams：过滤分词结果，保留与问题类型相关的。</li><li>Tile N-grams：组合分词结果，把重叠可拼接的词merge，置信度也相加。</li><li>N-best Answers：答案通常都是多个，以置信度排序。</li></ol><h3 id="重写查询-query-rewriting">重写查询 | Query Rewriting</h3><p>如果只是检索用户输入的问句，很可能返回的就是含有句子本身的文档。而根据我们的直觉：用户需要的答案通常在语义上与查询<strong>相近</strong>的句子中。为此，我们需要将疑问句改写成<strong>陈述句</strong>，如果在文本中有这样一句陈述句，它可能本身就包含了问题的答案。</p><p>重写后的陈述句不一定是满足语法规则的句子，但是这并不影响检索结果。一个query可以被改写成若干个问题，并通过搜索引擎查询更多个相似的文本片段。</p><p>此外，重写问句时，通常要对语言的语法规则有一点了解，从而对问题进行分类。如在英语中：Who对应人名，When 对应时间，Where对应地名等。这将用于后续的分词结果的过滤。</p><h2 id="模式挖掘-isi-surface-patterns-approach">模式挖掘 | ISI: Surfacepatterns approach</h2><p>在 AskMSR中，我们只在句法相近的句子中寻找答案，但是实际上很多答案会隐藏在不同的形式中。如果我们使用特征短语——<strong>模式</strong>（Pattern），就可以检索到不同形式的答案。</p><p>例如在问题 “When was <code>person</code> born ?”中，答案的形式可能是：</p><ul><li>Mozart was born in 1756.</li><li>Gandhi (1869-1948) ...</li></ul><p>这两种特征短语可以挖掘出如下的模式：</p><ul><li><code>NAME</code> was born in <code>BIRTHDATE</code></li><li><code>NAME</code> (<code>BIRTHDATE</code> -<code>DEATHDATE</code>)</li></ul><p>对于不同的模式，我们也可以赋予不同的置信度（准确度）。而后我们就可以用这两种模式在搜索引擎中匹配结果，并根据置信度返回结果排序。</p><p>在 2002 年 Hovy 等人还提出 QA Typology的问答分类体系，将常见的问答分为以下六种类型：BIRTHDATE、LOCATION、INVENTOR、DISCOVERER、DEFINITION、WHY-FAMOUS，并给出了对应类型下常用的模式和置信度。这在使用中取得了较高的MRR (Mean Reciprocal Rank)。</p><h3 id="shortcomings-extensions">Shortcomings &amp; Extensions</h3><p>接下来分析 ISI 可能出现的问题，以及改进的方法。</p><p>第一，由于使用了简单的字符串匹配，可能会在文档中出现「模式符合，但并非答案」的句子，这就需要利用<strong>词性标注</strong>（Part-Of-SpeechTagging）对答案内容加以分析，从而修改置信度。</p><p>第二，原始的模式不支持<strong>长距离</strong>答案的匹配，实际中的陈述句中如果含有形容词、副词或者更长的插入语，就容易割裂模式。如「Mozart,who was a famous classical composer, was born in1756.」中，原始的模式就会漏掉答案。因此我们需要在原始的模式中插入可任意文本填充的空白字段。</p><p>第三，当问句中的 <code>NAME</code>以<strong>同义词或释义</strong>的形式出现时，直接字符串匹配的模式也会漏掉答案。这时就需要对问句中的关键词进行语义上的扩展，需要用到WordNet 等词典。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>信息检索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IR</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IR学习笔记 #10 查询相关反馈</title>
    <link href="/IR-Note-10.html"/>
    <url>/IR-Note-10.html</url>
    
    <content type="html"><![CDATA[<p>用户在检索信息时，通常会以一个简短的 query开始，这样的查询往往得不到其最想要的结果。而用户会在得到结果后优化自己的query，如：增删词项、重新赋权、加入布尔运算符等。</p><p><strong>相关反馈</strong>（RelevanceFeedback）的主要思想就是：在信息检索的过程中通过用户交互来优化查询，从而提高最终的检索效果。我们的目的是实现一个良好的<strong>反馈机制</strong>。</p><h2 id="最佳查询-best-query">最佳查询 | Best Query</h2><p>为了使反馈能让 query 真正往「更好」的方向演变，需要定义评价 query的一个指标。通常我们在<strong>向量空间模型</strong>中评价之，因为可以较好地表达相似度。</p><p>假设我们要找一个最优查询向量 <spanclass="math inline">\(\vec{q}\)</span>，它与相关文档之间的相似度最大，和不相关文档之间的相似度最小。若<span class="math inline">\(C_r\)</span> 表示相关文档集，<spanclass="math inline">\(C_{nr}\)</span>表示不相关文档集，我们希望找到的最优的是 <spanclass="math inline">\(\vec{q}\)</span> 应当满足： <spanclass="math display">\[\begin{aligned}\vec{q}_{opt} &amp;= argmax[Sim(\vec{q},C_r)-Sim(\vec{q},C_{nr})]\\&amp;=argmax(\frac{1}{|C_r|}\sum_{\vec{d}_j\in C_r}{\vec{q}\cdot\vec{d}_j}-\frac{1}{|C_{nr}|}\sum_{\vec{d}_j\in C_{nr}}{\vec{q}\cdot\vec{d}_j})\\&amp;=argmax(\vec{q}\cdot \vec{a})\end{aligned}\]</span></p><p>其中 <span class="math inline">\(argmax(x)\)</span>​​​​ 函数是返回使<span class="math inline">\(x\)</span>​​​​ 最大的变量，相似度 <spanclass="math inline">\(Sim\)</span>​​​​ 的求法则采用余弦夹角， <spanclass="math inline">\(\vec{q}\)</span>​​​​ 和 <spanclass="math inline">\(\vec{d}\)</span>​​​​采用归一化后的<strong>单位向量</strong>。此外，我们令： <spanclass="math display">\[\vec{a}=\frac{1}{|C_r|}\sum_{\vec{d_j}\in{C_r}}{\vec{d_j}} -\frac{1}{|C_{nr}|}\sum_{\vec{d_j}\in{C_{nr}}}{\vec{d_j}}\]</span> 若使 <spanclass="math inline">\(\vec{q}\cdot{\vec{a}}\)</span> 最大，<spanclass="math inline">\(\vec{q}\)</span> 需要与 <spanclass="math inline">\(\vec{a}\)</span> 平行，且 <spanclass="math inline">\(\vec{q}\)</span>​ 为单位向量，故有最佳查询： <spanclass="math display">\[\vec{q}_{opt}=\frac{1}{|C_r|}\sum_{\vec{d_j}\in{C_r}}{\vec{d_j}} -\frac{1}{|C_{nr}|}\sum_{\vec{d_j}\in{C_{nr}}}{\vec{d_j}}\]</span>这就是说，最优的查询向量等于相关文档的质心向量和不相关文档的质心向量的差，相当于是最接近相关文档，同时最远离不相关文档。</p><h3 id="查询优化-query-modification">查询优化 | Query Modification</h3><p>然而，即使有了上述最佳查询的表示方法，也无法直接求出来——因为检索本来的目的就是要找相关文档，而所有的相关文档事先是未知的。</p><p>Rocchio 提出在真实的检索情景中，我们可以利用已检索到的部分相关文档<span class="math inline">\(D_r\)</span> 和不相关文档 <spanclass="math inline">\(D_{nr}\)</span>，逐步修改原始的查询向量： <spanclass="math display">\[\vec{q}_m=\alpha\vec{q}_0 + \beta\frac{1}{|D_r|}\sum_{\vec{d_j}\in{D_r}}{\vec{d_j}} -\gamma\frac{1}{|D_{nr}|}\sum_{\vec{d_j}\in{D_{nr}}}{\vec{d_j}}\]</span> 修改后的新查询从 <spanclass="math inline">\(\vec{q}_{0}\)</span>​​开始，向着相关文档的质心向量靠近了一段距离，而同时又与不相关文档的质心向量远离了一段距离——更加接近最优查询了。通过不断迭代，可以观察到查询效果确实有显著的提升。</p><h2 id="查询反馈-relevance-feedback">查询反馈 | Relevance Feedback</h2><p>通常情况下，反馈可分为以下两种：</p><ul><li>真实相关反馈：搜索引擎返回结果，用户提供反馈，搜索引擎根据反馈返回更好的结果。</li><li>假设相关反馈：搜索引擎得到结果但不返回，根据结果自动优化query，根据优化后的 query 返回「更好」的结果。</li></ul><h3 id="点击流数据-clickthrough-data">点击流数据 | ClickthroughData</h3><p>在真实相关反馈中，用户往往不愿意主动提供反馈信息（如标记相关或不相关文档），于是搜索引擎收集用户的<strong>间接反馈</strong>。</p><p>而点击流数据则是这个领域最常用的一种反馈，可以在不干扰用户的情况下大量收集（此外还有一种补充用户行为信息的方法是眼动追踪）。</p><p>同一搜索结果中，用户进行点击浏览的结果被认为是相关的，或者说是「用户更<strong>偏好</strong>的」。如果用户查看了每个搜索引擎下面显示的文本短摘要后，决定跳过它并点击在排序中低于它的结果，就可以说用户相对更喜欢这个被点击的结果。</p><h3 id="局部上下文分析-local-context-analysis">局部上下文分析 | LocalContext Analysis</h3><p>在假设相关反馈中，还可分为两种基本方法：</p><ul><li>局部分析（LocalAnalysis）：从结果集合排名靠前的文档中产生反馈信息。</li><li>全局分析（GlobalAnalysis）：从外部资源产生反馈信息，如同义词典（用于扩充查询）。</li></ul><p>同义词典构建的代价十分昂贵，通常考虑用<strong>上下文和短语结构</strong>进行分析获得。而如果把这个思想用于局部分析，则诞生了LCA 方法：一种聚焦于从反馈结果中筛选出与 query 相关性更高的term，再用这些 term 扩展 query 重新检索的方法。</p><p>大致的步骤如下：</p><ol type="1"><li>找到与这个 query 检索结果排名靠前的文章，使用一个固定长度（如 300个词）的滑动窗口，来划分<strong>段落</strong>。引入段落是为了最小化冗长文档中的无关内容。</li><li>对段落进行检索排序，找到结果排名靠前的段落，对其使用语义分析和文本分词、词性标注技术找到<strong>名词项</strong>。名词在搜索引擎中往往被视为最重要的词。</li><li>统计名词项的出现次数，以及出现时离 query的距离，通过特殊的加权方法来选择出候选 term。</li><li>将排名靠前的几个候选 term 加入到原始 query 中，进行新的查询。</li></ol><h2 id="排序学习-learning-to-rank">排序学习 | Learning to Rank</h2><p>相关反馈信息，包括前述文章中提到的相关度、重要度，其实只是 IR中许多因子的冰山一角。实际中可能还有若干、数十个因子，这些因子最后会加权构成一个统一的<strong>指标函数</strong>。</p><p>这个指标函数的输入是数据集（包括查询和文档集），输出是最终检索出的ranklist。如何构造这样一个复杂的函数呢？</p><p>对于构造函数，人们最原始的想法通常是拟合所有 &lt;query, ranklist&gt;点，但是这显然不适用于这种规模的问题。</p><h3 id="机器学习的使用-machine-learning-for-ir-ranking">机器学习的使用 |Machine learning for IR ranking</h3><p>过去的 IR系统较少用到机器学习，是因为缺乏训练集，特别是在真实世界中得到的数据集（而不是学术论文中），因为很难收集到用户检索的真实需求和对返回文档的相关反馈。</p><p>此外，过去的 IR系统往往只使用少量的<strong>特征</strong>（feature），如词项频率、逆文档频率、term出现的位置等。</p><p>少量的特征带来的是构造函数的便利。而随着现在网络的发展、算力的提升，大家开始关注数据集中大量的特征，并尝试用机器学习使用这些特征。</p><p>定义 loss function <spanclass="math inline">\(l(r_a,r_b)\)</span>​​​，其中 <spanclass="math inline">\(r_a\)</span>是基于用户反馈得到的「标准排名」，<spanclass="math inline">\(r_b\)</span> 是通过拟合的排序函数 <em>F</em>计算出的「模拟排名」。我们要寻找到一个 <em>F</em>使得损失最小——这就是机器学习的目标。</p><h3 id="example---title-body">Example - title &amp; body</h3><p>下面以一个例子说明机器学习在 IR 中的应用。考虑查询中的 term出现在文档的 title (标题) 或 body (正文) 中对返回结果排名的影响。</p><p>为此，我们需要对 term 出现的四种情况分别打分： <spanclass="math display">\[\operatorname{score}\left(d, q\right)=g {s_{t}}\left(d, q\right)+(1-g)s_{b}\left(d, q\right)\]</span> 其中 <span class="math inline">\(s_t\)</span> 和 <spanclass="math inline">\(s_b\)</span> 函数是关于 term是否存在于文档对应位置的布尔函数（0/1），故 score 的结果只有 0,<em>g</em>, 1-<em>g</em>, 1 四种。我们要求的就是权重 <em>g</em>。</p><p>在第 <em>j</em> 个查询中，我们对检索结果中的文档 <em>i</em>定义如下<strong>损失函数</strong>： <span class="math display">\[\varepsilon\left(g, \Phi_{j}\right)=\left(r\left(d_{i},q_{j}\right)-\operatorname{score}\left(d_{i}, q_{j}\right)\right)^{2}\]</span> 这里简单的定义 <em>r</em>函数是关于二者是否相关的布尔函数（0/1），使用平方误差是为了让结果更连续。</p><p>在训练集中，我们标注出所有结果的 <spanclass="math inline">\(s_t\)</span> 、 <spanclass="math inline">\(s_b\)</span> 和 <em>r</em>函数的取值——八种情况，并分别统计其次数。例如，<spanclass="math inline">\(n_{01r}\)</span> 表示 <spanclass="math inline">\(s_t=0,s_b=1\)</span> 且相关的例子，<spanclass="math inline">\(n_{01n}\)</span> 表示 <spanclass="math inline">\(s_t=0,s_b=1\)</span>​且不相关的例子，其平方误差之和为： <span class="math display">\[[1-(1-g)]^{2} n_{01 r}+[0-(1-g)]^{2} n_{01 n}\]</span> 同样的，我们对其他三组也进行计算后相加，化简可得： <spanclass="math display">\[\left(n_{01 r}+n_{10 n}\right) g^{2}+\left(n_{10 r}+n_{01n}\right)(1-g)^{2}+n_{00 r}+n_{11 n}\]</span> 要求这个函数的极小值，只需用对关于 <em>g</em>的导数求零点即可。如果考虑更多的变量，则需要求<strong>偏导</strong>，再用拉格朗日常数法等数值分析方法。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>信息检索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IR</tag>
      
      <tag>NLP</tag>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IR学习笔记 #09 网页排序</title>
    <link href="/IR-Note-9.html"/>
    <url>/IR-Note-9.html</url>
    
    <content type="html"><![CDATA[<p>当我们在一个小的文档库中查询时，即使 query很模糊，我们只要返回所有相关文档即可，甚至不需要<strong>猜测</strong>用户的查询需求。</p><p>但如果在一个大的文档集中查询时（比如谷歌），往往可以返回大量的相关文档。如果基于相关度的ranking，往往无法区分哪些文档该呈现在最前面，甚至可能时一些低质量的网页由于某些词的词频很高，从而排在了前面。</p><p>此时我们就不能只聚焦于「相关度」，PageRank算法通过计算一个页面的「<strong>重要度</strong>」，从而判别网页质量，得到排序。</p><h2 id="基本思想">基本思想</h2><p>如何衡量「重要度」？用点击率、权威性？然而，这些数据都是爬虫无法爬取到的，同时也难以正确衡量。</p><p>科学家们从 Bibliometrics (文献计量学) 中借鉴了如下观点：</p><ul><li>Bibliographic coupling (引文耦合)：两篇文章具有相近的引用。</li><li>Co-citation (协同引用)：两篇文章被大量其他文章同时引用。</li><li>Impact factor(影响因子)：一个期刊中的文章的年平均被引次数，衡量了一个期刊的「<strong>重要度</strong>」。</li></ul><p>例如，最权威的 SCI往往只收录其认为重要的期刊，也只记录其中的期刊相互引用的次数。当一篇文章被SCI收录的文章引用时，通常也可以说明其有一定的影响力——即重要度的「<strong>传递</strong>」。</p><p>对于文献引用的可视化，我们通常称为 Citation Graph，通常是一个Directed Acyclic Graph(有向无环图，DAG)，因为较早的文章无法修改而引用后来的文章。</p><p>在网页中，我们可以 Hyperlinks (超链接) 来类比引用，从而形成一个Hyperlink Graph，区别是这类图中可以有环路。</p><p>从而引出网页排序的基本假设：</p><ul><li>The rank of a web page is higher if many pages link to it.</li><li>Links from highly ranked pages are given greater weight than linksfrom less highly ranked pages.</li></ul><h2 id="pagerank-algorithm">PageRank Algorithm</h2><p>基于上述假设，我们很容易可以得到当前页面的链入、链出数。但是，要怎么知道链入当前页面的<strong>前序页面</strong>，其重要度是多少呢？</p><h3 id="随机游走模型-random-walk-model">随机游走模型 | Random WalkModel</h3><p>在一个封闭的 Hyperlink Graph中，随机选取一个页面作为访问起点，随机选取其链出的页面进行访问，再对下一个页面重复上述操作。</p><p>大量重复后，统计每个结点被访问的频率，用频率近似概率后，我们可以发现访问概率较大者通常有着较多的链入，或者其链入页面也有着较大的访问概率。用公式表示就是：<span class="math display">\[\mathrm{Pr}\left( P_i \right) =\mathrm{Pr}\left( P_i\mid P_1 \right)\mathrm{Pr}\left( P_1 \right) +\mathrm{Pr}\left( P_i\mid P_2 \right)\mathrm{Pr}\left( P_2 \right) +\cdots +\mathrm{Pr}\left( P_i\mid P_N\right) \mathrm{Pr}\left( P_N \right)\]</span> 其中 $( P_iP_1 ) $​​ 表示从编号为 1 的网页跳转到编号为<em>i</em> 的网页的概率，其计算方式为： <span class="math display">\[\mathrm{Pr}\left( P_i\mid P_1 \right) =\begin{cases}    0\text{，如果}P_1\text{到}P_i\text{没有链入}\\    \frac{1}{m}\text{，}m\text{为}P_1\text{的链出数}\\\end{cases}\]</span></p><h3 id="矩阵表示-matrix-representation">矩阵表示 | MatrixRepresentation</h3><p>令 $w_i=( P_i ) $，则 <spanclass="math inline">\(\boldsymbol{w}=\left[ \mathrm{Pr}\left( P_1\right) ,\mathrm{Pr}\left( P_2 \right) ,\cdots ,\mathrm{Pr}\left( P_N\right) \right] ^T\)</span>，再令 $_i=$，则有：</p><p><span class="math display">\[w_{i}=B_i\cdot \boldsymbol{w}\]</span></p><p>特别地，当 <em>i</em> 取遍 1 到 N 的所有值后， 得到矩阵形式： <spanclass="math display">\[\boldsymbol{w}=B\cdot \boldsymbol{w}\]</span> 其中 <em>B</em>称作<strong>标准化链接矩阵</strong>，矩阵中的每个元素代表列号对应的 Page链入行号对应的 Page 的概率，每列之和为1。当一个页面没有链出时，这一列全为 0。</p><p>于是我们可以用<strong>迭代</strong>方法求解这个方程的稳定解 <spanclass="math inline">\(\boldsymbol{w}_k\)</span>​​​——即我们想求的访问概率向量，也就是<strong>重要度</strong>向量。只需要将<span class="math inline">\(\boldsymbol{w}_0\)</span>​​​ 设为全 1向量（因为一开始随机访问到每个页面的概率都相同），不断代入即可。</p><h3 id="阻尼迭代-pagerank-with-damping">阻尼迭代 | PageRank withDamping</h3><p>然而，现在存在的问题是，上面的所有推导都是建立在理想状态下的，即假设所有网页组成的这个有向图是<strong>强连通</strong>的。</p><p>当 Hyperlink Graph 存在 link loops(<strong>循环陷阱</strong>)，即存在一个小的子图，只有链入没有链出，所有随机游走的用户到了这几个网页后，就如同进了黑洞一般，一直在里面“打转”，出不来了。</p><p>这样就使得当游走次数趋于无穷时，最终陷阱中结点的访问次数远大于其他结点。这样会使得计算出的<span class="math inline">\(\boldsymbol{w}\)</span>向量中，陷阱外的结点访问概率都为 0。</p><p>PageRank 算法最终采用了<strong>阻尼因子</strong>（dampingfactor）的修正，使得进入陷阱后仍有机会跳出循环。 <spanclass="math display">\[\boldsymbol{w}_k=d\boldsymbol{w}_0+\left( 1-d \right) B\cdot\boldsymbol{w}_{k-1}\]</span> 其中 <span class="math inline">\(\boldsymbol{w}_0\)</span>​为全 1 向量，<em>d</em> 是实验确定的常数，通常取 0.15。</p><h3 id="结合相关度-combined-method">结合相关度 | Combined Method</h3><p>有了重要度向量后，当有查询时，我们只需要先确定<strong>命中文档</strong>（至少有一个term 与 query 相同的文档），再将其用重要度排序即可。</p><p>然而，这样做的缺点是，没有考虑到查询和文档的相关性——即，有可能一篇文档虽然有相同的term，但主题却相去甚远。</p><p>于是，有人提出了结合 Term Weighting 和 PageRank的方法，在确定命中文档后，利用传统的权重计算方法，计算出 query 和每个doc 的相似度 <span class="math inline">\(s_j\)</span>。再和重要度 <spanclass="math inline">\(p_j\)</span>​ 线性加权算出排序指标： <spanclass="math display">\[c_j=\lambda s_j+\left( 1-\lambda \right) p_j\]</span> 其中 <span class="math inline">\(\lambda\)</span>​为实验确定的常数。</p><h3 id="pagerank-算法缺点">PageRank 算法缺点</h3><ol type="1"><li><p>忽略了查询，则忽略了 query 和 doc主题相关性，导致结果的相关性降低。</p></li><li><p>没有过滤广告链接和功能链接（例如常见的分享链接）。这些链接通常没有什么实际价值，前者链接到广告页面，后者常常链接到某个社交网站首页。</p></li><li><p>对新网页不友好。因为即使是非常好的新页面也不会有很多链入，要成为一个高重要度的页面仍需要很长时间的推广。</p></li></ol><h2 id="主题敏感-pr-topic-sensitive-pagerank">主题敏感 PR |Topic-Sensitive PageRank</h2><p>在实际的网络中，PageRank算法还存在「<strong>主题漂移</strong>」问题，特别对于大量随意交互外链的站点，会导致搜索引擎返回主题无关结果。</p><p>同时，前面的讨论提到，PageRank 忽略了 query的主题相关性，也导致了结果的相关性降低。同一查询词在<strong>不同语境</strong>下，语义上指向的可能是不同的主题，但PageRank 无论如何都是返回「重要度」最高的页面。</p><p>理想情况下，应为每个用户的偏好维护一套专用的「主题重要度」向量，但面对海量用户这种方法显然不可行。所以搜索引擎一般会选择一种称为主题敏感的折中方案。</p><h3 id="基本思想-basic-idea">基本思想 | Basic Idea</h3><p><strong>基本假设</strong>：在 PageRank的随机游走模型中，用户倾向于选择具有<strong>同一个主题</strong>的链出网页。</p><p>基于这个假设，可以预定义几个话题类别，例如体育、娱乐、科技等等，对于某个网页来说，对应某个主题类型都有相应的PageRank分值，然后想办法关联用户的话题倾向，根据用户的话题倾向排序结果。</p><h3 id="矩阵形式-matrix-form">矩阵形式 | Matrix Form</h3><p>与原始的 PageRank 不同，新的算法对出度为 0的网站加以处理以保证<strong>收敛性</strong>。引入了向量 <spanclass="math inline">\(\boldsymbol{d}\)</span>​​​ 来指示某一个网页是否出度为0，若为 0 则对应项为 1。 <span class="math display">\[d_{i}= \begin{cases}1 &amp; \text { if } \operatorname{deg}(j)=0 \\ 0&amp; \text { otherwise }\end{cases}\]</span></p><p>向量 <span class="math inline">\(\boldsymbol{p}\)</span>来表示访问各个网页的概率均等，代替 <spanclass="math inline">\(\boldsymbol{w}_0\)</span>​ 的写法：</p><p><span class="math display">\[\boldsymbol{p}=\left[\frac{1}{n}\right]_{n \times 1}\]</span></p><p>两个矩阵的乘积所得的矩阵 <em>D</em> 表示出度为 0的网页将以均等概率访问其他网页。与前述提到的矩阵 <spanclass="math inline">\(B\)</span>​​具有互补的特性，补充了在随机游走模型中，一个网页出度为 0时的访问页面的情况。这样做使得最终矩阵的每一列之和都为 1。 <spanclass="math display">\[D=\boldsymbol{p} \times \boldsymbol{d}^{T}\]</span> 则最终排名的计算方法为： <span class="math display">\[\boldsymbol{Rank} =d \boldsymbol{p} + (1-d)(B+D)\cdot  \boldsymbol{Rank}\]</span></p><h3 id="偏置向量-odp-biasing">偏置向量 | ODP-Biasing</h3><p>主题的预定义参考了 <a href="www.dmoz.org">ODP</a> (Open DirectoryProject) 网站，利用 ODP 中 16 个顶级分类下的 URLs 生成了 16 组偏置PageRank 向量 (biased PageRank vectors)。</p><p>为了实现这一点，算法中采用了新的向量 <spanclass="math inline">\(\boldsymbol{v_j}=\boldsymbol{p}\)</span>，针对每个主题有：<span class="math display">\[v_{j i}=\left\{\begin{array}{cl}\frac{1}{\left|T_{j}\right|} &amp; i \in T_{j} \\0 &amp; i \notin T_{j}\end{array}\right.\]</span> 其中 <span class="math inline">\(T_{j}\)</span>​ 表示在契合第<em>j</em>个主题的网页集合。包含在这些网页中的页面被赋予较大的跳转概率值，而其他网站则相对减少。</p><h3 id="查询打分-query-time-importance-score">查询打分 | Query-TimeImportance Score</h3><p>此外，还需要在给定一个查询 <em>q</em>的时候，估算出该查询落在某个主题 <spanclass="math inline">\(c_j\)</span>的概率。文章使用了<strong>朴素贝叶斯分类器</strong>（naive-Bayesclassifier），将查询 <em>q</em> 中的每个 term 分词记作 <spanclass="math inline">\(q_i\)</span>，利用贝叶斯公式： <spanclass="math display">\[P\left(c_{j} \mid q\right)=\frac{p\left(c_{j}\right) \cdot P\left(q \midc_{j}\right)}{P\left(q\right)} \propto P\left(c_{j}\right) \cdot\prod_{i} P\left(q_{i} \mid c_{j}\right)\]</span> 而 <span class="math inline">\(P\left(q_{i} \midc_{j}\right)\)</span> 则容易用统计的方法估计出来，对于 <spanclass="math inline">\(P\left(c_{j}\right)\)</span>​则采用<strong>先验概率</strong>的方法，根据用户的查询历史（上下文）进行动态调整。</p><p>计算出了查询落在各个主题的概率后，再用这个概率对各个主题下的<strong><em>Rank</em></strong>向量进行线性加权，即可得到最终排序用的评分： <spanclass="math display">\[s_{q d}=\sum_{j} P\left(c_{j} \mid q^{\prime}\right) \cdot r_{j d}\]</span></p><h2 id="hits-hyperlink-induced-topic-search">HITS: Hyperlink-InducedTopic Search</h2><p>这里再介绍一种基础的网页排序算法——基于超链接追敏的主题排序，对于一个查询，不再返回单一的网页排名，而是同时返回两个列表：</p><ul><li>包含链接的 Hub 网页，收录了主题相关的权威网页链接。</li><li>包含内容的 Authority 网页，有着与主题相关的高质量内容。</li></ul><p>那么，如何排序这两个列表呢？</p><p><strong>基本假设</strong>：</p><ul><li>一个好的 Hub 网页指向该主题的许多 Authority 网页。</li><li>一个好的 Authority 网页被许多好的 Hub 网页指向。</li></ul><p>基于这两个假设，我们可以提出两个指标来衡量每个页面：枢纽值（HubScores）和权威值（AuthorityScores），这两种值是互相依存、互相影响的。</p><ul><li>枢纽值，指的是页面上所有出链指向页面的权威值之和。</li><li>权威值，指的是页面的所有入链所在的原页面的枢纽值之和。</li></ul><h3 id="算法步骤-hits-algorithm">算法步骤 | HITS Algorithm</h3><ol type="1"><li><p>找出 root set：根据用户 query 中的term，在文档集中找出包含至少一个 term 的的文档，使他们构成 rootset。</p></li><li><p>找出 <strong>base set</strong>：在 root set的基础上，找出集合中网页链入或链出并且不在 root set中的网页，并把他们加入到集合中，从而构成 base set。</p></li><li><p>计算每一个网页的枢纽值 h(x) 和权威值 a(x)，初始时，所有 h 值和 a值均为 1。</p></li><li><p><strong>迭代</strong>更新两个值直至收敛。为了防止两个值太大，可以在每次迭代后归一化。归一化的指标不重要，因为我们只关注相对排名。</p></li><li><p>返回两个值分别排序的列表。</p></li></ol><h3 id="hits-算法缺点">HITS 算法缺点</h3><ol type="1"><li>尽管限制了计算对象在 base set 中，但在线计算效率还是太低，不如 PR快。</li><li>主题漂移现象仍未解决。如果在集合里包含与查询主题无关的页面，且含有大量相互链接，可能会排到前列。这种现象被称为<strong>紧密链接社区</strong>现象。</li></ol>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>信息检索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IR学习笔记 #08 倒排索引模型</title>
    <link href="/IR-Note-8.html"/>
    <url>/IR-Note-8.html</url>
    
    <content type="html"><![CDATA[<p>文件组织架构，也称 index (索引)，常用于提升一个检索系统的性能。</p><p>回顾向量空间模型，我们知道在查询时，命中的 doc 应该是与 query最为相近的几个向量。当查询时，若只在所有<strong>可能相似的文档</strong>（至少含有一个query 中的关键词）中查找，可以大大减少资源浪费。</p><p>那么就需要先得到 query 中各个 term出现过的文档，再取<strong>并集</strong>，最后在并集中进行相似度的计算——「<strong>过滤</strong>」思想。</p><p>此时用特殊的索引方式，就可以更快地实现文档的过滤。有人提出 Hash的设想，但是 Hash 的缺点在于不能模糊匹配，当用户的 Query 和词典中的 term略有差距时，可能在 hash table 中会相距十分遥远。</p><h2 id="倒排文件-inverted-files">倒排文件 | Inverted Files</h2><p>我们通过一组对比，引入「<strong>倒排</strong>」的概念：</p><ul><li>正排索引：已知文档 doc，得到 doc 的 所有 term的位置序列，实现方式是「文档编号 + term 数组」</li><li>倒排索引：已知 term，找到包含 term 的文档 d1, d2, d3...实现方式是「term做键的字典，值是文档编号数组」</li></ul><p>由此我们可以得到倒排文件组织架构的<strong>构成</strong>：</p><ul><li>Indexfile：索引文件，以<strong>词典</strong>（Lexicon）的形式存储词项，链接到Posting file 的空间。</li><li>Postings file：记录文件，以倒排表的形式存储每个 term 对应在 Doc file中的df、tf、DocID、pos 等相关信息。</li><li>Document file：文档文件，存储所有文档。</li></ul><p>有了上述的架构，当用户输入 query 时，我们可以提取出term，直接访问对应的 Index file，再根据链接来到 Posting file。对于多个term，可以先完成交、并等逻辑运算，得到结果后，再去访问过滤后的文档集。</p><h2 id="构建倒排文件">构建倒排文件</h2><p>由此，我们可以知道当爬取到新的文档时，构建索引的步骤：</p><ol type="1"><li>Tokenizer：提取 token 流。</li><li>Linguistic modules：规范化，得到 term 集合。</li><li>Indexer：在对应的 term 键值下新增该文档的编号。</li></ol><h3 id="文档解析-document-parsing">文档解析 | Document Parsing</h3><p>接下来介绍搜索引擎如何解析一个新爬取到的文档，这个过程往往是离线进行的（在线进行的是用户查询过程）。</p><p>而由于文档的多样性，往往解析过程中会面临各式各样的问题：文件格式、各国语言、字符编码、停用词等。这些问题往往用<strong>启发式</strong>（heuristically）的方法解决。</p><ul><li>断词、标记化 | Tokenization</li></ul><p>Token来自文档的原始字符串，是根据空格划分提取出的原始单词。在实际中，要考虑：是否保留's 、是否保留连字符、专有名词是否拆开、数字嵌入等子问题。</p><p>而针对不同语言，也有更多新的问题：法语中大量的 '使用、德语中名词复合现象、中文日文不适用空格分词、日语的平假片假、阿拉伯语的书写次序等。</p><ul><li>停用词 | Stop words</li></ul><p>在文本中，往往还需要把最频繁出现的无意义词停用。在文档解析中，如何利用停用词进行压缩空间？在查询优化中，如何判别停用词？当停用词有意义时，如何识别？这些都是需要考虑的问题。</p><ul><li>标准化词项 | Normalization</li></ul><p>在英语中，通常时以定义「<strong>等价集</strong>」（equivalenceclassing）来归并词项。通常将单词归并到其原型，而对于特殊的单词有特殊的规则，例如规定“U.S.A.” 归并于 “USA”，规定 “anti-discriminatory” 归并于“antidiscriminatory”。</p><p>对于有的单词，不同形式可能含有不同语义，例如window/windows/Windows。此时在查询时可以先做<strong>不对称展开</strong>（asymmetricexpansion），对展开项搜索后取并集。</p><ul><li>辞典和探测法纠错 | Thesauri &amp; Soundex</li></ul><p>主要针对 Synonyms (同义词)、Homonyms(同形同音异义词)，这种情况下也可以利用等价集和不对称展开解决。</p><p>此外，当用户查询中有英文拼写错误时，常用的方法是 Soundex(探测法)，返回同音字串。Soundex是基于语音启发式方法生成的<strong>语音等价集</strong>。这种方法在汉语拼音中同样有很大应用。</p><ul><li>词干分析与词形还原 | Stemming &amp; Lemmatization</li></ul><p>将单词的名词、动词、形容词等形式统一归并到<strong>词根</strong>，将单复数、人称所有格、时态等统一归并到<strong>原型</strong>。</p><h3 id="文档文件-document-file">文档文件 | Document file</h3><p>解析完文档后，我们可以将新的文档直接存入文档集，也可以利用<strong>摘要生成</strong>技术生成Surrogates (文档替代品)，减少存储空间。</p><p>此外，当我们搜索到页面文档时，其文件格式可能各不相同，如 HTML、XML等，故检索到网页后还需要进行 Page Purifing(文档净化)，从而获得便于识别的文本文档和内部链接。</p><h3 id="记录文件-posting-file">记录文件 | Posting File</h3><p>之前的文章介绍过，用于连接 term 和 doc的词典表往往是个稀疏矩阵。而倒排文件用<strong>链表</strong>的形式存储每一行的内容，即包含此term 的所有 doc 及其基本信息，串接而成。链表中的每个元素称为一个 posting(记录)。</p><p>其中，基本信息可以包含：Document ID (文档的唯一标识)、LocationPointer (该文档在 Doc file 中的位置)、原始的权重因子。</p><p>存储原始的权重因子，是为了在查找的时候更方便的计算词项权重。可以包括df、tf、最大频度、总文档数等等。</p><p>此外，链表中的元素以 Doc ID排序，这样存储有利于多页倒排表的<strong>合并</strong>匹配。</p><h3 id="索引文件-index-file">索引文件 | Index File</h3><p>索引文件通常以词典的形式存储 term ID、含有该 term 的文档数以及该 term在记录文件中的位置（指针）。</p><p>以下列出几种常用的索引文件组织形式：</p><ul><li><p>Linear Index | 线性索引</p></li><li><p>Binary Tree | 二叉树</p></li><li><p>Right Threaded Binary Tree | 右索二叉树</p></li><li><p>B-trees | B树</p></li><li><p>B+-tree | B+树</p></li><li><p>Tries | 搜索树</p></li></ul><h2 id="特征选取-feature-selection">特征选取 | Feature Selection</h2><p>前文提到，在解析一篇文档获得索引时，最简单的方法就是先提取token，再获得 term 作为索引。而在真正高效的索引模型（IndexModel）中，往往要先对文档进行<strong>特征选取</strong>，从而构成索引。</p><p>而特征选择问题，可以转化为词项权重（termweighting）计算，一篇文档中权重较大的 term 往往更能表示这篇文档。</p><h3 id="词项频率-tf">词项频率 | TF</h3><p>在前面的文章中有提到，tf 及其衍生的权重计算方法，是 IR模型中最常用的权重计算方法。这里就不再重复介绍，仅提及一个有趣的定理<code>Zipf's Law</code>。</p><p>该定理描述了如下现象：在一个大的文档集中，统计出各个词项的 tf排名后，记排名为 <em>r</em>，频率为 <em>f</em>，则有 <spanclass="math display">\[f\cdot r\approx \mathrm{const}\]</span>而在实际中，排名最高的词项通常都是停用词，最「<strong>重要</strong>」的词往往词频不是很高，而最罕见的词往往没有普遍价值。这也与<em>tf·idf</em> 的思想契合，下图说明了这一点。</p><p>在倒排文档中，移除停用词和罕见词、保留重要词，可以节约大量的记录空间。</p><p><img src="/img/blog/IR-Note-8-images/index_tf.png" alt="词项频率与重要度的关系" style="zoom:67%;" /></p><h3 id="索引规模-index-scale">索引规模 | Index Scale</h3><p>对于一个确定大小的文档集，需要多少词项才能很好的索引全部文档呢？这便是根据文档集大小确定词典大小（LexiconSize）的问题。<code>Heap's Law</code> 对此进行了估算： <spanclass="math display">\[n=|V|=K \cdot N^{\beta} \text { with constants } K, 0&lt;\beta&lt;1\]</span> 其中，<em>K</em> 通常取 10 到 100 间的整数，<spanclass="math inline">\(\beta\)</span>​​​ 通常取 0.4 到 0.6之间的小数。绘制出的图如下：</p><p><img src="/img/blog/IR-Note-8-images/index_scale.png" alt="索引规模的变化曲线" style="zoom:50%;" /></p><h3 id="词项判别模型-term-discrimination-model">词项判别模型 | TermDiscrimination Model</h3><p>在一个向量空间中，文档由<strong>基向量</strong>加权构成的向量表示。</p><p>我们可以计算文档之间的相似度，相似度越高，代表空间越紧凑，反之则越松散。计算文档集两两之间的相似度需要<span class="math inline">\(O(n^2)\)</span> 的复杂度。</p><p>当然，如果先计算出一个「<strong>平均文档</strong>」，再计算其他文档与其的相似度，则只需要<span class="math inline">\(O(n)\)</span>​ 的复杂度。</p><p>词项判别模型则是通过<strong>引入</strong>一个新的 term作为基向量，观察相似度的变化分析该 term 的重要性。大致的思想是：</p><ul><li>如果一个 term 引入后，向量空间变松散了，则说明这个 term有效的区分了不同文档，这个词通常是<strong>中频词</strong>（重要词）。</li><li>如果一个 term 引入后，向量空间没有变化，则说明这个 term没有太大价值，这个词通常是<strong>低频词</strong>（罕见词）。</li><li>如果一个 term 引入后，向量空间变紧凑了，则说明这个 term将文档同一化了，这个词通常是<strong>高频词</strong>（停用词）。</li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>信息检索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IR</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IR学习笔记 #07 IRLbot</title>
    <link href="/IR-Note-7.html"/>
    <url>/IR-Note-7.html</url>
    
    <content type="html"><![CDATA[<p>本文是 WWW2008 最佳论文「IRLbot: Scaling to 6 Billion Pages andBeyond」的阅读报告。相关领域：网络信息检索。</p><h2 id="摘要">摘要</h2><p>随着验证 URL 唯一性的复杂度平方增长、BFS爬行顺序和固定的每主机速率限制，现有的爬虫算法不能有效地应对在大型爬虫中产生的大量URL、高度分枝的垃圾页面（highly-branchingspam）、数百万页博客站点以及服务器端脚本（server-sidescripts）产生的无限循环。作者提供了一组处理这些问题的技术，并在称为IRLbot 的实现中测试它们的性能。</p><p><strong>关键词</strong>：IRLbot, large-scale, crawling</p><h2 id="问题提出">问题提出</h2><p>作者称本文不是针对数据挖掘器，而是致力于设计能够适应<strong>当前和未来网络规模</strong>的网络爬虫：从一个给定的seed URL集合开始，递归地访问集合中的页面，并在这个过程中动态地改变下载顺序，最终可以下载到所有有用的网页；同时，在下载的过程中，无论规模多大，都应当保持一定的速度。</p><p>当然，还有以下考虑：爬虫需要限制对于单一网站、单一服务器的访问频率（礼貌策略），避免陷入垃圾网站和服务器端脚本产生的无限循环。</p><p>具体而言，作者提出了三类问题：</p><h3 id="规模扩展问题-scalability">规模扩展问题 | Scalability</h3><p>每个爬虫系统都必须面对一个固有的取舍：在处理规模（<strong>scalability</strong>）、性能（performance）和硬件资源使用（resourceusage）三者中做出权衡。</p><p>一般来说，较大的规模将导致较低的性能与较高的资源使用，较高的性能需要降低规模与增加资源使用。因而，大多数爬虫只能兼顾三者之二（大型慢速爬虫、小型快速爬虫，大型快速却需要占用大量资源的爬虫）。</p><p>本文希望在给定性能标准和硬件资源的情况下，研究规模的扩展问题。</p><h3id="网站信誉与垃圾网站问题-reputation-and-spam">网站信誉与垃圾网站问题 |Reputation and Spam</h3><p>与早期的网络相比，如今的网络已发生了很大变化，主要是在服务器端脚本生成的动态网站和垃圾网站两个方面。二者性质不同，却都给爬虫带来了一个新的挑战：必须要有一种在爬虫爬取网页的过程中实时决定<strong>哪些站点包含有用信息</strong>、实时<strong>决定爬取优先级</strong>的方法。</p><p>因为，传统的广度优先搜索往往会由于以下原因而降低效率：</p><ul><li>来自垃圾网站 URL 分支过多，甚至可能取代合法的 URL；</li><li>单个域名中动态创建新主机名，DNS 解析不及；</li><li>来自网页的延迟攻击，故意在来自爬虫程序 IP 地址的所有请求中引入 HTTP和 DNS 延迟。</li></ul><h3 id="礼貌问题-politeness">礼貌问题 | Politeness</h3><p>网络爬虫对某一服务器的频繁访问，往往会对服务器的正常性能造成影响，因而也容易招致服务器的拒绝访问或是举报、诉讼，因而需要对爬虫设置一定速度的限制。</p><p>直接给爬虫设置这种对单一服务器、单一 IP地址的访问速度限制并不复杂，却容易导致爬虫的效率在特定情况下（待爬取的URL 只来自于极少的几个服务器或IP，由于限制不得不减慢速度）极大地降低效率。</p><p>因而需要设计一种可以避免这种情况的发生的爬虫。</p><h2 id="解决规模扩展问题">解决规模扩展问题</h2><h3 id="磁盘检查算法-disk-check-algorithms">磁盘检查算法 | Disk-checkAlgorithms</h3><p>规模问题最终体现在使用 <code>URLseen</code> 确认 URL 的唯一性和使用<code>RobotsCache</code> 检查 robot.txt 的符合上。此外，还要将新的 URL再传递给 <code>URLseen</code> ，以及在必要时更新<code>RobotsCache</code>。主要牵涉到磁盘与存储器的交互。</p><p>在先前的方法中，无论是使用 RAM 散列存储的<strong>Mrcator-B</strong>，还是使用内存中二叉搜索树的<strong>Ploybot</strong>，随着爬取规模的增大，执行这一步骤的开销都会快速增长。为了降低这一开销，需要一种更有效的数据存储结构。</p><h3 id="drum---disk-repository-with-update-management">DRUM - DiskRepository with Update Management</h3><p>论文中提出了 DRUM 技术，这个技术结合了桶排序和哈希算法。</p><p>DRUM 的目的是允许高效地存储大量 &lt;key, value&gt; 对的集合，其中 key是某些数据的唯一标识符（hash），value是附加到密钥的任意信息。通过这样的设计，可以实现对大规模键值对数据的存储，并实现快速的检查（check）、更新（update）、检查＋更新（check+update）的操作。</p><p>下图展示了 DRUM 的操作流程：</p><p><img src="/img/blog/IR-Note-7-images/DRUM.png" alt="DRUM" style="zoom:50%;" /></p><p>在该图中，一个连续的元组 &lt;key, value, aux&gt; 流到达了 DRUM，其中aux 是与每个键相关联的一些辅助数据。随后被分割为 &lt;key, value&gt; 与aux 两个部分，分入内存中的各桶，并在一次操作中将所有的桶中的 &lt;key,value&gt; 与磁盘存储阵列中的数据进行合并。此外，通过读取 cache<em>Z</em> 中的 <span class="math inline">\(\varDelta\)</span>字块，并与桶中的 key 比较，可以确定其唯一性。</p><p>利用这一技术，在论文中的爬虫系统创建了多个存储模块，包括<code>URLseen</code> 模块、<code>RobotsCache</code>模块、<code>RobotsRequested</code> 模块、<code>PLDindegree</code>模块，分别赋予元组一定的意义，赋予各模块特定的操作，以此大大提高系统规模化的效率。</p><h3 id="性能分析">性能分析</h3><p>论文中主要通过给定一系列参数来推导 <code>URLseen</code>的开销，从而比较各种数据结构的优劣。参数定义及推导过程详见论文。</p><p>Mrcator-B： <span class="math display">\[\omega(n, R)=\frac{2(H+P) p H}{R} n^{2}\]</span> Ploybot： <span class="math display">\[\omega(n, R)=\frac{2(b+4 P) p b q}{R} n^{2}\]</span> DRUM：（论文中公式有两种情况） <span class="math display">\[\omega(n, R)=n b\left(\frac{(H+b)(2 U H+p H n)}{b D}+2+p+\frac{2H}{b}\right)\]</span> 并做出了如下比较：</p><p><img src="/img/blog/IR-Note-7-images/DRUM2.png" alt="DRUM和其他模型的比较" style="zoom:67%;" /></p><p>可以看出 DRUM模型的效率远超过原来两个模型。此外论文中还针对磁盘性能和平均爬虫效率（下载速率）做出了比较，这里不再赘述。</p><h2 id="解决垃圾网站问题">解决垃圾网站问题</h2><h3 id="计算站点信誉-computing-domain-reputation">计算站点信誉 |Computing Domain Reputation</h3><p>拥有大量动态网页的合法网站与制造大量垃圾网页的恶意网站（quicklybranching site），都使得爬虫在礼貌性原则、DNS查找以及爬取本身的限制下变得低效，也会浪费带宽下载许多无用的内容。</p><p>然而，由于互联网规模不断扩大，拥有同样有用大量网页的合法网站与恶意网站相互混杂，使得简单的<strong>限制分支因子</strong>或限制<strong>每个域名的最大pages/hosts数量</strong>，并不能合理的解决这个问题。而在之前的研究中，无论是 BFS的爬取策略，还是 PageRank、BlockRank、SiteRank算法，也极易使爬虫陷入到这种海量网站中。</p><p>事实上，严格的<strong>页面级排名</strong>对于控制大量分支垃圾网站并不是绝对必要的。作者发现可以通过根据<strong>域名信誉</strong>来判断网页类型、决定对某一域名的网站搜索的深度，域名信誉是根据垃圾网站必须付费的域名资源程度来确定的。</p><h3 id="star---spam-tracking-and-avoidance-through-reputation">STAR -Spam Tracking and Avoidance through Reputation</h3><p>论文指出，只要在「域名信誉」的基础上给每个 PLD (Pay-Level Domain)分配「<strong>预算</strong>」，即可侦测出垃圾网站。论文在 PLD这个较粗的粒度上进行“预算”的计算，流程如下图：</p><p><img src="/img/blog/IR-Note-7-images/STAR.png" alt="STAR" style="zoom:50%;" /></p><p>利用 DRUM 的存储结构，存储爬虫在爬去过程中得到的 PLD网络图的信息，构造 <code>PLDindegree</code>模块。通过模块中考察域名的链入数，为各个域名动态地分配“预算”，并按照“预算”指示单位时间内爬虫能够从该域名爬取多少新的链接，最终避免垃圾网站的困扰。</p><h3 id="性能分析-1">性能分析</h3><p>从理论上说，从其他 PLD获得链入需要付出（金钱）代价，一般的垃圾网站在代价面前很可能不会获得高的“预算”，故使用这一方法来鉴别网站的质量。</p><h2 id="解决礼貌问题">解决礼貌问题</h2><h3 id="速率限制-rate-limiting">速率限制 | Rate Limiting</h3><p>IRLbot从一开始的主要目标之一就是在访问配置不佳（就带宽或服务器负载而言）的站点时遵守严格的速率限制策略。尽管较大的站点更难崩溃，但不受限制的访问频率往往也会被视为DOS 攻击。</p><p>而在之前的研究中，简单地设置单个主机的访问延迟，可能会导致“多主机共用”的<strong>托管服务器</strong>崩溃，若设置单个服务器的访问延迟，又将大大降低效率，甚至可能在大规模的网页中最终无法正常工作。</p><p>另一方面，在已经得到各网站“预算”的情况下，若仅仅只是重复地扫描未爬取的链接队列并从中选取需要爬取的链接，只能在高昂的花费下得到极少的有用链接。因而需要想办法更有效地利用“预算”的结果给出爬取各网页的延迟，才能实现高效的爬虫。</p><h3 id="beast---budget-enforcement-with-anti-spam-tactics">BEAST -Budget Enforcement with Anti-Spam Tactics</h3><p>此方法的目标不是丢弃URL，而是<strong>延迟</strong>它们的下载，直到更多地了解它们的<strong>合法性</strong>。</p><p>大多数网站的排名较低是因为它们没有权重高的链入，这并不一定意味着它们的内容是无用的，或者它们属于垃圾网站。在所有其他条件相同的情况下，排名较低的域名应该以某种近似循环（round-robin）的方式爬取，并谨慎控制它们的分支。</p><p>此外，随着爬取的进行，域名会改变它们的声誉，而先前未通过预算检查的URL 需要重新计算预算，并且以不同的速率爬取。</p><p><img src="/img/blog/IR-Note-7-images/BEAST.png" alt="BEAST" style="zoom:50%;" /></p><p>如图所示，在经过修正之后，论文给出了一种不需要依赖数据规模增大硬盘读写能力的实现方式:</p><p>将通过 STAR赋予了一定预算的成批链接进行检查，将通过检查的、有较高预算的链接按照预算排名高低分到<em>j</em>个队列中，将暂时未通过检查的、只有排名的链接分到一个单独的队列 <spanclass="math inline">\(Q_F\)</span> 中；当前 <em>j</em>个队列中的链接全部爬取完成后，重新检查队列 <spanclass="math inline">\(Q_F\)</span>，并将其中通过检查的链接分到已有队列的两倍数量的队列中。</p><p>不断重复上述过程，不断动态地增加队列的数量，直至达到某些停止条件。用这种办法可以合理地决定网页的爬取顺序。</p><h3 id="性能分析-2">性能分析</h3><p>采取 BEATS 的办法，一方面保留了队列的不同优先级，使得队列 <spanclass="math inline">\(Q_F\)</span>中具有较高预算的链接可以尽快地得到爬取；另一方面利用不断增长的<em>j</em>使得预算较低的链接不断地推迟被爬取的时机，实现爬取的延迟。</p><h2 id="实验验证">实验验证</h2><h3 id="模型构建">模型构建</h3><p>在充分整合 DRUM, STAR, BEAST技术之后，论文搭建形成了如图所示的爬虫系统 IRLbot。</p><p><img src="/img/blog/IR-Note-7-images/IRLbot.png" alt="IRLbot" style="zoom:50%;" /></p><p>并从 crawling threads 得到新链接开始，涉及到 URL唯一性确认，“预算”的确认，robot.txt的确认，“预算”的检查以及最终页面的下载，形成了一个完整的处理流程。</p><h3 id="效率验证">效率验证</h3><p>论文中提到，在 2007 年 6 月 9 日至 8 月 3 日的这段时间，IRLbot运行在单一服务器上，并以 1GB/s 的速度连接互联网。最终在 41天的运行过程中，爬取了 63 亿的页面。</p><p>此外，通过分析排名最高的 1000个网站，发现其中的大部分网站都非常权威知名，这也说明了 STAR信誉计算的有效性。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>信息检索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IR学习笔记 #06 网络信息检索</title>
    <link href="/IR-Note-6.html"/>
    <url>/IR-Note-6.html</url>
    
    <content type="html"><![CDATA[<p>随着互联网的兴起，Web 的增多，网络信息检索成为 IR 中的一大主题。</p><p>简要介绍以下几个相关概念：</p><ul><li>最广的是 Network ，一个物理层面的<strong>广义网络</strong>。</li><li>其次，是互联网。因特网和其他类似的由计算机相互连接而成的<strong>大型网络系统</strong>都可算是互联网，Internet是互联网中最大的一个。</li><li>再者，是Internet，即因特网。由许多小的<strong>子网</strong>互联而成的一个逻辑网，每个子网中连接着若干台计算机（主机）。</li><li>最后，是 Web，也称 World WideWeb，即万维网。它使用超文本技术将遍布全球的各种信息资源链接起来，以便于用户访问。Web只是 Internet 上的一个<strong>应用层服务</strong>。</li></ul><h2 id="网络信息检索-web-search">网络信息检索 | Web Search</h2><p><strong>Goal</strong> | 目标</p><p>Provide information discovery for large amounts of open accessmaterial on the web.</p><p><strong>Challenges</strong> | 挑战</p><ul><li><p>Volume of material -- several billion items, growingsteadily</p></li><li><p>Items created dynamically or in databases (deep web, about 150times of web pages of surface web)</p></li><li><p>Great variety -- length, formats, quality control, purpose,etc.</p></li><li><p>Inexperience of users -- range of needs</p></li><li><p>Economic models to pay for the service --订阅、广告、许可</p></li></ul><p><strong>Strategies</strong> | 策略</p><ul><li>Subject hierarchies (<strong>分类目录</strong>) + human indexing --1st Generation</li><li>Web crawling (<strong>网页爬取</strong>) + automatic indexing -- 2ndGeneration</li><li>Human directed web crawling and automatic indexing -- Mixedmodels</li></ul><p><strong>Components</strong> | 组成</p><ul><li>Web crawler: URL Server + <strong>Crawler</strong> + StoreServer</li><li>Indexing system: URL Resolver + Indexer + <strong>Pagerank</strong>(离线网页排名算法)</li><li>Search system: Sorter + Searcher (在线检索服务)</li></ul><h2 id="网络爬虫-web-crawler">网络爬虫 | Web crawler</h2><p>Web crawler，也称 Web spider，用于下载网页的一种程序。只要给出 seedURLs (Uniform Resource Locator)的初始集，就可以<strong>递归地</strong>（recursively）根据集合中的链接下载更多的页面。有两种特殊的爬虫：</p><ul><li><p>Focused webcrawler，针对特定类别的网站的专业爬虫，需要分类方法支持。</p></li><li><p>Deep webcrawler，针对动态网页的深网爬虫，需要脚本模拟动作支持。</p></li></ul><p>对于所有的爬虫，最重要的是抓取一个页面中的链接，扩充初始集。</p><p>此外，一个爬虫，还要考虑性能（爬取大量页面）、礼貌性（避免过载服务器、违法操作）、应对故障（破损链接、超时、爬虫陷阱）、搜索策略（DFS/BFS），存储网页（并行文件系统）等。</p><h3 id="礼貌性-politeness">礼貌性 | Politeness</h3><p>「恶意爬虫」往往会在短时间内大量访问同一个站点，造成 DDoS攻击（Distributed denial of serviceattack，分布式拒绝服务攻击），进而导致网站的瘫痪。此外，还存在非法爬取私人信息、非法收集数据等行为。</p><p>为了在法律上限制爬虫，有以下的协议：</p><p><strong>Robots ExclusionProtocol</strong>：网站管理者可以注明该网站的哪些路径是不可被爬虫访问的，这些协议会体现在http://.../robots.txt 中。</p><p><strong>Robots META tag</strong>：HTML作者可以注明该页面中的文件不可被索引，或该页面不可被用于解析以获得更多链接。只需要在HTML 文本中添加以下命令：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs HTML"><span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;robots&quot;</span> <span class="hljs-attr">content</span>=<span class="hljs-string">&quot;noindex, nofollow&quot;</span>&gt;</span><br></code></pre></td></tr></table></figure><h3 id="爬虫性能-performance">爬虫性能 | Performance</h3><p>爬虫面向的信息往往是极庞大的，超过了一台机器的性能范围。现实中，通常采用<strong>并行分布式</strong>爬虫，将任务量划分到各台机器。那么如何分配任务才能使得各台机器承受的压力均匀呢？</p><h4 id="distributing-the-workload">Distributing the Workload</h4><p>首先可以将机器编号 0 至 N-1，再对一个 URL 的主域名（Domainname）做<strong>哈希</strong>，得到一个 0 至 N-1的值，并分配到对应机器。这样做的好处有：</p><ul><li>同一域名只在一台机器上访问，这样就可以防止多台机器<strong>同时访问</strong>了同一域名（避免对其造成DoS 攻击）。</li><li>不需要主服务器的分配，减少了机器间的沟通，主域名下的子域名全在同一台机器中。</li><li>每台机器有独立的 DNS cache (域名缓存)，可以提高查询效率。</li></ul><h4 id="software-hazards">Software Hazards</h4><p>此外，要提高爬虫的性能，还要实现软件故障的处理：</p><ul><li>过慢、无应答的 DNS/HTTP 服务器</li><li>过大、无限大的页面（自动填充型）</li><li>无限的链接（随时间变化的路径）</li><li>破损的 HTML 页面</li></ul><h4 id="extract-links">Extract Links</h4><p>抓取页面中的链接、解析页面中的链接也会遇到许多难题：</p><ul><li>相对路径、绝对路径</li><li>CGI (Common Gateway Interface, 公共网关接口) 动态生成的页面</li><li>Server-side 服务端脚本</li><li>隐藏在 JavaScript 代码中的链接</li></ul><h2 id="爬虫架构-crawler-architecture">爬虫架构 | CrawlerArchitecture</h2><p>接下来介绍一种经典的<strong>分布式</strong>爬虫架构<code>High performance large scale web spider architecture</code>。</p><p><img src="/img/blog/IR-Note-6-images/WebCrawler1.png" alt="爬虫架构" style="zoom:67%;" /></p><ul><li><strong>URL Manager</strong></li></ul><p>存放所有访问过的 URL 记录，以及待访问队列。</p><ul><li><strong>Pool of data collector</strong></li></ul><p>数据收集池，按照 URL Manager分配的序列访问网页，内部通常由多台机器多线程地收集网页数据。</p><ul><li><strong>Set of filters</strong></li></ul><p>实现数据统计、页面索引、链接提取的功能，并返回新的连接到 URLManager。</p><ul><li><strong>Storage manager</strong></li></ul><p>负责压缩、解压缩、存储、检索数据。</p><ul><li><strong>Communicator</strong></li></ul><p>通信器，将新找到的页面和找到它的页面连接，完成 URL 去重等任务。</p><figure><img src="/img/blog/IR-Note-6-images/WebCrawler2.png" alt="工作流程" /><figcaption aria-hidden="true">工作流程</figcaption></figure><h3 id="url-队列-url-frontier">URL 队列 | URL Frontier</h3><p>URL Frontier 维护了一个包含大量 URL 的队列，并且每当有爬虫线程寻找URL 的时候，它都会按照某种顺序重新排序。以何种顺序返回队列中的URL，需要有两个方面的考虑：</p><ul><li>第一个要考虑的是具有很高更新频率的高质量页面，即<strong>页面的优先级</strong>。一个页面的优先级权值应该是由它的改变频率和它本身网页质量（使用一些恰当的质量评估方法）共同决定的。</li><li>第二个要考虑的就是<strong>礼貌策略</strong>：我们必须避免在很短的时间间隔内重复抓取同一个主机。因此，如果URL队列被设计成简单的优先级队列的话，可能会造成对某一主机的大量的访问请求。</li></ul><figure><img src="/img/blog/IR-Note-6-images/URL_Frontier.png"alt="URL队列工作流程" /><figcaption aria-hidden="true">URL队列工作流程</figcaption></figure><p>上图展示了一个基于<strong>礼貌性和权值策略</strong>的URL队列的实现。它的目标是确保：</p><ul><li>每次只有一个连接去访问一台主机；</li><li>连续对同一个主机的访问请求之间存在几秒钟的等待时间；</li><li>具有高优先级的页面将会被优先爬取。</li></ul><p>其中有两个重要的子模块，前部分的 Front 队列集合 F，以及后部分的 Back队列集合 B。这两种队列均是 FIFO 队列。</p><p>Front 队列实现了对权值相关处理，而 Back队列实现了对礼貌策略的相关处理。在一条 URL被添加到队列的过程中，它将会先后穿越 Front 和 Back 队列。</p><p>首先，权值计算器会给该 URL 分配一个介于 1 和 F之间的整数权值，再进入相应的 Front队列，具有<strong>很高更新频率</strong>的文档（如新闻页面）将会被赋予一个很高的权值（通过启发式方法）。而后高权值对应的Front 队列也会更高频率的吞吐 URL。</p><p>此外，我们需要维护一个<strong>堆</strong>，堆里存放着的条目对应每一个Back队列，该条目记录着该队列所对应的主机可以再次被连接的最早时间。注意：每个队列仅对应一个主机，即满足分布式的要求。</p><p>请求获取 URL的爬虫线程会抽取出<strong>堆顶元素</strong>（时间最早者），然后一直等到相应时间后访问之。从而避免访问频率过高。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>信息检索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IR学习笔记 #05 检索系统评价</title>
    <link href="/IR-Note-5.html"/>
    <url>/IR-Note-5.html</url>
    
    <content type="html"><![CDATA[<p>前述文章介绍了几种基本信息检索模型，本文将介绍如何评价一个现有的文档检索系统。</p><h2 id="evaluation-in-document-retrieval">Evaluation in DocumentRetrieval</h2><p>一个检索系统的好坏，通常取决于其检索结果与用户查询的相关性，此外还有检索用时、检索范围等等。这里仅针对评价相关性展开讨论。</p><h3 id="相关性-relevance">相关性 | Relevance</h3><p>如何度量相关性？考虑如下三个待实现的要素：</p><ul><li><p>A <strong>benchmark</strong> document collection(基准文档集)</p></li><li><p>A <strong>benchmark</strong> suite of queries(基准查询集)</p></li><li><p>A <strong>usually binary assessment</strong> of either Relevantor Nonrelevant for each query and each document(对基准查询结果打分)</p></li></ul><p>当然，这个「打分标准」可能会随每个人的<strong>信息需求</strong>而变化（theinformation need is <strong>translated</strong> into aquery），因此这个指标不是确定的（more than binary）。</p><p>有了以上三个基本要素，我们就可以构造出一个合理的<strong>测试集</strong>：包含文档集、查询集和有关评价机制。</p><h3 id="测试集-test-collections">测试集 | Test collections</h3><p>在制定测试集的时候，往往要先标注好相关的「查询-文档」对。对于小的测试，可以采用人工标注（遍历文档集和查询集）。</p><p>但对于较大的测试集则不行（如 TREC测试集）。此时，可以采用如下方法：</p><ul><li>Pooling | <strong>池化</strong></li></ul><p>直接用已有的几个检索系统在「总的基准文档集」中检索，取出每个检索的前n个结果，取<strong>并集</strong>，用这个「新的集合」作为「模拟基准文档集」进行标注，这样就可以大大减少范围。</p><ul><li>Sampling | 抽样</li></ul><p>可以通过随机抽样估计真实相关集的大小。</p><ul><li>Search-based</li></ul><p>与其阅读所有的文档，不如人工用较宽泛的 Query先得到一些检索结果，再在这些结果中标记。</p><h2 id="有效性度量-effectiveness-measures">有效性度量 | Effectivenessmeasures</h2><p>有了合理的测试集，只需要用待测试 IR查询「基准查询集」的内容，对查询结果与「查询-文档」对比较，即可得到有效性度量。</p><p>以下介绍两个在度量有效性过程中常用的变量。</p><h3 id="精确率和召回率-precision-and-recall">精确率和召回率 | Precisionand Recall</h3><p>在检索结果的 Top n 中，我们定义如下变量：</p><p>Precision (精确率): Proportion of a retrieved set that isrelevant.</p><p>Recall (召回率): Proportion of all relevant documents in thecollection included in the retrieved set.</p><p>与这两个概念相关的还有 Miss (漏识率) 和 Fallout (误报率)。</p><p>对应的混淆矩阵（Confusion Matrix）如下表：</p><table><thead><tr class="header"><th style="text-align: center;">/</th><th style="text-align: center;">相关</th><th style="text-align: center;">不相关</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">检索到</td><td style="text-align: center;">A</td><td style="text-align: center;">B</td></tr><tr class="even"><td style="text-align: center;">未检索到</td><td style="text-align: center;">C</td><td style="text-align: center;">D</td></tr></tbody></table><p><span class="math display">\[\text{精度}=\frac{A}{A+B}, \text{召回率}=\frac{A}{A+C},\text{漏识率}=\frac{C}{A+C}, \text{误报率}=\frac{B}{B+D}\]</span></p><p>这样的计算过程没有考虑到检索结果的顺序，事实上相关文档排在前列的搜索引擎才是我们最需要的。</p><h4 id="有序检索-ranked-retrieval">有序检索 | Ranked retrieval</h4><p>考虑搜索引擎返回的结果是有序的，取 Top n，则计算 P/R的方法可以加以修正：</p><p>对检索到的文档按照 ranking 排列，顺次计算 P/R，每次计算时考虑前 k个文档。最后会得到一组 n 个 P/R 值，再对 Top n 中的「相关文档」对应的Precision 取平均。</p><p><img src="/img/blog/IR-Note-5-images/PR_ranking1.png" alt="同一关键词的查询结果" style="zoom:67%;" /></p><p><img src="/img/blog/IR-Note-5-images/PR_ranking2.png" alt="平均值计算" style="zoom:50%;" /></p><p>上图中，我们对搜索引擎 A 和搜索引擎 B 查询了同一关键词，并取了 Top 10的查询结果，其中各有 5 篇相关文档，经过计算可发现，A的检索结果更优。</p><p>但是，如果我们要对同一个搜索引擎 A 用不同的关键词来查询呢？</p><p><img src="/img/blog/IR-Note-5-images/PR_ranking3.png" alt="不同关键词的查询结果" style="zoom:67%;" /></p><p>对于不同的 query 可能 Top n 中有数量不同的相关文档，此时的 Recall就会不一致。如果我们要计算同一 Recall值处的精度，则需要用到插值方法。</p><h4 id="跨查询平均-averaging-across-queries">跨查询平均 | Averagingacross queries</h4><p>仅用个别的 query 难以在数据巨大的文档集中得到准确的 P/R值。因此需要考虑更多的 query，并对结果再次平均。</p><p>由此，引出两种平均的思想：</p><ul><li>Micro-average (<strong>微平均</strong>): each relevant document is apoint in the average. 只针对该搜索引擎下一个 query的命中结果，求出平均精度。</li><li>Macro-average (<strong>宏平均</strong>): each query is a point inthe average (Most Common). 针对该搜索引擎下的许多 query的微平均精度，再求总的平均精度。</li></ul><p>做宏平均的过程中，最重要的是将所有 query视作平等的点。因为在微平均的过程中，我们往往只关注一些大样本、常见样本，而这些样本并不能完全体现搜索引擎的性能。而宏平均关注其他小样本、偏僻样本，这些样本的检索结果体现了搜索引擎内部的类别分布是否均匀。</p><p>这种方法也称作 MAP (<strong>Mean AveragePrecision</strong>)，平均之上的平均。</p><h4 id="绘图-recallprecision-graphs">绘图 | Recall/Precision graphs</h4><p>如果只关注平均精度，则会隐藏检索结果的一些有效信息。如果用图表的形式呈现，则更能观察到趋势。</p><p>如果直接把 ranked retrieval的结果画在图中，会得到一条「<strong>锯齿状</strong>」的曲线。因为在同一个召回率下，随着结果数的增长，精度是垂直向下的。</p><p><img src="/img/blog/IR-Note-5-images/PR_graph1.png" alt="锯齿状的PR图" style="zoom: 80%;" /></p><p>此时，如果我们想要关注曲线中的：</p><ul><li>特定召回率（10%、20%等）下的精度</li><li>零召回率（系统尚未返回结果）下的精度</li></ul><p>由于各个 query对应的相关文档总数不同，观测到的召回率点也不同。此时就需要对离散的点用interpolate(插值)，做出连续的曲线，才能确定这些点的精度。接下来讨论如何选取适合的插值方法。</p><blockquote><p>直接连接各点？连接最大值？连接最小值？连接平均值？</p><p>零召回率时假设为零？假设为最大精度？假设为平均精度？与起始点相等？</p></blockquote><p><strong>基本原则</strong>：从<strong>平均</strong>来看，随着召回率的增加，精度应该是单调递减的。</p><p>基于这个原则，可以得到 <span class="math display">\[P(R)=\max \left\{P^{\prime}: \quad R^{\prime} \geq R\wedge\left(R^{\prime}, P^{\prime}\right) \in S\right\}\]</span>即：选取「当前区间」最大的精度点，再以「召回率大于该点的区间」为「新区间」，选取最大的精度点，迭代至100%。</p><p>最后用「<strong>阶梯状</strong>」曲线连接以上各点，可以得到单调递减的曲线。</p><p><img src="/img/blog/IR-Note-5-images/PR_graph2.png" alt="阶梯状的PR图" style="zoom:67%;" /></p><h3 id="e-and-f">E and F</h3><p>综合考虑 P/R 值，可以计算出如下<strong>单值评价指标</strong>。</p><h4 id="e-measure">E Measure</h4><p>用于强调精度或召回率中的某一个指标，同时兼顾另一个指标。</p><p><span class="math display">\[E=1-\frac{1}{\alpha \frac{1}{P}+(1-\alpha) \frac{1}{R}}\]</span> 根据 <span class="math inline">\(\alpha\)</span> 的取值，增大<span class="math inline">\(\alpha\)</span>代表强调精度的重要性，反之强调召回率。</p><p>令 <span class="math inline">\(\alpha =\frac{1}{\beta ^2+1}\)</span>，可以得到 <span class="math display">\[E=1-\frac{\left(\beta^{2}+1\right) P R}{\beta^{2} P+R}\]</span> 当 <span class="math inline">\(\beta = 1\)</span>时可得到二者相同重要性的效果，此时的 <spanclass="math inline">\(E\)</span>具有的<strong>物理意义</strong>是所有相关文档 <spanclass="math inline">\(A+C\)</span> 和所有检索到文档 <spanclass="math inline">\(A+B\)</span>的集合的<strong>对称差</strong>的基数除以两个集合的基数。</p><h4 id="f-measure">F Measure</h4><p>将 <span class="math inline">\(E\)</span> 取补，可以得到 <spanclass="math display">\[F_{\beta}=1-E=\frac{\left(\beta^{2}+1\right) P R}{\beta^{2} P+R}\]</span></p><p>其中 <span class="math inline">\(F_1\)</span> 分数则是 P/R值的调和平均，较为平均的兼顾了二者。这是分类与信息检索中最常用的指标之一。<span class="math display">\[F_{1}=\frac{2 PR}{P+R}=\frac{1}{\frac{1}{2}\left(\frac{1}{R}+\frac{1}{P}\right)}\]</span></p><p>之所以使用<strong>调和平均</strong>而不是算术平均，是因为在<strong>算术平均</strong>中，任何一方对数值增长的贡献相当，任何一方对数值下降的责任也相当；而<strong>调和平均</strong>在增长的时候会偏袒较小值，也会惩罚精确率和召回率相差巨大的极端情况，很好地兼顾了精确率和召回率。</p><h3 id="单值评价指标-other-single-valued-measures">单值评价指标 | OtherSingle-Valued Measures</h3><p>类似 <span class="math inline">\(E\)</span> 和 <spanclass="math inline">\(F\)</span>这样的单值评价指标之所以重要，是因为这样能够更好的优化度量。此外，在文档评价中，我们还有如下指标：</p><ul><li>期望搜索长度 | Expected search length</li></ul><p>定义在弱顺序文档，量化的用户查找 K个相关文档所需工作量。这项指标计算预期用户在找到第 K个相关文档之前，按顺序浏览搜索结果列表将要看到的非相关文档的数量。</p><ul><li>损益平衡点 | Breakeven point</li></ul><p>寻找 Precision 等于 Recall 的点，通常在分类任务中用到。</p><ul><li><strong>平均排序倒数</strong> | <strong>MRR</strong> (MeanReciprocal Rank)</li></ul><p>对于某些 IR系统（如问答系统或主页发现系统），只关心第一个标准答案返回的rank，越前越好，这个位置的倒数称为 Reciprocal Rank (RR)，对问题集合求平均，则得到MRR。即，把标准答案在被评价系统给出结果中的排序取倒数作为它的准确度，再对所有的问题取平均。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>信息检索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IR</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IR学习笔记 #04 概率模型</title>
    <link href="/IR-Note-4.html"/>
    <url>/IR-Note-4.html</url>
    
    <content type="html"><![CDATA[<p>前面介绍的模型，都是通过对<strong>相似性</strong>的计算，得出最佳匹配的模型。而概率检索模型，则是基于概率原理，越过相似性，直接对<strong>相关性</strong>进行计算的一种检索模型。</p><p>利用相关性有一个好处，就是对于两个不相似的词，如果它们因为某些因素联系起来了，那么也会出现在检索结果中。</p><h2 id="朴素贝叶斯分类-naive-bayes">朴素贝叶斯分类 | Naive Bayes</h2><p>贝叶斯公式是概率论中非常基础的公式，其解决的核心点在于根据已有信息，对未知事物发生结果的概率计算。这里简单介绍一下作为模型的引入。</p><p>如果我们有文档 <em>D</em>，可以记 <spanclass="math inline">\(P(R=1|D)\)</span> 为文档和查询相关的概率（这里的<em>R</em> 只有 0 和 1 两种取值），这表示在文档确定的情况下，发生 <spanclass="math inline">\(R=1\)</span> 假设的<strong>后验概率</strong>。</p><p>与此同时，<span class="math inline">\(P(R=1)\)</span>可以表示该假设的<strong>先验概率</strong>，意思是在完全对文档无所知的情况下，这个文档的<strong>分类</strong>情况满足假设的概率。</p><p>以下我们将 <span class="math inline">\(R=1\)</span> 简写为<em>R</em>，<span class="math inline">\(R=0\)</span> 简写为<em>NR</em>，表示一下贝叶斯公式： <span class="math display">\[P(R|D) = \frac{P(RD)}{P(D)} = \frac{P(D|R)P(R)}{P(D)}\]</span>实际上，贝叶斯公式是做了一个转换，将复杂的概率式转化为三个更好计算的概率式。</p><ul><li><span class="math inline">\(P(D)\)</span>表示随机选取一篇文档，恰好是特定的 <em>D</em>的概率，这个概率对于所有文档都是一致的，如果只是作比较，就不需要考虑。</li><li><span class="math inline">\(P(R)\)</span> 表示假设 <em>R</em>成立的先验概率，如果有已知的数据集，我们可以用相似文档的频率近似概率；如果没有，也可以先设为0.5。但应用在比较中，也是不需要考虑的。</li><li><span class="math inline">\(P(D|R)\)</span>表示任意一篇文档被归类为相似后，恰好是特定的 <em>D</em>的概率，需要所用特殊的方法来估计。</li></ul><p>所以，<strong>判断</strong>一篇文档是否相似，只需要比较 <spanclass="math inline">\(P(R|D)\)</span> 和 <spanclass="math inline">\(P(NR|D)\)</span> 两个值的大小，就是比较 <spanclass="math inline">\(P(D|R)P(R)\)</span> 和 <spanclass="math inline">\(P(D|NR)P(NR)\)</span>。</p><h2 id="概率检索-probabilistic-retrieval">概率检索 | ProbabilisticRetrieval</h2><p>概率检索模型与贝叶斯分类的思想非常接近，但还是有本质区别的。概率检索模型的根本目的不是<strong>分类</strong>，它不需要根据查询判断一个文档属于“相关”或者“不相关”，而是计算这个文档属于属于“相关”或者“不相关”的<strong>概率大小</strong>为文档<strong>排序</strong>。</p><p>因此，在概率检索模型中，我们首先要定义一个<strong>相关度指标</strong>，考虑前文中提到的<span class="math inline">\(P(D|R)P(R)\)</span> 和 <spanclass="math inline">\(P(D|NR)P(NR)\)</span>，由于 <spanclass="math inline">\(P(R)\)</span> 和 <spanclass="math inline">\(P(NR)\)</span>在同一个查询下对所有文档都是一致的，因此只要关注剩余部分之比（也称为<strong>优势率</strong>）：<span class="math display">\[\alpha = \frac{P(D|R)}{P(D|NR)}\]</span>显然，这个比值越大，代表该文档与查询的相关度越大，因此我们最后就通过<span class="math inline">\(\alpha\)</span> 将文档排序。</p><h3 id="风险最小化-risk-minimization">风险最小化 | RiskMinimization</h3><p>此外，在检索过程中，我们还要决定一篇文档是否被召回，即设定一个<strong>召回阈值</strong>。一般我们会选择<strong>贝叶斯最优决策定理</strong>（Bayes’Optimal DecisionRule）来决定一个文档是否相关，进而确定是否将其返回。</p><p>所谓的贝叶斯最优决策定理其实很简单，当 $P( R|D ) &gt;P( NR|D ) $时，我们认定该文档是相关文档，将其返回。</p><p>但在实际中，我们还要考虑<strong>最小化期望损失</strong>（<strong>也称为贝叶斯风险</strong>，BayesRisk），即「返回一篇不相关文档」或「没有返回一篇相关文档」的损失。</p><p>举个例子，在就诊看病的过程中，将患病者诊断为「健康」而错失治疗时机，远比健康者诊断为「患病」代价大得多。因此我们也认为「没有返回一篇相关文档」的代价要大于「返回一篇不相关文档」。</p><p>如果记 <span class="math inline">\(c_{rr}\)</span> 为 cost ofdeciding <strong>relevant when relevant</strong>， <spanclass="math inline">\(c_{rn}\)</span> 为 cost of deciding<strong>relevant when not relevant</strong>，<spanclass="math inline">\(c_{nn}\)</span> 和 <spanclass="math inline">\(c_{nr}\)</span> 同理。那么就有： <spanclass="math display">\[c_{nr}P\left( R|D \right) +c_{nn}P\left( NR|D \right) &gt;c_{rn}P\left(NR|D \right) +c_{rr}P\left( R|D \right)\]</span> 移项，并引入贝叶斯公式后： <span class="math display">\[\left( c_{nr}-c_{rr} \right) P\left( D|R \right) P\left( R \right)&gt;\left( c_{rn}-c_{nn} \right) P\left( D|NR \right) P\left( NR \right)\]</span> 结合相关度指标，可以等到新的阈值： <spanclass="math display">\[\alpha =\frac{P(D|R)}{P(D|NR)}&gt;\frac{\left( c_{rn}-c_{nn}\right)P\left( NR \right)}{\left( c_{nr}-c_{rr} \right)P\left( R\right)}\]</span></p><h2 id="二值独立检索-binary-independence-retrieval">二值独立检索 |Binary Independence Retrieval</h2><p>前面提到，<span class="math inline">\(P(D|R)\)</span>表示任意一篇文档被归类为相似后，恰好是特定的 <em>D</em>的概率，在通常的<strong>朴素贝叶斯分类</strong>中，通常有两种方法来估计：</p><ul><li>用 <em>D</em> 在类别 <em>R</em>中的比例来估计，显然，这个方法在检索中不适用，因为同一文档 <em>D</em>几乎不可能在 <em>R</em> 中出现过。</li><li>将 <em>D</em> 看作由 0 和 1<strong>二值</strong>组成的向量，每个维度代表了一种词项是否包含在该文档中，默认词项之间是相互<strong>独立</strong>的，然后用下面的公式计算：</li></ul><p><span class="math display">\[P(D|R) = \prod_{T_i \in D}{P(T_i=1|R)}\prod_{T_j \notin D}{P(T_j=0|R)}\]</span></p><p>其中，<em>T</em> 就代表文档中的词项 term，<spanclass="math inline">\(P(T|R)\)</span>就是该词项在归类为相似的文档集中<strong>出现或不出现</strong>的概率。</p><h3 id="公式推演">公式推演</h3><p>在以上的概念下，不妨记：</p><ul><li>相似文档集中 <span class="math inline">\(P(T_k=1|R)\)</span> 为<span class="math inline">\(p_k\)</span>，<spanclass="math inline">\(P(T_k=0|R)\)</span> 为 <spanclass="math inline">\(1-p_k\)</span>。</li><li>不相似文档集中 <span class="math inline">\(P(T_k=1|NR)\)</span> 为<span class="math inline">\(q_k\)</span>，<spanclass="math inline">\(P(T_k=0|NR)\)</span> 为 <spanclass="math inline">\(1-q_k\)</span>。</li></ul><p>则相关度指标可表示为： <span class="math display">\[\alpha = \frac{\prod_{T_k \in D}p_k \prod_{T_k \notin D} 1 -p_k}{\prod_{T_k \in D}q_k \prod_{T_k \notin D}1 - q_k}\]</span> 再做一个数学上的等价变换，如下： <span class="math display">\[\begin{aligned}\alpha &amp;= \frac{\prod_{T_k \in D}p_k \prod_{T_k \notin D}1 -p_k}{\prod_{T_k \in D}q_k \prod_{T_k \notin D}1 - q_k} =\frac{\prod_{T_k \in D}p_k}{\prod_{T_k \in D}q_k} \cdot \frac{\prod_{T_k\notin D}1 - p_k}{\prod_{T_k \notin D}1 - q_k}\\&amp;= (\frac{\prod_{T_k \in D}p_k}{\prod_{T_k \in D}q_k} \cdot\frac{\prod_{T_k \in D}1 - q_k}{\prod_{T_k \in D}1 - p_k}) \cdot(\frac{\prod_{T_k \in D}1 - p_k}{\prod_{T_k \in D}1 - q_k}\frac{\prod_{T_k \notin D}1 - p_k}{\prod_{T_k \notin D}1 - q_k})\\&amp;= \frac{\prod_{T_k \in D}p_k(1 - q_k)}{\prod_{T_k \in D}q_k(1 -p_k)} \cdot \frac{\prod 1 - p_k}{\prod 1 - q_k}\end{aligned}\]</span> 在同一查询下，相似文档集和不相似文档集是固定的，也就是说 <spanclass="math inline">\(p_k\)</span> 和 <spanclass="math inline">\(q_k\)</span>的值也是相同的。故公式的第二部分（与文档 <em>D</em>无关）可以忽略，简化成 <span class="math display">\[\alpha=\prod_{T_k \in D}\frac{p_k(1 - q_k)}{q_k(1 - p_k)}\]</span> 取对数将乘积转化为求和得到用于排序的两，称为 RSV (RetrievalStatus Value，<strong>检索状态值</strong>)： <spanclass="math display">\[RSV_D=\sum_{T_k \in D} \left(\log \frac{p_k}{1 - p_k} + \log \frac{1 -q_k}{q_k} \right)\]</span></p><h3 id="estimation-using-training-data">Estimation using trainingdata</h3><p>现在我们只要计算出 <span class="math inline">\(p_k\)</span> 和 <spanclass="math inline">\(q_k\)</span>的值就成功了。在计算之前，我们先写出下面的索引项出现列联表：</p><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">相关文档</th><th style="text-align: center;">不相关文档</th><th style="text-align: center;">总数</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><strong>包含</strong>词项 <spanclass="math inline">\(T_k\)</span></td><td style="text-align: center;">r</td><td style="text-align: center;">n-r</td><td style="text-align: center;">n</td></tr><tr class="even"><td style="text-align: center;"><strong>不包含</strong>词项 <spanclass="math inline">\(T_k\)</span></td><td style="text-align: center;">R-r</td><td style="text-align: center;">N-n-R+r</td><td style="text-align: center;">N-n</td></tr><tr class="odd"><td style="text-align: center;"><strong>总数</strong></td><td style="text-align: center;">R</td><td style="text-align: center;">N-R</td><td style="text-align: center;">N</td></tr></tbody></table><p>则可以得到估算式： <span class="math display">\[p_k=\frac{r}{R}, q_k=\frac{n-r}{N-R}\]</span>同时，为了避免可能出现的<strong>零频问题</strong>（比如所有的相关文档都包含或不包含某个特定的词项），一种很常规的做法是在之前的表格中的每个量的基础上都加上0.5 来<strong>平滑处理</strong>，因此总数也做相应改变： <spanclass="math display">\[p_k=\frac{r+0.5}{R+1}, q_k=\frac{n-r+0.5}{N-R+1}\]</span></p><h3 id="estimation-without-training-data">Estimation without trainingdata</h3><p>用上式代入得到的计算式也称作 <code>Robertson-Sparck Jones</code>等式，这个式子的计算条件是知道相关文档总数R，但实际上大多数时候我们都是不知道的。</p><p>一种可行的方案是，初始时令相关文档数为0，这是因为在实际检索情景下，文档库中往往只有少部分是和查询词相关的内容：</p><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">相关文档</th><th style="text-align: center;">不相关文档</th><th style="text-align: center;">总数</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><strong>包含</strong>词项 <spanclass="math inline">\(T_k\)</span></td><td style="text-align: center;">0</td><td style="text-align: center;">n</td><td style="text-align: center;">n</td></tr><tr class="even"><td style="text-align: center;"><strong>不包含</strong>词项 <spanclass="math inline">\(T_k\)</span></td><td style="text-align: center;">0</td><td style="text-align: center;">N-n</td><td style="text-align: center;">N-n</td></tr><tr class="odd"><td style="text-align: center;"><strong>总数</strong></td><td style="text-align: center;">0</td><td style="text-align: center;">N</td><td style="text-align: center;">N</td></tr></tbody></table><p>此时的 <span class="math inline">\(p_k\)</span>值可以用常数来代替，如 0.3。</p><h2 id="修正公式">修正公式</h2><p>在本文的最后，我们再来讨论一个问题，在前面讲到的 <spanclass="math inline">\(p_k\)</span> 和 <spanclass="math inline">\(q_k\)</span>的值估算过程中，我们其实是用到了之前提过的<strong>文档频率</strong><em>df</em>。</p><p>而在之前的文章中，还有词频、逆文档频率、文档长度等等多种因素未被考虑到。因此，基于最初的原理和假设，可以对原来的RSV 公式增加修正因子，使得模型更加精确。</p><h3 id="bm25-weighting">BM25 Weighting</h3><p>这是一种最常用的加权方法，考虑了<strong>词频</strong>和<strong>文档长度</strong>。BM25模型为文档 <span class="math inline">\(D_i\)</span> 每个词项项 <spanclass="math inline">\(T_j\)</span> 分配了一个系数 <spanclass="math inline">\(B_{i,j}\)</span> ，由下计算生成： <spanclass="math display">\[B_{i,j}=\frac{\left( K_1+1 \right) f_{i,j}}{K_1\left[(1-b)+b\frac{\mathrm{len}\left( D_i\right)}{\,\,\mathrm{avg}\_\mathrm{doclen}} \right] +f_{i,j}}\]</span> 其中，<span class="math inline">\(K_1\)</span> 和 <em>b</em>为经验参数，用于调节词频和文档长度在权重计算中起到的作用，一般来讲，<span class="math inline">\(K_1\)</span> 取 1，<em>b</em> 取 0.75已经被证明是合理的假设。而 <span class="math inline">\(f_{i,j}\)</span>则为词项 <span class="math inline">\(T_j\)</span> 在文档 <spanclass="math inline">\(D_i\)</span> 中的词频，avg_doclen为平均文档长度。</p><h3 id="multiple-fields">Multiple Fields</h3><p>在 BM25的之后，还有一种针对其提出的修正方法。将文档划分成不同的<strong>域</strong>，如：title/abstract/body，并对不同域赋予不同的权重（每个term 出现的<strong>当量</strong>不同）。例如，term 在标题出现 1次相当于在 abstract 出现 1.5 次。</p><p>同理，文档长度也相应的进行加权调整，最后可以计算出新的修正因子：<span class="math display">\[\begin{aligned}\widetilde{t f}_{i} &amp;=\sum_{s=1}^{S} w_{s} t f_{s i} \\\widetilde{d l} &amp;=\sum_{s=1}^{S} w_{s} s l_{s}\end{aligned}\]</span> 最后计算出的频度替换原始的频度，代入 BM25 Weighting 公式。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>信息检索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IR</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IR学习笔记 #03 向量空间模型</title>
    <link href="/IR-Note-3.html"/>
    <url>/IR-Note-3.html</url>
    
    <content type="html"><![CDATA[<p>回忆前两个模型，我们发现统计语言模型在布尔模型上，做出了最佳匹配和排序结果的改进。但是，仍然没有考虑到「<strong>词项的权重</strong>」。</p><p>在向量空间模型中，我们容易联想到用向量来表示文档和查询，再通过计算余弦来得到两个向量的距离，从而得到相似性度量。</p><p>那么，如何选取向量空间 basis vector(基向量)？如何将目标转化为向量？如何为各个维度选取 magnitide(幅值)，从而考虑权重？如何在高维空间计算向量距离？</p><h2 id="向量空间模型-vector-space-model">向量空间模型 | Vector SpaceModel</h2><p>通常地，我们选择用 linearly independent (线性独立) 或 orthogonal(正交)的基向量来张成<strong>向量空间</strong>，这样可以使得维度最少。那么，如何选取基向量？</p><p>这是一个特征选择问题，在 IR 中，通常有两种方式：</p><ol type="1"><li><p>Core concept (核心概念)的思想：把词语的类型分类，按照其在不同分类上的「倾斜程度」决定向量的值，可以使维度尽量少。但是，由于语义上的多样性，很难实现。目前有WordNet, HowNet, HNC 等模型。</p></li><li><p>把出现过的 term都当作是一个基向量，并<strong>假设</strong>所有的基向量都是相互正交、相互独立的。这样将会得到一个维度不断增长的向量空间（随着词典表扩大）。</p></li></ol><p>以下我们采用第二种方式。一个 Doc 或 Query的向量表示就是：所有出现在文档中的 term 的向量之和。</p><h3 id="词项权重-term-weighting">词项权重 | Term Weighting</h3><p>当一个 term在文档中不断出现时，在这个方向上的向量幅值就会很大。这样比起布尔模型的0/1 二值，更能反映了这个 term 的重要性。这便是决定权重的 <em>tf</em>(<strong>term frequency</strong>，词项频率) 方法。</p><p>然而，原始的 <em>tf</em>值会面临这样一个严重的问题：即在和查询进行相关度计算时，所有 term都被认为是同等重要的。</p><p>实际上，某些 term对于相关度计算来说几乎没有或很少有区分能力。一个很直接的想法就是给包含在较多文档中的词项赋予较低的权重。为此，引入变量<em>df</em> (<strong>documentfrequency</strong>，文档集频率)，即有多少文档包含了该 term。df值越大，说明该 term 越不重要。</p><p>为了计算的方便，将其标准化得到 <em>idf</em> (<strong>inverse documentfrequency</strong>，逆文档频率)：</p><p><span class="math display">\[idf_t=\log \left( \frac{N}{df_t} \right)\]</span> 观察该式发现，<em>idf</em>虽然可以使得在较多文档中的词项权值降低，但与 <em>tf</em>相反的是，这样做的缺点是：对那些极少出现的词极度敏感。</p><p>为此，我们将二者结合在一起，诞生了 <strong><em>tf·idf</em></strong>方法——在文本处理领域中使用最广泛的数值权重计算方法。方法基于的思想和构造的统计量都很简单，但是在实际中却表现了很好的性能。</p><p>在 VSM 中，我们会将词项的 <em>tf·idf</em>存储在词典表（词项-文档）矩阵中，作为向量的幅值，用于后续的计算。</p><h3 id="相似度计算-similarity">相似度计算 | Similarity</h3><p>当我们已经把文档表示成 <span class="math inline">\(R^{v}\)</span>上的向量，从而可以计算文档与文档之间的相似度（根据向量内积或者<strong>余弦夹角</strong>）。</p><p>设 <span class="math inline">\(D_1\)</span> 和 <spanclass="math inline">\(D_2\)</span> 表示 VSM 中的两个向量： <spanclass="math display">\[\begin{aligned}&amp;D_{1}=D_{1}\left(w_{11}, w_{12}, \ldots, w_{1 n}\right) \\&amp;D_{2}=D_{2}\left(w_{21}, w_{22}, \ldots, w_{2 n}\right)\end{aligned}\]</span> 可以借助于 N维空间中两个向量之间的某种距离来表示文档之间的相似度，常用的方法是使用向量之间的內积来计算：<span class="math display">\[\operatorname{Sim}\left(D_{1}, D_{2}\right)=\sum_{k=1}^{n} w_{1 k}\times w_{2 k}\]</span>考虑到向量的<strong>归一化</strong>，则可以使用两个向量的余弦值来表示相似系数：<span class="math display">\[\operatorname{Sim}\left(D_{1}, D_{2}\right)=\cos\theta=\frac{\sum_{k=1}^{n} w_{1 k} \times w_{2 k}}{\sqrt{\sum_{k=1}^{n}w_{1 k}^{2} \sum_{k=1}^{n} w_{2 k}^{2}}}\]</span>要注意，这里使用向量内积，是基于对所有向量相互独立、相互正交的假设，否则计算内积也就失去了意义。对于相关的基向量，应该评估Term 之间的相关度 <spanclass="math inline">\(T_{i,j}\)</span>，再把向量当成多项式计算，最后代入<span class="math inline">\(T_{i,j}\)</span>。</p><p>此外，在其他的考虑权重的模型中，如Lucene，在计算相似度时引入了更多的因子，如 <em>tf·idf</em>，<spanclass="math inline">\(boost_t\)</span>，<em>overlap(q,d)</em>等，对应用情形、平滑度加以考量。</p><h3 id="vsm-实际应用">VSM 实际应用</h3><p>在 IR 中应用 VSM 模型时，相似度在检索结果中有两种体现：</p><ol type="1"><li><strong>Threshold</strong>(阈值)：对于每个查询，只在相似度大于一定阈值的文档中检索，如 Sim &gt;0.50 的文档中，减少查询范围。</li><li><strong>Ranking</strong>：对于每个查询，返回相似度排名 Top n的文档，以相似度排序。</li></ol><p>而 VSM 模型也有着致命的<strong>缺点</strong>：</p><ul><li><p>对于大的文档集（10w+term），向量维度太多导致难以存储和计算。</p></li><li><p>一篇文档的词数（1k+ term）远低于总的词数——高维稀疏矩阵。</p></li><li><p>词项之间的相关性，导致了大量冗余的基向量。</p></li></ul><h2 id="潜层语义索引-latent-semantic-indexing">潜层语义索引 | LatentSemantic Indexing</h2><p>潜层语义索引，也被称为 LSA (Latent SemanticAnalysis，潜在语义分析)，是针对向量空间的「<strong>高维稀疏</strong>」问题提出的解决方法，利用线性代数中的<strong>奇异值分解</strong>降低维度（去除噪音），同时尽量减少信息的损失。</p><h3 id="奇异值分解-singular-value-decomposition">奇异值分解 | SingularValue Decomposition</h3><p>参考：https://www.cnblogs.com/pinard/p/6251584.html</p><p>对于一个 <span class="math inline">\(t\times d\)</span>​​ 矩阵 <spanclass="math inline">\(A\)</span>​​​，可以分解为下面三个矩阵： <spanclass="math display">\[A_{t\times d}=U_{t\times t}\varSigma _{t\times d}V^T_{d\times d}\]</span> 其中 <span class="math inline">\(U\)</span>​ 和 <spanclass="math inline">\(V\)</span>​ 都是<strong>酉矩阵</strong>，即满足<span class="math inline">\(U^TU=I, V^TV=I\)</span>​。<spanclass="math inline">\(\varSigma\)</span>​ 一个 <spanclass="math inline">\(t\times d\)</span>​矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为<strong>奇异值</strong>。</p><p>利用酉矩阵性质得： <span class="math display">\[A=U\Sigma V^T \Rightarrow A^T=V\Sigma^T U^T \Rightarrow A^TA = V\Sigma^TU^TU\Sigma V^T = V\Sigma^2V^T\]</span> 可以看出 <span class="math inline">\(A^TA\)</span>的特征向量组成的矩阵，就是我们 SVD 中的 <spanclass="math inline">\(V^T_{d\times d}\)</span>​矩阵。进一步我们还可以看出我们的特征值矩阵等于奇异值矩阵的平方。</p><p>利用以上原理，我们可以得出 SVD <strong>分解步骤</strong>：</p><ol type="1"><li>假设词典矩阵为 <span class="math inline">\(A\)</span>，首先求出<span class="math inline">\(AA^T\)</span>，会得到一个 <spanclass="math inline">\(t\times t\)</span> 的方阵。</li><li>既然是方阵，就可以进行特征值分解，得到 <em>t</em>个特征值和对应的特征向量。</li><li>将特征值按方差大小排序，用所有的列向量张成一个 <spanclass="math inline">\(t\times t\)</span> 的矩阵 <spanclass="math inline">\(U_{t\times t}\)</span>。</li><li>同理可以用 <span class="math inline">\(A^TA\)</span> 求出 <spanclass="math inline">\(d\times d\)</span> 的矩阵 <spanclass="math inline">\(V^T_{d\times d}\)</span>。</li><li>利用前面求出的特征值，开方后得到 <spanclass="math inline">\(\varSigma _{t\times d}\)</span>。​</li></ol><h3 id="利用-svd-降维">利用 SVD 降维</h3><p>对于奇异值，它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列。通常，奇异值的<strong>衰减</strong>得特别快，在很多情况下，前10% 甚至 1% 的奇异值之和就占了全部的奇异值之和的 99% 以上的比例。</p><p>也就是说，我们也可以用最大的 k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说： <spanclass="math display">\[A_{t\times d}=U_{t\times t}\varSigma _{t\times d}V^T_{d\times d}\approxU_{t\times k}\varSigma _{k\times k}V^T_{k\times d}\]</span> 其中 <em>k</em> 要比 <em>t</em>小很多，也就是一个大的矩阵可以用三个小的矩阵，此时存储空间可以大量节省。通常<em>k</em> 的值即为我们假设的<strong>主题数</strong>。</p><p>SVD 分解后，<span class="math inline">\(U_{il}\)</span> 对应第<em>i</em> 个词和第 <em>l</em> 个词义的相关度。<spanclass="math inline">\(V_{jm}\)</span> 对应第 <em>j</em> 个文档和第<em>m</em> 个主题的相关度。<spanclass="math inline">\(\Sigma_{lm}\)</span> 对应第 <em>l</em> 个词义和第<em>m</em> 个主题的相关度。</p><p>这样我们通过一次SVD，就可以得到词和词义的相关度，词义和主题的相关度，以及文档和主题的相关度。</p><h3 id="lsi-的使用">LSI 的使用</h3><p>通过计算后，我们关注新的矩阵 <span class="math inline">\(V^T_{k\timesd}\)</span>​ ，所有的文档已经简化成了和 k个主题的相关度。假设此时的查询为 <spanclass="math inline">\(Q=q_1q_2\cdots q_t\)</span>​​​​，​其中 <em>q</em> 取 0或 1，则 <span class="math display">\[Q_{1\times k}=Q_{1\times t}U_{t\times k}\varSigma _{k\times k}\]</span> 可将 <em>t</em> 维的查询转化成 <em>k</em>维的「<strong>与主题的相关度</strong>」，此时就可以与文档进行相似度计算了。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>信息检索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IR</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IR学习笔记 #02 统计语言模型</title>
    <link href="/IR-Note-2.html"/>
    <url>/IR-Note-2.html</url>
    
    <content type="html"><![CDATA[<p>基于对布尔模型的改进，提出一种新的最佳匹配模型。</p><h2 id="统计语言模型-statistical-language-models">统计语言模型 |Statistical Language Models</h2><p>首先探讨的是 Doc (文档) 的呈现形式，引入 Topic(<strong>主题</strong>)来表述一个文档的隐含语义，起到索引作用。基于以下两个假设：</p><ul><li>Words common in document are common in topic.</li><li>Words not in document much less likely.</li></ul><p>可以得出，Topic 是由 Doc 中的一些关键词勾勒出来的。于是引入 <spanclass="math inline">\(P(w|Doc)\)</span>概率分布表：统计每个词在文档中出现频度（频率）——基于大数定律。</p><p>但 Topic的难确定性（语义理解不同、可能有多个主题）导致其难以直接计算，因此可以用近似估算。</p><p><span class="math display">\[P\left( w|Topic_D \right) \approx P\left( w|D \right) =tf\left( w,D\right) /len\left( D \right)\]</span></p><p>事实上，我们可以认为 Topic 是一种「<strong>语言模型</strong>」，<spanclass="math inline">\(P\left( w|Topic_D \right)\)</span> 可以认为是在Topic 下生成该 word 的概率，即该 word在这个「语言模型」中被生成的概率，故 word 可以不在 Topic中出现，但也有概率生成。</p><h3 id="语言模型化-language-modeling">语言模型化 | LanguageModeling</h3><p>定义 <em>M</em> 为我们试图描述的 language (语言)，<em>s</em>为该语言下观测到的文本串（由许多词条构成）。</p><ul><li><p><em>M</em> can be thought of as a “source” or a generator - a<strong>mechanism</strong> that can spit out strings that are legal inthe language.</p></li><li><p><span class="math inline">\(P(s|M)\)</span> is the probability ofgetting “<em>s</em>” during random sampling from <em>M</em>.</p></li></ul><p>语言的规模可大可小，把每种语言的规模缩小为一个Topic（对应着语料库中的一个文档）；这个 Topic就决定了任意一个字符串在这个 Topic所对应的「语言模型」中出现的概率：比如，在一个描述信息检索发展历史的文档中，“Washington”出现的概率就会远远小于 “Robertson”。</p><p>那么，一旦我们确定了这个 Doc 所对应的「语言模型」<spanclass="math inline">\(M_D\)</span> ，而 <em>Q</em> 是用户的Query，我们是不是可以求出这个「语言模型」下<strong>生成</strong><em>Q</em>的概率？概率最大者就是与查询最相关的文档。那么，我们就可以根据 $P(Q|M_D)$ 给所有的 Doc 排序，得到我们的查询结果。</p><h2 id="多元语言模型-n-gram-language-models">多元语言模型 | N-gramLanguage Models</h2><p>对于一个较长的Query，我们采用<strong>分词</strong>的方法来计算它的生成概率。为此，首先通过几个例子明确语言模型中N-gram 的概念：</p><ul><li>Unigram 一元分词，把句子分成一个一个的汉字，如：哈/工/大/深/圳</li><li>Bigram 二元分词，每两个字组成一个词语，如：哈工/工大/大深/深圳</li><li>Trigram三元分词，每三个字组成一个词语，如：哈工大/工大深/大深圳</li></ul><p>在以上例子中，我们可以知道一个文本串在一元语言中生成的概率将这样计算：<span class="math display">\[P\left( w_1w_2w_3 \right) =P\left( w_1 \right) \cdot P\left( w_2 \right)\cdot P\left( w_3 \right)\]</span> 在二元语言中将这样计算： <span class="math display">\[P\left( w_1w_2w_3 \right) =P\left( w_1 \right) \cdot P\left( w_2|w_1\right) \cdot P\left( w_3|w_2 \right)\]</span> 可以发现，在 Unigram中我们假设了单词之间的<strong>独立性</strong>，这就意味着它的本质是词的多项分布，而一个文本串可以看作是这个分布的一个实例。</p><p>对于更多元的分词 N-gram，我们是假设每个单词出现的概率只与它之前的 n-1个单词<strong>相关</strong>，因此采用了条件概率。事实上，这是一种基于马尔可夫假设的模型，此时的文本串应是有序相关的，这就不属于BoW 的范畴。</p><p>一般情况下，N的取值都很小，实际<strong>自然语言处理</strong>应用中最多的是将 N = 3的三元分词模型。原因如下：</p><ul><li>N 元模型的空间复杂度，是 N 的指数函数，即 $O( | V |^N ) <spanclass="math inline">\(，*V*是一种语言的词汇量，一般在几万到几十万个。时间复杂度也是一个指数函数\)</span>O(| V |^{N-1} ) $。</li><li>即使使用 N = 4 、N = 5也不可能覆盖所有词与词之间的相关性。某两个词可能是一段话和一段话之间才会出现的。</li></ul><h3 id="多元语言模型的参数估计">多元语言模型的参数估计</h3><p>针对一元模型，只需要统计该「语言模型」生成的文档中，出现该 term的频率，用频率近似概率即可——<strong>大数定律</strong>。</p><p>这里对二元模型展开探讨：估计 <span class="math inline">\(P\left(w_i|w_{i-1} \right)\)</span>，利用条件概率： <spanclass="math display">\[P\left(w_{i} \mid w_{i-1}\right)=\frac{P\left(w_{i-1},w_{i}\right)}{P\left(w_{i-1}\right)}\]</span> 于是，我们只需要统计 <spanclass="math inline">\(\left(w_{i-1}, w_{i}\right)\)</span>的有序词对在文档中的出现次数，再统计 <spanclass="math inline">\(w_{i-1}\)</span> 的出现次数，即可估计其概率。</p><p>然而，存在这样一个问题：在文本中，两个词没有连续出现过，即频度为0，那么它的概率就是 0 吗？如果词对 <spanclass="math inline">\(\left(w_{i-1}, w_{i}\right)\)</span> 和 <spanclass="math inline">\(w_{i-1}\)</span> 的出现次数相同，其概率就是 1吗？这就涉及到了统计的可靠性问题，也称「<strong>不平滑问题</strong>」。</p><p>解决这些问题的主要方法是<strong>古德-图灵估计</strong>（Good-TuringEstimate）和<strong>卡茨退避法</strong>（Katz backoff）。</p><ul><li><p>对出现次数大于某个阈值的词，频率不下调，即用频率代替概率；</p></li><li><p>对出现次数小于这个阈值的词，频率才下调，利用古德-图灵估计的相对频度来调整；</p></li><li><p>对出现次数等于 0的词，利用卡茨退避法给予一个比较小的概率值。</p></li></ul><p>这部分的内容属于语料库的自然语言处理，本文中不赘述，仅在后文针对零频问题介绍几种方法。</p><h2 id="查询排序问题-ranking">查询排序问题 | Ranking</h2><p>当给定查询 <em>Q</em>时，怎么根据统计语言模型进行排序呢？有三种排序方法，分别是：</p><ol type="1"><li><strong>查询似然排序</strong> | Query-likelihood</li></ol><p>为每个 Doc 确定其所对应的 <spanclass="math inline">\(M_D\)</span>，而用户的 Query 记为 <spanclass="math inline">\(q=(q_1,q_2,\cdots,q_k)\)</span>。则该查询在每个文档的「语言模型」下生成的概率可如下计算： <spanclass="math display">\[P\left(q_{1} \ldots q_{k} \mid M_{D}\right)=\prod_{i=1}^{k} P\left(q_{i}\mid M_{D}\right)\]</span> 将所有计算结果排序，即可得到检索结果。要注意，这种方法对每个Doc 计算出的概率都独立于其他 Doc，相关文档没有被利用到。</p><ol start="2" type="1"><li><strong>文档似然排序</strong> | Document-likelihood</li></ol><p>查询似然的翻转版本，为每个 Query 确定其所对应的 <spanclass="math inline">\(M_Q\)</span>，计算任意一个文档在该查询的「语言模型」下生成的概率：<span class="math display">\[P\left(D \mid M_{Q}\right)=\prod_{w \in D} P\left(w \mid M_{Q}\right)\]</span> 但是，这种方法存在如下问题：</p><ul><li>文档的长度相差很大，很难比较。</li><li>由于文档中出现的词很多没有出现在查询中，将会出现零频问题。</li><li>将会出现无意义的作弊网页，如将 Query 中的关键词无限重复。</li></ul><p>要解决这些问题，需要引入 Likelihood Ratio(似然比)，对文档长度加以归一。</p><p><span class="math display">\[P\left(M_{Q} \mid D\right)=\frac{P\left(M_{Q}\right) P\left(D \midM_{Q}\right)}{P(D)} \approx \frac{c \prod_{w \in D} P\left(w \midM_{Q}\right)}{\prod_{w \in D} P(w \mid G E)}\]</span> 其中，对每个文档计算其可能 「生成 <spanclass="math inline">\(M_Q\)</span>」的概率，在用贝叶斯公式展开，其中的<span class="math inline">\(P\left(M_{Q}\right)\)</span>对于每个文档可视作常数，再由分母的约束，对文档加以限制。</p><ol start="3" type="1"><li>Ranking by <strong>Model Comparison</strong></li></ol><p>结合前两种方法，提出了<strong>交叉熵</strong>（cross-entropy）的概念：<span class="math display">\[H\left(M_{Q} \| M_{D}\right)=-\sum_{w} P\left(w \mid M_{Q}\right) \logP\left(w \mid M_{D}\right)\]</span> 这种方法同时考虑了查询 <spanclass="math inline">\(M_Q\)</span> 和文档 <spanclass="math inline">\(M_D\)</span>，直接比较两种模型的相似度。要注意，<spanclass="math inline">\(M_Q\)</span> 和 <spanclass="math inline">\(M_D\)</span> 在公式中的顺序不能调换。</p><h3 id="零频问题-zero-frequency-problem">零频问题 | Zero frequencyproblem</h3><p>有了上述排序模型，现在我们只需要从查询和文档中估算出 <spanclass="math inline">\(M_Q\)</span> 和 <spanclass="math inline">\(M_D\)</span>。</p><p>在本文的「语言模型」中，我们只需采用<strong>一元分词模型</strong>，独立性和独立分布可以简化许多问题。然而，在<strong>极大似然估计</strong>下，还是有个问题急需解决——零频问题，即有的term 根本不出现在观测集中，我们该如何估算其概率？</p><p>这里介绍三种 Discounting Methods (折扣法) 来 Smoothing (平滑) ：</p><ol type="1"><li><p>Laplace correction：把每个词的词频都加 1，分母的总频数加上词项数N。但是这种方法不适合较大的词典表。</p></li><li><p>Lindstone correction：把每个词都加一个很小的值ε，分母的总频数加上 Nε。</p></li><li><p>Absolute Discounting：把词频不等于 0 的词减去一个很小的值ε，再把减去的总值平均分配到词频为 0 的词上去，不改变分母。</p></li></ol><p>除了折扣法，还有诸如插值法、退避法等方法也可以用于平滑。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>信息检索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IR</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IR学习笔记 #01 概论&amp;布尔模型</title>
    <link href="/IR-Note-1.html"/>
    <url>/IR-Note-1.html</url>
    
    <content type="html"><![CDATA[<p>该笔记是本人于哈尔滨工业大学（深圳）2021年夏季学期「信息检索」课程的笔记，授课教师为 <ahref="https://scholar.google.com/citations?hl=zh-CN&amp;user=7aR5D4sAAAAJ">陈清财</a>教授。姑且算是一门 NLP 入门课程。</p><h2 id="概论-overview">概论 | Overview</h2><h3 id="whats-information-retrieval">What’s Information Retrieval?</h3><p>Indexing, retrieving, and organizing text by probabilistic orstatistical.</p><p><strong>Comparing IR to Databases</strong>:</p><table><thead><tr class="header"><th></th><th>Databases</th><th>IR</th></tr></thead><tbody><tr class="odd"><td>Data</td><td>Structured</td><td>Unstructured</td></tr><tr class="even"><td>Fields</td><td>Clear semantics</td><td>No fields</td></tr><tr class="odd"><td>Queries</td><td>Defined(SQL)</td><td>Free text (自然语言) + Boolean</td></tr><tr class="even"><td>Recoverability</td><td>Critical</td><td>Downplayed</td></tr><tr class="odd"><td>Matching</td><td>Exact</td><td>Imprecise (need to measure)</td></tr></tbody></table><h3 id="信息检索的基本方法-basic-approach-to-ir">信息检索的基本方法 |Basic Approach to IR</h3><p>大多数成功的方法都是基于概论统计，而不是自然语言理解。因为自然语言在缺少约束的状态（unrestricteddomains）下具有极大不确定性，而人工标注又十分昂贵。</p><p><strong>统计方法的核心思想</strong>：Relevant (相关) Items areSimilar (相似). Usually look for documents matching query words.</p><p>The similarity can be measured by:</p><ul><li>String matching/comparison (字符串匹配)</li><li>Same vocabulary (词汇)</li><li>Probability that documents arise from same model (文档出现概率)</li><li>Same meaning of text (语义) -- Hard to achieve</li></ul><h4 id="词袋-bag-of-words">词袋 | “Bag of Words”</h4><p>Compares words <strong>without regard to order</strong>.</p><p><strong>Stop word (停用词)</strong>：屏蔽对文章分类无效的高频词。</p><h3 id="基础检索模型-retrieval-models">基础检索模型 | RetrievalModels</h3><p>检索模型：建立在 Doc 和 Query之间的模型，用于描述相似性、排序相似性。</p><p>检索变量：queries (查询), documents (文档), terms (术语), relevancejudgments (相关性判别)。</p><h4 id="exact-vs.-best-match">Exact vs. Best Match</h4><p>精确匹配：二值 (0/1) 匹配，检索结果无序，可以用 boolean queries(布尔查询)、proximity operators (邻接算子)、simple regular expressions(正则表达式)。对文档量级有限制。</p><p>最佳匹配：相似度 (0~1) 匹配，检索结果按照相似度排序。</p><h2 id="布尔模型-boolean-retrieval">布尔模型 | Boolean Retrieval</h2><p>一种最常见的精确匹配模型，通常结果是无序呈现（unranked），有的模型会增加简单的排序。</p><p>精确匹配模型最直接的想法：<strong>线性扫描</strong>，从头到尾扫描文档集，对每个文档都查看是否包含关键词。在Unix/Linux 系统中的文本扫描命令 grep做的就是这种工作。然而，当需要检索的文档规模非常大时，这种线性扫描的方式的效率会变得非常低下。</p><h3 id="如何实现-boolean-retrieval">如何实现 Boolean Retrieval</h3><p>需要实现如下的模块：</p><ul><li><p>Term-document incidence (<strong>词典表</strong>): 类似 index(索引) 的文档呈现的形式，一个矩阵中，用 0 和 1 标记文档中出现的 term(词项)。</p></li><li><p>Boolean queries (布尔查询): AND, OR, AND-NOT.</p></li><li><p>Incidence vector (关联向量): 0/1 vector, bitwise AND。</p></li><li><p>Proximity operators (邻接算子): phrases - “”、same sentence - “/s ”、same paragraph - “/p” 等等。</p></li></ul><h3 id="实现中的要点">实现中的要点</h3><p>在词典表实现中，为了避免矩阵过大，还可以引入 inverted index(<strong>倒排索引</strong>)存储矩阵，这里不再赘述。下面介绍两个实现步骤中的概念。</p><p><strong>token (词条) vs. term (词项)</strong>：</p><p>对于英文文本而言，词条就是根据空格把单词一个一个地提取出来，把原始文本分割开。词项则是更加统一规范的的词条。</p><p>例如在文本中可能出现 “apple”、“apples”、“Apple” 这类token，但我们知道这几个 token都是表达苹果（apple）的意思，因此，在构建索引的时候通常会把这几个 token统一还原为 “apple”，只为 “apple” 建立索引项，那么 “apple” 就是一个 term了。</p><h3 id="features-to-note-about-queries">Features to Note aboutQueries</h3><ul><li>Queries are developed incrementally.查询表达式是可增长的，往往一直增加直到查询出正确结果。</li><li>Queries are complex. 用到了一定公式，对初学者不友好。</li><li>Queries are long (av. 9-10 words). 不同于通常的自然语言询问，只需要1-2 个单词。</li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>信息检索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IR</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hexo配置与扩展</title>
    <link href="/Hexo-Configuration.html"/>
    <url>/Hexo-Configuration.html</url>
    
    <content type="html"><![CDATA[<p>Hello My World 的姊妹篇。</p><p>本站基于 Hexo + GitHub 搭建，采用 Fluid 主题。</p><p>这篇文章记录了博客的配置历程，包括：主题配置、域名配置、功能扩展。</p><h2 id="主题配置">主题配置</h2><p>本站采用的是 Fluid 主题，以下的配置在路径<code>hewei2001/_config.fluid.yml</code> 中可以实现。该文件的介绍参见 <ahref="https://hexo.fluid-dev.com/docs/guide/">主题配置指南</a>。以下仅介绍部分较为特殊的配置，其他内容可在指南中找到。</p><h3 id="代码高亮">代码高亮</h3><p><code>lib</code>: 选择生成高亮的库，可选项有 highlightjs 和prismjs，对应下面两组配置。</p><p>这里选择 <code>highlightjs</code>，将 <code>style</code> 修改为<code>Night Owl</code> 风格，将 <code>bg_color</code> 修改为<code>true</code> 以适配暗色代码框。</p><p>其他尝试过的主题还有<code>Atom One Dark Reasonable</code>、<code>Vs 2015</code>、<code>Github Dark Dimmed</code>，都是不错的暗色风格。</p><h3 id="mac-风格代码块">Mac 风格代码块</h3><p>在 GitHub 的 Issue 发现有人提供了自定义样式实现 Mac风格代码块的方法，遂尝试之。首先在路径<code>hewei2001/themes/fluid/source/css</code> 下新建文件<code>mac.styl</code>，复制以下代码：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-class">.highlight</span><br>    <span class="hljs-attribute">background</span>: <span class="hljs-number">#011627</span><br>    <span class="hljs-attribute">border-radius</span>: <span class="hljs-number">5px</span><br>    <span class="hljs-attribute">box-shadow</span>: <span class="hljs-number">0</span> <span class="hljs-number">10px</span> <span class="hljs-number">30px</span> <span class="hljs-number">0</span> rgba(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, .<span class="hljs-number">4</span>)<br>    <span class="hljs-attribute">padding-top</span>: <span class="hljs-number">30px</span><br><br>    <span class="hljs-selector-pseudo">&amp;::before</span><br>      <span class="hljs-attribute">background</span>: <span class="hljs-number">#fc625d</span><br>      <span class="hljs-attribute">border-radius</span>: <span class="hljs-number">50%</span><br>      <span class="hljs-attribute">box-shadow</span>: <span class="hljs-number">20px</span> <span class="hljs-number">0</span> <span class="hljs-number">#fdbc40</span>, <span class="hljs-number">40px</span> <span class="hljs-number">0</span> <span class="hljs-number">#35cd4b</span><br>      <span class="hljs-attribute">content</span>: <span class="hljs-string">&#x27; &#x27;</span><br>      <span class="hljs-attribute">height</span>: <span class="hljs-number">12px</span><br>      <span class="hljs-attribute">left</span>: <span class="hljs-number">12px</span><br>      <span class="hljs-attribute">margin-top</span>: -<span class="hljs-number">20px</span><br>      <span class="hljs-attribute">position</span>: absolute<br>      <span class="hljs-attribute">width</span>: <span class="hljs-number">12px</span><br></code></pre></td></tr></table></figure><p>然后在路径 <code>hewei2001/_config.fluid.yml</code> 中找到<code>custom_css</code> 选项，加入 <code>/css/mac.css</code>代码，注意这里后缀名依然使用 <code>.css</code>，不然无法识别！</p><h3 id="行内代码颜色">行内代码颜色</h3><p>默认的行内代码颜色和正文颜色是继承关系，且行内代码背景色也不明显，因此视觉上难以区分。但是配置文件中又没有对应选项可以修改，查阅GitHub 的 Issue 发现，有人曾提供过一个解决方案。</p><p>打开路径 <code>hewei2001/themes/fluid/source/css/_pages/_base</code>下的 <code>base.styl</code> 文件，找到 <code>code</code>配置项，修改颜色为 <code>#E05B35</code>。</p><h3 id="评论功能">评论功能</h3><p>Valine 是国内的一款极简风格的评论软件，也是 Fluid支持的评论软件之一。在 <code>comment</code> 中选择<code>valine</code>，之后找到相应的配置区域进行如下操作：</p><p>进入官网 <a href="https://leancloud.cn/">LeanCloud</a>完成注册，然后在控制台创建一个项目 <code>Blog.Comments</code>后，获取密钥（App ID 和 AppKey），在对应位置填入。其他内容选项可以在官网找到说明。</p><h3 id="访问人数统计">访问人数统计</h3><p>Fluid 主题提供两种网站的 PV、UV 统计数来源：<ahref="https://www.leancloud.cn/">LeanCloud</a> 与 <ahref="http://busuanzi.ibruce.info/">不蒜子</a>。不蒜子不需要申请账号，直接开启即可，但有时候会响应缓慢拖慢整个页面加载。LeanCloud使用前需要申请账号，由于前面使用评论功能时已经注册，我们这边直接使用就行。</p><p>在控制台创建一个项目 <code>Blog.Counter</code> 后，获取密钥（App ID和 App Key）和大陆服务器地址，填入 <code>web_analytics</code> 配置项中<code>leancloud</code> API 相关参数。</p><h3 id="内置-tag-插件">内置 Tag 插件</h3><p>Fluid 内置了一些 Tag 插件，用于实现 Markdown不容易生成的样式，以下仅列出两种常用的使用语法，添加在 md 文件中：</p><ul><li>脚注（需要在设置中开启 <code>footnote</code> 才会渲染）</li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs markdown">正文[^1]<br><br><span class="hljs-section">## 参考</span><br>[<span class="hljs-symbol">^1</span>]: <span class="hljs-link">参考资料1</span><br>[<span class="hljs-symbol">^2</span>]: <span class="hljs-link">参考资料2</span><br></code></pre></td></tr></table></figure><ul><li>Tag 便签</li></ul><p>在 markdown中加入如下的代码来使用便签，注意<strong>头尾标志需要单独一行</strong>：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% note success %&#125;<br>文字 或者 <span class="hljs-code">`Markdown`</span> 均可<br>可选便签：primary紫/secondary灰/success绿/danger红/warning黄/info蓝/light黑<br>&#123;% endnote %&#125;<br></code></pre></td></tr></table></figure><p>或者使用 HTML 形式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;note note-primary&quot;</span>&gt;</span>标签<span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></td></tr></table></figure><ul><li>行内标签</li></ul><p>在 markdown 中加入如下的代码来使用 Label：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% label primary @text %&#125;<br>可选 Label：primary紫/default灰/success绿/danger红/warning黄/info蓝<br></code></pre></td></tr></table></figure><p>或者使用 HTML 形式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">span</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;label label-primary&quot;</span>&gt;</span>Label<span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><br></code></pre></td></tr></table></figure><h3 id="折叠块">折叠块</h3><p>Fluid 没有支持原生的折叠块，但是可以通过内嵌 HTML代码实现，为了更加美观，可以用使用 Tag 便签包裹：</p><div class="note note-secondary">            <details><summary><b>点击查看详细推导</b></summary>Surprise!</details>          </div><p>实现代码为：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs html">&#123;% note secondary %&#125;<br><br><span class="hljs-tag">&lt;<span class="hljs-name">details</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">summary</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">b</span>&gt;</span>点击查看详细推导<span class="hljs-tag">&lt;/<span class="hljs-name">b</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">summary</span>&gt;</span><br>Surprise!<br><span class="hljs-tag">&lt;/<span class="hljs-name">details</span>&gt;</span><br><br>&#123;% endnote %&#125;<br></code></pre></td></tr></table></figure><h3 id="latex-数学公式">Latex 数学公式</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">post:</span><br>  <span class="hljs-attr">math:</span><br>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">specific:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">engine:</span> <span class="hljs-string">mathjax</span> <span class="hljs-string">或</span> <span class="hljs-string">katex</span><br></code></pre></td></tr></table></figure><p>其中 <code>specific</code> 建议开启：当为 true 时，只有在文章Front-matter 里指定 <code>math: true</code>才会在文章页启动公式转换，以便在页面不包含公式时提高加载速度。</p><p>由于 Hexo 默认的 Markdown 渲染器不支持复杂公式，所以必须更换渲染器为MathJax + Kramed。新版的 Fluid 建议使用 MathJax + Pandoc渲染器，这样就可以避免一系列字符错乱。下面仅介绍 MathJax + Kramed的配置方法，更推荐的 Pandoc 配置<strong>参考官网</strong>即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ npm uninstall hexo-renderer-marked --save  <span class="hljs-comment"># 卸载原渲染器</span><br>$ npm install hexo-renderer-kramed --save    <span class="hljs-comment"># mathjax + kramed</span><br>$ npm install hexo-renderer-pandoc --save<span class="hljs-comment"># mathjax + pandoc</span><br>$ npm install @upupming/hexo-renderer-markdown-it-plus --save  <span class="hljs-comment"># katex</span><br></code></pre></td></tr></table></figure><p>这里选择 MathJax 而不是 Katex，是因为其对 LaTeX语法支持全面，且右键点击公式有扩展功能菜单。</p><p>但是 Hexo 中会对一些字符转义，使得用 MathJax 渲染公式有时会出错，根据GitHub 上的 Issue，需要找到路径<code>node_modules\kramed\lib\rules\inline.js</code>，修改：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-comment">// 第11行：取消对 \ 和 &#123;&#125; 的转义 escape</span><br><span class="hljs-attr">escape</span>: <span class="hljs-regexp">/^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/</span>,<span class="hljs-comment">// 错误的</span><br><span class="hljs-built_in">escape</span>: <span class="hljs-regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,<span class="hljs-comment">// 正确的</span><br><span class="hljs-comment">//第20行：避免下划线 _ 被转义为斜体，而非 LaTeX 下标</span><br>em: <span class="hljs-regexp">/^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,<span class="hljs-comment">// 错误的</span><br>em: <span class="hljs-regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,<span class="hljs-comment">// 正确的</span><br><span class="hljs-comment">//第64行：避免反斜杠加竖线 \| 被转义为 |，而非 LaTeX 双竖线</span><br><span class="hljs-built_in">escape</span>: replace(inline.escape)(<span class="hljs-string">&#x27;])&#x27;</span>, <span class="hljs-string">&#x27;~|])&#x27;</span>)(),<span class="hljs-comment">// 错误的</span><br><span class="hljs-attr">escape</span>: replace(inline.escape)(<span class="hljs-string">&#x27;])&#x27;</span>, <span class="hljs-string">&#x27;~])&#x27;</span>)(),<span class="hljs-comment">// 正确的</span><br></code></pre></td></tr></table></figure><p>如果发现 MathJax公式虽然能显示，但字体异常，右键点击公式，检查是否默认设置了<code>Math Settings &gt; Math Renderer &gt; CHTML</code>，将其改为<code>SVG</code> 可以恢复正常。打开博客主题路径<code>hewei2001\themes\fluid\layout\_partials\plugins\math.ejs</code>，修改如下内容即可：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-comment">// import_js(theme.static_prefix.mathjax.replace(&#x27;es5/&#x27;, &#x27;&#x27;), &#x27;es5/tex-mml-chtml.js&#x27;)</span><br>import_js(theme.static_prefix.mathjax.replace(<span class="hljs-string">&#x27;es5/&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>), <span class="hljs-string">&#x27;es5/tex-svg-full.js&#x27;</span>)<br></code></pre></td></tr></table></figure><blockquote><p>Tips1：在使用数学公式时，应当避免使用两个连续的<code>&#123;&#125;</code>，否则会被 Hexo 解释为特殊标签，从而报错。</p><p>Tips2：Hexo 对公式的支持不如 Typora好，譬如<strong>多行公式</strong>需要用<code>\begin&#123;aligned&#125; ... \end&#123;aligned&#125;</code>，换行符 <code>\\</code>和定位符 <code>&amp;</code>。</p><p>Tips3：Hexo 中变量的<strong>上下标</strong>只能用 LaTeX 实现而不能用Enhanced Markdown 语法。</p><p>Tips4：行内公式、公式块里的联立公式，如果含有<strong>分数</strong>或者<strong>大运算符</strong>（如求和），需要用<code>\begin&#123;aligned&#125; ... \end&#123;aligned&#125;</code>夹住，否则会被渲染器压扁。</p><p>Tips5：公式块内如果一行有多个式子，间隙可能会被压缩，需要用<code>\quad</code> 分隔。</p></blockquote><h2 id="域名配置">域名配置</h2><h3 id="自定义域名">自定义域名</h3><p>有了 GitHub Pages服务器自带的域名后，还可以到阿里云再购买一个自定义域名，然后将域名解析到博客的域名，具体过程如下：</p><ol type="1"><li>注册阿里云，实名认证后在购买下 hwcoder.top 域名。</li><li>打开域名控制台，进入<strong>域名解析</strong>列表，进入新买的域名，添加两条记录：<ul><li>主机记录：@；记录类型：A；记录值为 GitHub Pages 域名的 IP。</li><li>主机记录：www；记录类型：CNAME；记录值为 GitHub Pages 域名。</li></ul></li><li>在路径 <code>hewei2001/source</code> 下新建一个 <code>CNAME</code>文件，里面填写我们买的域名，注意文件不需要任何后缀。</li><li>GitHub 中打开对应仓库，在 Setting 中找到 Pages，添加 Custom Domain为新买的域名，旁边的一个 <code>Enforce HTTPS</code>勾选后我们的网站就变为 https://hwcoder.top。</li><li>路径 <code>hewei2001/_config.yml</code> 的<code>#URL</code>部分，更改为新域名。</li></ol><p>以上操作后就可以在自定义的域名访问博客站点了，如果显示的内容与本地服务器查看内容不同，<strong>清除浏览器缓存</strong>后即可解决。如果不能解决，检查是否以上步骤有错。</p><h3 id="部署到-coding-pages">部署到 Coding Pages</h3><p>Coding 可以算是国内的GitHub，尽管并不是特别流行，但部署到上面可以使国内访问速度更快，还可以提交百度收录（GitHub禁止了百度的爬取）。</p><p>注意：由于 Coding 在前段时间改版后，原有的个人版 Pages下架，以企业版的形式重新开放，新版的静态网站服务需调用腾讯云对象存储COS、内容分发网络 CDN、SSL 证书产品等资源，其中 COS 和 CDN采用<strong>用量计费模式</strong>。故本博客暂不采用 Coding 部署。</p><h3 id="部署到-cloudflare-pages">部署到 Cloudflare Pages</h3><p>无意中发现了 Cloudflare 这个平台在 2021年初开始提供了静态网页托管的服务，且对免费用户也比较友好。</p><p>下面是部署的过程：</p><ol type="1"><li>注册 Cloudflare 账户，注册后与 GitHub 绑定。</li><li>绑定账户后可以直接 clone 博客仓库，按照引导选择Build，等待若干分钟即可，部署后访问 https://hewei2001.pages.dev/可以看到博客页面。</li><li><strong>自定义域名设置</strong>：输入<code>hwcoder.top</code>，提示需要进行将 DNS <strong>转移</strong>到Cloudflare 服务器，选择 Free 免费版本。</li><li><strong>更改名称服务器</strong>：打开之前购买域名的阿里云，进入域名控制台，选择DNS 修改，按照 Cloudflare 的提示将原 DNS 服务器修改为指定的内容。</li><li>等待 DNS 转移成功后，重复第 4 步操作，此时 Cloudflare会<strong>自动</strong>将 DNS 记录为 A 的那条修改为如下内容：<ul><li>主机记录：hwcoder；记录类型：CNAME；记录值为 Cloudflare Pages域名。</li></ul></li></ol><p>通过 Cloudflare 双线部署后，不需要在 <code>_config.yml</code>文件中加入新的部署仓库，因为自带的 Hook 会在每次部署到 GitHub后拉取更新。</p><h3 id="搜索路径优化">搜索路径优化</h3><p>Hexo 博客默认的文章路径是<code>域名/年/月/日/文章标题</code>，这样的多层目录搜索引擎爬虫爬起来非常费力，平时查阅也很困难，因此需要优化文章的URL 路径。</p><p>打开 <code>hewei2001/_config.yml</code>，找到 <code>permalink</code>项，将 <code>:year/:month/:day/:title/</code> 修改为<code>:name.html</code>，就可以用 <code>域名/文章标题</code>访问了。</p><blockquote><p><code>:title</code> 和 <code>:name</code>的区别是：前者访问时会保留相对于 <code>_post</code>目录的路径，改成后者后就是纯粹的文章标题。</p></blockquote><p>此外，还可以将下方的 <code>pretty_url</code> 项中的两个<code>true</code> 改为 <code>false</code> 用于美化URL，两项分别是去除连接中的后缀 <code>index.html</code> 和<code>.html</code> 的。</p><h3 id="添加百度谷歌收录">添加百度谷歌收录</h3><p>如果仅部署在 GitHub Pages，是无法被百度收录的，因为 GitHub禁止了百度爬虫，最常见的解决办法是进行双线部署。</p><p>下面介绍如何提交百度和谷歌搜索：</p><ol type="1"><li><p>进入<ahref="https://link.jianshu.com?t=http://zhanzhang.baidu.com/linksubmit/url">百度搜索提交入口</a>或 <ahref="https://link.jianshu.com?t=https://www.google.com/webmasters/tools/home?hl=zh-CN">Google搜索提交入口</a>，登录。百度的提交入口比较隐蔽，需要进入<code>用户中心 &gt; 站点管理</code>。</p></li><li><p>输入网站后，需要验证对网站的所有权，有三种验证方式，这里选择「HTML<strong>标签验证</strong>」，打开<code>hewei2001/themes/fluid/layout/_partial/head.ejs</code>文件，将验证标签放入 <code>&lt;head&gt;...&lt;/head&gt;</code> 中。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">head</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;baidu-site-verification&quot;</span> <span class="hljs-attr">content</span>=<span class="hljs-string">&quot;code-J3wrn8WJYJ&quot;</span> /&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;google-site-verification&quot;</span> <span class="hljs-attr">content</span>=<span class="hljs-string">&quot;0p_KJKTfB8EcahVDp0vYRjVRhHFw1SBWHi15OakKHY0&quot;</span> /&gt;</span> <br><span class="hljs-tag">&lt;/<span class="hljs-name">head</span>&gt;</span><br><span class="hljs-comment">&lt;!-- 注意每个站点的验证标签都不一样，请勿复制！ --&gt;</span><br></code></pre></td></tr></table></figure></li><li><p>重新 <code>hexo d</code>后，等待数分钟，点击完成验证，就会出现成功提示。</p></li></ol><p>提交搜索后，可以选择添加<strong>站点地图</strong>使搜索引擎更智能地抓取内容：</p><ol type="1"><li><p>在博客目录下打开 Git Bash，输入如下命令安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ npm install hexo-generator-sitemap --save<br>$ npm install hexo-generator-baidu-sitemap --save<br></code></pre></td></tr></table></figure></li><li><p>打开<code>hewei2001/_config.yml</code>，在最下方添加如下字段：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># 自动生成sitemap</span><br><span class="hljs-attr">sitemap:</span><br>  <span class="hljs-attr">path:</span> <span class="hljs-string">sitemap.xml</span><br><span class="hljs-attr">baidusitemap:</span><br>  <span class="hljs-attr">path:</span> <span class="hljs-string">baidusitemap.xml</span><br></code></pre></td></tr></table></figure></li><li><p>重新 <code>hexo d</code> 后，等待数分钟。</p></li><li><p>打开刚才验证网站的页面，找到 <code>sitemap</code>相关字样，输入：</p><ul><li>百度：https://hwcoder.top/baidusitemap.xml</li><li>谷歌：https://hwcoder.top/sitemap.xml</li></ul></li></ol><p>完成以上内容后，就可以静待两个搜索引擎的收录啦，其他搜索引擎也同理，在搜索引擎中输入<code>site:hwcoder.top</code> 就可以实时查看收录结果。</p><h2 id="其他功能扩展">其他功能扩展</h2><p>以下配置是在 Fluid 主题中不具有的功能，通过各种插件实现。</p><h3 id="备份博客到-github">备份博客到 GitHub</h3><p>由于 Hexo博客是静态托管的，所有的原始数据都保存在本地，如果哪一天电脑坏了，或者是误删了本地数据就很危险了。</p><p>GitHub 上可以找到一个 <code>hexo-git-backup</code>插件，但似乎已经不再更新了，仅支持 Hexo 3.x.x 版本，尝试后放弃。</p><h3 id="压缩静态资源">压缩静态资源</h3><p>博客中有大量 HTML、CSS、JS文件，这些文件为了阅读方便会加入许多回车和空行，但在页面解析时其实会浪费部分时间，此外如果有许多插图，也会拖慢网页加载，并占据GitHub 仓库的存储空间。</p><p>目前有关插件有<code>gulp</code>、<code>hexo-neat</code>、<code>hexo-all-minifier</code>。推荐采用集成度比较高的<code>hexo-all-minifier</code>来实现，由于在安装依赖包过程报错，本站最终采用了<code>hexo-neat</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ npm install hexo-all-minifier --save  <span class="hljs-comment"># 出现 npm ERR! code ELIFECYCLE 错误</span><br>$ npm install hexo-neat --save <span class="hljs-comment"># 换成这个后成功安装</span><br></code></pre></td></tr></table></figure><p>之后在配置文件 <code>hewei2001/_config.yml</code>中增加如下内容就行：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># hexo-neat</span><br><span class="hljs-comment">## Docs: https://github.com/rozbo/hexo-neat</span><br><span class="hljs-attr">neat_enable:</span> <span class="hljs-literal">true</span><br><span class="hljs-comment"># 压缩 html</span><br><span class="hljs-attr">neat_html:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">exclude:</span><br><span class="hljs-comment"># 压缩 css  </span><br><span class="hljs-attr">neat_css:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">exclude:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">&#x27;**/*.min.css&#x27;</span><br><span class="hljs-comment"># 压缩 js</span><br><span class="hljs-attr">neat_js:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">mangle:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">output:</span><br>  <span class="hljs-attr">compress:</span><br>  <span class="hljs-attr">exclude:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">&#x27;**/*.min.js&#x27;</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">&#x27;**/jquery.fancybox.pack.js&#x27;</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">&#x27;**/index.js&#x27;</span> <br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>博客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Git</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello My World</title>
    <link href="/Hello-My-World.html"/>
    <url>/Hello-My-World.html</url>
    
    <content type="html"><![CDATA[<p>谨以此文纪念该个人网站的诞生。</p><p>本站基于 Hexo + GitHub 搭建，采用 Fluid 主题。</p><p>这篇文章记录了博客的搭建历程，以及 Hexo 的使用指南。</p><h2 id="想法">想法</h2><p>早在大学入学时，夏老师就告知了个人博客的重要性。</p><p>期间也看过许多漂亮的个人网站，也看到很多大神的技术博客，偶尔会心动一下。</p><p>于是乎，咕到了大二快结束时，打开了这个新世界的大门。</p><h2 id="搭建历程">搭建历程</h2><h3 id="安装-node.js">1. 安装 Node.js</h3><p>官网：<a href="https://nodejs.org/en/">nodejs.org</a></p><p>安装后在 cmd 命令行输入 <code>node -v</code> 即可查看版本。</p><h3 id="注册-github">2. 注册 GitHub</h3><p>注册后，创建仓库：https://github.com/hewei2001/hewei2001.github.io</p><p>此时在浏览器中已经可以访问域名：https://hewei2001.github.io</p><p>注意，仓库名必须取<code>&lt;username&gt;.github.io</code>，如果仓库取了其他名字，最后访问的域名会是：https://用户名.github.io/仓库名/</p><h3 id="下载-git">3. 下载 Git</h3><p>官网：<a href="https://gitforwindows.org/">gitforwindows.org</a></p><p>安装时按照默认配置即可，其中有个 MinTTY终端模拟器的选项会默认选上，如果不选则会使用 Windows 自带的终端 cmd 显示Git Bash。</p><p>安装后在 cmd 命令行输入 <code>git</code> 即可调出功能目录。</p><p>此后就在 cmd 中配置 Git 个人基本信息（绑定 GitHub）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git config --global user.name <span class="hljs-string">&quot;hewei2001&quot;</span><br>$ git config --global user.email <span class="hljs-string">&quot;631670924@qq.com&quot;</span><br></code></pre></td></tr></table></figure><p>配置完才后续才可以正常提交远程仓库（详见博客 Git分类下其他文章）。</p><h3 id="配置-github-ssh">4. 配置 GitHub SSH</h3><p>该步骤旨在建立当前主机与 GitHub的安全连接，以后提交/拉取仓库才不需要重复输入密码。如果你这台主机已经建立过SSH 连接，那么可以跳过这一步。</p><p>打开 Git Bash 输入命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ssh-keygen -t rsa -C <span class="hljs-string">&quot;631670924@qq.com&quot;</span> <span class="hljs-comment"># 获取 SSH Key</span><br></code></pre></td></tr></table></figure><p>完成后，在提示路径下找到 .ssh 文件夹中的 id_rsa.pub公钥文件，用记事本打开拷贝。</p><p>在 GitHub 个人设置中找到 SSH，新建，输入 Key。</p><p>配置后可以用如下命令测试是否成功：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ssh -T git@github.com<br><span class="hljs-comment"># 运行结果出现类似如下即表示成功</span><br><span class="hljs-comment"># Hi hewei2001! You&#x27;ve successfully authenticated, but GitHub does not provide shell access.</span><br></code></pre></td></tr></table></figure><h3 id="安装-hexo">5. 安装 Hexo</h3><p>官网：<a href="https://hexo.io/zh-cn/">hexo.io</a>，有中文官网。</p><p>在电脑中新建 Blog 文件夹，如 <code>D:\Blog</code>。</p><p>在 cmd 命令行打开 D 盘，用 <code>cd Blog</code>命令进入 Blog子目录。或者在文件夹路径栏直接输入 <code>cmd</code>。</p><p>输入 Hexo 官网上的全局安装命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ npm install hexo-cli -g<br></code></pre></td></tr></table></figure><p>输入初始化部署命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo init hewei2001<br></code></pre></td></tr></table></figure><p>即可在 Blog 目录下新建<strong>博客文件夹</strong> hewei2001。用<code>cd hewei2001</code>命令进入博客子目录。</p><p>输入安装 Node.js 包管理器命令，安装所有的依赖：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ npm install<br></code></pre></td></tr></table></figure><p>输入生成本地预览命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo s<br></code></pre></td></tr></table></figure><p>默认会生成 <code>localhost:4000</code>端口的网址，在浏览器中可以访问。</p><h3 id="安装编辑器">6. 安装编辑器</h3><p>主要用于编写文档和修改配置，这里选择 VsCode 即可，打开路径<code>hewei2001/source/_posts/hello-world.md</code>，随便修改内容后保存。</p><p>在 hewei2001 目录中运行 Git Bash。</p><p>依次输入清理缓存、生成博客文件（静态页面）、<strong>生成本地预览</strong>命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo cl <span class="hljs-comment"># 表示 clean</span><br>$ hexo g  <span class="hljs-comment"># 表示 generate</span><br>$ hexo s  <span class="hljs-comment"># 表示 server</span><br></code></pre></td></tr></table></figure><p>在浏览器中访问即可查看更改。之后按下 Ctrl + C 即可退出预览进程。</p><h3 id="将-hexo-发布至-github">7. 将 Hexo 发布至 GitHub</h3><p>打开路径 <code>hewei2001/_config.yml</code>，更改基础参数。</p><ul><li><code>#Site</code> 部分的站点描述自行修改</li><li><code>#URL</code> 部分，将链接改为 https://hewei2001.github.io</li><li><code>#Deployment</code> 部分</li><li><code>type</code> 改为 git</li><li><code>repo</code> 改为https://github.com/hewei2001/hewei2001.github.io.git</li><li><code>branch</code> 改为 main（旧版本默认分支叫 master）</li></ul><p>进入 Hexo 官网，复制 hexo-deployer-git 的安装命令，在 Git Bash中安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ npm install hexo-deployer-git --save<br></code></pre></td></tr></table></figure><p>依次输入清理缓存、生成博客文件（静态页面）、<strong>提交远程仓库</strong>命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo cl<br>$ hexo g<br>$ hexo d  <span class="hljs-comment"># 表示 deploy，安装了上面的 hexo-deployer-git 才可用</span><br></code></pre></td></tr></table></figure><p>初次使用该操作可能会弹出一个 GitHub登录界面，用于当前主机的身份验证。</p><p>在浏览器中访问 https://hewei2001.github.io 即可查看站点。</p><h2 id="hexo-使用">Hexo 使用</h2><p>以下介绍其他常用的 Hexo 操作命令，需要在博客文件夹下打开 Git Bash使用。</p><h3 id="新建文章">新建文章</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo n <span class="hljs-string">&quot;文章名&quot;</span> <span class="hljs-comment"># 代表 new</span><br></code></pre></td></tr></table></figure><p>文章名不需要文件后缀，会自动生成 Markdown文件，且带有预先定义的参数（在 Front-matter中），如标题、日期、标签等。</p><p>以下是一些常用的参数及默认设置：</p><table><thead><tr class="header"><th>参数</th><th>描述</th><th>默认值</th></tr></thead><tbody><tr class="odd"><td><code>title</code></td><td>标题，最好用<strong>引号</strong>括起来</td><td>文章的文件名</td></tr><tr class="even"><td><code>date</code></td><td>建立日期</td><td>文件建立日期</td></tr><tr class="odd"><td><code>updated</code></td><td>更新日期</td><td>文件更新日期</td></tr><tr class="even"><td><code>comments</code></td><td>开启文章的评论功能</td><td>true</td></tr><tr class="odd"><td><code>categories</code></td><td>分类（不适用于分页）</td><td>无</td></tr><tr class="even"><td><code>tags</code></td><td>标签（不适用于分页）</td><td>无</td></tr><tr class="odd"><td><code>excerpt</code></td><td>摘要，优先于 <code>&lt;!-- more --&gt;</code></td><td>无</td></tr><tr class="even"><td><code>index_img</code></td><td>文章封面图，用 /img/ 相对路径</td><td>无</td></tr><tr class="odd"><td><code>math</code></td><td>公式转换，关闭时加速加载</td><td>true</td></tr><tr class="even"><td><code>sticky</code></td><td>文章置顶，数值越大越靠前</td><td>无</td></tr></tbody></table><p>注意 Hexo 采用的是 GitHub Favored Markdown，书写规范与标准 Markdown有微小区别，可以查阅 GitHub 上的说明。</p><h3 id="布局管理">布局管理</h3><p>在上面新建文章时，我们还可以制定布局：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo n [layout] <span class="hljs-string">&quot;文章名&quot;</span> <br></code></pre></td></tr></table></figure><p>其中，<code>layout</code> 可替换为post（文章，默认）、draft（草稿）、page（页面）。默认值可以在配置文件中修改<code>default_layout</code> 来改动。不同布局的文件会存储在不同位置。</p><p>对于 page 布局，Hexo 会创建一个以标题为名字的目录，并在目录中放置一个index.md 文件，页面布局顾名思义就是用来 DIY我们博客页面的，不会被渲染。</p><p>对于 draft 布局，在建立时会被保存到 <code>source/drafts</code>文件夹中，但不会显示在页面上，如果我们不想某一篇文章显示在页面上，也可以把它移动到该文件夹中。</p><p>此外，还有关于 draft 的一些操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo s --draft  <span class="hljs-comment"># 在服务器预览时加入草稿文件</span><br>$ hexo publish [layout] <span class="hljs-string">&quot;文章名&quot;</span>  <span class="hljs-comment"># 将草稿发布为 post 或 page</span><br></code></pre></td></tr></table></figure><h3 id="修改默认布局">修改默认布局</h3><p>上述布局管理中的三种布局，还对应这三种模板，这些模板可以在<code>hewei2001/scaffolds</code> 路径下找到，我们可以提前修改 post模板，这样每次新建文章时就不需要自己添加 Front-matter 了。</p><p>以下是我修改后的 Front-matter 布局：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">title:</span> &#123;&#123; <span class="hljs-string">title</span> &#125;&#125;<br><span class="hljs-attr">excerpt:</span> <span class="hljs-string">&#x27;&#x27;</span><br><span class="hljs-attr">index_img:</span> <span class="hljs-string">/img/home/</span><br><span class="hljs-attr">math:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">date:</span> &#123;&#123; <span class="hljs-string">date</span> &#125;&#125;<br><span class="hljs-attr">updated:</span><br><span class="hljs-attr">category_bar:</span> [<span class="hljs-string">&quot;分类&quot;</span>]<br><span class="hljs-attr">categories:</span><br><span class="hljs-attr">tags:</span><br><span class="hljs-attr">sticky:</span> <br></code></pre></td></tr></table></figure><h3 id="分类和标签">分类和标签</h3><p>在其他系统中，分类和标签听起来很接近，但是在 Hexo中两者有着明显的差别：分类具有顺序性和层次性，也就是说<code>Foo, Bar</code> 不等于<code>Bar, Foo</code>；而标签没有顺序和层次。</p><p>首先要创建「分类」和「标签」本身所在的<strong>页面</strong>，打开博客文件夹，执行<code>hexo new page xxx</code> 命令。Fluid主题不需要，已经自带「分类」、「标签」和「归档」页面了，故这里不展开介绍。</p><p>此后就可以 post 布局的文章指定分类和标签了，也需要在对应文章的Front-matter 中设置。Hexo不鼓励给一篇文章指定<strong>多个同级分类</strong>，因此需要规划好。下面是指定方法：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">categories:</span><br> <span class="hljs-bullet">-</span> <span class="hljs-string">技术经验</span><br> <span class="hljs-bullet">-</span> <span class="hljs-string">博客</span>  <span class="hljs-comment"># 「博客」会作为「技术经验」的子类</span><br><span class="hljs-attr">tags:</span><br> <span class="hljs-bullet">-</span> <span class="hljs-string">Hexo</span><br> <span class="hljs-bullet">-</span> <span class="hljs-string">Git</span>  <span class="hljs-comment"># 「Hexo」与「Git」是同级标签</span><br></code></pre></td></tr></table></figure><p>当然，Hexo 还提供了一种类 WordPress的分类方式，用于指定多个同级分类：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">categories:</span><br><span class="hljs-bullet">-</span> [<span class="hljs-string">Diary</span>, <span class="hljs-string">PlayStation</span>]<br><span class="hljs-bullet">-</span> [<span class="hljs-string">Diary</span>, <span class="hljs-string">Games</span>]<br><span class="hljs-bullet">-</span> [<span class="hljs-string">Life</span>]  <span class="hljs-comment"># 文章会同时出现在三个类别中</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>博客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Git</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
