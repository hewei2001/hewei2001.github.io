<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#01213a"><meta name="description" content="「信息检索」课程笔记，本文介绍向量空间模型的概念与应用、潜层语义索引及奇异值分解的应用。"><meta name="author" content="He Wei"><meta name="keywords" content="Computer science"><title>IR学习笔记 #3 向量空间模型 | Hwcoder</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/night-owl.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"hwcoder.top",root:"/",version:"1.8.11",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h2",placement:"left",visible:"always",icon:"§"},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"XGNf4GEnaFLiUijMUvz8HSBF-gzGzoHsz",app_key:"JPeVLcug6EcWdBTpGrDJSdKi",server_url:"https://xgnf4gen.lc-cn-n1-shared.com"}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>Hwcoder</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于我</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(/img/bg/other.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="IR学习笔记 #3 向量空间模型"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-08-22 22:26" pubdate>2021年8月22日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 2.2k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 26 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">IR学习笔记 #3 向量空间模型</h1><p class="note note-info">本文最后更新于：2021年8月25日 晚上</p><div class="markdown-body"><p>回忆前两个模型，我们发现统计语言模型在布尔模型上，做出了最佳匹配和排序结果的改进。但是，仍然没有考虑到「<strong>词项的权重</strong>」。</p><p>在向量空间模型中，我们容易联想到用向量来表示文档和查询，再通过计算余弦来得到两个向量的距离，从而得到相似性度量。</p><p>那么，如何选取向量空间 basis vector (基向量)？如何将目标转化为向量？如何为各个维度选取 magnitide (幅值)，从而考虑权重？如何在高维空间计算向量距离？</p><h2 id="Vector-Space-Model-向量空间模型"><a href="#Vector-Space-Model-向量空间模型" class="headerlink" title="Vector Space Model | 向量空间模型"></a>Vector Space Model | 向量空间模型</h2><p>通常地，我们选择用 linearly independent (线性独立) 或 orthogonal (正交) 的基向量来张成<strong>向量空间</strong>，这样可以使得维度最少。那么，如何选取基向量？</p><p>这是一个特征选择问题，在 IR 中，通常有两种方式：</p><ol><li><p>Core concept (核心概念) 的思想：把词语的类型分类，按照其在不同分类上的「倾斜程度」决定向量的值，可以使维度尽量少。但是，由于语义上的多样性，很难实现。目前有 WordNet, HowNet, HNC 等模型。</p></li><li><p>把出现过的 term 都当作是一个基向量，并<strong>假设</strong>所有的基向量都是相互正交、相互独立的。这样将会得到一个维度不断增长的向量空间（随着词典表扩大）。</p></li></ol><p>以下我们采用第二种方式。一个 Doc 或 Query 的向量表示就是：所有出现在文档中的 term 的向量之和。</p><h3 id="Term-Weighting-词项权重"><a href="#Term-Weighting-词项权重" class="headerlink" title="Term Weighting | 词项权重"></a>Term Weighting | 词项权重</h3><p>当一个 term 在文档中不断出现时，在这个方向上的向量幅值就会很大。这样比起布尔模型的 0/1 二值，更能反映了这个 term 的重要性。这便是决定权重的 <em>tf</em> (<strong>term frequency</strong>，词项频率) 方法。</p><p>然而，原始的 <em>tf</em> 值会面临这样一个严重的问题：即在和查询进行相关度计算时，所有 term 都被认为是同等重要的。</p><p>实际上，某些 term 对于相关度计算来说几乎没有或很少有区分能力。一个很直接的想法就是给包含在较多文档中的词项赋予较低的权重。为此，引入变量 <em>df</em> (<strong>document frequency</strong>，文档集频率)，即有多少文档包含了该 term。df 值越大，说明该 term 越不重要。</p><p>为了计算的方便，将其标准化得到 <em>idf</em> (<strong>inverse document frequency</strong>，逆文档频率)：</p><script type="math/tex;mode=display">idf_t=\log \left( \frac{N}{df_t} \right)</script><p>观察该式发现，<em>idf</em> 虽然可以使得在较多文档中的词项权值降低，但与 <em>tf</em> 相反的是，这样做的缺点是：对那些极少出现的词极度敏感。</p><p>为此，我们将二者结合在一起，诞生了 <strong><em>tf·idf</em></strong> 方法——在文本处理领域中使用最广泛的数值权重计算方法。方法基于的思想和构造的统计量都很简单，但是在实际中却表现了很好的性能。</p><p>在 VSM 中，我们会将词项的 <em>tf·idf</em> 存储在词典表（词项-文档）矩阵中，作为向量的幅值，用于后续的计算。</p><h3 id="Similarity-相似度计算"><a href="#Similarity-相似度计算" class="headerlink" title="Similarity | 相似度计算"></a>Similarity | 相似度计算</h3><p>当我们已经把文档表示成 $R^{v}$ 上的向量，从而可以计算文档与文档之间的相似度（根据向量内积或者<strong>余弦夹角</strong>）。</p><p>设 $D_1$ 和 $D_2$ 表示 VSM 中的两个向量：</p><script type="math/tex;mode=display">\begin{aligned}
&D_{1}=D_{1}\left(w_{11}, w_{12}, \ldots, w_{1 n}\right) \\
&D_{2}=D_{2}\left(w_{21}, w_{22}, \ldots, w_{2 n}\right)
\end{aligned}</script><p>可以借助于 N 维空间中两个向量之间的某种距离来表示文档之间的相似度，常用的方法是使用向量之间的內积来计算：</p><script type="math/tex;mode=display">\operatorname{Sim}\left(D_{1}, D_{2}\right)=\sum_{k=1}^{n} w_{1 k} \times w_{2 k}</script><p>考虑到向量的<strong>归一化</strong>，则可以使用两个向量的余弦值来表示相似系数：</p><script type="math/tex;mode=display">\operatorname{Sim}\left(D_{1}, D_{2}\right)=\cos \theta=\frac{\sum_{k=1}^{n} w_{1 k} \times w_{2 k}}{\sqrt{\sum_{k=1}^{n} w_{1 k}^{2} \sum_{k=1}^{n} w_{2 k}^{2}}}</script><p>要注意，这里使用向量内积，是基于对所有向量相互独立、相互正交的假设，否则计算内积也就失去了意义。对于相关的基向量，应该评估 Term 之间的相关度 $T_{i,j}$，再把向量当成多项式计算，最后代入 $T_{i,j}$。</p><p>此外，在其他的考虑权重的模型中，如 Lucene，在计算相似度时引入了更多的因子，如 <em>tf·idf</em>，$boost_t$，<em>overlap(q,d)</em> 等，对应用情形、平滑度加以考量。</p><h3 id="VSM-实际应用"><a href="#VSM-实际应用" class="headerlink" title="VSM 实际应用"></a>VSM 实际应用</h3><p>在 IR 中应用 VSM 模型时，相似度在检索结果中有两种体现：</p><ol><li><strong>Threshold</strong> (阈值)：对于每个查询，只在相似度大于一定阈值的文档中检索，如 Sim &gt; 0.50 的文档中，减少查询范围。</li><li><strong>Ranking</strong>：对于每个查询，返回相似度排名 Top n 的文档，以相似度排序。</li></ol><p>而 VSM 模型也有着致命的<strong>缺点</strong>：</p><ul><li><p>对于大的文档集（10w+ term），向量维度太多导致难以存储和计算。</p></li><li><p>一篇文档的词数（1k+ term）远低于总的词数——高维稀疏矩阵。</p></li><li>词项之间的相关性，导致了大量冗余的基向量。</li></ul><h2 id="Latent-Semantic-Indexing-潜层语义索引"><a href="#Latent-Semantic-Indexing-潜层语义索引" class="headerlink" title="Latent Semantic Indexing | 潜层语义索引"></a>Latent Semantic Indexing | 潜层语义索引</h2><p>潜层语义索引，也被称为 LSA (Latent Semantic Analysis，潜在语义分析)，是针对向量空间的「<strong>高维稀疏</strong>」问题提出的解决方法，利用线性代数中的<strong>奇异值分解</strong>降低维度（去除噪音），同时尽量减少信息的损失。</p><h3 id="Singular-Value-Decomposition-奇异值分解"><a href="#Singular-Value-Decomposition-奇异值分解" class="headerlink" title="Singular Value Decomposition | 奇异值分解"></a>Singular Value Decomposition | 奇异值分解</h3><p>参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6251584.html">https://www.cnblogs.com/pinard/p/6251584.html</a></p><p>对于一个 $t\times d$​​ 矩阵 $A$​​​，可以分解为下面三个矩阵：</p><script type="math/tex;mode=display">A_{t\times d}=U_{t\times t}\varSigma _{t\times d}V^T_{d\times d}</script><p>其中 $U$​ 和 $V$​ 都是<strong>酉矩阵</strong>，即满足 $U^TU=I, V^TV=I$​。$\varSigma$​ 一个 $t\times d$​ 矩阵，除了主对角线上的元素以外全为 0，主对角线上的每个元素都称为<strong>奇异值</strong>。</p><p>利用酉矩阵性质得：</p><script type="math/tex;mode=display">A=U\Sigma V^T \Rightarrow A^T=V\Sigma^T U^T \Rightarrow A^TA = V\Sigma^T U^TU\Sigma V^T = V\Sigma^2V^T</script><p>可以看出 $A^TA$ 的特征向量组成的矩阵，就是我们 SVD 中的 $V^T_{d\times d}$​ 矩阵。进一步我们还可以看出我们的特征值矩阵等于奇异值矩阵的平方。</p><p>利用以上原理，我们可以得出 SVD <strong>分解步骤</strong>：</p><ol><li>假设词典矩阵为 $A$，首先求出 $AA^T$，会得到一个 $t\times t$ 的方阵。</li><li>既然是方阵，就可以进行特征值分解，得到 <em>t</em> 个特征值和对应的特征向量。</li><li>将特征值按方差大小排序，用所有的列向量张成一个 $t\times t$ 的矩阵 $U_{t\times t}$。</li><li>同理可以用 $A^TA$ 求出 $d\times d$ 的矩阵 $V^T_{d\times d}$。</li><li>利用前面求出的特征值，开方后得到 $\varSigma _{t\times d}$。​</li></ol><h3 id="利用-SVD-降维"><a href="#利用-SVD-降维" class="headerlink" title="利用 SVD 降维"></a>利用 SVD 降维</h3><p>对于奇异值，它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列。通常，奇异值的<strong>衰减</strong>得特别快，在很多情况下，前 10% 甚至 1% 的奇异值之和就占了全部的奇异值之和的 99% 以上的比例。</p><p>也就是说，我们也可以用最大的 k 个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：</p><script type="math/tex;mode=display">A_{t\times d}=U_{t\times t}\varSigma _{t\times d}V^T_{d\times d}\approx U_{t\times k}\varSigma _{k\times k}V^T_{k\times d}</script><p>其中 <em>k</em> 要比 <em>t</em> 小很多，也就是一个大的矩阵可以用三个小的矩阵，此时存储空间可以大量节省。通常 <em>k</em> 的值即为我们假设的<strong>主题数</strong>。</p><p>SVD 分解后，$U_{il}$ 对应第 <em>i</em> 个词和第 <em>l</em> 个词义的相关度。$V_{jm}$ 对应第 <em>j</em> 个文档和第 <em>m</em> 个主题的相关度。$\Sigma_{lm}$ 对应第 <em>l</em> 个词义和第 <em>m</em> 个主题的相关度。</p><p>这样我们通过一次 SVD，就可以得到词和词义的相关度，词义和主题的相关度，以及文档和主题的相关度。</p><h3 id="LSI-的使用"><a href="#LSI-的使用" class="headerlink" title="LSI 的使用"></a>LSI 的使用</h3><p>通过计算后，我们关注新的矩阵 $V^T_{k\times d}$​ ，所有的文档已经简化成了和 k 个主题的相关度。假设此时的查询为 $Q=q_1q_2\cdots q_t$​​​​，​其中 <em>q</em> 取 0 或 1，则</p><script type="math/tex;mode=display">Q_{1\times k}=Q_{1\times t}U_{t\times k}\varSigma _{k\times k}</script><p>可将 <em>t</em> 维的查询转化成 <em>k</em> 维的「<strong>与主题的相关度</strong>」，此时就可以与文档进行相似度计算了。</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="hover-with-bg" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/">信息检索</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/IR/">IR</a> <a class="hover-with-bg" href="/tags/NLP/">NLP</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow noopener noopener"><u>CC BY-NC-SA 4.0</u></a> 协议，转载请注明来自 <a href="https://hwcoder.top" rel="noopener"><u>Hwcoder</u></a>！</p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/08/25/IR-Note-4/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">IR学习笔记 #4 概率模型</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/08/22/IR-Note-2/"><span class="hidden-mobile">IR学习笔记 #2 统计语言模型</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",function(){var e=Object.assign({appId:"jdbBr3BddTiqSCPnXw6sXFv7-gzGzoHsz",appKey:"2bjgwDr2opjVCwhgjDMpk53c",placeholder:"说点什么吧( •̀ ω •́ )✧",path:"window.location.pathname",avatar:"retro",meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",emojiCDN:null,emojiMaps:null,enableQQ:!1,requiredFields:["nick"]},{el:"#valine",path:window.location.pathname});new Valine(e)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script src="/js/local-search.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>