<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#01213a"><meta name="description" content="「信息检索」课程笔记，本文由朴素贝叶斯分类引入，介绍概率检索模型、二值独立检索公式的推导和估算、BM25 权重修正。"><meta name="author" content="He Wei"><meta name="keywords" content="Computer Science and Technology, Machine Learning, Deep Learning, Computer Vision, Natural Language Processing"><meta name="baidu-site-verification" content="code-J3wrn8WJYJ"><meta name="google-site-verification" content="0p_KJKTfB8EcahVDp0vYRjVRhHFw1SBWHi15OakKHY0"><meta name="description" content="「信息检索」课程笔记，本文由朴素贝叶斯分类引入，介绍概率检索模型、二值独立检索公式的推导和估算、BM25 权重修正。"><meta property="og:type" content="article"><meta property="og:title" content="IR学习笔记 #4 概率模型"><meta property="og:url" content="https://hwcoder.top/IR-Note-4"><meta property="og:site_name" content="Hwcoder - Life Oriented Programming"><meta property="og:description" content="「信息检索」课程笔记，本文由朴素贝叶斯分类引入，介绍概率检索模型、二值独立检索公式的推导和估算、BM25 权重修正。"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2021-08-25T08:24:45.000Z"><meta property="article:modified_time" content="2021-10-05T04:53:10.365Z"><meta property="article:author" content="He Wei"><meta property="article:tag" content="IR"><meta property="article:tag" content="NLP"><meta name="twitter:card" content="summary_large_image"><title>IR学习笔记 #4 概率模型 | Hwcoder - Life Oriented Programming</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/night-owl.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/css/mac.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"hwcoder.top",root:"/",version:"1.8.12",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h2, h3",placement:"left",visible:"always",icon:"§"},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"XGNf4GEnaFLiUijMUvz8HSBF-gzGzoHsz",app_key:"JPeVLcug6EcWdBTpGrDJSdKi",server_url:"https://xgnf4gen.lc-cn-n1-shared.com",path:"window.location.pathname"}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Hwcoder</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于我</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(/img/bg/other.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="IR学习笔记 #4 概率模型"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-08-25 16:24" pubdate>2021年8月25日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 4.6k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 19 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">IR学习笔记 #4 概率模型</h1><p class="note note-info">本文最后更新于：2021年10月5日 下午</p><div class="markdown-body"><p>前面介绍的模型，都是通过对<strong>相似性</strong>的计算，得出最佳匹配的模型。而概率检索模型，则是基于概率原理，越过相似性，直接对<strong>相关性</strong>进行计算的一种检索模型。</p><p>利用相关性有一个好处，就是对于两个不相似的词，如果它们因为某些因素联系起来了，那么也会出现在检索结果中。</p><h2 id="朴素贝叶斯分类-Naive-Bayes"><a href="#朴素贝叶斯分类-Naive-Bayes" class="headerlink" title="朴素贝叶斯分类 | Naive Bayes"></a>朴素贝叶斯分类 | Naive Bayes</h2><p>贝叶斯公式是概率论中非常基础的公式，其解决的核心点在于根据已有信息，对未知事物发生结果的概率计算。这里简单介绍一下作为模型的引入。</p><p>如果我们有文档 <em>D</em>，可以记 $P(R=1|D)$ 为文档和查询相关的概率（这里的 <em>R</em> 只有 0 和 1 两种取值），这表示在文档确定的情况下，发生 $R=1$ 假设的<strong>后验概率</strong>。</p><p>与此同时，$P(R=1)$ 可以表示该假设的<strong>先验概率</strong>，意思是在完全对文档无所知的情况下，这个文档的<strong>分类</strong>情况满足假设的概率。</p><p>以下我们将 $R=1$ 简写为 <em>R</em>，$R=0$ 简写为 <em>NR</em>，表示一下贝叶斯公式：</p><script type="math/tex;mode=display">P(R|D) = \frac{P(RD)}{P(D)} = \frac{P(D|R)P(R)}{P(D)}</script><p>实际上，贝叶斯公式是做了一个转换，将复杂的概率式转化为三个更好计算的概率式。</p><ul><li>$P(D)$ 表示随机选取一篇文档，恰好是特定的 <em>D</em> 的概率，这个概率对于所有文档都是一致的，如果只是作比较，就不需要考虑。</li><li>$P(R)$ 表示假设 <em>R</em> 成立的先验概率，如果有已知的数据集，我们可以用相似文档的频率近似概率；如果没有，也可以先设为 0.5。但应用在比较中，也是不需要考虑的。</li><li>$P(D|R)$ 表示任意一篇文档被归类为相似后，恰好是特定的 <em>D</em> 的概率，需要所用特殊的方法来估计。</li></ul><p>所以，<strong>判断</strong>一篇文档是否相似，只需要比较 $P(R|D)$ 和 $P(NR|D)$ 两个值的大小，就是比较 $P(D|R)P(R)$ 和 $P(D|NR)P(NR)$。</p><h2 id="概率检索-Probabilistic-Retrieval"><a href="#概率检索-Probabilistic-Retrieval" class="headerlink" title="概率检索 | Probabilistic Retrieval"></a>概率检索 | Probabilistic Retrieval</h2><p>概率检索模型与贝叶斯分类的思想非常接近，但还是有本质区别的。概率检索模型的根本目的不是<strong>分类</strong>，它不需要根据查询判断一个文档属于“相关”或者“不相关”，而是计算这个文档属于属于“相关”或者“不相关”的<strong>概率大小</strong>为文档<strong>排序</strong>。</p><p>因此，在概率检索模型中，我们首先要定义一个<strong>相关度指标</strong>，考虑前文中提到的 $P(D|R)P(R)$ 和 $P(D|NR)P(NR)$，由于 $P(R)$ 和 $P(NR)$ 在同一个查询下对所有文档都是一致的，因此只要关注剩余部分之比（也称为<strong>优势率</strong>）：</p><script type="math/tex;mode=display">\alpha = \frac{P(D|R)}{P(D|NR)}</script><p>显然，这个比值越大，代表该文档与查询的相关度越大，因此我们最后就通过 $\alpha$ 将文档排序。</p><h3 id="风险最小化-Risk-Minimization"><a href="#风险最小化-Risk-Minimization" class="headerlink" title="风险最小化 | Risk Minimization"></a>风险最小化 | Risk Minimization</h3><p>此外，在检索过程中，我们还要决定一篇文档是否被召回，即设定一个<strong>召回阈值</strong>。一般我们会选择<strong>贝叶斯最优决策定理</strong>（Bayes’ Optimal Decision Rule）来决定一个文档是否相关，进而确定是否将其返回。</p><p>所谓的贝叶斯最优决策定理其实很简单，当 $P\left( R|D \right) &gt;P\left( NR|D \right) $ 时，我们认定该文档是相关文档，将其返回。</p><p>但在实际中，我们还要考虑<strong>最小化期望损失</strong>（<strong>也称为贝叶斯风险</strong>，Bayes Risk），即「返回一篇不相关文档」或「没有返回一篇相关文档」的损失。</p><p>举个例子，在就诊看病的过程中，将患病者诊断为「健康」而错失治疗时机，远比健康者诊断为「患病」代价大得多。因此我们也认为「没有返回一篇相关文档」的代价要大于「返回一篇不相关文档」。</p><p>如果记 $c_{rr}$ 为 cost of deciding <strong>relevant when relevant</strong>， $c_{rn}$ 为 cost of deciding <strong>relevant when not relevant</strong>，$c_{nn}$ 和 $c_{nr}$ 同理。那么就有：</p><script type="math/tex;mode=display">c_{nr}P\left( R|D \right) +c_{nn}P\left( NR|D \right) >c_{rn}P\left( NR|D \right) +c_{rr}P\left( R|D \right)</script><p>移项，并引入贝叶斯公式后：</p><script type="math/tex;mode=display">\left( c_{nr}-c_{rr} \right) P\left( D|R \right) P\left( R \right) >\left( c_{rn}-c_{nn} \right) P\left( D|NR \right) P\left( NR \right)</script><p>结合相关度指标，可以等到新的阈值：</p><script type="math/tex;mode=display">\alpha =\frac{P(D|R)}{P(D|NR)}>\frac{\left( c_{rn}-c_{nn} \right)P\left( NR \right)}{\left( c_{nr}-c_{rr} \right)P\left( R \right)}</script><h2 id="二值独立检索-Binary-Independence-Retrieval"><a href="#二值独立检索-Binary-Independence-Retrieval" class="headerlink" title="二值独立检索 | Binary Independence Retrieval"></a>二值独立检索 | Binary Independence Retrieval</h2><p>前面提到，$P(D|R)$ 表示任意一篇文档被归类为相似后，恰好是特定的 <em>D</em> 的概率，在通常的<strong>朴素贝叶斯分类</strong>中，通常有两种方法来估计：</p><ul><li>用 <em>D</em> 在类别 <em>R</em> 中的比例来估计，显然，这个方法在检索中不适用，因为同一文档 <em>D</em> 几乎不可能在 <em>R</em> 中出现过。</li><li>将 <em>D</em> 看作由 0 和 1 <strong>二值</strong>组成的向量，每个维度代表了一种词项是否包含在该文档中，默认词项之间是相互<strong>独立</strong>的，然后用下面的公式计算：</li></ul><script type="math/tex;mode=display">P(D|R) = \prod_{T_i \in D}{P(T_i=1|R)}\prod_{T_j \notin D}{P(T_j=0|R)}</script><p>其中，<em>T</em> 就代表文档中的词项 term，$P(T|R)$ 就是该词项在归类为相似的文档集中<strong>出现或不出现</strong>的概率。</p><h3 id="公式推演"><a href="#公式推演" class="headerlink" title="公式推演"></a>公式推演</h3><p>在以上的概念下，不妨记：</p><ul><li>相似文档集中 $P(T_k=1|R)$ 为 $p_k$，$P(T_k=0|R)$ 为 $1-p_k$。</li><li>不相似文档集中 $P(T_k=1|NR)$ 为 $q_k$，$P(T_k=0|NR)$ 为 $1-q_k$。</li></ul><p>则相关度指标可表示为：</p><script type="math/tex;mode=display">\alpha = \frac{\prod_{T_k \in D}p_k \prod_{T_k \notin D} 1 - p_k}{\prod_{T_k \in D}q_k \prod_{T_k \notin D}1 - q_k}</script><p>再做一个数学上的等价变换，如下：</p><script type="math/tex;mode=display">\begin{aligned}
\alpha &= \frac{\prod_{T_k \in D}p_k \prod_{T_k \notin D}1 - p_k}{\prod_{T_k \in D}q_k \prod_{T_k \notin D}1 - q_k} = \frac{\prod_{T_k \in D}p_k}{\prod_{T_k \in D}q_k} \cdot \frac{\prod_{T_k \notin D}1 - p_k}{\prod_{T_k \notin D}1 - q_k}\\
&= (\frac{\prod_{T_k \in D}p_k}{\prod_{T_k \in D}q_k} \cdot \frac{\prod_{T_k \in D}1 - q_k}{\prod_{T_k \in D}1 - p_k}) \cdot (\frac{\prod_{T_k \in D}1 - p_k}{\prod_{T_k \in D}1 - q_k} \frac{\prod_{T_k \notin D}1 - p_k}{\prod_{T_k \notin D}1 - q_k})\\
&= \frac{\prod_{T_k \in D}p_k(1 - q_k)}{\prod_{T_k \in D}q_k(1 - p_k)} \cdot \frac{\prod 1 - p_k}{\prod 1 - q_k} 
\end{aligned}</script><p>在同一查询下，相似文档集和不相似文档集是固定的，也就是说 $p_k$ 和 $q_k$ 的值也是相同的。故公式的第二部分（与文档 <em>D</em> 无关）可以忽略，简化成</p><script type="math/tex;mode=display">\alpha=\prod_{T_k \in D}\frac{p_k(1 - q_k)}{q_k(1 - p_k)}</script><p>取对数将乘积转化为求和得到用于排序的两，称为 RSV (Retrieval Status Value，<strong>检索状态值</strong>)：</p><script type="math/tex;mode=display">RSV_D=\sum_{T_k \in D} \left(\log \frac{p_k}{1 - p_k} + \log \frac{1 - q_k}{q_k} \right)</script><h3 id="Estimation-using-training-data"><a href="#Estimation-using-training-data" class="headerlink" title="Estimation using training data"></a>Estimation using training data</h3><p>现在我们只要计算出 $p_k$ 和 $q_k$ 的值就成功了。在计算之前，我们先写出下面的索引项出现列联表：</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">相关文档</th><th style="text-align:center">不相关文档</th><th style="text-align:center">总数</th></tr></thead><tbody><tr><td style="text-align:center"><strong>包含</strong>词项 $T_k$</td><td style="text-align:center">r</td><td style="text-align:center">n-r</td><td style="text-align:center">n</td></tr><tr><td style="text-align:center"><strong>不包含</strong>词项 $T_k$</td><td style="text-align:center">R-r</td><td style="text-align:center">N-n-R+r</td><td style="text-align:center">N-n</td></tr><tr><td style="text-align:center"><strong>总数</strong></td><td style="text-align:center">R</td><td style="text-align:center">N-R</td><td style="text-align:center">N</td></tr></tbody></table></div><p>则可以得到估算式：</p><script type="math/tex;mode=display">p_k=\frac{r}{R}, q_k=\frac{n-r}{N-R}</script><p>同时，为了避免可能出现的<strong>零频问题</strong>（比如所有的相关文档都包含或不包含某个特定的词项），一种很常规的做法是在之前的表格中的每个量的基础上都加上 0.5 来<strong>平滑处理</strong>，因此总数也做相应改变：</p><script type="math/tex;mode=display">p_k=\frac{r+0.5}{R+1}, q_k=\frac{n-r+0.5}{N-R+1}</script><h3 id="Estimation-without-training-data"><a href="#Estimation-without-training-data" class="headerlink" title="Estimation without training data"></a>Estimation without training data</h3><p>用上式代入得到的计算式也称作 <code>Robertson-Sparck Jones</code> 等式，这个式子的计算条件是知道相关文档总数 R，但实际上大多数时候我们都是不知道的。</p><p>一种可行的方案是，初始时令相关文档数为 0，这是因为在实际检索情景下，文档库中往往只有少部分是和查询词相关的内容：</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">相关文档</th><th style="text-align:center">不相关文档</th><th style="text-align:center">总数</th></tr></thead><tbody><tr><td style="text-align:center"><strong>包含</strong>词项 $T_k$</td><td style="text-align:center">0</td><td style="text-align:center">n</td><td style="text-align:center">n</td></tr><tr><td style="text-align:center"><strong>不包含</strong>词项 $T_k$</td><td style="text-align:center">0</td><td style="text-align:center">N-n</td><td style="text-align:center">N-n</td></tr><tr><td style="text-align:center"><strong>总数</strong></td><td style="text-align:center">0</td><td style="text-align:center">N</td><td style="text-align:center">N</td></tr></tbody></table></div><p>此时的 $p_k$ 值可以用常数来代替，如 0.3。</p><h2 id="修正公式"><a href="#修正公式" class="headerlink" title="修正公式"></a>修正公式</h2><p>在本文的最后，我们再来讨论一个问题，在前面讲到的 $p_k$ 和 $q_k$的值估算过程中，我们其实是用到了之前提过的<strong>文档频率</strong> <em>df</em>。</p><p>而在之前的文章中，还有词频、逆文档频率、文档长度等等多种因素未被考虑到。因此，基于最初的原理和假设，可以对原来的 RSV 公式增加修正因子，使得模型更加精确。</p><h3 id="BM25-Weighting"><a href="#BM25-Weighting" class="headerlink" title="BM25 Weighting"></a>BM25 Weighting</h3><p>这是一种最常用的加权方法，考虑了<strong>词频</strong>和<strong>文档长度</strong>。BM25 模型为文档 $D_i$ 每个词项项 $T_j$ 分配了一个系数 $B_{i,j}$ ，由下计算生成：</p><script type="math/tex;mode=display">B_{i,j}=\frac{\left( K_1+1 \right) f_{i,j}}{K_1\left[ (1-b)+b\frac{\mathrm{len}\left( D_i \right)}{\,\,\mathrm{avg}\_\mathrm{doclen}} \right] +f_{i,j}}</script><p>其中，$K_1$ 和 <em>b</em> 为经验参数，用于调节词频和文档长度在权重计算中起到的作用，一般来讲， $K_1$ 取 1，<em>b</em> 取 0.75 已经被证明是合理的假设。而 $f_{i,j}$ 则为词项 $T_j$ 在文档 $D_i$ 中的词频，avg_doclen 为平均文档长度。</p><h3 id="Multiple-Fields"><a href="#Multiple-Fields" class="headerlink" title="Multiple Fields"></a>Multiple Fields</h3><p>在 BM25 的之后，还有一种针对其提出的修正方法。将文档划分成不同的<strong>域</strong>，如：title/abstract/body，并对不同域赋予不同的权重（每个 term 出现的<strong>当量</strong>不同）。例如，term 在标题出现 1 次相当于在 abstract 出现 1.5 次。</p><p>同理，文档长度也相应的进行加权调整，最后可以计算出新的修正因子：</p><script type="math/tex;mode=display">\begin{aligned}
\widetilde{t f}_{i} &=\sum_{s=1}^{S} w_{s} t f_{s i} \\
\widetilde{d l} &=\sum_{s=1}^{S} w_{s} s l_{s}
\end{aligned}</script><p>最后计算出的频度替换原始的频度，代入 BM25 Weighting 公式。</p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="hover-with-bg" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/">信息检索</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/IR/">IR</a> <a class="hover-with-bg" href="/tags/NLP/">NLP</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow noopener noopener"><u>CC BY-NC-SA 4.0</u></a> 协议，转载请注明来自 <a href="https://hwcoder.top" rel="noopener"><u>Hwcoder</u></a>！</p><div class="post-prevnext"><article class="post-prev col-6"><a href="/IR-Note-5"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">IR学习笔记 #5 检索系统评价</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/IR-Note-3"><span class="hidden-mobile">IR学习笔记 #3 向量空间模型</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"jdbBr3BddTiqSCPnXw6sXFv7-gzGzoHsz","appKey":"2bjgwDr2opjVCwhgjDMpk53c","path":"window.location.pathname","placeholder":"说点什么吧( •̀ ω •́ )✧","avatar":"retro","meta":["nick","mail","link"],"requiredFields":["nick"],"pageSize":10,"lang":"zh-CN","highlight":true,"recordIP":true,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          Fluid.plugins.initFancyBox('#valine .vcontent img:not(.vemoji)');
        })
      });
    });</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/local-search.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script>MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>