<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#01213a"><meta name="author" content="Wei He"><meta name="keywords" content="Computer Science and Technology, Machine Learning, Deep Learning, Computer Vision, Natural Language Processing"><meta name="description" content="「强化学习」阅读笔记，本节介绍了 DPO 算法的理论推导（如何绕过显式奖励建模，建立策略和偏好的映射关系），将 DPO 与 PPO 进行了对比，分析了两种算法的局限性。"><meta property="og:type" content="article"><meta property="og:title" content="RL 学习笔记 #13 直接偏好优化（DPO）理论"><meta property="og:url" content="https://hwcoder.top/RL-Note-13"><meta property="og:site_name" content="Hwcoder - Life Oriented Programming"><meta property="og:description" content="「强化学习」阅读笔记，本节介绍了 DPO 算法的理论推导（如何绕过显式奖励建模，建立策略和偏好的映射关系），将 DPO 与 PPO 进行了对比，分析了两种算法的局限性。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hwcoder.top/img/home/RL.jpg"><meta property="article:published_time" content="2025-03-02T04:22:14.000Z"><meta property="article:modified_time" content="2025-03-22T14:27:01.633Z"><meta property="article:author" content="Wei He"><meta property="article:tag" content="RL"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://hwcoder.top/img/home/RL.jpg"><title>RL 学习笔记 #13 直接偏好优化（DPO）理论 | Hwcoder - Life Oriented Programming</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/mac.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"hwcoder.top",root:"/",version:"1.9.5",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h2, h3",placement:"left",visible:"always",icon:"§"},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:null,google:{measurement_id:null},tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"XGNf4GEnaFLiUijMUvz8HSBF-gzGzoHsz",app_key:"JPeVLcug6EcWdBTpGrDJSdKi",server_url:"https://xgnf4gen.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!0}},search_path:"/local-search.xml",include_content_in_search:!0};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>Fluid.ctx.dnt||Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=",function(){function a(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],a("js",new Date),a("config","")})</script><meta name="generator" content="Hexo 5.4.2"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Hwcoder</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/bg/other.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="RL 学习笔记 #13 直接偏好优化（DPO）理论"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-03-02 12:22" pubdate>2025年3月2日 中午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 12k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 66 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="学习笔记" id="heading-078425eaf316a180b0989442e53f920b" role="tab" data-toggle="collapse" href="#collapse-078425eaf316a180b0989442e53f920b" aria-expanded="true">学习笔记 <span class="list-group-count">(46)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-078425eaf316a180b0989442e53f920b" role="tabpanel" aria-labelledby="heading-078425eaf316a180b0989442e53f920b"><div class="category-post-list"></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="信息检索" id="heading-b10f39c13b5f9af517dfde91c4da0ec4" role="tab" data-toggle="collapse" href="#collapse-b10f39c13b5f9af517dfde91c4da0ec4" aria-expanded="false">信息检索 <span class="list-group-count">(11)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-b10f39c13b5f9af517dfde91c4da0ec4" role="tabpanel" aria-labelledby="heading-b10f39c13b5f9af517dfde91c4da0ec4"><div class="category-post-list"><a href="/IR-Note-1" title="IR学习笔记 #01 概论&amp;布尔模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #01 概论&amp;布尔模型</span> </a><a href="/IR-Note-2" title="IR学习笔记 #02 统计语言模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #02 统计语言模型</span> </a><a href="/IR-Note-3" title="IR学习笔记 #03 向量空间模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #03 向量空间模型</span> </a><a href="/IR-Note-4" title="IR学习笔记 #04 概率模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #04 概率模型</span> </a><a href="/IR-Note-5" title="IR学习笔记 #05 检索系统评价" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #05 检索系统评价</span> </a><a href="/IR-Note-6" title="IR学习笔记 #06 网络信息检索" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #06 网络信息检索</span> </a><a href="/IR-Note-7" title="IR学习笔记 #07 IRLbot" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #07 IRLbot</span> </a><a href="/IR-Note-8" title="IR学习笔记 #08 倒排索引模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #08 倒排索引模型</span> </a><a href="/IR-Note-9" title="IR学习笔记 #09 网页排序" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #09 网页排序</span> </a><a href="/IR-Note-10" title="IR学习笔记 #10 查询相关反馈" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #10 查询相关反馈</span> </a><a href="/IR-Note-11" title="IR学习笔记 #11 问答系统" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #11 问答系统</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem list-group-item category-item-action col-10 col-md-11 col-xm-11" title="强化学习" id="heading-6964a39570837d485ed73b611a392391" role="tab" data-toggle="collapse" href="#collapse-6964a39570837d485ed73b611a392391" aria-expanded="true">强化学习 <span class="list-group-count">(13)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-6964a39570837d485ed73b611a392391" role="tabpanel" aria-labelledby="heading-6964a39570837d485ed73b611a392391"><div class="category-post-list"><a href="/RL-Note-1" title="RL 学习笔记 #01 基本概念" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #01 基本概念</span> </a><a href="/RL-Note-2" title="RL 学习笔记 #02 贝尔曼公式" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #02 贝尔曼公式</span> </a><a href="/RL-Note-3" title="RL 学习笔记 #03 值迭代和策略迭代" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #03 值迭代和策略迭代</span> </a><a href="/RL-Note-4" title="RL 学习笔记 #04 蒙特卡洛学习算法" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #04 蒙特卡洛学习算法</span> </a><a href="/RL-Note-5" title="RL 学习笔记 #05 随机近似与随机梯度下降" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #05 随机近似与随机梯度下降</span> </a><a href="/RL-Note-6" title="RL 学习笔记 #06 时序差分学习算法" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #06 时序差分学习算法</span> </a><a href="/RL-Note-7" title="RL 学习笔记 #07 值函数近似和 DQN 算法" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #07 值函数近似和 DQN 算法</span> </a><a href="/RL-Note-8" title="RL 学习笔记 #08 策略梯度方法" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #08 策略梯度方法</span> </a><a href="/RL-Note-9" title="RL 学习笔记 #09 Actor-Critic 方法" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #09 Actor-Critic 方法</span> </a><a href="/RL-Note-10" title="RL 学习笔记 #10 近端策略优化（PPO）理论" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #10 近端策略优化（PPO）理论</span> </a><a href="/RL-Note-11" title="RL 学习笔记 #11 PPO 在 RLHF 中的应用" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #11 PPO 在 RLHF 中的应用</span> </a><a href="/RL-Note-12" title="RL 学习笔记 #12 OpenRLHF-PPO 实践" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #12 OpenRLHF-PPO 实践</span> </a><a href="/RL-Note-13" title="RL 学习笔记 #13 直接偏好优化（DPO）理论" class="list-group-item list-group-item-action active"><span class="category-post">RL 学习笔记 #13 直接偏好优化（DPO）理论</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="本科课程" id="heading-d577b913fa605a2342ab24bd1b0bea8b" role="tab" data-toggle="collapse" href="#collapse-d577b913fa605a2342ab24bd1b0bea8b" aria-expanded="false">本科课程 <span class="list-group-count">(4)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-d577b913fa605a2342ab24bd1b0bea8b" role="tabpanel" aria-labelledby="heading-d577b913fa605a2342ab24bd1b0bea8b"><div class="category-post-list"><a href="/Database-System" title="数据库系统 应试笔记" class="list-group-item list-group-item-action"><span class="category-post">数据库系统 应试笔记</span> </a><a href="/Mathematical-Logic" title="数理逻辑 应试笔记" class="list-group-item list-group-item-action"><span class="category-post">数理逻辑 应试笔记</span> </a><a href="/Software-Engineering-1" title="软件工程 应试笔记 #1" class="list-group-item list-group-item-action"><span class="category-post">软件工程 应试笔记 #1</span> </a><a href="/Software-Engineering-2" title="软件工程 应试笔记 #2" class="list-group-item list-group-item-action"><span class="category-post">软件工程 应试笔记 #2</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="机器学习" id="heading-61bb1751fd355596e307767d1927c855" role="tab" data-toggle="collapse" href="#collapse-61bb1751fd355596e307767d1927c855" aria-expanded="false">机器学习 <span class="list-group-count">(13)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-61bb1751fd355596e307767d1927c855" role="tabpanel" aria-labelledby="heading-61bb1751fd355596e307767d1927c855"><div class="category-post-list"><a href="/ML-Note-1" title="ML学习笔记 #01 梯度下降：一元线性回归" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #01 梯度下降：一元线性回归</span> </a><a href="/ML-Note-2" title="ML学习笔记 #02 梯度下降：多元线性回归" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #02 梯度下降：多元线性回归</span> </a><a href="/ML-Note-3" title="ML学习笔记 #03 正规方程：多元线性回归" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #03 正规方程：多元线性回归</span> </a><a href="/ML-Note-4" title="ML学习笔记 #04 逻辑回归：二分类到多分类" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #04 逻辑回归：二分类到多分类</span> </a><a href="/ML-Note-5" title="ML学习笔记 #05 过拟合与正则化" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #05 过拟合与正则化</span> </a><a href="/ML-Note-6" title="ML学习笔记 #06 神经网络基础" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #06 神经网络基础</span> </a><a href="/ML-Note-7" title="ML学习笔记 #07 神经网络：反向传播" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #07 神经网络：反向传播</span> </a><a href="/ML-Note-8" title="ML学习笔记 #08 数据集划分与误差分析" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #08 数据集划分与误差分析</span> </a><a href="/ML-Note-9" title="ML学习笔记 #09 支持向量机" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #09 支持向量机</span> </a><a href="/ML-Note-10" title="ML学习笔记 #10 K-Means 聚类" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #10 K-Means 聚类</span> </a><a href="/ML-Note-11" title="ML学习笔记 #11 主成分分析" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #11 主成分分析</span> </a><a href="/ML-Note-12" title="ML学习笔记 #12 异常检测" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #12 异常检测</span> </a><a href="/ML-Note-13" title="ML学习笔记 #13 协同过滤推荐算法" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #13 协同过滤推荐算法</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="模式识别" id="heading-03b289aaa6b21d8730ca1e736a2796eb" role="tab" data-toggle="collapse" href="#collapse-03b289aaa6b21d8730ca1e736a2796eb" aria-expanded="false">模式识别 <span class="list-group-count">(5)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-03b289aaa6b21d8730ca1e736a2796eb" role="tabpanel" aria-labelledby="heading-03b289aaa6b21d8730ca1e736a2796eb"><div class="category-post-list"><a href="/PR-Note-1" title="PR学习笔记 #1 KNN 分类器" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #1 KNN 分类器</span> </a><a href="/PR-Note-2" title="PR学习笔记 #2 贝叶斯分类器" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #2 贝叶斯分类器</span> </a><a href="/PR-Note-3" title="PR学习笔记 #3 概率密度：参数估计" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #3 概率密度：参数估计</span> </a><a href="/PR-Note-4" title="PR学习笔记 #4 概率密度：非参数估计" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #4 概率密度：非参数估计</span> </a><a href="/PR-Note-5" title="PR学习笔记 #5 判别式 vs. 生成式" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #5 判别式 vs. 生成式</span></a></div></div></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">RL 学习笔记 #13 直接偏好优化（DPO）理论</h1><p class="note note-info">本文最后更新于：2025年3月22日 晚上</p><div class="markdown-body"><p>以 PPO 为代表的经典 RLHF 流程中，存在两个显著缺陷：</p><ol type="1"><li><strong>奖励模型偏差</strong>：PPO 需要训练一个 RM，奖励建模误差可能会通过 RL 传导到策略网络。</li><li><strong>训练流程复杂</strong>：PPO 需要交替训练 Actor 和 Critic 网络，加载多个模型，涉及大量超参数调整（如 clip 阈值、GAE 参数等），导致实现成本极高。</li></ol><p>DPO（Direct Preference Optimization）的提出正是为了解决这些问题。其核心思想是<strong>绕过奖励模型建模，直接利用人类偏好数据优化策略网络</strong>。这种方法的革命性在于发现：语言模型本身可以视为一个隐式的奖励函数，通过数学变换可以直接建立策略与偏好的映射关系。</p><p>附上一些参考资料：</p><ul><li><p>猛猿老师的文章：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/c_qkRj7cxzHGIR_3oqnkHA">人人都能看懂的 DPO 数学原理</a></p></li><li><p>DPO 论文：<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></p></li></ul><h2 id="dpo-理论推导">DPO 理论推导</h2><p>给定一个预训练语言模型 <span class="math inline">\(\pi_{\text{ref}}(y\mid x)\)</span>，以及一个人类偏好数据集 <span class="math inline">\(\mathcal{D} = \{(x, y_w, y_l)\}\)</span>，其中 <span class="math inline">\(y_w\)</span> 是人类偏好的回答，<span class="math inline">\(y_l\)</span> 是人类不偏好的回答。我们的目标是找到一个新的策略 <span class="math inline">\(\pi_{\theta}(y\mid x)\)</span> 来最大化与人类偏好的一致性。</p><p>不管是 PPO 还是 DPO，我们的优化目标通常是一个<strong>正则化（Behavior-Regularized）的 RL 问题</strong>： <span class="math display">\[ \max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y|x)}\left[r(x, y) - \beta \mathbb{D}_{\mathrm{KL}}(\pi_\theta(y\mid x) \parallel \pi_{\text{ref}}(y\mid x))\right] \]</span> 其中： - <span class="math inline">\(r(x, y)\)</span> 是奖励函数，评估响应 <span class="math inline">\(y\)</span> 对于输入 <span class="math inline">\(x\)</span> 的质量； - <span class="math inline">\(\beta\)</span> 是控制 KL 散度惩罚强度的超参数； - <span class="math inline">\(\mathbb{D}_{\mathrm{KL}}\)</span> 是 KL 散度，防止优化后的策略 <span class="math inline">\(\pi_\theta\)</span> 与参考策略 <span class="math inline">\(\pi_{\text{ref}}\)</span> 相差太远，两个模型都初始化自 SFT 模型。</p><h3 id="最优奖励与最优策略模型">最优奖励与最优策略模型</h3><p>现在我们尽可能去<strong>简化这个优化目标</strong>，考虑到 KL 散度的表达式： <span class="math display">\[ \mathbb{D}_{\mathrm{KL}}\left[\pi(y \mid x) \| \pi_{\mathrm{ref}}(y \mid x)\right]=\mathbb{E}_{y \sim \pi(y \mid x)}\left[\log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}\right] \]</span> 优化目标可以改写为： <span class="math display">\[ \max _\pi \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)}\left[r(x, y)-\beta \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}\right] \]</span> 除以 <span class="math inline">\(\beta\)</span>，再取反，因此 <span class="math inline">\(\max\)</span> 改为 <span class="math inline">\(\min\)</span> 得到： <span class="math display">\[ \min _\pi \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)}\left[\log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}-\frac{1}{\beta} r(x, y)\right] \]</span> 将 <span class="math inline">\(\frac{1}{\beta} r(x, y)\)</span> 改为 <span class="math inline">\(\log \exp\left(\frac{1}{\beta} r(x, y)\right)\)</span> 后可以将两项合并： <span class="math display">\[ \min _\pi \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)}\left[\log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)}\right] \]</span> 根据这里的分母，我们人为定义一个<strong>配分函数</strong>（Partition Function）： <span class="math display">\[ Z(x) = \sum_{y} \pi_{\mathrm{ref}}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right) \]</span> 其中 <span class="math inline">\(\sum_{y}\)</span> 表示在给定某个 Prompt <span class="math inline">\(x\)</span> 的前提下，参考模型可能生成的所有 <span class="math inline">\(y\)</span>，因此有 <span class="math inline">\(\sum_{y} \pi_{\mathrm{ref}}(y \mid x)=1\)</span>。由这个定义可知，<span class="math inline">\(Z(x)\)</span> 是<strong>关于 <span class="math inline">\(x\)</span> 的函数</strong>，且和我们准备优化的模型 <span class="math inline">\(\pi\)</span> 没有关系。</p><blockquote><p>暂时可以理解这个式子是<strong>为了跟分母对齐而凑出来的归一化因子</strong>，但实际上这种配分函数在逆强化学习、离线强化学习中大量存在，涉及到比较麻烦的理论推导，这里就不展开解释了。</p></blockquote><p>为了引入 <span class="math inline">\(Z(x)\)</span>，将优化目标进一步变形为： <span class="math display">\[ \min _\pi \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)}\left[\log \frac{\pi(y \mid x)}{\frac{1}{Z(x)} \pi_{\mathrm{ref}}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)}-\log Z(x)\right] \]</span> 对于式子的左半部分，可以发现它非常像 <strong>KL 散度的形式</strong>（即衡量了两个分布之间的相似性），因此我们将分母再定义为 <span class="math inline">\(\pi^*(y \mid x)\)</span>： <span class="math display">\[ \pi^*(y \mid x)=\frac{1}{Z(x)} \pi_{\mathrm{ref}}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right) \]</span> 可以证明上述 <span class="math inline">\(\pi^*\)</span> 的满足<strong>概率分布大于 <span class="math inline">\(0\)</span> 且和为 <span class="math inline">\(1\)</span></strong> 的定义，是一个合法的概率分布——这也是我们将 <span class="math inline">\(Z(x)\)</span> 定义为这个形式的原因！</p><p>此时，将期望移入括号，优化目标进一步改写为： <span class="math display">\[ \begin{aligned} &amp; \min _\pi \mathbb{E}_{x \sim \mathcal{D}}\left[\mathbb{E}_{y \sim \pi(y \mid x)}\left[\log \frac{\pi(y \mid x)}{\pi^*(y \mid x)}\right]-\log Z(x)\right] \\ &amp; = \min _\pi \mathbb{E}_{x \sim \mathcal{D}}\left[\mathbb{D}_{\mathrm{KL}}\left(\pi(y \mid x) \parallel \pi^*(y \mid x)\right)-\log Z(x)\right] \end{aligned} \]</span> 前面我们说过和 <span class="math inline">\(Z(x)\)</span> 和我们准备优化的模型 <span class="math inline">\(\pi\)</span> 没有关系，所以在 <span class="math inline">\(\min_{\pi}\)</span> 中可以<strong>将第二项忽略</strong>。那么现在我们只需要关心 KL 散度这一项。我们知道 KL 散度在<strong>两个分布完全相等时达到最小</strong>，由此我们可以写出模型的显式解： <span class="math display">\[ \pi(y \mid x)=\pi^*(y \mid x)=\frac{1}{Z(x)} \pi_{\mathrm{ref}}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right) \]</span> 因为以上推导都是在<strong>假设我们有一个固定的奖励函数 <span class="math inline">\(r\)</span> 的基础上进行的</strong>，所以我们可以加一个下标来强调这一点： <span class="math display">\[ \pi_r(y \mid x)=\frac{1}{Z(x)} \pi_{\mathrm{ref}}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right) \]</span> 可是，在正常的对齐训练中，这个奖励函数 <span class="math inline">\(r(x, y)\)</span> 可不是任意的，它是我们先用数据训练出来的<strong>最优奖励模型</strong>，然后在这个最优奖励模型的基础上，我们再通过训练去找到<strong>最优策略模型</strong>。最优奖励模型和基于它训练出的最优的对齐模型依然满足上式，我们分别设它们为 <span class="math inline">\(r^*(x, y)\)</span> 和 <span class="math inline">\(\pi^*(y \mid x)\)</span>，则有： <span class="math display">\[ \pi^*(y \mid x)=\frac{1}{Z(x)} \pi_{\mathrm{ref}}(y \mid x) \exp \left(\frac{1}{\beta} r^*(x, y)\right) \]</span></p><h3 id="建立策略到偏好的映射">建立策略到偏好的映射</h3><p>虽然我们现在得到了策略模型的显式解 <span class="math inline">\(\pi_r(y \mid x)\)</span>，但是我们却很难直接利用起这个显式解形式，原因如下：</p><ul><li><span class="math inline">\(Z(x)\)</span> 的值很难估计。根据 <span class="math inline">\(Z(x)\)</span> 的形式可知，想要估计它，就需要对一个 Prompt <span class="math inline">\(x\)</span> 采样足够多的回答 <span class="math inline">\(y\)</span>。这个代价是十分昂贵的。</li><li>回顾最开始我们的目标：<strong>省略训练奖励模型这个步骤，直接优化策略模型</strong>。而目前我们得到的的显式解仍然需要一个确定的奖励函数 <span class="math inline">\(r\)</span>。</li></ul><p>所以现在我们继续来迭代。考虑到第二个原因，我们可以先从 <span class="math inline">\(\pi^*\)</span> 的显式解中推出最优奖励模型 <span class="math inline">\(r^*\)</span> 的形式： <span class="math display">\[ r^*(x, y)=\beta \log \frac{\pi^*(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}+\beta \log Z(x) \]</span> 既然有这层映射关系，我们就可以直接<strong>将 <span class="math inline">\(\pi^*\)</span> 代入到奖励模型的训练优化目标</strong>中去，从而实现「<strong>明面上训练奖励模型，实际上却一步到位训练出了对齐模型</strong>」！</p><p>所以现在回到奖励模型的训练优化目标——在介绍 PPO 的时候我们曾提过，我们通常使用「偏好排序」这种数据标注方式来对奖励模型进行训练，一般有两种偏好排序方法：</p><ul><li>只生成 <span class="math inline">\(2\)</span> 个回答，<code>&lt;prompt x, chosen y1, reject y2&gt;</code>，即对于一个 Prompt，我们生成 2 个回答，人工对这两个回答的偏好做排序，我们希望奖励模型对 Chosen 回答的给分尽量高于对 Reject 回答的给分。</li><li>生成 <span class="math inline">\(K &gt; 2\)</span> 个回答，<code>&lt;prompt x, y1, ..., yK&gt;</code>，假设人工标注后的偏好排序组合为 <span class="math inline">\(\tau\)</span>，那么我们希望奖励模型对 <span class="math inline">\(\tau\)</span> 这个排序的总得分要大于其余任何可能的偏好排序。</li></ul><p>在某些框架（比如 InstructGPT）的训练中，当生成的回答 <span class="math inline">\(&gt;2\)</span> 个时，会将回答<strong>两两配对</strong>，这样就可以和只生成 <span class="math inline">\(2\)</span> 个回答时的目标函数做统一。但在更一般的场景中，对于 <span class="math inline">\(&gt;2\)</span> 个回答的场景，我们是把<strong>每一种可能的回答偏好排序当成一个整体数据</strong>进行处理的，然后希望真值排序的得分最高，DPO 的推导就是基于后者进行的。</p><p>接下来，我们也对 <span class="math inline">\(K=2\)</span> 和 <span class="math inline">\(K&gt;2\)</span> 这两种情况下 DPO 最终的目标函数形式进行推导。</p><h3 id="bradley-terry-bt-偏好模型">Bradley-Terry (BT) 偏好模型</h3><p>我们希望一个好的奖励模型对 Chosen 回答的给分尽量高于对 Reject 回答的给分，这其实表示了一种相对偏好。1952 年提出的统计模型 Bradley-Terry (BT) Model 可用于<strong>分析成对数据间的相对优势或者偏好</strong>，其被广泛应用于体育比赛分析、市场研究等场景。</p><p>在该模型下，我们假设有两个回答 <span class="math inline">\(y_w\)</span> 和 <span class="math inline">\(y_l\)</span>，<strong>人类偏好 <span class="math inline">\(y_w\)</span> 而非 <span class="math inline">\(y_l\)</span> 的概率</strong>可以表示为： <span class="math display">\[ P(y_w \succ y_l\mid x) = \frac{\exp[r(x, y_w)]}{\exp[r(x, y_w)] + \exp[r(x, y_l)]} \]</span> 现在，我们希望对于整个标注数据集 <span class="math inline">\(\mathcal{D} = \{(x, y_w, y_l)\}\)</span>，Chosen 打败 Reject 的期望概率尽量大，所以奖励函数的总体优化目标可以设计成<strong>负对数似然</strong>（Negative Log-Likelihood，NLL）： <span class="math display">\[ L_R\left(r_\phi, D\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim D}\left[\log P\left(y_w \succ y_l \mid x\right)\right] \]</span> 将 <span class="math inline">\(P\)</span> 的具体形式代入化简，则有： <span class="math display">\[ \begin{aligned} L_R\left(r_\phi, D\right) &amp; =-\mathbb{E}_{\left(x, y_w, y_l\right) \sim D}\left[\log P\left(y_w \succ y_l \mid x\right)\right] \\ &amp; =-\mathbb{E}_{\left(x, y_w, y_l\right) \sim D}\left[\log \frac{\exp \left[r\left(x, y_w\right)\right]}{\exp \left[r\left(x, y_w\right)\right]+\exp \left[r\left(x, y_l\right)\right]}\right] \\ &amp; =-\mathbb{E}_{\left(x, y_w, y_l\right) \sim D}\left[\log \frac{1}{1+\frac{\exp \left[r\left(x, y_l\right)\right]}{\exp \left[r\left(x, y_w\right)\right]}}\right] \\ &amp; =-\mathbb{E}_{\left(x, y_w, y_l\right) \sim D}\left[\log \frac{1}{1+e^{-\left(r\left(x, y_w\right)-r\left(x, y_l\right)\right)}}\right] \\ &amp; =-\mathbb{E}_{\left(x, y_w, y_l\right) \sim D}\left[\log \sigma\left(r\left(x, y_w\right)-r\left(x, y_l\right)\right)\right] \end{aligned} \]</span></p><p>其中 <span class="math inline">\(\sigma\)</span> 是 sigmoid 函数。这时我们震惊地发现：最后一个式子就是 OpenAI 2020 年论文《<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.01325">Learning to Summarize with Human Feedback</a>》介绍 PPO 的<strong>成对排序损失（Pairwise Ranking Loss）</strong>： <span class="math display">\[ \mathcal{L}^{\text{RM}} = \log \sigma(r(x, y_w) - r(x, y_l)) \]</span></p><p>这个公式表明，<strong>两个回答之间的相对偏好与它们奖励值的差异成正比</strong>。</p><p>现在，我们将前面求出的最优奖励模型 <span class="math inline">\(r^*\)</span> 代入 Bradley-Terry 偏好模型，可以得到： <span class="math display">\[ \begin{aligned} P(y_w \succ y_l \mid x) &amp; = \sigma(r^*(x, y_w) - r^*(x, y_l))\\ &amp;= \sigma\left(\beta \log \frac{\pi^*(y_w\mid x)}{\pi_{\text{ref}}(y_w\mid x)} + \beta \log Z(x) - \beta \log \frac{\pi^*(y_l\mid x)}{\pi_{\text{ref}}(y_l\mid x)} - \beta \log Z(x)\right)\\ \end{aligned} \]</span> 注意到 <span class="math inline">\(\log Z(x)\)</span> 项相互抵消，我们得到： <span class="math display">\[ P^*(y_w \succ y_l \mid x) = \sigma\left(\beta \log \frac{\pi^*(y_w\mid x)}{\pi_{\text{ref}}(y_w\mid x)} - \beta \log \frac{\pi^*(y_l\mid x)}{\pi_{\text{ref}}(y_l\mid x)}\right) \]</span></p><p>到这里，我们已经把训<strong>练奖励模型的目标函数转化成只和策略模型 <span class="math inline">\(\pi\)</span> 相关</strong>了！也就是说，我们可以一步到位，绕开训练奖励模型的过程，直接用标注好的成对偏好数据，像 SFT 一样直接训练策略模型了。</p><h3 id="plackett-luce-偏好模型">Plackett-Luce 偏好模型</h3><p>现在，如果我不想使用成对偏好数据，而是对于一个 Prompt，我给 <span class="math inline">\(K&gt;2\)</span> 个回答进行偏好排序，在这种方式下，我要怎么设计奖励模型优化目标呢？</p><p>类似于 BT 模型的一个 Plackett-Luce 模型可以<strong>对多数据的偏好排序进行建模</strong>：假设 <span class="math inline">\(\tau\)</span> 为人工标注出的真值排序，我们希望其能够打败其余任何一种可能的偏好排序，这一概率可以表示成： <span class="math display">\[ P\left(\tau \mid y_1, \ldots, y_K, x\right)=\prod_{k=1}^K \frac{\exp \left[r\left(x, y_{\tau(k)}\right)\right]}{\sum_{j=k}^K \exp \left[r\left(x, y_{\tau(j)}\right)\right]} \]</span> 其中，<span class="math inline">\(\tau_k\)</span> 表示人类标注的偏好序列 <span class="math inline">\(\tau\)</span> 中的第 <span class="math inline">\(k\)</span> 个数据，序列 <span class="math inline">\(\tau\)</span> 中的 <span class="math inline">\(K\)</span> 个回答已经按照偏好从高到低进行排序。</p><p>这个公式从直观上理解的话：</p><ul><li><p>对于真值 <span class="math inline">\(\tau\)</span> 中的第一个回答 <span class="math inline">\(\tau_1\)</span> ，它是人工标注的偏好最高的数据，我们当然希望它的得分在 <span class="math inline">\(\tau_1 \sim \tau_K\)</span> 中占大头；</p></li><li><p>对于真值 <span class="math inline">\(\tau\)</span> 中的第一个回答 <span class="math inline">\(\tau_2\)</span> ，我们当然希望它的得分在 <span class="math inline">\(\tau_2 \sim \tau_K\)</span> 中占大头；</p></li><li><p>对于真值 <span class="math inline">\(\tau\)</span> 中的第一个回答 <span class="math inline">\(\tau_3\)</span> ，我们当然希望它的得分在 <span class="math inline">\(\tau_3 \sim \tau_K\)</span> 中占大头；</p></li><li><p>以此类推，则不难理解上述在 Plackett-Luce 模型下概率的表达方式。</p></li></ul><p>同样，我们把最优奖励函数 <span class="math inline">\(r^*(x, y)\)</span> 代入上面的 P 中，则有：</p><p><span class="math display">\[ P\left(\tau \mid y_1, \ldots, y_K, x\right)=\prod_{k=1}^K \frac{\exp \left[r^*\left(x, y_{\tau(k)}\right)\right]}{\sum_{j=k}^K \exp \left[r^*\left(x, y_{\tau(j)}\right)\right]} \]</span></p><p>然后我们再用 <span class="math inline">\(\pi^*\)</span> 去表示 <span class="math inline">\(r^*\)</span> ，则有（这里我们可以把 <span class="math inline">\(Z(x)\)</span> 省略掉，因为正如前文所说，它和对齐模型 <span class="math inline">\(\pi\)</span> 没有关系）：</p><p><span class="math display">\[ P^*\left(\tau \mid y_1, \ldots, y_K, x\right)=\prod_{k=1}^K \frac{\exp \left(\beta \log \frac{\pi^*\left(y_{\tau(k)} \mid x\right)}{\pi_{\mathrm{ref}}\left(y_{\tau(k)} \mid x\right)}\right)}{\sum_{j=k}^K \exp \left(\beta \log \frac{\pi^*\left(y_{\tau(j)} \mid x\right)}{\pi_{\mathrm{ref}}\left(y_{\tau(j)} \mid x\right)}\right)} \]</span></p><h2 id="dpo-实现细节">DPO 实现细节</h2><h3 id="损失函数推导">损失函数推导</h3><p>通过极大似然估计，我们将 BT 模型的优化目标转化为二元交叉熵损失： <span class="math display">\[ \mathcal{L}_{\text{DPO}}(\theta) = -\mathbb{E}_{(x,y_w,y_l) \sim D} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w\mid x)}{\pi_{\text{ref}}(y_w\mid x)} - \beta \log \frac{\pi_\theta(y_l\mid x)}{\pi_{\text{ref}}(y_l\mid x)} \right) \right] \]</span> 这个损失函数具有三个关键特性：</p><ol type="1"><li><strong>隐式奖励建模</strong>：通过策略网络与参考策略的比值隐式学习奖励函数，避免显式奖励模型的偏差；</li><li><strong>数据高效性</strong>：直接利用成对偏好数据，无需从策略网络中采样（对比 PPO 需要 on-policy 采样）；</li><li><strong>动态权重调节</strong>：<span class="math inline">\(\beta\)</span> 控制策略更新幅度，防止过度偏离参考策略（类似 PPO 中的 KL 约束）。</li></ol><p>链式法则求梯度可以得到： <span class="math display">\[ \nabla_\theta \mathcal{L}_{\mathrm{DPO}}\left(\theta \right)= -\beta \mathbb{E}_{\left(x, y_w, y_l\right) \sim D}[\underbrace{\sigma\left(\hat{r}_\theta\left(x, y_l\right)-\hat{r}_\theta\left(x, y_w\right)\right)}_{\text {higher weight when reward estimate is wrong }}[\underbrace{\nabla_\theta \log \pi\left(y_w \mid x\right)}_{\text {increase likelihood of } y_w}-\underbrace{\nabla_\theta \log \pi\left(y_l \mid x\right)}_{\text {decrease likelihood of } y_l}]] \]</span> 其中 <span class="math inline">\(\hat{r}_\theta\left(x, y\right)=\beta \log \frac{\pi_\theta(y\mid x)}{\pi_{\text{ref}}(y\mid x)}\)</span>，注意这一项是 <span class="math inline">\(l-w\)</span>，表示当隐式奖励估计错误时，会用更大的幅度去纠正策略模型。</p><p>有了这个损失函数后，理想的结果就是，Chosen 回答概率上升，Reject 回答概率下降，从而达到跟人类偏好对齐的自的。当然实践过程中可能会到 <strong>Chosen 和 Reject 的概率都下降的情况</strong>，毕竟 DPO 只约束差值。之后会介绍一些改进工作，包含了相应的优化。</p><h3 id="dpo-loss-简易实现">DPO Loss 简易实现</h3><p>DPO 原论文附录给出了 DPO Loss 的简易实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dpo_loss</span>(<span class="hljs-params">pi_logps, ref_logps, yw_idxs, yl_idxs, beta</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算DPO（Direct Preference Optimization）损失和奖励</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    参数说明：</span><br><span class="hljs-string">    pi_logps:   策略模型（待优化模型）的log概率，形状为 (B,)，B是批量大小</span><br><span class="hljs-string">    ref_logps:  参考模型（固定模型）的log概率，形状为 (B,)</span><br><span class="hljs-string">    yw_idxs:    偏好完成的索引（优选答案），形状为 (T,)，每个元素在 [0, B-1] 之间</span><br><span class="hljs-string">    yl_idxs:    非偏好完成的索引（非优选答案），形状为 (T,)</span><br><span class="hljs-string">    beta:       控制KL惩罚强度的温度参数</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    每个 (yw_idxs[i], yl_idxs[i]) 对表示一个偏好对</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <br>    <span class="hljs-comment"># 从策略模型中提取优选和非优选的 log 概率</span><br>    <span class="hljs-comment"># 结果形状均为 (T,)</span><br>    pi_yw_logps = pi_logps[yw_idxs]  <span class="hljs-comment"># 策略模型对优选答案的 log 概率</span><br>    pi_yl_logps = pi_logps[yl_idxs]  <span class="hljs-comment"># 策略模型对非优选答案的 log 概率</span><br>    <br>    <span class="hljs-comment"># 从参考模型中提取对应的 log 概率</span><br>    ref_yw_logps = ref_logps[yw_idxs]  <span class="hljs-comment"># 参考模型对优选答案的 log 概率</span><br>    ref_yl_logps = ref_logps[yl_idxs]  <span class="hljs-comment"># 参考模型对非优选答案的 log 概率</span><br>    <br>    <span class="hljs-comment"># 计算策略模型和参考模型的 log 概率比（logits 差）</span><br>    pi_logratios = pi_yw_logps - pi_yl_logps  <span class="hljs-comment"># 策略模型的优选 vs 非优选概率比的对数</span><br>    ref_logratios = ref_yw_logps - ref_yl_logps  <span class="hljs-comment"># 参考模型的对应概率比</span><br>    <br>    <span class="hljs-comment"># 计算DPO损失（核心公式）</span><br>    <span class="hljs-comment"># 公式等效于 -log σ(beta * (策略比值 - 参考比值))</span><br>    losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))<br>    <br>    <span class="hljs-comment"># 计算奖励信号（用于监控训练过程，不影响梯度）</span><br>    <span class="hljs-comment"># 奖励 = beta * (策略模型logp - 参考模型logp)</span><br>    <span class="hljs-comment"># detach() 使参考模型的梯度不反向传播</span><br>    rewards = beta * (pi_logps - ref_logps).detach()<br>    <br>    <span class="hljs-keyword">return</span> losses, rewards<br></code></pre></td></tr></table></figure><h2 id="讨论online-vs.-offlineon-policy-vs.-off-policy">讨论：Online vs. Offline，On-Policy vs. Off-Policy</h2><p>引用知乎上 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/688806682"><span class="citation" data-cites="Wei">@Wei</span> Xiong</a> 的说法：</p><blockquote><p>我们用 Offline 指代从一个给定的偏好数据集中学习，并且在学习中，我们无法进一步让 Human 给出偏好信号，而相应的 Online 指的是我们可以在训练过程中让 Human 为我们标数据。</p><p><strong>换言之，区分 online / offline 最关键的在于 preference signal 采集的模式不同。</strong>因此，下面的这些算法都是 offline 的：</p><ul><li>DPO 直接在给定的数据集上进行偏好学习：offline</li><li>我们从一个给定的数据集训练得到一个奖励函数，并使用 PPO 优化这个奖励函数：offline</li><li>我们从一个给定的数据集训练得到一个奖励函数，并使用 rejection sampling finetuning 优化：offline</li></ul><p>一个相关的概念是 on-policy 与 off-policy，我们用 on-policy 指代那些采集数据的策略与要改进的策略是同一个策略的情况，而 off-policy 指代那些使用某个行为策略采集数据，但是用以改进另一个策略的算法。</p><p><strong>换言之，区分 on-policy / off-policy 最关键的在于 responses 采集的模型不同。</strong>我们给出以下例子：</p><ul><li>DPO 是 off-policy 的</li><li>我们从一个给定的数据集训练得到一个奖励函数，并使用 PPO 优化这个奖励函数：on-policy</li></ul></blockquote><p>总而言之，对于 DPO 而言，没有争议，就是 offline、off-policy 算法。因为他<strong>使用离线的偏好数据集</strong>（不与环境交互，甚至没有采样环节）。</p><p>而对于 PPO 就比较灵活： - 如果从偏好信号采集的角度，<strong>PPO 在 System level 上是不与环境交互的</strong>，只从固定的 RM 获取，所以认为是 offline【主流观点】；但有些人也会认为 PPO 是实时从 RM 处获取偏好，所以也是 online。 - 从采样和更新策略角度，对于整个训练过程，<strong>PPO 使用一个策略模型采样并更新之</strong>，所以是 on-policy【主流观点】；但是如果细到每个 step，由于 rollout 模型跟 training 模型不一定一致（存在延迟），PPO 也可以说是 off-policy。</p><h2 id="讨论dpo-的局限性">讨论：DPO 的局限性</h2><blockquote><p>参考资料：</p><ul><li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/11913305485">DPO vs PPO：深度解读谁是 LLM Alignment 的未来 - 知乎</a></li><li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/688806682">为什么我们应该做 online RLHF/DPO? - 知乎</a></li><li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1082394115">DPO 的局限性 - 知乎</a></li><li>论文：Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study.</li><li>论文：Towards Analyzing and Understanding the Limitations of DPO: A Theoretical Perspective.</li></ul></blockquote><p>以下截取一些简单易懂的局限性进行分析：</p><ol type="1"><li><strong>训练目标不一致</strong>：DPO 的训练目标实际上是 RM 的训练目标，只不过将其迁移到了策略模型上。而 RM 的训练目标其实是让模型拥有 Evaluate 能力（拉开好坏响应的 Margin），但我们希望策略模型拥有的是 Generate 能力 ——「<strong>模型的 Evaluate 能力和 Generate 能力到底是不是相互促进的？</strong>」这个前提成立与否，才是 DPO 有效的关键。在训练 DPO 时，基本都遇到过 good_sentence 和 bad_sentence 的 Loss 都上升的尴尬现象，这就表示模型的 Generate 能力不一定同步上升了，往往需要魔改 Loss 才能有效。</li><li><strong>缺少在线采样</strong>：PPO 通过在线采样<strong>将 RM 的 Evaluate 能力转化为 Generate 能力</strong>让策略模型学习。这种 Online 方式让模型能够探索出更多的结果并获取来自真实世界的反馈，也能有更大的 Generate 空间来优化。因此，一些 Online-DPO 迭代策略，就是<strong>通过用 DPO 采样多个 path 后构造偏好对</strong>，从而引入采样和探索机制来增大策略空间。</li><li><strong>场景适应性限制</strong>：DPO 在生成多样性要求高的任务（如创意写作）中表现较弱，因为其偏好学习偏向「安全回答」。此外，其标注方式也注定其只能用于<strong>偏好关系简单且静态</strong>的任务（如文本风格迁移、简单对话生成）。</li></ol></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="category-chain-item">学习笔记</a> <span>></span> <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="category-chain-item">强化学习</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/RL/" class="print-no-link">#RL</a></div></div><div class="license-box my-3"><div class="license-title"><div>RL 学习笔记 #13 直接偏好优化（DPO）理论</div><div>https://hwcoder.top/RL-Note-13</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Wei He</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2025年3月2日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - 非商业性使用"><i class="iconfont icon-nc"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - 相同方式共享"><i class="iconfont icon-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/Manual-Coding-5" title="手撕经典算法 #5 RLHF 篇"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">手撕经典算法 #5 RLHF 篇</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/RL-Note-12" title="RL 学习笔记 #12 OpenRLHF-PPO 实践"><span class="hidden-mobile">RL 学习笔记 #12 OpenRLHF-PPO 实践</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"jdbBr3BddTiqSCPnXw6sXFv7-gzGzoHsz","appKey":"2bjgwDr2opjVCwhgjDMpk53c","path":"window.location.pathname","placeholder":"说点什么吧( •̀ ω •́ )✧","avatar":"retro","meta":["nick","mail","link"],"requiredFields":["nick"],"pageSize":10,"lang":"zh-CN","highlight":true,"recordIP":true,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,i=t.getElementById("subtitle");i&&e&&e(i.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;0<t.find(".toc-list-item").length&&t.css("visibility","visible")}}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-svg-full.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>