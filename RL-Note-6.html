<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg"><link rel="icon" href="/img/logo.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#01213a"><meta name="author" content="Wei He"><meta name="keywords" content="Computer Science and Technology, Machine Learning, Deep Learning, Computer Vision, Natural Language Processing"><meta name="description" content="「强化学习」课程笔记，本节介绍了时序差分学习（TD Learning），包括最基础的求解动作价值的方法、求解动作价值的 SARSA 算法及其两种变体、求解最优动作价值的 Q-Learning 算法。本节还重点对比了 On-Policy 和 Off-Policy 算法。"><meta property="og:type" content="article"><meta property="og:title" content="RL 学习笔记 #6 时序差分学习算法"><meta property="og:url" content="https://hwcoder.top/RL-Note-6"><meta property="og:site_name" content="Hwcoder - Life Oriented Programming"><meta property="og:description" content="「强化学习」课程笔记，本节介绍了时序差分学习（TD Learning），包括最基础的求解动作价值的方法、求解动作价值的 SARSA 算法及其两种变体、求解最优动作价值的 Q-Learning 算法。本节还重点对比了 On-Policy 和 Off-Policy 算法。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hwcoder.top/img/home/RL.jpg"><meta property="article:published_time" content="2024-12-06T03:43:07.000Z"><meta property="article:modified_time" content="2024-12-19T12:58:27.207Z"><meta property="article:author" content="Wei He"><meta property="article:tag" content="RL"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://hwcoder.top/img/home/RL.jpg"><title>RL 学习笔记 #6 时序差分学习算法 | Hwcoder - Life Oriented Programming</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/mac.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"hwcoder.top",root:"/",version:"1.9.5",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h2, h3",placement:"left",visible:"always",icon:"§"},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:null,google:{measurement_id:null},tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"XGNf4GEnaFLiUijMUvz8HSBF-gzGzoHsz",app_key:"JPeVLcug6EcWdBTpGrDJSdKi",server_url:"https://xgnf4gen.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!0}},search_path:"/local-search.xml",include_content_in_search:!0};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>Fluid.ctx.dnt||Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=",function(){function a(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],a("js",new Date),a("config","")})</script><meta name="generator" content="Hexo 5.4.2"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Hwcoder</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/bg/other.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="RL 学习笔记 #6 时序差分学习算法"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2024-12-06 11:43" pubdate>2024年12月6日 中午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 11k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 63 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="学习笔记" id="heading-078425eaf316a180b0989442e53f920b" role="tab" data-toggle="collapse" href="#collapse-078425eaf316a180b0989442e53f920b" aria-expanded="true">学习笔记 <span class="list-group-count">(42)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-078425eaf316a180b0989442e53f920b" role="tabpanel" aria-labelledby="heading-078425eaf316a180b0989442e53f920b"><div class="category-post-list"></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="信息检索" id="heading-b10f39c13b5f9af517dfde91c4da0ec4" role="tab" data-toggle="collapse" href="#collapse-b10f39c13b5f9af517dfde91c4da0ec4" aria-expanded="false">信息检索 <span class="list-group-count">(11)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-b10f39c13b5f9af517dfde91c4da0ec4" role="tabpanel" aria-labelledby="heading-b10f39c13b5f9af517dfde91c4da0ec4"><div class="category-post-list"><a href="/IR-Note-1" title="IR学习笔记 #01 概论&amp;布尔模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #01 概论&amp;布尔模型</span> </a><a href="/IR-Note-2" title="IR学习笔记 #02 统计语言模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #02 统计语言模型</span> </a><a href="/IR-Note-3" title="IR学习笔记 #03 向量空间模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #03 向量空间模型</span> </a><a href="/IR-Note-4" title="IR学习笔记 #04 概率模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #04 概率模型</span> </a><a href="/IR-Note-5" title="IR学习笔记 #05 检索系统评价" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #05 检索系统评价</span> </a><a href="/IR-Note-6" title="IR学习笔记 #06 网络信息检索" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #06 网络信息检索</span> </a><a href="/IR-Note-7" title="IR学习笔记 #07 IRLbot" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #07 IRLbot</span> </a><a href="/IR-Note-8" title="IR学习笔记 #08 倒排索引模型" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #08 倒排索引模型</span> </a><a href="/IR-Note-9" title="IR学习笔记 #09 网页排序" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #09 网页排序</span> </a><a href="/IR-Note-10" title="IR学习笔记 #10 查询相关反馈" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #10 查询相关反馈</span> </a><a href="/IR-Note-11" title="IR学习笔记 #11 问答系统" class="list-group-item list-group-item-action"><span class="category-post">IR学习笔记 #11 问答系统</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem list-group-item category-item-action col-10 col-md-11 col-xm-11" title="强化学习" id="heading-6964a39570837d485ed73b611a392391" role="tab" data-toggle="collapse" href="#collapse-6964a39570837d485ed73b611a392391" aria-expanded="true">强化学习 <span class="list-group-count">(9)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-6964a39570837d485ed73b611a392391" role="tabpanel" aria-labelledby="heading-6964a39570837d485ed73b611a392391"><div class="category-post-list"><a href="/RL-Note-1" title="RL 学习笔记 #1 基本概念" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #1 基本概念</span> </a><a href="/RL-Note-2" title="RL 学习笔记 #2 贝尔曼公式" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #2 贝尔曼公式</span> </a><a href="/RL-Note-3" title="RL 学习笔记 #3 值迭代和策略迭代" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #3 值迭代和策略迭代</span> </a><a href="/RL-Note-4" title="RL 学习笔记 #4 蒙特卡洛学习算法" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #4 蒙特卡洛学习算法</span> </a><a href="/RL-Note-5" title="RL 学习笔记 #5 随机近似与随机梯度下降" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #5 随机近似与随机梯度下降</span> </a><a href="/RL-Note-6" title="RL 学习笔记 #6 时序差分学习算法" class="list-group-item list-group-item-action active"><span class="category-post">RL 学习笔记 #6 时序差分学习算法</span> </a><a href="/RL-Note-7" title="RL 学习笔记 #7 值函数近似和 DQN 算法" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #7 值函数近似和 DQN 算法</span> </a><a href="/RL-Note-8" title="RL 学习笔记 #8 策略梯度方法" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #8 策略梯度方法</span> </a><a href="/RL-Note-9" title="RL 学习笔记 #9 Actor-Critic 方法" class="list-group-item list-group-item-action"><span class="category-post">RL 学习笔记 #9 Actor-Critic 方法</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="本科课程" id="heading-d577b913fa605a2342ab24bd1b0bea8b" role="tab" data-toggle="collapse" href="#collapse-d577b913fa605a2342ab24bd1b0bea8b" aria-expanded="false">本科课程 <span class="list-group-count">(4)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-d577b913fa605a2342ab24bd1b0bea8b" role="tabpanel" aria-labelledby="heading-d577b913fa605a2342ab24bd1b0bea8b"><div class="category-post-list"><a href="/Database-System" title="数据库系统 应试笔记" class="list-group-item list-group-item-action"><span class="category-post">数据库系统 应试笔记</span> </a><a href="/Mathematical-Logic" title="数理逻辑 应试笔记" class="list-group-item list-group-item-action"><span class="category-post">数理逻辑 应试笔记</span> </a><a href="/Software-Engineering-1" title="软件工程 应试笔记 #1" class="list-group-item list-group-item-action"><span class="category-post">软件工程 应试笔记 #1</span> </a><a href="/Software-Engineering-2" title="软件工程 应试笔记 #2" class="list-group-item list-group-item-action"><span class="category-post">软件工程 应试笔记 #2</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="机器学习" id="heading-61bb1751fd355596e307767d1927c855" role="tab" data-toggle="collapse" href="#collapse-61bb1751fd355596e307767d1927c855" aria-expanded="false">机器学习 <span class="list-group-count">(13)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-61bb1751fd355596e307767d1927c855" role="tabpanel" aria-labelledby="heading-61bb1751fd355596e307767d1927c855"><div class="category-post-list"><a href="/ML-Note-1" title="ML学习笔记 #01 梯度下降：一元线性回归" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #01 梯度下降：一元线性回归</span> </a><a href="/ML-Note-2" title="ML学习笔记 #02 梯度下降：多元线性回归" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #02 梯度下降：多元线性回归</span> </a><a href="/ML-Note-3" title="ML学习笔记 #03 正规方程：多元线性回归" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #03 正规方程：多元线性回归</span> </a><a href="/ML-Note-4" title="ML学习笔记 #04 逻辑回归：二分类到多分类" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #04 逻辑回归：二分类到多分类</span> </a><a href="/ML-Note-5" title="ML学习笔记 #05 过拟合与正则化" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #05 过拟合与正则化</span> </a><a href="/ML-Note-6" title="ML学习笔记 #06 神经网络基础" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #06 神经网络基础</span> </a><a href="/ML-Note-7" title="ML学习笔记 #07 神经网络：反向传播" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #07 神经网络：反向传播</span> </a><a href="/ML-Note-8" title="ML学习笔记 #08 数据集划分与误差分析" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #08 数据集划分与误差分析</span> </a><a href="/ML-Note-9" title="ML学习笔记 #09 支持向量机" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #09 支持向量机</span> </a><a href="/ML-Note-10" title="ML学习笔记 #10 K-Means 聚类" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #10 K-Means 聚类</span> </a><a href="/ML-Note-11" title="ML学习笔记 #11 主成分分析" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #11 主成分分析</span> </a><a href="/ML-Note-12" title="ML学习笔记 #12 异常检测" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #12 异常检测</span> </a><a href="/ML-Note-13" title="ML学习笔记 #13 协同过滤推荐算法" class="list-group-item list-group-item-action"><span class="category-post">ML学习笔记 #13 协同过滤推荐算法</span></a></div></div></div><div class="category-sub row nomargin-x"><a class="category-subitem collapsed list-group-item category-item-action col-10 col-md-11 col-xm-11" title="模式识别" id="heading-03b289aaa6b21d8730ca1e736a2796eb" role="tab" data-toggle="collapse" href="#collapse-03b289aaa6b21d8730ca1e736a2796eb" aria-expanded="false">模式识别 <span class="list-group-count">(5)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse" id="collapse-03b289aaa6b21d8730ca1e736a2796eb" role="tabpanel" aria-labelledby="heading-03b289aaa6b21d8730ca1e736a2796eb"><div class="category-post-list"><a href="/PR-Note-1" title="PR学习笔记 #1 KNN 分类器" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #1 KNN 分类器</span> </a><a href="/PR-Note-2" title="PR学习笔记 #2 贝叶斯分类器" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #2 贝叶斯分类器</span> </a><a href="/PR-Note-3" title="PR学习笔记 #3 概率密度：参数估计" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #3 概率密度：参数估计</span> </a><a href="/PR-Note-4" title="PR学习笔记 #4 概率密度：非参数估计" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #4 概率密度：非参数估计</span> </a><a href="/PR-Note-5" title="PR学习笔记 #5 判别式 vs. 生成式" class="list-group-item list-group-item-action"><span class="category-post">PR学习笔记 #5 判别式 vs. 生成式</span></a></div></div></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">RL 学习笔记 #6 时序差分学习算法</h1><p class="note note-info">本文最后更新于：2024年12月19日 晚上</p><div class="markdown-body"><p>前面我们介绍了首个无模型的强化学习算法 —— 蒙特卡洛学习，这是一种非增量（Non-Incremental）的方法，尽管它通过多次采样逐步逼近价值函数，但更新时需要<strong>完整的采样回合</strong>。本节将介绍一种重要的<strong>增量式</strong>（Incremental）无模型强化学习方法——<strong>时序差分学习</strong>（Temporal Difference Learning），其结合了蒙特卡洛方法和动态规划的优点：</p><ol type="1"><li><strong>类似动态规划</strong>：TD 学习不需要完整的回报序列，只需利用<strong>当前状态和下一状态</strong>的信息。</li><li><strong>类似蒙特卡洛方法</strong>：TD 学习能够通过与环境交互的数据进行更新，而<strong>不需要已知的环境模型</strong>。</li></ol><p>这里我们首先用上一节介绍的 RM 算法引入，考虑如下的估计问题： <span class="math display">\[ \theta=\mathbb{E}[R+\gamma v(X)] \]</span> 其中，<span class="math inline">\(R, X\)</span> 为随机变量，<span class="math inline">\(\gamma\)</span> 为常数，<span class="math inline">\(v(\cdot)\)</span> 为一个未知函数。假设我们获取了采样序列 <span class="math inline">\(\{x\}\)</span> 和 <span class="math inline">\(\{r\}\)</span>，则有： <span class="math display">\[ \begin{aligned} g(\theta) &amp; =\theta-\mathbb{E}[R+\gamma v(X)] \\ \tilde{g}(\theta, \eta) &amp; =\theta-[r+\gamma v(x)] \\ &amp; =(\theta-\mathbb{E}[R+\gamma v(X)])+(\mathbb{E}[R+\gamma v(X)]-[r+\gamma v(x)]) \\ &amp; \doteq g(\theta)+\eta \end{aligned} \]</span> 于是，我们将估计问题转化为方程 <span class="math inline">\(g(\theta)=0\)</span> 的根的求解，最后 RM 算法可以表示为： <span class="math display">\[ \theta_{n+1}=\theta_n-\alpha_n \tilde{g}\left(\theta_n, \eta_n\right)=\theta_n-\alpha_n\left[\theta_n-\left(r_n+\gamma v\left(x_n\right)\right)\right] \]</span> 上述式子其实已经蕴含了 TD Learning 的经典形式，接下来我们先介绍用于估计状态值的 TD 算法。</p><h2 id="状态价值-td-learning">状态价值 TD Learning</h2><p>给定策略 <span class="math inline">\(\pi\)</span>，我们希望能估计状态价值 <span class="math inline">\(v_\pi(s)\)</span>，完成策略评估（PE）。增量更新的特点在于<strong>动态调整</strong>其估计值 <span class="math inline">\(v_t(s)\)</span>，最终逐步接近真实值 <span class="math inline">\(v_\pi(s)\)</span>。</p><p>由于无模型，我们仅依赖策略 <span class="math inline">\(\pi\)</span> 产生的经验数据 <span class="math inline">\(\{(s_t, r_{t+1}, s_{t+1})\}\)</span>。其更新公式如下： <span class="math display">\[ \left\{\begin{aligned} v_{t+1}\left(s_t\right) &amp;=v_t\left(s_t\right)-\alpha_t\left(s_t\right)\left[v_t\left(s_t\right)-\left[r_{t+1}+\gamma v_t\left(s_{t+1}\right)\right]\right], \\ v_{t+1}(s) &amp;=v_t(s), \quad \forall s \neq s_t \end{aligned}\right. \]</span> 其中：</p><ul><li>在 <span class="math inline">\(t\)</span> 时刻，我们对当前状态 <span class="math inline">\(s_t\)</span> 对应的 <span class="math inline">\(v_t(s_t)\)</span> 进行了修正，而其他状态对应的状态值则保持原样；</li><li>修正项中，<span class="math inline">\(\alpha_t\left(s_t\right)\)</span> 表示学习率，控制更新步长；</li><li>第一个大括号表示 <strong>TD Error</strong> <span class="math inline">\(\delta_t\)</span>，是估计值 <span class="math inline">\(v_t(s_t)\)</span> 与目标值的差距；</li><li>第二个大括号表示 <strong>TD Target</strong> <span class="math inline">\(\bar{v}_t\)</span>，我们希望 <span class="math inline">\(v_t(s_t)\)</span> 能朝着这个目标值去修改。</li></ul><p>为什么能控制修改方向呢？我们先对更新公式简单变形，两边都减去 <span class="math inline">\(\bar{v}_t\)</span>： <span class="math display">\[ v_{t+1}\left(s_t\right)-\bar{v}_t=v_t\left(s_t\right)-\bar{v}_t-\alpha_t\left(s_t\right)\left[v_t\left(s_t\right)-\bar{v}_t\right] \]</span> 合并得到： <span class="math display">\[ v_{t+1}\left(s_t\right)-\bar{v}_t=\left[1-\alpha_t\left(s_t\right)\right]\left[v_t\left(s_t\right)-\bar{v}_t\right] \]</span> 各项取绝对值： <span class="math display">\[ \left|v_{t+1}\left(s_t\right)-\bar{v}_t\right|=\left|1-\alpha_t\left(s_t\right)\right|\left|v_t\left(s_t\right)-\bar{v}_t\right| \]</span> 考虑到 <span class="math inline">\(\alpha_t\left(s_t\right)\)</span> 是一个小的正数，我们有： <span class="math display">\[ 0&lt;1-\alpha_t\left(s_t\right)&lt;1 \]</span></p><p>进而得到：</p><p><span class="math display">\[ \left|v_{t+1}\left(s_t\right)-\bar{v}_t\right| \leq\left|v_t\left(s_t\right)-\bar{v}_t\right| \]</span> 显然，在一次更新后估计值 <span class="math inline">\(v_t(s_t)\)</span> 与目标值的差距变小了！现在另一个问题是，为什么我们需要将 TD Target 定义为这种形式呢？考虑： <span class="math display">\[ \delta_t=v\left(s_t\right)-\left[r_{t+1}+\gamma v\left(s_{t+1}\right)\right] \]</span> 这个式子可以认为是<strong>两个相邻时间步的差</strong>，也就是时序差分算法名字的由来。它反映的是 <span class="math inline">\(v_t\)</span> 和 <span class="math inline">\(v_\pi\)</span> 的误差，当 <span class="math inline">\(\delta_t=0\)</span> 时，这个式子就收敛到状态值的递归定义。</p><p>因此，为了减小误差、改进估计的准确度，每当一个新的<strong>单步经验数据</strong> <span class="math inline">\((s_t, r_{t+1}, s_{t+1})\)</span> 产生时，我们都可以用这个公式进行增量更新。</p><h3 id="rm-算法求解贝尔曼公式">RM 算法求解贝尔曼公式</h3><p>TD Learning 算法在数学上究竟在干什么呢？实际上其目标是求解无模型时策略 <span class="math inline">\(\pi\)</span> 的贝尔曼公式。这里我们将从理论上更深入探讨 RM 算法如何用于这一目标。首先引入贝尔曼公式：</p><p><span class="math display">\[ v_\pi(s)=\mathbb{E}[R+\gamma G \mid S=s], \quad s \in \mathcal{S} \]</span></p><p>其中 <span class="math inline">\(G\)</span> 为累计奖励回报，由于：</p><p><span class="math display">\[ \mathbb{E}[G \mid S=s]=\sum_a \pi(a \mid s) \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right)=\mathbb{E}\left[v_\pi\left(S^{\prime}\right) \mid S=s\right] \]</span></p><p>其中 <span class="math inline">\(S^{\prime}\)</span> 是下一个状态，因此我们可以改写定义得到：</p><p><span class="math display">\[ v_\pi(s)=\mathbb{E}\left[R+\gamma v_\pi\left(S^{\prime}\right) \mid S=s\right], \quad s \in \mathcal{S} . \]</span></p><p>接下来我们继续用 RM 算法来求解上式表示的估计问题，首先定义： <span class="math display">\[ g(v(s))=v(s)-\mathbb{E}\left[R+\gamma v_\pi\left(S^{\prime}\right) \mid s\right] \]</span></p><p>问题转化为求解 <span class="math inline">\(g(v(s))=0\)</span>。由于环境的概率模型未知，我们通过采样数据 <span class="math inline">\({(s, r, s&#39;)}\)</span>，定义一个噪声版 <span class="math inline">\(g\)</span>：</p><p><span class="math display">\[ \begin{aligned} \tilde{g}(v(s)) &amp; =v(s)-\left[r+\gamma v_\pi\left(s^{\prime}\right)\right] \\ &amp; =\underbrace{\left(v(s)-\mathbb{E}\left[R+\gamma v_\pi\left(S^{\prime}\right) \mid s\right]\right)}_{g(v(s))}+\underbrace{\left(\mathbb{E}\left[R+\gamma v_\pi\left(S^{\prime}\right) \mid s\right]-\left[r+\gamma v_\pi\left(s^{\prime}\right)\right]\right)}_\eta . \end{aligned} \]</span> 套用 RM 算法，更新公式为： <span class="math display">\[ v_{t+1}(s) = v_t(s) - \alpha_t \tilde{g}(v_t(s)) = v_t(s) - \alpha_t \left( v_t(s)-\left[r_k +\gamma v_\pi\left(s^{\prime}_k\right)\right]\right),\quad k=1,2,3 \]</span> 此公式与我们之前的 TD 更新形式非常相似，但请注意，这里是需要采样一系列 <span class="math inline">\(\{(s, r_k, s&#39;_k)\}\)</span> 进行迭代更新的。换句话说，我们得从 <span class="math inline">\(s\)</span> 出发计算多次来估计 <span class="math inline">\(v_\pi(s)\)</span>。这就有两点关键的实际问题需要解决：</p><ol type="1"><li><strong>数据采样的形式</strong>：如何用一个经验轨迹更新多个状态的估计值？</li><li><strong>估计目标的替代</strong>：实际中 <span class="math inline">\(v_\pi(s&#39;)\)</span> 无法直接获取。</li></ol><p>TD Learning 的解决方案如下：</p><ol type="1"><li><strong>采样形式的改进</strong>：改用轨迹数据，在序列 <span class="math inline">\(\{(s_t, r_{t+1}, s_{t+1})\}\)</span> 中，仅当访问某状态 <span class="math inline">\(s\)</span> 时更新其估计值，而<strong>未访问到的状态保持不变</strong>。这样，一条轨迹可以同时用于多个状态的更新。</li><li><strong>估计目标的替代</strong>：用当前的估计值 <span class="math inline">\(v_t(s&#39;)\)</span> 替代真实的 <span class="math inline">\(v_\pi(s&#39;)\)</span>，虽然这会引入偏差，但随着更新的累积，估计值会逐步收敛到真实值。</li></ol><p>通过以上调整，RM 算法可以高效求解无模型时的贝尔曼公式。</p><h3 id="收敛性分析">收敛性分析</h3><p>通过 RM 算法框架，可以严格证明 TD 算法的收敛性。其核心定理为，若满足以下学习率条件：</p><ol type="1"><li><span class="math inline">\(\sum_{t=1}^\infty \alpha_t(s) = \infty\)</span>，注意这里要对每个状态都成立，即<strong>每个状态都需要被访问无数次</strong>——实际中只需要较多次即可；</li><li><span class="math inline">\(\sum_{t=1}^\infty \alpha_t^2 (s) &lt; \infty\)</span>，即学习率逐渐减小——实际中学习率通常设为一个小的常数（如 <span class="math inline">\(0.01\)</span>），防止最后经验失去作用，虽然严格来说不满足条件，但实践效果良好。</li></ol><p>则 <span class="math inline">\(v_t(s)\)</span> 收敛到真实值 <span class="math inline">\(v_\pi(s)\)</span>，得到最终的策略评估（PE）。</p><h3 id="td-learning-与-mc-learning-的对比">TD Learning 与 MC Learning 的对比</h3><p>我们已经了解了时序差分学习和蒙特卡洛学习两种无模型算法的基本性质，以下是两者的详细对比：</p><table><thead><tr class="header"><th style="text-align:center">特性</th><th style="text-align:center">TD/Sarsa Learning</th><th style="text-align:center">MC Learning</th></tr></thead><tbody><tr class="odd"><td style="text-align:center"><strong>更新时机</strong></td><td style="text-align:center">在线（Online）逐状态更新</td><td style="text-align:center">离线（Offline）Episode 结束才更新</td></tr><tr class="even"><td style="text-align:center"><strong>任务类型</strong></td><td style="text-align:center">Continuing 无终止任务、Episode 任务</td><td style="text-align:center">仅 Episode 任务</td></tr><tr class="odd"><td style="text-align:center"><strong>更新方式</strong></td><td style="text-align:center">Bootstrapping 从猜测出发修正</td><td style="text-align:center">直接用采样结果估计</td></tr><tr class="even"><td style="text-align:center"><strong>估计方差</strong></td><td style="text-align:center">较小，因为单步采样变量少</td><td style="text-align:center">较大，整个 Episode 不确定因素多</td></tr><tr class="odd"><td style="text-align:center"><strong>估计偏差</strong></td><td style="text-align:center">有偏，来自于初始猜测，但会收敛</td><td style="text-align:center">无偏，直接求期望</td></tr></tbody></table><h2 id="动作价值-td-learning">动作价值 TD Learning</h2><p>基础的 TD Learning 仍存在一些局限性：</p><ul><li>无法直接估计动作价值函数 <span class="math inline">\(q(s,a)\)</span>，因为 <span class="math inline">\(r\)</span> 仍具有不确定性；</li><li>进而无法直接找到最优策略 <span class="math inline">\(\pi^*\)</span>，通常我们会贪心选取动作价值最高者。</li></ul><p>如果能将 TD Learning 从状态值函数扩展到动作价值函数，就能在策略评估（PE）后进行<strong>策略改进</strong>（PI），从而找到最优策略。本小节将介绍的 <strong>SARSA</strong> 是一种估计动作价值的时序差分算法，其名称来源于更新公式中的五元组 <span class="math inline">\((S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})\)</span>。</p><h3 id="sarsa-算法">SARSA 算法</h3><p>由于无模型，我们仅依赖策略 <span class="math inline">\(\pi\)</span> 产生的经验数据 <span class="math inline">\(\{(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})\}\)</span>。其更新公式如下： <span class="math display">\[ \left\{\begin{aligned} q_{t+1}\left(s_t, a_t\right) &amp;=q_t\left(s_t, a_t\right)-\alpha_t\left(s_t, a_t\right)\left[q_t\left(s_t, a_t\right)-\left[r_{t+1}+\gamma q_t\left(s_{t+1}, a_{t+1}\right)\right]\right], \\ q_{t+1}(s, a) &amp;=q_t(s, a), \quad \forall(s, a) \neq\left(s_t, a_t\right), \end{aligned}\right. \]</span> 其中：</p><ul><li>在 <span class="math inline">\(t\)</span> 时刻，我们对当前状态 <span class="math inline">\(s_t\)</span> 和动作 <span class="math inline">\(a_t\)</span> 对应的 <span class="math inline">\(q_t(s_t, a_t)\)</span> 进行了修正，而其他状态对应的状态值则保持原样；</li><li>算法的整体形式和前面介绍的状态值 TD Learning 是完全一样的，包含 TD Error 和 TD Target。</li></ul><p>同理，SARSA 在数学上也是求解了一个贝尔曼公式： <span class="math display">\[ q_\pi(s,a)=\mathbb{E}\left[R+\gamma q_\pi\left(S^{\prime}, A^{\prime}\right) \mid S=s, A=a\right], \quad \forall s, a . \]</span> 其收敛性条件和上一小节所述的也完全一样，这里略去证明过程。我们直接来讨论 SARSA 的重要优化：如何与策略改进结合起来？其<strong>算法步骤</strong>可以概括如下：</p><ol type="1"><li>初始化：初始化动作价值函数 <span class="math inline">\(q(s, a)\)</span>（通常为零或随机值），初始化初始状态 <span class="math inline">\(s\)</span> 和动作 <span class="math inline">\(a\)</span>；</li><li><strong>执行策略</strong> <span class="math inline">\(\pi\)</span>：在环境中执行动作 <span class="math inline">\(a\)</span>，观察到即时奖励 <span class="math inline">\(r\)</span> 和下一状态 <span class="math inline">\(s^{\prime}\)</span>；</li><li><strong>策略评估</strong>：根据当前策略选择下一动作 <span class="math inline">\(a^{\prime}\)</span>，再根据经验数据更新动作价值函数；</li><li><strong>策略改进</strong>：更新策略，通常选择 <span class="math inline">\(\varepsilon\)</span>-贪心策略来平衡探索和利用；</li><li>重复：继续采样，直到到达终止状态或者策略收敛至 <span class="math inline">\(\pi^*\)</span>。</li></ol><h3 id="expected-sarsa-算法">Expected SARSA 算法</h3><p>SARSA 的更新公式基于采样的 <span class="math inline">\(q_t\left(s_{t+1}, a_{t+1}\right)\)</span>，而 Expected SARSA 则<strong>改用期望更新</strong>： <span class="math display">\[ \left\{ \begin{aligned} q_{t+1}(s_t, a_t) &amp;= q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t, a_t) - \left( r_{t+1} + \gamma \mathbb{E}\left[q_t \left(s_{t+1}, A\right) \right] \right) \right], \\ q_{t+1}(s, a) &amp;= q_t(s, a), \quad \forall (s, a) \neq (s_t, a_t), \end{aligned} \right. \]</span></p><p>其中，<span class="math inline">\(\mathbb{E}\left[q_t(s_{t+1}, A) \right]\)</span> 是下一状态 <span class="math inline">\(s_{t+1}\)</span> 下，根据策略 <span class="math inline">\(\pi\)</span> 所有可能动作的加权期望值，展开即为： <span class="math display">\[ \mathbb{E}\left[q_t(s_{t+1}, A) \right]=\sum_a \pi_t(a\mid s_{t+1})q_t (s_{t+1}, a) \doteq v_t(s_{t+1}) \]</span></p><p>Expected SARSA 的关键是，其 <strong>TD Target</strong> 由随机采样的一个动作价值转变为了 <span class="math inline">\(s_{t+1}\)</span> 的状态值。相比 SARSA，这里进行期望估计的计算量更大，但是也减少了对单一随机动作的依赖，从而<strong>降低了方差</strong>，适用于某些需要<strong>更稳定更新</strong>的任务。</p><h3 id="n-step-sarsa-算法">N-Step SARSA 算法</h3><p>N-Step SARSA 是 SARSA 的延伸，通过引入<strong>多个步骤的累计回报</strong>来改进更新，从而平衡偏差和方差。可以认为这是 SARSA 和蒙特卡洛方法的平衡，其更新公式如下： <span class="math display">\[ \left\{ \begin{aligned} q_{t+1}(s_t, a_t) &amp;= q_t(s_t, a_t) - \alpha_t(s_t, a_t) \left[ q_t(s_t, a_t) - G_t^{(N)} \right], \\ q_{t+1}(s, a) &amp;= q_t(s, a), \quad \forall (s, a) \neq (s_t, a_t), \end{aligned} \right. \]</span></p><p>其中，TD Target <span class="math inline">\(G_t^{(N)}\)</span> 为 <span class="math inline">\(N\)</span> 步的累计回报，定义为：</p><p><span class="math display">\[ G_t^{(N)} = \sum_{k=0}^{N-1} \gamma^k r_{t+k+1} + \gamma^N q(s_{t+N}, a_{t+N}). \]</span></p><p>在 <span class="math inline">\(N\)</span> 步过程中：</p><ul><li>前 <span class="math inline">\(N\)</span> 步的奖励 <span class="math inline">\(r_{t+k+1}\)</span> 被折扣为 <span class="math inline">\(\gamma^k r_{t+k+1}\)</span>；</li><li>最后一步的估计值 <span class="math inline">\(q(s_{t+N}, a_{t+N})\)</span> 则被用于最终的回报估计。</li></ul><p>N-Step SARSA 的优势在于，它能够利用多步经验数据 <span class="math inline">\(\{(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}, \cdots, r_{t+N}, s_{t+N}, a_{t+N})\}\)</span>，从而提高估计的准确性。在实际中，<span class="math inline">\(N\)</span> 的选择会影响算法的性能：</p><ul><li>当 <span class="math inline">\(N = 1\)</span> 时，N-Step SARSA 退化为<strong>标准的单步 SARSA</strong>；</li><li>当 <span class="math inline">\(N \to \infty\)</span> 时，N-Step SARSA 逐渐<strong>接近蒙特卡洛方法</strong>。</li></ul><p>通过调整 <span class="math inline">\(N\)</span>，可以在偏差和方差之间找到合适的平衡。</p><h2 id="最优动作价值-td-learning">最优动作价值 TD Learning</h2><p>SARSA 通过估计动作价值解决了策略改进（PI）的问题，但还有一种思路，就是直接学习最优动作价值函数 <span class="math inline">\(q^*(s, a)\)</span> 来推导出最优策略 <span class="math inline">\(\pi^*\)</span>。</p><p>这需要对动作价值函数进行<strong>无策略依赖</strong>的迭代更新。<strong>Q-Learning</strong> 就是解决这一问题的经典算法。</p><h3 id="q-learning-算法">Q-Learning 算法</h3><p>Q-Learning 不依赖于当前的行为策略 <span class="math inline">\(\pi\)</span>，而是通过<strong>选择最大动作价值来进行更新</strong>，从而逐步逼近最优策略。</p><p>其更新公式如下：</p><p><span class="math display">\[ \left\{ \begin{aligned} q_{t+1}(s_t, a_t) &amp;= q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t, a_t) - \left( r_{t+1} + \gamma \max_{a&#39; \in A} q_t(s_{t+1}, a&#39;) \right) \right], \\ q_{t+1}(s, a) &amp;= q_t(s, a), \quad \forall (s, a) \neq (s_t, a_t). \end{aligned} \right. \]</span></p><p>其中，<span class="math inline">\(\max_{a&#39;\in A} q_t(s_{t+1}, a&#39;)\)</span> 表示在下一状态 <span class="math inline">\(s_{t+1}\)</span> 下，所有可能动作 <span class="math inline">\(a&#39;\)</span> 中的最大动作价值。这个选择是 Q-Learning 与 SARSA 的关键区别之一，<strong>Q-Learning 总是选择最大值，而不依赖于当前策略</strong>。</p><p>在数学上，Q-Learning 求解了如下贝尔曼<strong>最优</strong>公式（不同于之前的所有方法）： <span class="math display">\[ q(s, a)=\mathbb{E}\left[R_{t+1}+\gamma \max _a q\left(S_{t+1}, a\right) \mid S_t=s, A_t=a\right], \quad \forall s, a . \]</span></p><p>Q-Learning 算法的核心是通过对每一个状态-动作对的更新，逐渐逼近最优动作价值函数 <span class="math inline">\(q^*(s, a)\)</span>。最终通过贪心策略 <span class="math inline">\(\pi^*(s) = \arg \max_a q^*(s, a)\)</span>，得到最优策略。</p><p>至此大家可能已经迫不及待想要查看 Q-Learning 的具体步骤了，但此时我们还需要引入一个重要概念。</p><h3 id="off-policy-vs.-on-policy">Off-Policy vs. On-Policy</h3><p>我们在之前的学习中已经提到过两种策略：</p><ul><li><strong>行为策略</strong>（Behavior Policy）：用于与环境交互生成经验数据。</li><li><strong>目标策略</strong>（Target Policy）：我们希望最终达到的策略，也就是我们要不断更新直到收敛到最优策略。</li></ul><p>根据策略之间的关系，可以将强化学习算法分为 <strong>On-Policy</strong> 和 <strong>Off-Policy</strong> 两大类。</p><ul><li><strong>On-Policy</strong>：在学习过程中，<strong>使用的行为策略与目标策略是相同的</strong>。也就是说，行为策略直接与目标策略同步，所有的经验数据都是根据当前的目标策略生成的。</li><li><strong>Off-Policy</strong>：在学习过程中，<strong>使用的行为策略和目标策略不需要相同</strong>。也就是说，算法可以通过一个<strong>探索性较强的行为策略</strong>和环境交互得到大量的经验，再独立地去改进另一个目标策略。并且积累的经验数据也可以不断<strong>回放</strong>（Replay）。</li></ul><p>在本课程学习过的内容中，SARSA 和 MC Learning 属于经典的 On-Policy 算法，而 Q-Learning 则属于 Off-Policy 算法：</p><ul><li><p>SARSA 的更新公式中，动作价值的更新是直接基于<strong>当前的行为策略</strong> <span class="math inline">\(\pi\)</span> 进行的： <span class="math display">\[ q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t, a_t) - \left( r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1}) \right) \right], \]</span> 其使用的经验数据 <span class="math inline">\(\{(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})\}\)</span> 中，<span class="math inline">\(s_t\)</span> 和 <span class="math inline">\(a_t\)</span> 属于条件，<span class="math inline">\(r_{t+1}\)</span> 和 <span class="math inline">\(s_{t+1}\)</span> 由环境决定也与策略无关，但是 <span class="math inline">\(a_{t+1}\)</span> 则完全依赖策略 <span class="math inline">\(\pi_t(s_{t+1})\)</span>。</p></li><li><p>MC Learning 的更新公式中，轨迹的采样也是直接基于<strong>当前的行为策略</strong> <span class="math inline">\(\pi\)</span> 进行的： <span class="math display">\[ q_\pi(s,a) = \mathbb{E}_\pi\left[G_t \mid S_t = s, A_t = a\right] \approx \frac{1}{N} \sum_{i=1}^N g^{(i)}(s, a) \]</span></p></li><li><p>Q-Learning 的更新公式中，<span class="math inline">\(\max q_t(s_{t+1}, a&#39;)\)</span> 代表的是对<strong>目标策略</strong>的估计，与当前的采样所用的行为策略完全无关： <span class="math display">\[ q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t, a_t) - \left( r_{t+1} + \gamma \max_{a&#39; \in A} q_t(s_{t+1}, a&#39;) \right) \right], \]</span> 其使用的经验数据 <span class="math inline">\(\{(s_t, a_t, r_{t+1}, s_{t+1}\}\)</span> 中，都与策略无关。此时行为策略可以是任意的（例如 <span class="math inline">\(\varepsilon\)</span>-greedy 策略），甚至越灵活越好。</p></li></ul><h3 id="q-learning-算法步骤">Q-Learning 算法步骤</h3><p>既然 Q-Learning 的行为策略可以是任意的，那么我们也可以强行让行为策略和目标策略一致，就会得到一个 <strong>On-Policy</strong> 版本的步骤（与 SARSA 几乎一致）：</p><ol type="1"><li>初始化：初始化动作价值函数 <span class="math inline">\(q(s, a)\)</span>，初始化初始状态 <span class="math inline">\(s\)</span> 和动作 <span class="math inline">\(a\)</span>；</li><li>执行策略 <span class="math inline">\(\pi\)</span>：在环境中执行动作 <span class="math inline">\(a\)</span>，观察到即时奖励 <span class="math inline">\(r\)</span> 和下一状态 <span class="math inline">\(s^{\prime}\)</span>；</li><li>策略评估：<strong>无需选择下一动作</strong>，直接用 <span class="math inline">\(\max q(s^{\prime}, a)\)</span> 更新动作价值函数；</li><li>策略改进：更新策略，通常选择 <span class="math inline">\(\varepsilon\)</span>-贪心策略来平衡探索和利用；</li><li>重复：继续采样，<strong>直到到动作价值收敛</strong>后，求出最优策略 <span class="math inline">\(\pi^*\)</span>。</li></ol><hr><p>反之，如果行为策略独立，则 <strong>Off-Policy</strong> 版本的步骤如下：</p><ol type="1"><li>初始化：初始化动作价值函数 <span class="math inline">\(q(s, a)\)</span>，初始化初始状态 <span class="math inline">\(s\)</span> 和动作 <span class="math inline">\(a\)</span>；</li><li>执行策略 <span class="math inline">\(\pi_b\)</span>：根据行为策略随机采样一组经验数据 <span class="math inline">\(\{s_0,a_0,r_1,s_1,a_1,r_2,\cdots\}\)</span>；</li><li>策略评估：对于经验数据中的<strong>每个时刻</strong>，直接用 <span class="math inline">\(\max q(s^{\prime}, a)\)</span> 更新动作价值函数；</li><li>策略改进 <span class="math inline">\(\pi_t\)</span>：更新策略，<strong>直接用贪心策略更新</strong>，因为此时目标策略不用于采样，无需探索性；</li><li>重复：继续采样，直到到动作价值收敛后，求出最优策略 <span class="math inline">\(\pi^*\)</span>。</li></ol><h2 id="td-learning-的统一视角">TD Learning 的统一视角</h2><p>本节中介绍的算法都可以用一个公式概括： <span class="math display">\[ q_{t+1}\left(s_t, a_t\right)=q_t\left(s_t, a_t\right)-\alpha_t\left(s_t, a_t\right)\left[q_t\left(s_t, a_t\right)-\bar{q}_t\right], \]</span> 其中 <span class="math inline">\(\bar{q}_t\)</span> 表示 TD Target，根据取值的不同有如下变种：</p><table><thead><tr class="header"><th style="text-align:center">算法</th><th style="text-align:center"><span class="math inline">\(\bar{q}_t\)</span> 形式</th></tr></thead><tbody><tr class="odd"><td style="text-align:center">SARSA</td><td style="text-align:center"><span class="math inline">\(\bar{q}_t=r_{t+1}+\gamma q_t\left(s_{t+1}, a_{t+1}\right)\)</span></td></tr><tr class="even"><td style="text-align:center">N-Step SARSA</td><td style="text-align:center"><span class="math inline">\(\bar{q}_t=r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^n q_t\left(s_{t+n}, a_{t+n}\right)\)</span></td></tr><tr class="odd"><td style="text-align:center">Expected SARSA</td><td style="text-align:center"><span class="math inline">\(\bar{q}_t=r_{t+1}+\gamma \sum_a \pi_t\left(a \mid s_{t+1}\right) q_t\left(s_{t+1}, a\right)\)</span></td></tr><tr class="even"><td style="text-align:center">Q-Learning</td><td style="text-align:center"><span class="math inline">\(\bar{q}_t=r_{t+1}+\gamma \max _a q_t\left(s_{t+1}, a\right)\)</span></td></tr><tr class="odd"><td style="text-align:center">MC Learning</td><td style="text-align:center"><span class="math inline">\(\bar{q}_t=r_{t+1}+\gamma r_{t+2}+\cdots\)</span></td></tr></tbody></table><p>同理，他们所解决的数学问题也可以统一表达：</p><table><thead><tr class="header"><th style="text-align:center">算法</th><th style="text-align:center">数学问题</th></tr></thead><tbody><tr class="odd"><td style="text-align:center">SARSA</td><td style="text-align:center"><span class="math inline">\(\mathrm{BE}: q_\pi(s, a)=\mathbb{E}\left[R_{t+1}+\gamma q_\pi\left(S_{t+1}, A_{t+1}\right) \mid S_t=s, A_t=a\right]\)</span></td></tr><tr class="even"><td style="text-align:center">N-Step SARSA</td><td style="text-align:center"><span class="math inline">\(\mathrm{BE}: q_\pi(s, a)=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^n q_\pi\left(s_{t+n}, a_{t+n}\right) \mid S_t=s, A_t=a\right]\)</span></td></tr><tr class="odd"><td style="text-align:center">Expected SARSA</td><td style="text-align:center"><span class="math inline">\(\mathrm{BE}: q_\pi(s, a)=\mathbb{E}\left[R_{t+1}+\gamma \mathbb{E}_{A_{t+1}}\left[q_\pi\left(S_{t+1}, A_{t+1}\right)\right] \mid S_t=s, A_t=a\right]\)</span></td></tr><tr class="even"><td style="text-align:center">Q-Learning</td><td style="text-align:center"><span class="math inline">\(\textcolor{red}{\mathrm{BOE}}: q(s, a)=\mathbb{E}\left[R_{t+1}+\max _a q\left(S_{t+1}, a\right) \mid S_t=s, A_t=a\right]\)</span></td></tr><tr class="odd"><td style="text-align:center">MC Learning</td><td style="text-align:center"><span class="math inline">\(\mathrm{BE}: q_\pi(s, a)=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\ldots \mid S_t=s, A_t=a\right]\)</span></td></tr></tbody></table></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="category-chain-item">学习笔记</a> <span>></span> <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="category-chain-item">强化学习</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/RL/" class="print-no-link">#RL</a></div></div><div class="license-box my-3"><div class="license-title"><div>RL 学习笔记 #6 时序差分学习算法</div><div>https://hwcoder.top/RL-Note-6</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Wei He</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2024年12月6日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - 非商业性使用"><i class="iconfont icon-nc"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - 相同方式共享"><i class="iconfont icon-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/RL-Note-7" title="RL 学习笔记 #7 值函数近似和 DQN 算法"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">RL 学习笔记 #7 值函数近似和 DQN 算法</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/RL-Note-5" title="RL 学习笔记 #5 随机近似与随机梯度下降"><span class="hidden-mobile">RL 学习笔记 #5 随机近似与随机梯度下降</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"jdbBr3BddTiqSCPnXw6sXFv7-gzGzoHsz","appKey":"2bjgwDr2opjVCwhgjDMpk53c","path":"window.location.pathname","placeholder":"说点什么吧( •̀ ω •́ )✧","avatar":"retro","meta":["nick","mail","link"],"requiredFields":["nick"],"pageSize":10,"lang":"zh-CN","highlight":true,"recordIP":true,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,i=t.getElementById("subtitle");i&&e&&e(i.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;0<t.find(".toc-list-item").length&&t.css("visibility","visible")}}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-svg-full.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>